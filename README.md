# 结合大模型的程序优化

本仓库是熊英飞老师组为 2026 年北京大学图灵班轮转设计的实践项目之一，主要涉及和大模型相关的程序优化技术。

如果在完成项目的过程中遇到了任何问题，请随时通过微信或者邮件 (zhaoyuwei@stu.pku.edu.cn) 与负责的同学联系。另外，请大家善用大模型，可以解决很多问题。

## 项目概述

### 动机

程序优化在现代计算领域具有重要意义，它直接关系到软件的执行效率、资源利用率以及用户体验。在性能需求不断提高和硬件环境日益复杂的背景下，传统的优化手段往往面临规则设计繁琐、自动化程度不足等瓶颈。而大模型凭借强大的自然语言理解与生成能力，以及在模式识别和知识推理中的卓越表现，为解决这些问题提供了新的可能。通过将大模型应用于程序优化任务，我们可以探索更加智能化、自动化的优化手段，从而提升程序开发效率并实现更高效的代码性能。

### 主要构成

- Part1：尝试使用 API 来调用大模型，了解 API 相关的设置。了解提示工程技术。
- Part2：了解如何构建性能优化知识库。从 GitHub 获取 commit 信息，使用 LLM 判断是否为性能优化，最终理解 SemOpt 知识库的构建流程和使用方式。
- Part3：使用大模型自动优化代码。实现并对比三种方法：Direct Prompt、RAG、Claude Code Skill。
- Part4：阅读相关论文，了解程序优化领域的前沿工作。
- Part5：尝试使用 SemOpt 的完整流程。
- Part6：进阶探索，改进现有方法或提出新的想法。

注意：Part4-6 的顺序不是固定的，可以交叉完成。Part5 和 Part6 为可选内容。

### 时间安排

推荐时间分配
基础路线：
- Part1：1 周（基础工具）
- Part2：0.5 周（知识库理解）
- Part3：1.5-2 周（自动优化代码，至少完成 Direct Prompt + 一种进阶方法）
- Part4：1 周（阅读论文）

深度探索：
- Part1：0.5 周
- Part2：0.5 周
- Part3：1.5-2 周（实现全部三种方法并深入对比）
- Part4：1 周（精读多篇论文）
- Part5-6：1+ 周（可选，深度探索）


## Part 1 - 基础工具

本部分目标： 掌握项目所需的基础工具，包括 Git、命令行、LLM API 调用和提示工程技术。

该部分的构成如下：
- 1.1：简单学习 Git 以及命令行的使用
- 1.2：尝试使用 API 来调用大模型，了解 API 相关的设置
- 1.3：了解提示工程技术（重点：RAG 和 Claude Code Skill）

### 1.1

- 学习命令行的基本使用方式
    - 参考 https://missing-semester-cn.github.io/2020/shell-tools/
- 运行 python 脚本
    - 在本地配置 python 环境，并尝试在命令行运行 `python hello_world.py`
    - 如果需要的话，可以尝试使用 pyenv 工具管理 python 版本。
- 学习 Git
    - 参考 https://missing-semester-cn.github.io/2020/version-control/
    - 了解如何进行简单的版本管理
- 了解 GitHub 的主要用法，例如可以从以下几个问题入手
    - 配置环境，并在命令行使用 `git clone` 来下载代码，例如可以下载本项目。尝试使用 `git pull` 来获取代码库的更新。
    - 尝试在 GitHub 网页上查看 commit 信息。
    - 尝试在 GitHub 上创建仓库，并使用 `git push` 将本地的代码同步更新到 GitHub。
        - 注意：如果你需要在本项目的基础上进行修改，并希望将代码保存到 GitHub，请另外新开一个仓库来存储你自己的代码，而不要直接修改本项目对应的仓库。例如你可以使用 GitHub 上的 fork。

### 1.2 - 了解并尝试使用 api 来调用大模型

#### 1.2.1 - 配置参数

我们组使用[大模型门户](https://llm.xmcp.ltd/)来调用各类模型，包括 Claude、OpenAI、DeepSeek 等模型，请联系负责同学获取账号。每位同学的限额是每月100刀，组内统一报销费用，只能用于自己的科研轮转项目，不能转借或者倒卖。

另外注意保护好自己的 api key，防止泄露。如果要在 GitHub 上存放项目，注意不要将 api key 同步更新上去（`config.py` 已在 `.gitignore` 中，不会被提交）。如果发现 api key 有可能已经泄露，请立刻联系管理员重置秘钥。

配置步骤：
1. 复制根目录下的 `config_template.py` 文件并重命名为 `config.py`

2. 编辑 `config.py` 文件，填入以下变量：
   - `GITHUB_TOKEN`：使用 GitHub api 时需要的配置，在 GitHub - Settings - Developer Settings - Personal access tokens - Tokens (classic) 中生成一个然后复制进来就行
   - `xmcp_base_url`：调用大模型相关，在大模型门户上的"API 调用秘钥/BASE_URL"一栏获取，已设置好
   - `xmcp_api_key`：调用大模型相关，在大模型门户上的"API 调用秘钥/API_KEY"一栏获取自己账号的 api key 并放到这里
   - `xmcp_model`：调用大模型相关，在大模型门户上查看模型列表，选择想要调用的模型，然后将"模型名称"复制到这里，例如 `volc/deepseek-v3-250324`

3. `part1/test_api.py` 是一个简单的大模型调用示例，尝试使用 `python test_api.py` 来运行这个脚本


#### 1.2.2 - 了解 api 相关的设置

参考以下资料了解如何通过 api 调用大模型以及相关的参数设置（OpenAI 和 DeepSeek 用的同一个库，所以可以先阅读 DeepSeek 文档，了解基本设置）：
- https://api-docs.deepseek.com/zh-cn
- https://platform.openai.com/docs
- https://cookbook.openai.com/

主要的设置包括：
- message：https://api-docs.deepseek.com/zh-cn/
    - role
    - content
- temperature：https://api-docs.deepseek.com/zh-cn/quick_start/parameter_settings
- logprobs: https://cookbook.openai.com/examples/using_logprobs & https://api-docs.deepseek.com/zh-cn/api/create-chat-completion
- 对话补全：https://api-docs.deepseek.com/zh-cn/api/create-chat-completion
- JSON Output：https://api-docs.deepseek.com/zh-cn/guides/json_mode
- 上下文硬盘缓存：https://api-docs.deepseek.com/zh-cn/guides/kv_cache
- ...

尝试基于 `test_api.py` 来调用大模型并解决简单的问题，尝试调整输入的参数以及获取输出的各类信息。


### 1.3 - 了解提示工程技术

参考以下资料了解提示工程技术：
- https://www.promptingguide.ai/zh
- OpenAI Prompt Engineering: https://platform.openai.com/docs/guides/prompt-engineering
- Anthropic Prompt Engineering: https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview

基础技术包括：
- 零样本提示
- 少样本提示
- 链式思考（CoT）提示

相对复杂的技术包括（这两个技术都将在后续 Part3 中使用）：
- 检索增强生成 (RAG)
    - RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术，通过从知识库中检索相关信息并作为上下文提供给大模型，从而提高生成内容的准确性和相关性。
    - https://www.promptingguide.ai/zh/techniques/rag
    - https://www.zhihu.com/tardis/zm/art/675509396?source_id=1003
    - https://arxiv.org/abs/2005.11401: 提出 RAG 技术的论文
        - https://blog.csdn.net/weixin_43221845/article/details/142610477: 随便找的一个论文解读
- Claude Code Skill
    - Claude Code Skill 是一种让 AI 助手能够使用预定义技能（Skills）来完成特定任务的技术。通过提供结构化的策略库或技能库，可以让 LLM 在面对特定问题时，先从策略库中选择合适的策略，再应用到具体任务中。这种方法结合了专家知识和 LLM 的推理能力，提供了更好的可解释性。
    - Claude Code Skill 官方文档：https://code.claude.com/docs/en/skills


## Part 2 - 理解性能优化知识库

本部分目标：了解如何构建性能优化知识库。通过学习获取 commit 信息和判断性能优化这两个步骤，理解 SemOpt 知识库的构建流程，为后续的自动优化任务做准备。

该部分的构成如下：
- 2.1：从 GitHub 获取 commit 信息
- 2.2：使用大模型判断 commit 是否为性能优化
- 2.3：了解 SemOpt 知识库的结构和使用

为什么要了解知识库构建？ 在 Part3 中，我们将使用大模型自动优化代码，其中 RAG 和 Claude Code Skill 两种方法都需要使用历史优化案例。理解知识库的构建过程和结构，可以帮助你更好地使用这些方法。

该部分将主要使用 [RocksDB](https://github.com/facebook/rocksdb) 代码库作为示例。

### 2.1 - 从 GitHub 获取 commit 信息

编写脚本，在给定代码库以及 commit hash 后，自动获取该 commit 对应的信息。主要有以下两种思路：

方法 1（推荐）：如果 commit 集中来自于一个或多个代码库，可以考虑将整个代码库下载到本地，并且直接从 git 信息中获取需要的部分。

方法 2：如果 commit 分散在许多不同的代码库，可以考虑直接调用 GitHub API，获取对应的 commit 信息。GitHub API 有调用频率的限制，主要有以下几种解决方法：
- 多注册几个账号获得更多的 key
- 纯等待，例如每调用一次 API 之后 `sleep(n)`
- 改用方法 1

练习：使用 [RocksDB](https://github.com/facebook/rocksdb) 代码库来尝试实现上述功能。尝试获取任意一个 commit 的信息并用合适的文件类型和格式来保存这些信息，可以提取的信息包括：
- Commit message
- 修改的文件列表
- 每个文件的具体修改内容（diff）
- 修改前后的完整文件内容

### 2.2 - 使用大模型判断是否为性能优化

编写 Python 脚本，判断一个 commit 中代码修改的主要目的是否为性能优化（只包括提高代码运行效率 / 减少运行所需资源，不包括改善代码可读性和可维护性等）

具体步骤：
1. 从代码库的 git 信息中获取 commit 信息（使用 2.1 中的方法）

2. 给出 commit 的具体信息（例如 commit message，具体的代码修改信息），让大模型给出答案
   - 如何让大模型只回答 `true` / `false` / `unknown`（在大模型不确定的时候就回答 `unknown`）
   - 如何提高大模型的回答准确率
   - 是否有一些类型的任务大模型认为不是性能优化，但在人类判断的结果上属于性能优化（比如优化内存访问的效率），尝试通过修改 prompt 来改善这一问题

总结： 通过 2.1 和 2.2，你已经了解了构建性能优化知识库的前两个步骤：获取 commit 信息和判断是否为性能优化。这正是 SemOpt 论文中知识库构建的核心流程。在 2.3 中，我们将进一步了解完整的 SemOpt 知识库。

### 2.3 - 了解 SemOpt 知识库

在这一小节中，我们将了解本项目提供的 SemOpt 知识库的结构和内容。SemOpt 是一个结合大语言模型的自动代码优化工具，目前可用于优化 C/C++ 代码中的函数性能。这个知识库是通过 2.1 和 2.2 中的方法，从 100 个 C/C++ 开源项目中构建而成的。

#### 2.3.1 - 知识库构建流程

SemOpt 知识库的构建流程如下：

1. 收集代码库：选择 100 个活跃的 C/C++ 开源项目（如 Redis、RocksDB、FFmpeg 等）
2. 获取 commits：从每个项目的 git 历史中获取所有 commits（使用 2.1 的方法）
3. 筛选 commits：只保留满足以下条件的 commits
   - 只修改了一个 C/C++ 文件中的代码
   - 只修改了该文件中的一个函数
   - 没有修改任何不属于该函数的代码
4. 判断优化类型：使用 LLM 判断每个 commit 是否为性能优化（使用 2.2 的方法）
5. 提取修改信息：对于筛选后的性能优化 commits，提取修改前后的代码
6. 聚类分析：
   - 首先使用 LLM 为每个 commit 生成文字描述，总结其具体使用的优化策略
   - 然后对这些文字描述进行 embedding（向量化）
   - 最后使用聚类算法（如 DBSCAN），将采用相似优化策略的 commits 归为一类
7. 构建知识库：保存所有信息，形成结构化的知识库

最终得到 1968 个性能优化 commits，聚类后形成 151 个策略簇（覆盖约 25% 的 commits）。

#### 2.3.2 - 知识库目录结构

知识库位于 `knowledge_base/` 目录下，结构如下：

```
knowledge_base/
├── {repo_name}/                    # 每个代码库一个文件夹
│   ├── is_opt_final.json          # 该代码库的所有性能优化 commits 元数据
│   └── modified_file/
│       └── {commit_hash}/         # 每个 commit 一个文件夹
│           ├── before.c           # 优化前的完整文件
│           ├── after.c            # 优化后的完整文件
│           ├── before_func.c      # 优化前的函数（如果只修改了一个函数）
│           ├── after_func.c       # 优化后的函数
│           ├── diff.txt           # 代码差异
│           └── *_api.json         # API 调用信息（部分有，不重要）
```


#### 2.3.3 - 聚类结果

聚类结果位于 `cluster_result/0_76.json`，其中 `0_76` 表示使用的相似度阈值为 0.76。

聚类统计：
- 总 commits 数：1968 个
- 聚类簇数：151 个
- 聚类覆盖率：约 24.7%（487 个 commits 被分到某个簇中）
- 噪声点：约 75.3%（1481 个 commits，可能包含更复杂或独特的策略）


#### 2.3.4 - 如何使用知识库

在后续的 Part3 中，我们将使用这个知识库来辅助大模型进行代码优化。主要有两种使用方式：

1. RAG（检索增强生成）：
   - 给定待优化的代码，从知识库中检索相似的优化案例
   - 将检索到的案例作为 few-shot 示例提供给 LLM
   - LLM 参考这些示例进行优化

2. Claude Code Skill（策略库）：
   - 从聚类结果中提取优化策略的描述
   - 构建策略库（每个策略包含描述和代表性示例）
   - LLM 先从策略库中选择合适的策略，再应用到代码上

（可选）探索知识库
你可以编写简单的脚本来探索知识库，例如：
- 列出所有代码库及其 commits 数量
- 查看某个代码库的所有优化 commits
- 查看某个策略簇的代表性案例
- 按优化类型筛选 commits

这将帮助你更好地理解知识库的内容，为 Part3 的任务做准备。


## Part 3 - 使用大模型自动优化代码

本部分目标： 实现并对比三种自动代码优化方法：Direct Prompt（baseline）、RAG、Claude Code Skill。理解不同方法的优劣，学习如何评估优化结果。

现在我们只考虑那些只修改了一个函数并且主要实现性能优化的 commit，然后尝试使用大模型来自动优化代码（即尝试复现 commit 中的修改）。我们将实现三种优化思路（3.1必做，3.3和3.4二选一即可）：
- 3.1：Direct Prompt 方法（必做）- 统一的 prompt，作为 baseline
- 3.2：编写评估脚本（必做）- 评估优化结果的正确性和性能提升
- 3.3：RAG 方法（二选一）- 检索相似案例作为参考
- 3.4：Claude Code Skill 方法（二选一）- 使用策略库引导优化
- 3.5：总结与分析（必做）- 对比不同方法的效果

测试数据集：
在这一部分中，提供两类 commit 可用于优化。你可以先从第一类中挑选几个并尝试优化，然后再考虑第二类中的 commit。
- 第一类：来自 [RAPGen](https://arxiv.org/abs/2306.17077)，每个 commit 只做了 API 调用层面的修改。具体内容在 `part2/rapgen_benchmark.json` 中。
- 第二类：来自 [RocksDB](https://github.com/facebook/rocksdb)，每个 commit 只修改了一个函数。具体内容在 `part2/rocksdb_benchmark.json` 中。

### 3.0 - 理解 commits

虽然我们给出的 commit 都是只修改了一个函数甚至一行代码的，但部分 commit 的修改内容依然可能非常复杂，对于一个并不了解该代码库的程序员来说，也很难快速读懂每一处修改的目的。所以在开始自动优化之前，你可以首先浏览一下 benchmark 中给出的 commit 具体都是做了什么样的修改，例如可以结合 commit message 和代码修改内容分析，或者可以让大模型辅助分析。

在分析一个 commit 时，你可以关注以下几个方面：
- 人工分析后是否觉得代码修改确实能够提升性能，还是说人类也无法理解这样的改动是否有效。如果是后者，可以暂时先不考虑这样的 commit。
- 是否将几个独立的修改内容放在了一个 commit 中，还是所有代码修改都为一个共同的目的服务。如果是前者，也可以暂时不考虑。
- 是否需要提前知道项目特定的一些信息，才能做出这些修改
    - 依赖项目特定信息：例如项目中的一些 API 接口，或者项目运行时的瓶颈是什么以及需要如何改进
    - 几乎不依赖项目特定信息：例如尝试调整 if 条件中的子条件顺序这种不太依赖项目特定环境，而是相对比较通用的修改
- 具体修改是否过分复杂（例如涉及几十甚至上百行代码，或者需要处理一个几百行的函数并且在里面修改很多地方），导致想要让大模型给出正确的优化结果较为困难

在后续的自动优化过程中，你可以根据上面几个方面对 commit 进行分类，并思考什么样的 commit 适合让大模型来自动复现优化，什么样的 commit 是人类也很难理解或者复现的。

### 3.1 - Direct Prompt 方法（baseline）

方法描述： 在所有任务上使用统一的 prompt，作为 baseline 方法。我们不会在 prompt 中告诉大模型当前代码可以在什么方面进行优化，而是完全让大模型自行决定每一步的结果。这是一种完全自动化的方法。

可能的技术方案：
1. 单轮对话：直接在 prompt 中给出需要被优化的代码，并要求大模型分析代码中可能存在的性能优化问题并给出优化结果。

2. 使用 CoT：引导大模型先阅读并理解给出的代码，然后分析有什么性能优化的可能（大模型可能会给出多个方向，也可以在 prompt 中加入限制，要求大模型给出不超过三个的优化方向），然后依次要求大模型给出每种方向上的优化结果。

需要关注的问题：- 分析大模型给出的优化方式有哪些类型，是否有一些是我们不想要的（例如提升代码可读性、可维护性等），调整 prompt 来规避这些可能。
- 判断大模型给出的优化是否是正确的方向，以及优化的结果是否保持语义不变并且有真正的性能提升。

提示： 从少量 commits（5-10 个）开始测试，记录结果用于后续对比。

### 3.2 - 编写评估脚本

无论使用哪种方法优化代码，都需要评估优化结果的正确性和性能提升。编写评估脚本，主要包含以下几个方面：

#### 3.2.1 - Exact Match

最严格的评估标准：生成的代码是否与原始优化后的代码完全一致（忽略空格和注释）。

```python
def exact_match(generated_code, target_code):
    # 去除空格、注释，比较是否完全一致
    return normalize(generated_code) == normalize(target_code)
```

#### 3.2.2 - Syntax Correctness

代码是否可以编译通过。可以使用编译器检查语法错误。

#### 3.2.3 - Semantic Equivalence

代码语义是否保持不变。主要有以下几种判断方法：

1. Unit Test：若代码库中包含 unit test，尝试在优化前后分别运行
2. 人工判断：阅读代码，判断语义是否等价
3. LLM 辅助判断（辅助手段）：让 LLM 分析两段代码的语义是否等价（但不完全可靠）

#### 3.2.4 - Performance Improvement

是否真正实现了性能提升。主要有以下几种判断方法：

1. Performance Test：若代码库中包含 performance test，尝试在优化前后分别运行
2. Benchmark：编写简单的 micro-benchmark 测试
3. 人工判断：分析代码，判断是否应该有性能提升

关于测试：- 第一类 commit（RAPGen）没有直接可以使用的测试，主要依赖人工判断
- 第二类 commit（RocksDB）可以参考 `part2/rocksdb_test.md` 来尝试运行 unit test 和 performance test（如果运行不起来的话手动判断就可以了）

评估脚本输出格式建议：
```python
{
    "exact_match": 0.15,      # 15% 完全匹配
    "syntax_correct": 0.85,   # 85% 可编译
    "semantic_equiv": 0.60,   # 60% 语义等价（人工标注）
    "performance": 0.45       # 45% 有性能提升
}
```

### 3.3 - RAG 方法（检索增强生成）

方法描述： RAG（Retrieval-Augmented Generation）是一种结合检索和生成的方法。给定待优化的代码，先从知识库中检索相似的优化案例，然后将这些案例作为 few-shot 示例提供给 LLM，引导 LLM 进行优化。

RAG 流程：
1. 检索相似案例：从 SemOpt 知识库中检索与当前代码相似的优化案例
2. 构建 prompt：将检索到的案例（优化前后的代码对比）加入到 prompt 中
3. 生成优化结果：LLM 参考这些示例进行优化
4. 评估结果：使用 3.2 中的评估方法

检索策略：
有两种主要的检索方法：

方法 1：基于代码相似度（推荐）
```python
def retrieve_by_code_similarity(target_code, knowledge_base, top_k=3):
    """
    使用 embedding 计算代码相似度
    优点：能找到结构相似的代码
    缺点：计算开销大，需要 embedding 模型
    """
    # 1. 获取目标代码的 embedding
    # 2. 计算与知识库中所有案例的相似度
    # 3. 返回 top_k 个最相似的案例
```

方法 2：基于优化类型
```python
def retrieve_by_optimization_type(target_code, cluster_result, top_k=3):
    """
    先让 LLM 分类当前代码可能的优化类型
    然后从对应的策略簇中检索案例
    优点：计算快，更精准
    缺点：需要预先分类
    """
    # 1. 让 LLM 分类当前代码的优化类型
    # 2. 从聚类结果中找到对应的策略簇
    # 3. 从策略簇中采样代表性案例
```

对比实验： 对比 Direct Prompt 和 RAG 方法的效果差异，分析 RAG 在什么情况下表现更好。

### 3.4 - Claude Code Skill 方法（策略库）

方法描述： Claude Code Skill 是一种基于策略库的方法。先从聚类结果中提取优化策略，构建策略库（每个策略包含描述和代表性示例），然后让 LLM 先从策略库中选择合适的策略，再应用到代码上。这种方法结合了专家知识和 LLM 的推理能力，提供了更好的可解释性。

流程：
1. 构建策略库：从 `cluster_result/0_76.json` 中提取优化策略
2. 策略选择：给定待优化代码，让 LLM 从策略库中选择合适的策略
3. 应用策略：根据选定的策略和代表性示例，优化代码
4. 评估结果：使用 3.2 中的评估方法

策略库构建：
从聚类结果中提取信息，构建策略库：

```python
# 策略库格式示例
{
  "strategies": [
    {
      "id": 0,
      "description": "使用哈希表替代线性查找，降低时间复杂度",
      "frequency": 45,  # 该策略在知识库中出现的频率
      "examples": [
        {
          "repo": "redis",
          "commit": "003cc8a4",
          "before": "// 优化前的代码",
          "after": "// 优化后的代码",
          "diff": "// 代码差异"
        },
        ...  # 2-3 个代表性案例
      ]
    },
    ...
  ]
}
```

Prompt 模板示例：
```
你是一个代码优化专家。以下是一些常见的性能优化策略：

策略 1：使用哈希表替代线性查找
策略 2：避免重复计算，缓存中间结果
策略 3：调整条件判断顺序，将常见情况前置
...

给定以下代码：
[待优化的代码]

请：
1. 分析代码，判断哪个策略最适合
2. 参考该策略的示例，优化代码
3. 输出优化后的完整代码
```

对比实验： 对比 Direct Prompt、RAG 和 Claude Code Skill 三种方法的效果，分析各自的优劣。

### 3.5 - 总结与分析

完成 3.1-3.4 后，进行总结与分析：

对比维度：
| 方法 | 准确率 | 成功率 | 优势 | 劣势 | 适用场景 |
|------|--------|--------|------|------|---------|
| Direct Prompt | ? | ? | 简单、通用 | 准确率低 | Baseline 对比 |
| RAG | ? | ? | 提供具体参考 | 需要检索系统 | 有大量历史案例 |
| Claude Code Skill | ? | ? | 可解释性强 | 策略库需要人工总结 | 优化类型已知 |

思考问题：
1. 哪种方法在你的测试集上表现最好？为什么？
2. 不同方法在什么类型的优化任务上表现更好？
3. 如何结合多种方法的优势？
4. 还有哪些可能的改进方向？

（可选）进阶方向：- 尝试组合不同方法（例如：先用 RAG 检索，再用 Claude Code Skill 选择策略）
- 分析失败案例，总结大模型的局限性
- 在更大的测试集上验证结果


## Part 4 - 阅读相关论文

本部分目标： 通过阅读论文，了解程序优化领域的前沿工作，理解理论和实践的关系。

阅读以下论文，可以先粗略过一遍所有论文，然后在优化小规模代码 / 大规模代码中选择一个感兴趣的方向进行精读。

### 4.1 - SemOpt（本项目知识库的来源，建议重点阅读）

- SemOpt: A Semantic Optimization Framework for Large Language Models  - 论文链接：https://arxiv.org/abs/2510.16384
  - PDF：https://arxiv.org/pdf/2510.16384
  - 说明：这是本项目知识库的来源论文。阅读这篇论文可以帮助你理解：
    - 知识库是如何构建的
    - 聚类方法的设计思路
    - 完整的 SemOpt 方法（比 Part3 中的方法更复杂）
    - 实验设计和评估方法

### 4.2 - 优化大规模代码

- 同一个作者的前后两篇连续的工作  - DeepDev-PERF：https://dl.acm.org/doi/abs/10.1145/3540250.3549096
  - RAPGen：https://arxiv.org/abs/2306.17077
  
- Performance Bug 相关  - TANDEM：https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9110902
  - A Large-Scale Empirical Study of Real-Life Performance Issues in Open Source Projects：https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9757842

### 4.3 - 优化小规模代码（算法竞赛题）

PIE 和 SBLLM 是比较主要的论文。

- PIE：https://arxiv.org/abs/2302.07867
- Learning to Improve Code Efficiency：https://arxiv.org/abs/2208.05297
- Supersonic：https://ieeexplore.ieee.org/abstract/document/10606318/
- SBLLM：https://arxiv.org/abs/2408.12159

### 4.4 - 阅读建议

1. 粗读所有论文（1-2 天）
   - 阅读摘要、引言和结论
   - 了解每篇论文的核心思想和贡献
   - 选择一个感兴趣的方向

2. 精读选定方向的论文（3-5 天）
   - 重点精读 SemOpt 论文，理解本项目的理论基础
   - 精读选定方向的 1-2 篇论文
   - 关注方法设计、实验设置、评估指标
   - 思考与 Part3 中实现的方法的联系和区别

3. 思考问题   - 这些论文使用了哪些技术？
   - 它们的优势和局限性是什么？
   - 如何将论文中的技术应用到你的项目中？
   - 有哪些可能的改进方向？


## Part 5 - 尝试使用 SemOpt（可选）

本部分目标： 尝试使用 SemOpt 的完整流程，或基于简化版本进行改进。

在 Part3 中，我们实现了简化版的代码优化方法。SemOpt 论文中描述的完整方法更加复杂，包含多个步骤：知识库构建、策略提取、策略选择、代码生成等。在这一部分，你可以选择以下方向之一：

### 5.1 - 尝试完整的 SemOpt 流程

参考 SemOpt 论文，尝试实现完整的优化流程：

1. 知识库准备：使用现有的 SemOpt 知识库（已提供）
2. 策略提取：从聚类结果中提取优化策略（类似 Part3 的 3.4）
3. 策略匹配：给定新代码，找到最适合的优化策略
4. 代码生成：根据策略和示例生成优化后的代码
5. 验证和迭代：验证生成的代码，必要时迭代优化

实践任务：- 在新的代码样本上测试 SemOpt 方法
- 对比与 Part3 中方法的效果差异
- 分析 SemOpt 的优势和局限性

### 5.2 - 改进简化版本

基于 Part3 中实现的方法，提出改进方向：

1. 改进检索策略   - 尝试不同的相似度计算方法
   - 结合多种检索策略（代码相似度 + 优化类型）

2. 改进策略库   - 增加更多策略类型
   - 改进策略描述的质量
   - 使用更好的代表性案例

3. 改进 Prompt 设计   - 尝试不同的 prompt 模板
   - 加入更多上下文信息
   - 使用多轮对话优化结果

实践任务：- 选择 1-2 个改进方向实施
- 在测试集上验证改进效果
- 分析改进的原因


## Part 6 - 进阶探索（可选）

本部分目标： 发现现有方法的局限性，提出改进方案并验证。这是一个开放性的探索部分，鼓励提出自己的想法。

### 6.1 - 问题：策略库的局限性

在使用 SemOpt 知识库的过程中，你可能会发现策略库存在一些局限性：

观察到的问题：1. 策略覆盖率低：只有约 25% 的 commits 被聚类，75% 的 commits 是噪声点
2. 策略过于简单：聚类得到的策略大多是简单的优化模式（如哈希表替代线性查找），缺少复杂的优化策略
3. 策略不够细粒度：一些策略簇包含的案例差异较大，可能需要更细的分类

可能的原因：1. 聚类阈值设置：当前使用 0.76 的相似度阈值可能过高，导致很多复杂案例被归为噪声
2. 聚类算法选择：当前使用的聚类算法可能不适合代码优化场景
3. 筛选策略问题：可能在筛选过程中过滤掉了一些有价值的复杂优化案例

### 6.2 - 改进方向 1：修改聚类流程

目标： 提高策略覆盖率，发现更多有价值的优化模式。

方案 1：调整聚类阈值- 尝试降低相似度阈值（如 0.6、0.7），观察聚类结果的变化
- 分析新增的策略簇是否包含有价值的优化模式
- 权衡覆盖率和策略质量

方案 2：使用层次聚类- 使用层次聚类算法，构建策略的层次结构
- 顶层是粗粒度的策略类别，底层是细粒度的具体策略
- 允许根据不同场景选择不同粒度的策略

方案 3：改进代码表示- 当前使用的 embedding 可能不够准确
- 尝试使用更先进的代码 embedding 模型（如 CodeBERT、GraphCodeBERT）
- 尝试结合代码结构信息（AST、控制流图）

实验验证：1. 实施一种改进方案
2. 对比改进前后的聚类结果
3. 评估新策略库在代码优化任务上的效果
4. 分析改进的原因和局限性

### 6.3 - 改进方向 2：修改筛选流程

目标： 更好地利用"噪声点"中的复杂优化策略。

当前筛选方式回顾：- 使用聚类算法，将相似的优化案例归为一类
- 未能聚类的案例（噪声点）被暂时忽略
- 这些噪声点可能包含复杂但有价值的优化策略

方案：两阶段筛选策略
阶段 1：自动聚类（已完成）
- 使用聚类算法识别常见的简单优化模式
- 适合大量重复出现的优化策略

阶段 2：人工筛选或 LLM 辅助筛选（新增）
- 从噪声点中挑选有价值的复杂优化案例
- 使用 LLM 分析噪声点的优化策略
- 人工验证并构建"复杂策略库"

分层策略库设计：
```
策略库
├── 常见策略（自动聚类）
│   ├── 策略 1：哈希表替代线性查找（45 个案例）
│   ├── 策略 2：缓存中间结果（32 个案例）
│   └── ...
└── 复杂策略（人工/LLM 筛选）
    ├── 策略 A：复杂的数据结构优化（5 个案例）
    ├── 策略 B：算法替换（3 个案例）
    └── ...
```

使用场景：- 对于简单代码：主要使用常见策略
- 对于复杂代码：结合常见策略和复杂策略

实验验证：1. 从噪声点中筛选出 10-20 个复杂优化案例
2. 构建复杂策略库
3. 在测试集上验证效果
4. 分析分层策略库的优势

### 6.4 - 其他探索方向

除了上述两个主要方向，你还可以探索：

方向 1：复现其他论文- 复现 RAPGen、PIE、SBLLM 等论文的方法
- 对比不同方法的效果
- 尝试组合不同论文的技术

相关资源：- DeepDev-PERF：https://github.com/glGarg/DeepDev-PERF
- PIE：https://github.com/LearningOpt/pie
- SBLLM：https://github.com/shuzhenggao/sbllm

方向 2：扩展到其他编程语言- 将方法应用到 Python、Java 等其他语言
- 对比不同语言的优化模式
- 构建多语言的优化知识库

方向 3：优化其他方面- 除了运行时性能，考虑内存优化、能耗优化等
- 探索 LLM 在不同优化类型上的表现

方向 4：工具化- 将方法封装成可用的工具
- 集成到 IDE 或 CI/CD 流程
- 设计用户友好的界面

### 6.5 - 开放探索

除了上述具体方向，我们鼓励你：
1. 发现新问题：在实践中发现现有方法的其他局限性
2. 提出新想法：基于你的理解提出改进方案
3. 大胆尝试：不要害怕失败，探索性研究就是要尝试各种可能
4. 记录过程：记录你的想法、实验过程和结果，这些都是宝贵的经验

思考问题：- 大模型在代码优化中的优势和局限性是什么？
- 如何结合传统编译器优化和 LLM 方法？
- 如何评估代码优化的效果？
- 未来的代码优化工具会是什么样的？

---

## 总结

完成本项目后，你应该：

✅ 掌握了 LLM API 的使用和提示工程技术  
✅ 了解了性能优化知识库的构建流程  
✅ 实现并对比了多种自动代码优化方法  
✅ 阅读了相关论文，理解了理论基础  
✅ （可选）探索了改进方向，提出了自己的想法  

下一步：- 整理你的代码和实验结果
- 总结你的发现和思考
- 与其他同学交流分享
- 考虑是否继续深入研究这个方向

如果有任何问题，请随时联系负责同学（zhaoyuwei@stu.pku.edu.cn）或在项目群中讨论。祝你在轮转项目中收获满满！

