diff --git a/src/execution/aggregate_hashtable.cpp b/src/execution/aggregate_hashtable.cpp
index 589fe4fbb0..8a2dd448ce 100644
--- a/src/execution/aggregate_hashtable.cpp
+++ b/src/execution/aggregate_hashtable.cpp
@@ -627,10 +627,17 @@ idx_t GroupedAggregateHashTable::FindOrCreateGroupsInternal(DataChunk &groups, V
 	// and precompute the hash salts for faster comparison below
 	const auto ht_offsets = FlatVector::GetData<uint64_t>(state.ht_offsets);
 	const auto hash_salts = FlatVector::GetData<hash_t>(state.hash_salts);
+
+	// We also compute the occupied count, which is essentially useless.
+	// However, this loop is branchless, while the main lookup loop below is not.
+	// So, by doing the lookups here, we better amortize cache misses.
+	idx_t occupied_count = 0;
 	for (idx_t r = 0; r < chunk_size; r++) {
 		const auto &hash = hashes[r];
-		ht_offsets[r] = ApplyBitMask(hash);
-		D_ASSERT(ht_offsets[r] == hash % capacity);
+		auto &ht_offset = ht_offsets[r];
+		ht_offset = ApplyBitMask(hash);
+		occupied_count += entries[ht_offset].IsOccupied(); // Lookup
+		D_ASSERT(ht_offset == hash % capacity);
 		hash_salts[r] = ht_entry_t::ExtractSalt(hash);
 	}
 
@@ -671,6 +678,13 @@ idx_t GroupedAggregateHashTable::FindOrCreateGroupsInternal(DataChunk &groups, V
 			}
 		}
 
+		if (DUCKDB_UNLIKELY(occupied_count > new_entry_count + need_compare_count)) {
+			// We use the useless occupied_count we summed above here so the variable is used,
+			// and the compiler cannot optimize away the vectorized lookups above. This should never be triggered.
+			throw InternalException("Internal validation failed in GroupedAggregateHashTable");
+		}
+		occupied_count = 0; // Have to set to 0 for next iterations
+
 		if (new_entry_count != 0) {
 			// Append everything that belongs to an empty group
 			optional_ptr<PartitionedTupleData> data;
