diff --git a/modified_file.cpp b/modified_file.cpp
index 0000000..1111111 100644
--- a/modified_file.cpp
+++ b/modified_file.cpp
--- a/modified_file.cpp
+++ b/modified_file.cpp
@@ -627,10 +627,17 @@
 	// and precompute the hash salts for faster comparison below

 	const auto ht_offsets = FlatVector::GetData<uint64_t>(state.ht_offsets);

 	const auto hash_salts = FlatVector::GetData<hash_t>(state.hash_salts);

+

+	// We also compute the occupied count, which is essentially useless.

+	// However, this loop is branchless, while the main lookup loop below is not.

+	// So, by doing the lookups here, we better amortize cache misses.

+	idx_t occupied_count = 0;

 	for (idx_t r = 0; r < chunk_size; r++) {

 		const auto &hash = hashes[r];

-		ht_offsets[r] = ApplyBitMask(hash);

-		D_ASSERT(ht_offsets[r] == hash % capacity);

+		auto &ht_offset = ht_offsets[r];

+		ht_offset = ApplyBitMask(hash);

+		occupied_count += entries[ht_offset].IsOccupied(); // Lookup

+		D_ASSERT(ht_offset == hash % capacity);

 		hash_salts[r] = ht_entry_t::ExtractSalt(hash);

 	}

 

@@ -670,6 +677,13 @@
 				throw InternalException("Maximum inner iteration count reached in GroupedAggregateHashTable");

 			}

 		}

+

+		if (DUCKDB_UNLIKELY(occupied_count > new_entry_count + need_compare_count)) {

+			// We use the useless occupied_count we summed above here so the variable is used,

+			// and the compiler cannot optimize away the vectorized lookups above. This should never be triggered.

+			throw InternalException("Internal validation failed in GroupedAggregateHashTable");

+		}

+		occupied_count = 0; // Have to set to 0 for next iterations

 

 		if (new_entry_count != 0) {

 			// Append everything that belongs to an empty group
