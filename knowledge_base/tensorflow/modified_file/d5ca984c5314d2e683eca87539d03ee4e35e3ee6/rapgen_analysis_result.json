{
  "status": "completed",
  "commit_dir": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/d5ca984c5314d2e683eca87539d03ee4e35e3ee6",
  "file_info": {
    "before_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/d5ca984c5314d2e683eca87539d03ee4e35e3ee6/before.cc",
    "after_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/d5ca984c5314d2e683eca87539d03ee4e35e3ee6/after.cc",
    "diff_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/d5ca984c5314d2e683eca87539d03ee4e35e3ee6/diff.txt"
  },
  "semgrep_analysis": {
    "is_api_change": false,
    "api_changes": {
      "replacements": [
        {
          "line": 177,
          "old_api": "in_backprop->tensor<T, 4>()",
          "new_api": "out_backprop.tensor<T, 4>()",
          "old_text": "in_backprop->tensor<T, 4>()",
          "new_text": "out_backprop.tensor<T, 4>()",
          "old_line_content": "    auto in_backprop_t = in_backprop->tensor<T, 4>();",
          "new_line_content": "    auto out_backprop_t = out_backprop.tensor<T, 4>();",
          "content_same": false
        },
        {
          "line": 178,
          "old_api": "out_backprop.tensor<T, 4>()",
          "new_api": "filter.tensor<T, 4>()",
          "old_text": "out_backprop.tensor<T, 4>()",
          "new_text": "filter.tensor<T, 4>()",
          "old_line_content": "    auto out_backprop_t = out_backprop.tensor<T, 4>();",
          "new_line_content": "    auto filter_t = filter.tensor<T, 4>();",
          "content_same": false
        },
        {
          "line": 188,
          "old_api": "functor::SpatialConvolutionBackwardInputFunc<Device, T>()(\n          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,\n          col_stride, row_stride, col_dilation, row_dilation)",
          "new_api": "ctx->eigen_device<Device>()",
          "old_text": "functor::SpatialConvolutionBackwardInputFunc<Device, T>()(\n          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,\n          col_stride, row_stride, col_dilation, row_dilation)",
          "new_text": "ctx->eigen_device<Device>()",
          "old_line_content": "      functor::SpatialConvolutionBackwardInputFunc<Device, T>()(",
          "new_line_content": "          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,",
          "content_same": false
        },
        {
          "line": 194,
          "old_api": "ctx->eigen_device<Device>()",
          "new_api": "dimension",
          "old_text": "ctx->eigen_device<Device>()",
          "new_text": "in_backprop_t.dimension(2)",
          "old_line_content": "          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,",
          "new_line_content": "          in_backprop_t.dimension(2) + (padding_left + padding_right),",
          "content_same": false
        },
        {
          "line": 387,
          "old_api": "GetAttr",
          "new_api": "FormatFromString",
          "old_text": "context->GetAttr(\"data_format\", &data_format)",
          "new_text": "FormatFromString(data_format, &data_format_)",
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));",
          "new_line_content": "    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),",
          "content_same": false
        },
        {
          "line": 388,
          "old_api": "FormatFromString",
          "new_api": "errors::InvalidArgument(\"Invalid data format\")",
          "old_text": "FormatFromString(data_format, &data_format_)",
          "new_text": "errors::InvalidArgument(\"Invalid data format\")",
          "old_line_content": "    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),",
          "new_line_content": "                errors::InvalidArgument(\"Invalid data format\"));",
          "content_same": false
        },
        {
          "line": 391,
          "old_api": "GetAttr",
          "new_api": "size",
          "old_text": "context->GetAttr(\"strides\", &strides_)",
          "new_text": "strides_.size()",
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));",
          "new_line_content": "    OP_REQUIRES(context, strides_.size() == 4,",
          "content_same": false
        },
        {
          "line": 392,
          "old_api": "size",
          "new_api": "errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\")",
          "old_text": "strides_.size()",
          "new_text": "errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\")",
          "old_line_content": "    OP_REQUIRES(context, strides_.size() == 4,",
          "new_line_content": "                errors::InvalidArgument(\"Sliding window strides field must \"",
          "content_same": false
        },
        {
          "line": 398,
          "old_api": "GetTensorDim",
          "new_api": "OP_REQUIRES",
          "old_text": "GetTensorDim(strides_, data_format_, 'W')",
          "new_text": "OP_REQUIRES(\n        context, (stride_n == 1 && stride_c == 1),\n        errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\"))",
          "old_line_content": "    int stride_w = GetTensorDim(strides_, data_format_, 'W');",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 403,
          "old_api": "OP_REQUIRES",
          "new_api": "errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\")",
          "old_text": "OP_REQUIRES(context, stride_h > 0 && stride_w > 0,\n                errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\"))",
          "new_text": "errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\")",
          "old_line_content": "    OP_REQUIRES(context, stride_h > 0 && stride_w > 0,",
          "new_line_content": "                errors::InvalidArgument(",
          "content_same": false
        },
        {
          "line": 407,
          "old_api": "GetAttr",
          "new_api": "size",
          "old_text": "context->GetAttr(\"dilations\", &dilations_)",
          "new_text": "dilations_.size()",
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilations_));",
          "new_line_content": "    OP_REQUIRES(context, dilations_.size() == 4,",
          "content_same": false
        },
        {
          "line": 408,
          "old_api": "size",
          "new_api": "errors::InvalidArgument(\"Sliding window dilations field must \"\n                                        \"specify 4 dimensions\")",
          "old_text": "dilations_.size()",
          "new_text": "errors::InvalidArgument(\"Sliding window dilations field must \"\n                                        \"specify 4 dimensions\")",
          "old_line_content": "    OP_REQUIRES(context, dilations_.size() == 4,",
          "new_line_content": "                errors::InvalidArgument(\"Sliding window dilations field must \"",
          "content_same": false
        },
        {
          "line": 414,
          "old_api": "GetTensorDim",
          "new_api": "OP_REQUIRES",
          "old_text": "GetTensorDim(dilations_, data_format_, 'W')",
          "new_text": "OP_REQUIRES(context, (dilation_n == 1 && dilation_c == 1),\n                errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\"))",
          "old_line_content": "    int dilation_w = GetTensorDim(dilations_, data_format_, 'W');",
          "new_line_content": "    OP_REQUIRES(context, (dilation_n == 1 && dilation_c == 1),",
          "content_same": false
        },
        {
          "line": 415,
          "old_api": "OP_REQUIRES",
          "new_api": "errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\")",
          "old_text": "OP_REQUIRES(context, (dilation_n == 1 && dilation_c == 1),\n                errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\"))",
          "new_text": "errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\")",
          "old_line_content": "    OP_REQUIRES(context, (dilation_n == 1 && dilation_c == 1),",
          "new_line_content": "                errors::InvalidArgument(",
          "content_same": false
        },
        {
          "line": 425,
          "old_api": "GetAttr",
          "new_api": "CheckValidPadding",
          "old_text": "context->GetAttr(\"explicit_paddings\", &explicit_paddings_)",
          "new_text": "CheckValidPadding(padding_, explicit_paddings_,\n                                              /*num_dims=*/4, data_format_)",
          "old_line_content": "                   context->GetAttr(\"explicit_paddings\", &explicit_paddings_));",
          "new_line_content": "    OP_REQUIRES_OK(context, CheckValidPadding(padding_, explicit_paddings_,",
          "content_same": false
        },
        {
          "line": 429,
          "old_api": "GetAttr",
          "new_api": "CanUseCudnn",
          "old_text": "context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_)",
          "new_text": "CanUseCudnn()",
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));",
          "new_line_content": "    use_cudnn_ &= CanUseCudnn();",
          "content_same": false
        },
        {
          "line": 430,
          "old_api": "CanUseCudnn",
          "new_api": "CudnnUseAutotune",
          "old_text": "CanUseCudnn()",
          "new_text": "CudnnUseAutotune()",
          "old_line_content": "    use_cudnn_ &= CanUseCudnn();",
          "new_line_content": "    cudnn_use_autotune_ = CudnnUseAutotune();",
          "content_same": false
        },
        {
          "line": 476,
          "old_api": "VLOG",
          "new_api": "DebugString",
          "old_text": "VLOG(2)",
          "new_text": "input_shape.DebugString()",
          "old_line_content": "    VLOG(2) << \"Conv2DBackpropInput:\"",
          "new_line_content": "            << \" input: \" << input_shape.DebugString()",
          "content_same": false
        },
        {
          "line": 477,
          "old_api": "DebugString",
          "new_api": "shape",
          "old_text": "input_shape.DebugString()",
          "new_text": "filter.shape().DebugString()",
          "old_line_content": "            << \" input: \" << input_shape.DebugString()",
          "new_line_content": "            << \" filter:\" << filter.shape().DebugString()",
          "content_same": false
        },
        {
          "line": 509,
          "old_api": "GetAttr",
          "new_api": "FormatFromString",
          "old_text": "context->GetAttr(\"data_format\", &data_format)",
          "new_text": "FormatFromString(data_format, &data_format_)",
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));",
          "new_line_content": "    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),",
          "content_same": false
        },
        {
          "line": 510,
          "old_api": "FormatFromString",
          "new_api": "errors::InvalidArgument(\"Invalid data format\")",
          "old_text": "FormatFromString(data_format, &data_format_)",
          "new_text": "errors::InvalidArgument(\"Invalid data format\")",
          "old_line_content": "    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),",
          "new_line_content": "                errors::InvalidArgument(\"Invalid data format\"));",
          "content_same": false
        },
        {
          "line": 511,
          "old_api": "errors::InvalidArgument(\"Invalid data format\")",
          "new_api": "OP_REQUIRES",
          "old_text": "errors::InvalidArgument(\"Invalid data format\")",
          "new_text": "OP_REQUIRES(context, data_format_ == FORMAT_NHWC,\n                errors::InvalidArgument(\n                    \"Conv2DCustomBackpropInputOp only supports NHWC.\"))",
          "old_line_content": "                errors::InvalidArgument(\"Invalid data format\"));",
          "new_line_content": "    OP_REQUIRES(context, data_format_ == FORMAT_NHWC,",
          "content_same": false
        },
        {
          "line": 512,
          "old_api": "OP_REQUIRES",
          "new_api": "errors::InvalidArgument(\n                    \"Conv2DCustomBackpropInputOp only supports NHWC.\")",
          "old_text": "OP_REQUIRES(context, data_format_ == FORMAT_NHWC,\n                errors::InvalidArgument(\n                    \"Conv2DCustomBackpropInputOp only supports NHWC.\"))",
          "new_text": "errors::InvalidArgument(\n                    \"Conv2DCustomBackpropInputOp only supports NHWC.\")",
          "old_line_content": "    OP_REQUIRES(context, data_format_ == FORMAT_NHWC,",
          "new_line_content": "                errors::InvalidArgument(",
          "content_same": false
        },
        {
          "line": 515,
          "old_api": "GetAttr",
          "new_api": "size",
          "old_text": "context->GetAttr(\"strides\", &strides_)",
          "new_text": "strides_.size()",
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));",
          "new_line_content": "    OP_REQUIRES(context, strides_.size() == 4,",
          "content_same": false
        },
        {
          "line": 516,
          "old_api": "size",
          "new_api": "errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\")",
          "old_text": "strides_.size()",
          "new_text": "errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\")",
          "old_line_content": "    OP_REQUIRES(context, strides_.size() == 4,",
          "new_line_content": "                errors::InvalidArgument(\"Sliding window strides field must \"",
          "content_same": false
        },
        {
          "line": 523,
          "old_api": "OP_REQUIRES",
          "new_api": "errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\")",
          "old_text": "OP_REQUIRES(context, strides_[1] > 0 && strides_[2] > 0,\n                errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\"))",
          "new_text": "errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\")",
          "old_line_content": "    OP_REQUIRES(context, strides_[1] > 0 && strides_[2] > 0,",
          "new_line_content": "                errors::InvalidArgument(",
          "content_same": false
        },
        {
          "line": 527,
          "old_api": "GetAttr",
          "new_api": "size",
          "old_text": "context->GetAttr(\"dilations\", &dilations_)",
          "new_text": "dilations_.size()",
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilations_));",
          "new_line_content": "    OP_REQUIRES(context, dilations_.size() == 4,",
          "content_same": false
        },
        {
          "line": 528,
          "old_api": "size",
          "new_api": "errors::InvalidArgument(\"Sliding window dilations field must \"\n                                        \"specify 4 dimensions\")",
          "old_text": "dilations_.size()",
          "new_text": "errors::InvalidArgument(\"Sliding window dilations field must \"\n                                        \"specify 4 dimensions\")",
          "old_line_content": "    OP_REQUIRES(context, dilations_.size() == 4,",
          "new_line_content": "                errors::InvalidArgument(\"Sliding window dilations field must \"",
          "content_same": false
        },
        {
          "line": 531,
          "old_api": "OP_REQUIRES",
          "new_api": "errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\")",
          "old_text": "OP_REQUIRES(context, (dilations_[0] == 1 && dilations_[3] == 1),\n                errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\"))",
          "new_text": "errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\")",
          "old_line_content": "    OP_REQUIRES(context, (dilations_[0] == 1 && dilations_[3] == 1),",
          "new_line_content": "                errors::InvalidArgument(",
          "content_same": false
        },
        {
          "line": 536,
          "old_api": "OP_REQUIRES",
          "new_api": "errors::InvalidArgument(\n                    \"Current libxsmm and customized CPU implementations do \"\n                    \"not yet support dilation rates larger than 1.\")",
          "old_text": "OP_REQUIRES(context, (dilations_[1] == 1 && dilations_[2] == 1),\n                errors::InvalidArgument(\n                    \"Current libxsmm and customized CPU implementations do \"\n                    \"not yet support dilation rates larger than 1.\"))",
          "new_text": "errors::InvalidArgument(\n                    \"Current libxsmm and customized CPU implementations do \"\n                    \"not yet support dilation rates larger than 1.\")",
          "old_line_content": "    OP_REQUIRES(context, (dilations_[1] == 1 && dilations_[2] == 1),",
          "new_line_content": "                errors::InvalidArgument(",
          "content_same": false
        },
        {
          "line": 541,
          "old_api": "GetAttr",
          "new_api": "CheckValidPadding",
          "old_text": "context->GetAttr(\"explicit_paddings\", &explicit_paddings_)",
          "new_text": "CheckValidPadding(padding_, explicit_paddings_,\n                                              /*num_dims=*/4, data_format_)",
          "old_line_content": "                   context->GetAttr(\"explicit_paddings\", &explicit_paddings_));",
          "new_line_content": "    OP_REQUIRES_OK(context, CheckValidPadding(padding_, explicit_paddings_,",
          "content_same": false
        },
        {
          "line": 594,
          "old_api": "LaunchXsmmBackwardInputConvolution<Device, T>()(\n              context, context->eigen_device<Device>(),\n              in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),\n              out_backprop.tensor<T, 4>(), dims.spatial_dims[0].input_size,\n              dims.spatial_dims[1].input_size,\n              static_cast<int>(dims.spatial_dims[0].stride),\n              static_cast<int>(dims.spatial_dims[1].stride),\n              static_cast<int>(pad_top), static_cast<int>(pad_left),\n              data_format_)",
          "new_api": "context->eigen_device<Device>()",
          "old_text": "LaunchXsmmBackwardInputConvolution<Device, T>()(\n              context, context->eigen_device<Device>(),\n              in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),\n              out_backprop.tensor<T, 4>(), dims.spatial_dims[0].input_size,\n              dims.spatial_dims[1].input_size,\n              static_cast<int>(dims.spatial_dims[0].stride),\n              static_cast<int>(dims.spatial_dims[1].stride),\n              static_cast<int>(pad_top), static_cast<int>(pad_left),\n              data_format_)",
          "new_text": "context->eigen_device<Device>()",
          "old_line_content": "      if (LaunchXsmmBackwardInputConvolution<Device, T>()(",
          "new_line_content": "              context, context->eigen_device<Device>(),",
          "content_same": false
        },
        {
          "line": 595,
          "old_api": "context->eigen_device<Device>()",
          "new_api": "filter.tensor<T, 4>()",
          "old_text": "context->eigen_device<Device>()",
          "new_text": "filter.tensor<T, 4>()",
          "old_line_content": "              context, context->eigen_device<Device>(),",
          "new_line_content": "              in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),",
          "content_same": false
        },
        {
          "line": 596,
          "old_api": "filter.tensor<T, 4>()",
          "new_api": "out_backprop.tensor<T, 4>()",
          "old_text": "filter.tensor<T, 4>()",
          "new_text": "out_backprop.tensor<T, 4>()",
          "old_line_content": "              in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),",
          "new_line_content": "              out_backprop.tensor<T, 4>(), dims.spatial_dims[0].input_size,",
          "content_same": false
        },
        {
          "line": 599,
          "old_api": "static_cast<int>(dims.spatial_dims[0].stride)",
          "new_api": "static_cast<int>(dims.spatial_dims[1].stride)",
          "old_text": "static_cast<int>(dims.spatial_dims[0].stride)",
          "new_text": "static_cast<int>(dims.spatial_dims[1].stride)",
          "old_line_content": "              static_cast<int>(dims.spatial_dims[0].stride),",
          "new_line_content": "              static_cast<int>(dims.spatial_dims[1].stride),",
          "content_same": false
        },
        {
          "line": 600,
          "old_api": "static_cast<int>(dims.spatial_dims[1].stride)",
          "new_api": "static_cast<int>(pad_left)",
          "old_text": "static_cast<int>(dims.spatial_dims[1].stride)",
          "new_text": "static_cast<int>(pad_left)",
          "old_line_content": "              static_cast<int>(dims.spatial_dims[1].stride),",
          "new_line_content": "              static_cast<int>(pad_top), static_cast<int>(pad_left),",
          "content_same": false
        },
        {
          "line": 681,
          "old_api": "static_cast<int64>(shard_size)",
          "new_api": "static_cast<int64>(output_image_size)",
          "old_text": "static_cast<int64>(shard_size)",
          "new_text": "static_cast<int64>(output_image_size)",
          "old_line_content": "                       TensorShape({static_cast<int64>(shard_size),",
          "new_line_content": "                                    static_cast<int64>(output_image_size),",
          "content_same": false
        },
        {
          "line": 682,
          "old_api": "static_cast<int64>(output_image_size)",
          "new_api": "static_cast<int64>(filter_total_size)",
          "old_text": "static_cast<int64>(output_image_size)",
          "new_text": "static_cast<int64>(filter_total_size)",
          "old_line_content": "                                    static_cast<int64>(output_image_size),",
          "new_line_content": "                                    static_cast<int64>(filter_total_size)}),",
          "content_same": false
        },
        {
          "line": 697,
          "old_api": "in_backprop->template flat<T>()",
          "new_api": "data",
          "old_text": "in_backprop->template flat<T>()",
          "new_text": "in_backprop_flat.data()",
          "old_line_content": "    auto in_backprop_flat = in_backprop->template flat<T>();",
          "new_line_content": "    T* input_backprop_data = in_backprop_flat.data();",
          "content_same": false
        },
        {
          "line": 698,
          "old_api": "data",
          "new_api": "context->eigen_device<Device>()",
          "old_text": "in_backprop_flat.data()",
          "new_text": "context->eigen_device<Device>()",
          "old_line_content": "    T* input_backprop_data = in_backprop_flat.data();",
          "new_line_content": "    in_backprop_flat.device(context->eigen_device<Device>()) =",
          "content_same": false
        },
        {
          "line": 699,
          "old_api": "context->eigen_device<Device>()",
          "new_api": "T",
          "old_text": "context->eigen_device<Device>()",
          "new_text": "T(0)",
          "old_line_content": "    in_backprop_flat.device(context->eigen_device<Device>()) =",
          "new_line_content": "        in_backprop_flat.constant(T(0));",
          "content_same": false
        },
        {
          "line": 738,
          "old_api": "static_cast<int>(shard_size)",
          "new_api": "static_cast<int>(dims.batch_size)",
          "old_text": "static_cast<int>(shard_size)",
          "new_text": "static_cast<int>(dims.batch_size)",
          "old_line_content": "            std::min(static_cast<int>(shard_size),",
          "new_line_content": "                     static_cast<int>(dims.batch_size) - image_id);",
          "content_same": false
        },
        {
          "line": 801,
          "old_api": "TF_CALL_half",
          "new_api": "TF_CALL_float",
          "old_text": "TF_CALL_half(REGISTER_CPU_KERNELS)",
          "new_text": "TF_CALL_float(REGISTER_CPU_KERNELS)",
          "old_line_content": "TF_CALL_half(REGISTER_CPU_KERNELS);",
          "new_line_content": "TF_CALL_float(REGISTER_CPU_KERNELS);",
          "content_same": false
        },
        {
          "line": 802,
          "old_api": "TF_CALL_float",
          "new_api": "TF_CALL_double",
          "old_text": "TF_CALL_float(REGISTER_CPU_KERNELS)",
          "new_text": "TF_CALL_double(REGISTER_CPU_KERNELS)",
          "old_line_content": "TF_CALL_float(REGISTER_CPU_KERNELS);",
          "new_line_content": "TF_CALL_double(REGISTER_CPU_KERNELS);",
          "content_same": false
        },
        {
          "line": 803,
          "old_api": "TF_CALL_double",
          "new_api": "TF_CALL_int32",
          "old_text": "TF_CALL_double(REGISTER_CPU_KERNELS)",
          "new_text": "TF_CALL_int32(REGISTER_CPU_KERNELS)",
          "old_line_content": "TF_CALL_double(REGISTER_CPU_KERNELS);",
          "new_line_content": "TF_CALL_int32(REGISTER_CPU_KERNELS);",
          "content_same": false
        },
        {
          "line": 869,
          "old_api": "DCHECK_EQ",
          "new_api": "GetWindowedOutputSizeVerboseV2",
          "old_text": "DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows)",
          "new_text": "GetWindowedOutputSizeVerboseV2(\n      dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n      col_dilation, col_stride, padding, &expected_out_cols, &padding_left,\n      &padding_right)",
          "old_line_content": "  DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows);",
          "new_line_content": "  TF_CHECK_OK(GetWindowedOutputSizeVerboseV2(",
          "content_same": false
        },
        {
          "line": 876,
          "old_api": "op_device_context",
          "new_api": "errors::Internal(\"No GPU stream available.\")",
          "old_text": "ctx->op_device_context()->stream()",
          "new_text": "errors::Internal(\"No GPU stream available.\")",
          "old_line_content": "  auto* stream = ctx->op_device_context()->stream();",
          "new_line_content": "  OP_REQUIRES(ctx, stream, errors::Internal(\"No GPU stream available.\"));",
          "content_same": false
        },
        {
          "line": 901,
          "old_api": "data",
          "new_api": "size",
          "old_text": "out_backprop.template flat<T>().data()",
          "new_text": "out_backprop.template flat<T>().size()",
          "old_line_content": "    auto a_ptr = AsDeviceMemory(out_backprop.template flat<T>().data(),",
          "new_line_content": "                                out_backprop.template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 902,
          "old_api": "size",
          "new_api": "data",
          "old_text": "out_backprop.template flat<T>().size()",
          "new_text": "filter.template flat<T>().data()",
          "old_line_content": "                                out_backprop.template flat<T>().size());",
          "new_line_content": "    auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 903,
          "old_api": "data",
          "new_api": "size",
          "old_text": "filter.template flat<T>().data()",
          "new_text": "filter.template flat<T>().size()",
          "old_line_content": "    auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),",
          "new_line_content": "                                filter.template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 904,
          "old_api": "size",
          "new_api": "data",
          "old_text": "filter.template flat<T>().size()",
          "new_text": "in_backprop->template flat<T>().data()",
          "old_line_content": "                                filter.template flat<T>().size());",
          "new_line_content": "    auto c_ptr = AsDeviceMemory(in_backprop->template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 905,
          "old_api": "data",
          "new_api": "size",
          "old_text": "in_backprop->template flat<T>().data()",
          "new_text": "in_backprop->template flat<T>().size()",
          "old_line_content": "    auto c_ptr = AsDeviceMemory(in_backprop->template flat<T>().data(),",
          "new_line_content": "                                in_backprop->template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 934,
          "old_api": "data",
          "new_api": "size",
          "old_text": "out_backprop.template flat<T>().data()",
          "new_text": "out_backprop.template flat<T>().size()",
          "old_line_content": "    auto a_ptr = AsDeviceMemory(out_backprop.template flat<T>().data(),",
          "new_line_content": "                                out_backprop.template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 935,
          "old_api": "size",
          "new_api": "data",
          "old_text": "out_backprop.template flat<T>().size()",
          "new_text": "filter.template flat<T>().data()",
          "old_line_content": "                                out_backprop.template flat<T>().size());",
          "new_line_content": "    auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 936,
          "old_api": "data",
          "new_api": "size",
          "old_text": "filter.template flat<T>().data()",
          "new_text": "filter.template flat<T>().size()",
          "old_line_content": "    auto b_ptr = AsDeviceMemory(filter.template flat<T>().data(),",
          "new_line_content": "                                filter.template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 937,
          "old_api": "size",
          "new_api": "data",
          "old_text": "filter.template flat<T>().size()",
          "new_text": "in_backprop->template flat<T>().data()",
          "old_line_content": "                                filter.template flat<T>().size());",
          "new_line_content": "    auto c_ptr = AsDeviceMemory(in_backprop->template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 938,
          "old_api": "data",
          "new_api": "size",
          "old_text": "in_backprop->template flat<T>().data()",
          "new_text": "in_backprop->template flat<T>().size()",
          "old_line_content": "    auto c_ptr = AsDeviceMemory(in_backprop->template flat<T>().data(),",
          "new_line_content": "                                in_backprop->template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 956,
          "old_api": "std::min(padding_top, padding_bottom)",
          "new_api": "std::min(padding_left, padding_right)",
          "old_text": "std::min(padding_top, padding_bottom)",
          "new_text": "std::min(padding_left, padding_right)",
          "old_line_content": "  const int64 common_padding_rows = std::min(padding_top, padding_bottom);",
          "new_line_content": "  const int64 common_padding_cols = std::min(padding_left, padding_right);",
          "content_same": false
        },
        {
          "line": 963,
          "old_api": "std::abs(padding_bottom - padding_top)",
          "new_api": "std::abs(padding_right - padding_left)",
          "old_text": "std::abs(padding_bottom - padding_top)",
          "new_text": "std::abs(padding_right - padding_left)",
          "old_line_content": "    const int64 padding_rows_diff = std::abs(padding_bottom - padding_top);",
          "new_line_content": "    const int64 padding_cols_diff = std::abs(padding_right - padding_left);",
          "content_same": false
        },
        {
          "line": 992,
          "old_api": "VLOG",
          "new_api": "ToString",
          "old_text": "VLOG(3)",
          "new_text": "ToString(data_format)",
          "old_line_content": "  VLOG(3) << \"Compute Conv2DBackpropInput with cuDNN:\"",
          "new_line_content": "          << \" data_format=\" << ToString(data_format)",
          "content_same": false
        },
        {
          "line": 996,
          "old_api": "constexpr",
          "new_api": "std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,\n                      se::dnn::FilterLayout::kOutputYXInput)",
          "old_text": "constexpr",
          "new_text": "std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,\n                      se::dnn::FilterLayout::kOutputYXInput)",
          "old_line_content": "  constexpr auto kComputeInNHWC =",
          "new_line_content": "      std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,",
          "content_same": false
        },
        {
          "line": 999,
          "old_api": "constexpr",
          "new_api": "std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,\n                      se::dnn::FilterLayout::kOutputInputYX)",
          "old_text": "constexpr",
          "new_text": "std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,\n                      se::dnn::FilterLayout::kOutputInputYX)",
          "old_line_content": "  constexpr auto kComputeInNCHW =",
          "new_line_content": "      std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,",
          "content_same": false
        },
        {
          "line": 1010,
          "old_api": "set_count",
          "new_api": "GetTensorDim",
          "old_text": "input_desc.set_count(dims.batch_size)\n      .set_height(GetTensorDim(compatible_input_shape, data_format, 'H'))\n      .set_width(GetTensorDim(compatible_input_shape, data_format, 'W'))\n      .set_feature_map_count(dims.in_depth)\n      .set_layout(compute_data_layout)",
          "new_text": "GetTensorDim(compatible_input_shape, data_format, 'H')",
          "old_line_content": "  input_desc.set_count(dims.batch_size)",
          "new_line_content": "      .set_height(GetTensorDim(compatible_input_shape, data_format, 'H'))",
          "content_same": false
        },
        {
          "line": 1055,
          "old_api": "functor::TransformFilter<GPUDevice, T, int, 4>()(\n        ctx->eigen_device<GPUDevice>(), dst_format,\n        To32Bit(filter.tensor<T, 4>()),\n        To32Bit(transformed_filter.tensor<T, 4>()))",
          "new_api": "ctx->eigen_device<GPUDevice>()",
          "old_text": "functor::TransformFilter<GPUDevice, T, int, 4>()(\n        ctx->eigen_device<GPUDevice>(), dst_format,\n        To32Bit(filter.tensor<T, 4>()),\n        To32Bit(transformed_filter.tensor<T, 4>()))",
          "new_text": "ctx->eigen_device<GPUDevice>()",
          "old_line_content": "    functor::TransformFilter<GPUDevice, T, int, 4>()(",
          "new_line_content": "        ctx->eigen_device<GPUDevice>(), dst_format,",
          "content_same": false
        },
        {
          "line": 1056,
          "old_api": "ctx->eigen_device<GPUDevice>()",
          "new_api": "filter.tensor<T, 4>()",
          "old_text": "ctx->eigen_device<GPUDevice>()",
          "new_text": "filter.tensor<T, 4>()",
          "old_line_content": "        ctx->eigen_device<GPUDevice>(), dst_format,",
          "new_line_content": "        To32Bit(filter.tensor<T, 4>()),",
          "content_same": false
        },
        {
          "line": 1057,
          "old_api": "filter.tensor<T, 4>()",
          "new_api": "transformed_filter.tensor<T, 4>()",
          "old_text": "filter.tensor<T, 4>()",
          "new_text": "transformed_filter.tensor<T, 4>()",
          "old_line_content": "        To32Bit(filter.tensor<T, 4>()),",
          "new_line_content": "        To32Bit(transformed_filter.tensor<T, 4>()));",
          "content_same": false
        },
        {
          "line": 1068,
          "old_api": "errors::InvalidArgument(\"Invalid compute data format: \",\n                                           ToString(compute_data_format))",
          "new_api": "ToString",
          "old_text": "errors::InvalidArgument(\"Invalid compute data format: \",\n                                           ToString(compute_data_format))",
          "new_text": "ToString(compute_data_format)",
          "old_line_content": "    ctx->SetStatus(errors::InvalidArgument(\"Invalid compute data format: \",",
          "new_line_content": "                                           ToString(compute_data_format)));",
          "content_same": false
        },
        {
          "line": 1075,
          "old_api": "VLOG",
          "new_api": "ShapeFromFormat",
          "old_text": "VLOG(4)",
          "new_text": "ShapeFromFormat(\n        compute_data_format, dims.batch_size, dims.spatial_dims[0].output_size,\n        dims.spatial_dims[1].output_size, dims.out_depth)",
          "old_line_content": "    VLOG(4) << \"Convert the `out_backprop` tensor from NHWC to NCHW.\";",
          "new_line_content": "    TensorShape compute_shape = ShapeFromFormat(",
          "content_same": false
        },
        {
          "line": 1083,
          "old_api": "functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          ctx->eigen_device<GPUDevice>(), out_backprop.tensor<T, 4>(),\n          transformed_out_backprop.tensor<T, 4>())",
          "new_api": "out_backprop.tensor<T, 4>()",
          "old_text": "functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          ctx->eigen_device<GPUDevice>(), out_backprop.tensor<T, 4>(),\n          transformed_out_backprop.tensor<T, 4>())",
          "new_text": "out_backprop.tensor<T, 4>()",
          "old_line_content": "      functor::NHWCToNCHW<GPUDevice, T, 4>()(",
          "new_line_content": "          ctx->eigen_device<GPUDevice>(), out_backprop.tensor<T, 4>(),",
          "content_same": false
        },
        {
          "line": 1084,
          "old_api": "out_backprop.tensor<T, 4>()",
          "new_api": "transformed_out_backprop.tensor<T, 4>()",
          "old_text": "out_backprop.tensor<T, 4>()",
          "new_text": "transformed_out_backprop.tensor<T, 4>()",
          "old_line_content": "          ctx->eigen_device<GPUDevice>(), out_backprop.tensor<T, 4>(),",
          "new_line_content": "          transformed_out_backprop.tensor<T, 4>());",
          "content_same": false
        },
        {
          "line": 1107,
          "old_api": "data",
          "new_api": "size",
          "old_text": "transformed_out_backprop.template flat<T>().data()",
          "new_text": "transformed_out_backprop.template flat<T>().size()",
          "old_line_content": "      AsDeviceMemory(transformed_out_backprop.template flat<T>().data(),",
          "new_line_content": "                     transformed_out_backprop.template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 1110,
          "old_api": "data",
          "new_api": "size",
          "old_text": "transformed_filter.template flat<T>().data()",
          "new_text": "transformed_filter.template flat<T>().size()",
          "old_line_content": "      AsDeviceMemory(transformed_filter.template flat<T>().data(),",
          "new_line_content": "                     transformed_filter.template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 1113,
          "old_api": "data",
          "new_api": "size",
          "old_text": "pre_transformed_in_backprop.template flat<T>().data()",
          "new_text": "pre_transformed_in_backprop.template flat<T>().size()",
          "old_line_content": "      AsDeviceMemory(pre_transformed_in_backprop.template flat<T>().data(),",
          "new_line_content": "                     pre_transformed_in_backprop.template flat<T>().size());",
          "content_same": false
        },
        {
          "line": 1120,
          "old_api": "parent",
          "new_api": "dtype",
          "old_text": "stream->parent()->device_ordinal()",
          "new_text": "out_backprop.dtype()",
          "old_line_content": "  int device_id = stream->parent()->device_ordinal();",
          "new_line_content": "  DataType dtype = out_backprop.dtype();",
          "content_same": false
        },
        {
          "line": 1125,
          "old_api": "height",
          "new_api": "width",
          "old_text": "input_desc.height()",
          "new_text": "input_desc.width()",
          "old_line_content": "      {{input_desc.height(),               // in_rows",
          "new_line_content": "        input_desc.width()}},              // in_cols",
          "content_same": false
        },
        {
          "line": 1176,
          "old_api": "RedzoneCheckDisabled",
          "new_api": "static_cast<se::ScratchAllocator*>(&rz_scratch_allocator)",
          "old_text": "RedzoneCheckDisabled()",
          "new_text": "static_cast<se::ScratchAllocator*>(&rz_scratch_allocator)",
          "old_line_content": "          !RedzoneCheckDisabled()",
          "new_line_content": "              ? static_cast<se::ScratchAllocator*>(&rz_scratch_allocator)",
          "content_same": false
        },
        {
          "line": 1177,
          "old_api": "static_cast<se::ScratchAllocator*>(&rz_scratch_allocator)",
          "new_api": "static_cast<se::ScratchAllocator*>(&scratch_allocator)",
          "old_text": "static_cast<se::ScratchAllocator*>(&rz_scratch_allocator)",
          "new_text": "static_cast<se::ScratchAllocator*>(&scratch_allocator)",
          "old_line_content": "              ? static_cast<se::ScratchAllocator*>(&rz_scratch_allocator)",
          "new_line_content": "              : static_cast<se::ScratchAllocator*>(&scratch_allocator);",
          "content_same": false
        },
        {
          "line": 1187,
          "old_api": "is_valid",
          "new_api": "emplace_back",
          "old_text": "profile_result.is_valid()",
          "new_text": "results.emplace_back()",
          "old_line_content": "      if (cudnn_launch_status && profile_result.is_valid()) {",
          "new_line_content": "        results.emplace_back();",
          "content_same": false
        },
        {
          "line": 1188,
          "old_api": "emplace_back",
          "new_api": "back",
          "old_text": "results.emplace_back()",
          "new_text": "results.back()",
          "old_line_content": "        results.emplace_back();",
          "new_line_content": "        auto& result = results.back();",
          "content_same": false
        },
        {
          "line": 1189,
          "old_api": "back",
          "new_api": "algo_id",
          "old_text": "results.back()",
          "new_text": "profile_algorithm.algo_id()",
          "old_line_content": "        auto& result = results.back();",
          "new_line_content": "        result.mutable_conv()->set_algorithm(profile_algorithm.algo_id());",
          "content_same": false
        },
        {
          "line": 1190,
          "old_api": "algo_id",
          "new_api": "mutable_conv",
          "old_text": "profile_algorithm.algo_id()",
          "new_text": "result.mutable_conv()->set_tensor_ops_enabled(\n            profile_algorithm.tensor_ops_enabled())",
          "old_line_content": "        result.mutable_conv()->set_algorithm(profile_algorithm.algo_id());",
          "new_line_content": "        result.mutable_conv()->set_tensor_ops_enabled(",
          "content_same": false
        },
        {
          "line": 1191,
          "old_api": "mutable_conv",
          "new_api": "tensor_ops_enabled",
          "old_text": "result.mutable_conv()->set_tensor_ops_enabled(\n            profile_algorithm.tensor_ops_enabled())",
          "new_text": "profile_algorithm.tensor_ops_enabled()",
          "old_line_content": "        result.mutable_conv()->set_tensor_ops_enabled(",
          "new_line_content": "            profile_algorithm.tensor_ops_enabled());",
          "content_same": false
        },
        {
          "line": 1192,
          "old_api": "tensor_ops_enabled",
          "new_api": "set_scratch_bytes",
          "old_text": "profile_algorithm.tensor_ops_enabled()",
          "new_text": "result.set_scratch_bytes(\n            !RedzoneCheckDisabled()\n                ? rz_scratch_allocator.TotalAllocatedBytesExcludingRedzones()\n                : scratch_allocator.TotalByteSize())",
          "old_line_content": "            profile_algorithm.tensor_ops_enabled());",
          "new_line_content": "        result.set_scratch_bytes(",
          "content_same": false
        },
        {
          "line": 1193,
          "old_api": "set_scratch_bytes",
          "new_api": "RedzoneCheckDisabled",
          "old_text": "result.set_scratch_bytes(\n            !RedzoneCheckDisabled()\n                ? rz_scratch_allocator.TotalAllocatedBytesExcludingRedzones()\n                : scratch_allocator.TotalByteSize())",
          "new_text": "RedzoneCheckDisabled()",
          "old_line_content": "        result.set_scratch_bytes(",
          "new_line_content": "            !RedzoneCheckDisabled()",
          "content_same": false
        },
        {
          "line": 1194,
          "old_api": "RedzoneCheckDisabled",
          "new_api": "TotalAllocatedBytesExcludingRedzones",
          "old_text": "RedzoneCheckDisabled()",
          "new_text": "rz_scratch_allocator.TotalAllocatedBytesExcludingRedzones()",
          "old_line_content": "            !RedzoneCheckDisabled()",
          "new_line_content": "                ? rz_scratch_allocator.TotalAllocatedBytesExcludingRedzones()",
          "content_same": false
        },
        {
          "line": 1195,
          "old_api": "TotalAllocatedBytesExcludingRedzones",
          "new_api": "TotalByteSize",
          "old_text": "rz_scratch_allocator.TotalAllocatedBytesExcludingRedzones()",
          "new_text": "scratch_allocator.TotalByteSize()",
          "old_line_content": "                ? rz_scratch_allocator.TotalAllocatedBytesExcludingRedzones()",
          "new_line_content": "                : scratch_allocator.TotalByteSize());",
          "content_same": false
        },
        {
          "line": 1196,
          "old_api": "TotalByteSize",
          "new_api": "elapsed_time_in_ms",
          "old_text": "scratch_allocator.TotalByteSize()",
          "new_text": "proto_utils::ToDurationProto(\n            absl::Milliseconds(profile_result.elapsed_time_in_ms()))",
          "old_line_content": "                : scratch_allocator.TotalByteSize());",
          "new_line_content": "        *result.mutable_run_time() = proto_utils::ToDurationProto(",
          "content_same": false
        },
        {
          "line": 1222,
          "old_api": "emplace_back",
          "new_api": "back",
          "old_text": "results.emplace_back()",
          "new_text": "results.back()",
          "old_line_content": "      results.emplace_back();",
          "new_line_content": "      auto& result = results.back();",
          "content_same": false
        },
        {
          "line": 1223,
          "old_api": "back",
          "new_api": "mutable_conv",
          "old_text": "results.back()",
          "new_text": "result.mutable_conv()->set_algorithm(\n          profile_result.algorithm().algo_id())",
          "old_line_content": "      auto& result = results.back();",
          "new_line_content": "      result.mutable_conv()->set_algorithm(",
          "content_same": false
        },
        {
          "line": 1224,
          "old_api": "mutable_conv",
          "new_api": "algorithm",
          "old_text": "result.mutable_conv()->set_algorithm(\n          profile_result.algorithm().algo_id())",
          "new_text": "profile_result.algorithm().algo_id()",
          "old_line_content": "      result.mutable_conv()->set_algorithm(",
          "new_line_content": "          profile_result.algorithm().algo_id());",
          "content_same": false
        },
        {
          "line": 1225,
          "old_api": "algorithm",
          "new_api": "mutable_conv",
          "old_text": "profile_result.algorithm().algo_id()",
          "new_text": "result.mutable_conv()->set_tensor_ops_enabled(\n          profile_result.algorithm().tensor_ops_enabled())",
          "old_line_content": "          profile_result.algorithm().algo_id());",
          "new_line_content": "      result.mutable_conv()->set_tensor_ops_enabled(",
          "content_same": false
        },
        {
          "line": 1226,
          "old_api": "mutable_conv",
          "new_api": "algorithm",
          "old_text": "result.mutable_conv()->set_tensor_ops_enabled(\n          profile_result.algorithm().tensor_ops_enabled())",
          "new_text": "profile_result.algorithm().tensor_ops_enabled()",
          "old_line_content": "      result.mutable_conv()->set_tensor_ops_enabled(",
          "new_line_content": "          profile_result.algorithm().tensor_ops_enabled());",
          "content_same": false
        },
        {
          "line": 1229,
          "old_api": "scratch_size",
          "new_api": "elapsed_time_in_ms",
          "old_text": "profile_result.scratch_size()",
          "new_text": "proto_utils::ToDurationProto(\n          absl::Milliseconds(profile_result.elapsed_time_in_ms()))",
          "old_line_content": "      result.set_scratch_bytes(profile_result.scratch_size());",
          "new_line_content": "      *result.mutable_run_time() = proto_utils::ToDurationProto(",
          "content_same": false
        },
        {
          "line": 1247,
          "old_api": "is_valid",
          "new_api": "emplace_back",
          "old_text": "profile_result.is_valid()",
          "new_text": "results.emplace_back()",
          "old_line_content": "        if (miopen_launch_status && profile_result.is_valid()) {",
          "new_line_content": "          results.emplace_back();",
          "content_same": false
        },
        {
          "line": 1248,
          "old_api": "emplace_back",
          "new_api": "back",
          "old_text": "results.emplace_back()",
          "new_text": "results.back()",
          "old_line_content": "          results.emplace_back();",
          "new_line_content": "          auto& result = results.back();",
          "content_same": false
        },
        {
          "line": 1249,
          "old_api": "back",
          "new_api": "algo_id",
          "old_text": "results.back()",
          "new_text": "profile_algorithm.algo_id()",
          "old_line_content": "          auto& result = results.back();",
          "new_line_content": "          result.mutable_conv()->set_algorithm(profile_algorithm.algo_id());",
          "content_same": false
        },
        {
          "line": 1250,
          "old_api": "algo_id",
          "new_api": "mutable_conv",
          "old_text": "profile_algorithm.algo_id()",
          "new_text": "result.mutable_conv()->set_tensor_ops_enabled(\n              profile_algorithm.tensor_ops_enabled())",
          "old_line_content": "          result.mutable_conv()->set_algorithm(profile_algorithm.algo_id());",
          "new_line_content": "          result.mutable_conv()->set_tensor_ops_enabled(",
          "content_same": false
        },
        {
          "line": 1251,
          "old_api": "mutable_conv",
          "new_api": "tensor_ops_enabled",
          "old_text": "result.mutable_conv()->set_tensor_ops_enabled(\n              profile_algorithm.tensor_ops_enabled())",
          "new_text": "profile_algorithm.tensor_ops_enabled()",
          "old_line_content": "          result.mutable_conv()->set_tensor_ops_enabled(",
          "new_line_content": "              profile_algorithm.tensor_ops_enabled());",
          "content_same": false
        },
        {
          "line": 1252,
          "old_api": "tensor_ops_enabled",
          "new_api": "TotalByteSize",
          "old_text": "profile_algorithm.tensor_ops_enabled()",
          "new_text": "scratch_allocator.TotalByteSize()",
          "old_line_content": "              profile_algorithm.tensor_ops_enabled());",
          "new_line_content": "          result.set_scratch_bytes(scratch_allocator.TotalByteSize());",
          "content_same": false
        },
        {
          "line": 1253,
          "old_api": "TotalByteSize",
          "new_api": "elapsed_time_in_ms",
          "old_text": "scratch_allocator.TotalByteSize()",
          "new_text": "proto_utils::ToDurationProto(\n              absl::Milliseconds(profile_result.elapsed_time_in_ms()))",
          "old_line_content": "          result.set_scratch_bytes(scratch_allocator.TotalByteSize());",
          "new_line_content": "          *result.mutable_run_time() = proto_utils::ToDurationProto(",
          "content_same": false
        },
        {
          "line": 1263,
          "old_api": "parent",
          "new_api": "BestCudnnConvAlgorithm",
          "old_text": "stream->parent()",
          "new_text": "BestCudnnConvAlgorithm(results, &algorithm_config)",
          "old_line_content": "        output_desc, conv_desc, stream->parent(), results);",
          "new_line_content": "    OP_REQUIRES_OK(ctx, BestCudnnConvAlgorithm(results, &algorithm_config));",
          "content_same": false
        },
        {
          "line": 1264,
          "old_api": "BestCudnnConvAlgorithm",
          "new_api": "Insert",
          "old_text": "BestCudnnConvAlgorithm(results, &algorithm_config)",
          "new_text": "AutoTuneConvBwdData::GetInstance()->Insert(conv_parameters,\n                                               algorithm_config)",
          "old_line_content": "    OP_REQUIRES_OK(ctx, BestCudnnConvAlgorithm(results, &algorithm_config));",
          "new_line_content": "    AutoTuneConvBwdData::GetInstance()->Insert(conv_parameters,",
          "content_same": false
        },
        {
          "line": 1289,
          "old_api": "ShapeFromFormat",
          "new_api": "GetTensorDim",
          "old_text": "ShapeFromFormat(compute_data_format,\n                                 GetTensorDim(input_shape, data_format, 'N'),\n                                 GetTensorDim(input_shape, data_format, 'H'),\n                                 GetTensorDim(input_shape, data_format, 'W'),\n                                 GetTensorDim(input_shape, data_format, 'C'))",
          "new_text": "GetTensorDim(input_shape, data_format, 'N')",
          "old_line_content": "                 ShapeFromFormat(compute_data_format,",
          "new_line_content": "                                 GetTensorDim(input_shape, data_format, 'N'),",
          "content_same": false
        },
        {
          "line": 1301,
          "old_api": "functor::PadInput<GPUDevice, T, int, 4>()(\n        ctx->template eigen_device<GPUDevice>(),\n        To32Bit(const_cast<const Tensor&>(pre_transformed_in_backprop)\n                    .tensor<T, 4>()),\n        {{static_cast<int>(-input_pad_top), static_cast<int>(-input_pad_left)}},\n        {{static_cast<int>(-input_pad_bottom),\n          static_cast<int>(-input_pad_right)}},\n        To32Bit(in_backprop_remove_padding.tensor<T, 4>()),\n        compute_data_format)",
          "new_api": "ctx->template eigen_device<GPUDevice>()",
          "old_text": "functor::PadInput<GPUDevice, T, int, 4>()(\n        ctx->template eigen_device<GPUDevice>(),\n        To32Bit(const_cast<const Tensor&>(pre_transformed_in_backprop)\n                    .tensor<T, 4>()),\n        {{static_cast<int>(-input_pad_top), static_cast<int>(-input_pad_left)}},\n        {{static_cast<int>(-input_pad_bottom),\n          static_cast<int>(-input_pad_right)}},\n        To32Bit(in_backprop_remove_padding.tensor<T, 4>()),\n        compute_data_format)",
          "new_text": "ctx->template eigen_device<GPUDevice>()",
          "old_line_content": "    functor::PadInput<GPUDevice, T, int, 4>()(",
          "new_line_content": "        ctx->template eigen_device<GPUDevice>(),",
          "content_same": false
        },
        {
          "line": 1302,
          "old_api": "ctx->template eigen_device<GPUDevice>()",
          "new_api": "const_cast<const Tensor&>(pre_transformed_in_backprop)\n                    .tensor<T, 4>()",
          "old_text": "ctx->template eigen_device<GPUDevice>()",
          "new_text": "const_cast<const Tensor&>(pre_transformed_in_backprop)\n                    .tensor<T, 4>()",
          "old_line_content": "        ctx->template eigen_device<GPUDevice>(),",
          "new_line_content": "        To32Bit(const_cast<const Tensor&>(pre_transformed_in_backprop)",
          "content_same": false
        },
        {
          "line": 1305,
          "old_api": "static_cast<int>(-input_pad_left)",
          "new_api": "static_cast<int>(-input_pad_bottom)",
          "old_text": "static_cast<int>(-input_pad_left)",
          "new_text": "static_cast<int>(-input_pad_bottom)",
          "old_line_content": "        {{static_cast<int>(-input_pad_top), static_cast<int>(-input_pad_left)}},",
          "new_line_content": "        {{static_cast<int>(-input_pad_bottom),",
          "content_same": false
        },
        {
          "line": 1306,
          "old_api": "static_cast<int>(-input_pad_bottom)",
          "new_api": "static_cast<int>(-input_pad_right)",
          "old_text": "static_cast<int>(-input_pad_bottom)",
          "new_text": "static_cast<int>(-input_pad_right)",
          "old_line_content": "        {{static_cast<int>(-input_pad_bottom),",
          "new_line_content": "          static_cast<int>(-input_pad_right)}},",
          "content_same": false
        },
        {
          "line": 1307,
          "old_api": "static_cast<int>(-input_pad_right)",
          "new_api": "in_backprop_remove_padding.tensor<T, 4>()",
          "old_text": "static_cast<int>(-input_pad_right)",
          "new_text": "in_backprop_remove_padding.tensor<T, 4>()",
          "old_line_content": "          static_cast<int>(-input_pad_right)}},",
          "new_line_content": "        To32Bit(in_backprop_remove_padding.tensor<T, 4>()),",
          "content_same": false
        },
        {
          "line": 1317,
          "old_api": "functor::NCHWToNHWC<GPUDevice, T, 4>()(\n        ctx->eigen_device<GPUDevice>(),\n        toConstTensor(pre_transformed_in_backprop).template tensor<T, 4>(),\n        in_backprop->tensor<T, 4>())",
          "new_api": "ctx->eigen_device<GPUDevice>()",
          "old_text": "functor::NCHWToNHWC<GPUDevice, T, 4>()(\n        ctx->eigen_device<GPUDevice>(),\n        toConstTensor(pre_transformed_in_backprop).template tensor<T, 4>(),\n        in_backprop->tensor<T, 4>())",
          "new_text": "ctx->eigen_device<GPUDevice>()",
          "old_line_content": "    functor::NCHWToNHWC<GPUDevice, T, 4>()(",
          "new_line_content": "        ctx->eigen_device<GPUDevice>(),",
          "content_same": false
        },
        {
          "line": 1318,
          "old_api": "ctx->eigen_device<GPUDevice>()",
          "new_api": "toConstTensor",
          "old_text": "ctx->eigen_device<GPUDevice>()",
          "new_text": "toConstTensor(pre_transformed_in_backprop).template tensor<T, 4>()",
          "old_line_content": "        ctx->eigen_device<GPUDevice>(),",
          "new_line_content": "        toConstTensor(pre_transformed_in_backprop).template tensor<T, 4>(),",
          "content_same": false
        },
        {
          "line": 1319,
          "old_api": "toConstTensor",
          "new_api": "in_backprop->tensor<T, 4>()",
          "old_text": "toConstTensor(pre_transformed_in_backprop).template tensor<T, 4>()",
          "new_text": "in_backprop->tensor<T, 4>()",
          "old_line_content": "        toConstTensor(pre_transformed_in_backprop).template tensor<T, 4>(),",
          "new_line_content": "        in_backprop->tensor<T, 4>());",
          "content_same": false
        }
      ],
      "additions": [
        {
          "line": 514,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"strides\", &strides_)",
          "old_line_content": "                    \"Conv2DCustomBackpropInputOp only supports NHWC.\"));",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));",
          "content_same": false
        },
        {
          "line": 1027,
          "old_api": null,
          "new_api": "set_vertical_dilation_rate",
          "old_text": null,
          "new_text": "conv_desc.set_vertical_dilation_rate(dims.spatial_dims[0].dilation)\n      .set_horizontal_dilation_rate(dims.spatial_dims[1].dilation)\n      .set_vertical_filter_stride(dims.spatial_dims[0].stride)\n      .set_horizontal_filter_stride(dims.spatial_dims[1].stride)\n      .set_zero_padding_height(common_padding_rows)\n      .set_zero_padding_width(common_padding_cols)\n      .set_group_count(dims.in_depth / filter_shape.dim_size(2))",
          "old_line_content": "  se::dnn::ConvolutionDescriptor conv_desc;",
          "new_line_content": "  conv_desc.set_vertical_dilation_rate(dims.spatial_dims[0].dilation)",
          "content_same": false
        },
        {
          "line": 518,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(\n        context, (strides_[0] == 1 && strides_[3] == 1),\n        errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\"))",
          "old_line_content": "                                        \"specify 4 dimensions\"));",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 520,
          "old_api": null,
          "new_api": "errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\")",
          "old_text": null,
          "new_text": "errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\")",
          "old_line_content": "        context, (strides_[0] == 1 && strides_[3] == 1),",
          "new_line_content": "        errors::InvalidArgument(\"Current implementation does not yet support \"",
          "content_same": false
        },
        {
          "line": 1033,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "filter_shape.dim_size(2)",
          "old_line_content": "      .set_zero_padding_width(common_padding_cols)",
          "new_line_content": "      .set_group_count(dims.in_depth / filter_shape.dim_size(2));",
          "content_same": false
        },
        {
          "line": 522,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(context, strides_[1] > 0 && strides_[2] > 0,\n                errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\"))",
          "old_line_content": "                                \"strides in the batch and depth dimensions.\"));",
          "new_line_content": "    OP_REQUIRES(context, strides_[1] > 0 && strides_[2] > 0,",
          "content_same": false
        },
        {
          "line": 525,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"padding\", &padding_)",
          "old_line_content": "                    \"Row and column strides should be larger than 0.\"));",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));",
          "content_same": false
        },
        {
          "line": 530,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(context, (dilations_[0] == 1 && dilations_[3] == 1),\n                errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\"))",
          "old_line_content": "                                        \"specify 4 dimensions\"));",
          "new_line_content": "    OP_REQUIRES(context, (dilations_[0] == 1 && dilations_[3] == 1),",
          "content_same": false
        },
        {
          "line": 1042,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "ToString(FORMAT_HWIO)",
          "old_line_content": "  const auto transform_filter = [&](FilterTensorFormat dst_format) -> Status {",
          "new_line_content": "    VLOG(4) << \"Transform filter tensor from \" << ToString(FORMAT_HWIO)",
          "content_same": false
        },
        {
          "line": 535,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(context, (dilations_[1] == 1 && dilations_[2] == 1),\n                errors::InvalidArgument(\n                    \"Current libxsmm and customized CPU implementations do \"\n                    \"not yet support dilation rates larger than 1.\"))",
          "old_line_content": "    // TODO(yangzihao): Add a CPU implementation for dilated convolution.",
          "new_line_content": "    OP_REQUIRES(context, (dilations_[1] == 1 && dilations_[2] == 1),",
          "content_same": false
        },
        {
          "line": 1047,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "filter.dim_size(2)",
          "old_line_content": "        dst_format == FORMAT_OIHW",
          "new_line_content": "            ? TensorShape({filter.dim_size(3), filter.dim_size(2),",
          "content_same": false
        },
        {
          "line": 539,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(context,\n                   context->GetAttr(\"explicit_paddings\", &explicit_paddings_))",
          "old_line_content": "                    \"not yet support dilation rates larger than 1.\"));",
          "new_line_content": "    OP_REQUIRES_OK(context,",
          "content_same": false
        },
        {
          "line": 1052,
          "old_api": null,
          "new_api": "allocate_temp",
          "old_text": null,
          "new_text": "ctx->allocate_temp(DataTypeToEnum<T>::value, dst_shape,\n                                          &transformed_filter)",
          "old_line_content": "",
          "new_line_content": "    TF_RETURN_IF_ERROR(ctx->allocate_temp(DataTypeToEnum<T>::value, dst_shape,",
          "content_same": false
        },
        {
          "line": 1054,
          "old_api": null,
          "new_api": "functor::TransformFilter<GPUDevice, T, int, 4>()(\n        ctx->eigen_device<GPUDevice>(), dst_format,\n        To32Bit(filter.tensor<T, 4>()),\n        To32Bit(transformed_filter.tensor<T, 4>()))",
          "old_text": null,
          "new_text": "functor::TransformFilter<GPUDevice, T, int, 4>()(\n        ctx->eigen_device<GPUDevice>(), dst_format,\n        To32Bit(filter.tensor<T, 4>()),\n        To32Bit(transformed_filter.tensor<T, 4>()))",
          "old_line_content": "                                          &transformed_filter));",
          "new_line_content": "    functor::TransformFilter<GPUDevice, T, int, 4>()(",
          "content_same": false
        },
        {
          "line": 546,
          "old_api": null,
          "new_api": "input",
          "old_text": null,
          "new_text": "context->input(0)",
          "old_line_content": "  void Compute(OpKernelContext* context) override {",
          "new_line_content": "    const Tensor& input_sizes = context->input(0);",
          "content_same": false
        },
        {
          "line": 1059,
          "old_api": null,
          "new_api": "Status::OK()",
          "old_text": null,
          "new_text": "Status::OK()",
          "old_line_content": "",
          "new_line_content": "    return Status::OK();",
          "content_same": false
        },
        {
          "line": 551,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(context,\n                   Conv2DBackpropComputeInputShape(input_sizes, filter.shape(),\n                                                   out_backprop.shape(),\n                                                   data_format_, &input_shape))",
          "old_line_content": "    TensorShape input_shape;",
          "new_line_content": "    OP_REQUIRES_OK(context,",
          "content_same": false
        },
        {
          "line": 1063,
          "old_api": null,
          "new_api": "transform_filter",
          "old_text": null,
          "new_text": "transform_filter(FORMAT_OIHW)",
          "old_line_content": "  if (compute_data_format == FORMAT_NCHW) {",
          "new_line_content": "    OP_REQUIRES_OK(ctx, transform_filter(FORMAT_OIHW));",
          "content_same": false
        },
        {
          "line": 1065,
          "old_api": null,
          "new_api": "transform_filter",
          "old_text": null,
          "new_text": "transform_filter(FORMAT_OHWI)",
          "old_line_content": "  } else if (compute_data_format == FORMAT_NHWC) {",
          "new_line_content": "    OP_REQUIRES_OK(ctx, transform_filter(FORMAT_OHWI));",
          "content_same": false
        },
        {
          "line": 1067,
          "old_api": null,
          "new_api": "errors::InvalidArgument(\"Invalid compute data format: \",\n                                           ToString(compute_data_format))",
          "old_text": null,
          "new_text": "errors::InvalidArgument(\"Invalid compute data format: \",\n                                           ToString(compute_data_format))",
          "old_line_content": "  } else {",
          "new_line_content": "    ctx->SetStatus(errors::InvalidArgument(\"Invalid compute data format: \",",
          "content_same": false
        },
        {
          "line": 557,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(context,\n                   ConvBackpropComputeDimensionsV2(\n                       \"Conv2DCustomBackpropInput\", /*num_spatial_dims=*/2,\n                       input_shape, filter.shape(), out_backprop.shape(),\n                       /*dilations=*/{1, 1, 1, 1}, strides_, padding_,\n                       explicit_paddings_, data_format_, &dims))",
          "old_line_content": "    ConvBackpropDimensions dims;",
          "new_line_content": "    OP_REQUIRES_OK(context,",
          "content_same": false
        },
        {
          "line": 560,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "out_backprop.shape()",
          "old_line_content": "                       \"Conv2DCustomBackpropInput\", /*num_spatial_dims=*/2,",
          "new_line_content": "                       input_shape, filter.shape(), out_backprop.shape(),",
          "content_same": false
        },
        {
          "line": 1074,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(4)",
          "old_line_content": "  if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "new_line_content": "    VLOG(4) << \"Convert the `out_backprop` tensor from NHWC to NCHW.\";",
          "content_same": false
        },
        {
          "line": 565,
          "old_api": null,
          "new_api": "allocate_output",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input_shape, &in_backprop))",
          "old_line_content": "    Tensor* in_backprop = nullptr;",
          "new_line_content": "    OP_REQUIRES_OK(context,",
          "content_same": false
        },
        {
          "line": 1079,
          "old_api": null,
          "new_api": "allocate_temp",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(ctx,\n                     ctx->allocate_temp(DataTypeToEnum<T>::value, compute_shape,\n                                        &transformed_out_backprop))",
          "old_line_content": "    if (dims.out_depth > 1) {",
          "new_line_content": "      OP_REQUIRES_OK(ctx,",
          "content_same": false
        },
        {
          "line": 569,
          "old_api": null,
          "new_api": "num_elements",
          "old_text": null,
          "new_text": "input_shape.num_elements()",
          "old_line_content": "    // If there is nothing to compute, return.",
          "new_line_content": "    if (input_shape.num_elements() == 0) {",
          "content_same": false
        },
        {
          "line": 1082,
          "old_api": null,
          "new_api": "functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          ctx->eigen_device<GPUDevice>(), out_backprop.tensor<T, 4>(),\n          transformed_out_backprop.tensor<T, 4>())",
          "old_text": null,
          "new_text": "functor::NHWCToNCHW<GPUDevice, T, 4>()(\n          ctx->eigen_device<GPUDevice>(), out_backprop.tensor<T, 4>(),\n          transformed_out_backprop.tensor<T, 4>())",
          "old_line_content": "                                        &transformed_out_backprop));",
          "new_line_content": "      functor::NHWCToNCHW<GPUDevice, T, 4>()(",
          "content_same": false
        },
        {
          "line": 1087,
          "old_api": null,
          "new_api": "CopyFrom",
          "old_text": null,
          "new_text": "transformed_out_backprop.CopyFrom(out_backprop, compute_shape)",
          "old_line_content": "      // If depth <= 1, then just reshape.",
          "new_line_content": "      CHECK(transformed_out_backprop.CopyFrom(out_backprop, compute_shape));",
          "content_same": false
        },
        {
          "line": 579,
          "old_api": null,
          "new_api": "OP_REQUIRES_OK",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom))",
          "old_line_content": "    int64 pad_left, pad_right;",
          "new_line_content": "    OP_REQUIRES_OK(",
          "content_same": false
        },
        {
          "line": 581,
          "old_api": null,
          "new_api": "GetWindowedOutputSizeVerbose",
          "old_text": null,
          "new_text": "GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom)",
          "old_line_content": "        context,",
          "new_line_content": "        GetWindowedOutputSizeVerbose(",
          "content_same": false
        },
        {
          "line": 1094,
          "old_api": null,
          "new_api": "allocate_temp",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(\n      ctx, ctx->allocate_temp(\n               DataTypeToEnum<T>::value,\n               ShapeFromFormat(\n                   compute_data_format,\n                   GetTensorDim(compatible_input_shape, data_format, 'N'),\n                   GetTensorDim(compatible_input_shape, data_format, 'H'),\n                   GetTensorDim(compatible_input_shape, data_format, 'W'),\n                   GetTensorDim(compatible_input_shape, data_format, 'C')),\n               &pre_transformed_in_backprop))",
          "old_line_content": "  Tensor pre_transformed_in_backprop;",
          "new_line_content": "  OP_REQUIRES_OK(",
          "content_same": false
        },
        {
          "line": 585,
          "old_api": null,
          "new_api": "OP_REQUIRES_OK",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right))",
          "old_line_content": "            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom));",
          "new_line_content": "    OP_REQUIRES_OK(",
          "content_same": false
        },
        {
          "line": 1097,
          "old_api": null,
          "new_api": "ShapeFromFormat",
          "old_text": null,
          "new_text": "ShapeFromFormat(\n                   compute_data_format,\n                   GetTensorDim(compatible_input_shape, data_format, 'N'),\n                   GetTensorDim(compatible_input_shape, data_format, 'H'),\n                   GetTensorDim(compatible_input_shape, data_format, 'W'),\n                   GetTensorDim(compatible_input_shape, data_format, 'C'))",
          "old_line_content": "               DataTypeToEnum<T>::value,",
          "new_line_content": "               ShapeFromFormat(",
          "content_same": false
        },
        {
          "line": 587,
          "old_api": null,
          "new_api": "GetWindowedOutputSizeVerbose",
          "old_text": null,
          "new_text": "GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right)",
          "old_line_content": "        context,",
          "new_line_content": "        GetWindowedOutputSizeVerbose(",
          "content_same": false
        },
        {
          "line": 1099,
          "old_api": null,
          "new_api": "GetTensorDim",
          "old_text": null,
          "new_text": "GetTensorDim(compatible_input_shape, data_format, 'N')",
          "old_line_content": "                   compute_data_format,",
          "new_line_content": "                   GetTensorDim(compatible_input_shape, data_format, 'N'),",
          "content_same": false
        },
        {
          "line": 593,
          "old_api": null,
          "new_api": "LaunchXsmmBackwardInputConvolution<Device, T>()(\n              context, context->eigen_device<Device>(),\n              in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),\n              out_backprop.tensor<T, 4>(), dims.spatial_dims[0].input_size,\n              dims.spatial_dims[1].input_size,\n              static_cast<int>(dims.spatial_dims[0].stride),\n              static_cast<int>(dims.spatial_dims[1].stride),\n              static_cast<int>(pad_top), static_cast<int>(pad_left),\n              data_format_)",
          "old_text": null,
          "new_text": "LaunchXsmmBackwardInputConvolution<Device, T>()(\n              context, context->eigen_device<Device>(),\n              in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),\n              out_backprop.tensor<T, 4>(), dims.spatial_dims[0].input_size,\n              dims.spatial_dims[1].input_size,\n              static_cast<int>(dims.spatial_dims[0].stride),\n              static_cast<int>(dims.spatial_dims[1].stride),\n              static_cast<int>(pad_top), static_cast<int>(pad_left),\n              data_format_)",
          "old_line_content": "    if (pad_left == pad_right && pad_top == pad_bottom) {",
          "new_line_content": "      if (LaunchXsmmBackwardInputConvolution<Device, T>()(",
          "content_same": false
        },
        {
          "line": 1106,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "transformed_out_backprop.template flat<T>().data()",
          "old_line_content": "  auto out_backprop_ptr =",
          "new_line_content": "      AsDeviceMemory(transformed_out_backprop.template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 1109,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "transformed_filter.template flat<T>().data()",
          "old_line_content": "  auto filter_ptr =",
          "new_line_content": "      AsDeviceMemory(transformed_filter.template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 598,
          "old_api": null,
          "new_api": "static_cast<int>(dims.spatial_dims[0].stride)",
          "old_text": null,
          "new_text": "static_cast<int>(dims.spatial_dims[0].stride)",
          "old_line_content": "              dims.spatial_dims[1].input_size,",
          "new_line_content": "              static_cast<int>(dims.spatial_dims[0].stride),",
          "content_same": false
        },
        {
          "line": 1112,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "pre_transformed_in_backprop.template flat<T>().data()",
          "old_line_content": "  auto in_backprop_ptr =",
          "new_line_content": "      AsDeviceMemory(pre_transformed_in_backprop.template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 1115,
          "old_api": null,
          "new_api": "GetDnnWorkspaceLimit",
          "old_text": null,
          "new_text": "GetDnnWorkspaceLimit(\n      \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default\n  )",
          "old_line_content": "",
          "new_line_content": "  static int64 ConvolveBackwardDataScratchSize = GetDnnWorkspaceLimit(",
          "content_same": false
        },
        {
          "line": 1119,
          "old_api": null,
          "new_api": "parent",
          "old_text": null,
          "new_text": "stream->parent()->device_ordinal()",
          "old_line_content": "  DnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize, ctx);",
          "new_line_content": "  int device_id = stream->parent()->device_ordinal();",
          "content_same": false
        },
        {
          "line": 1124,
          "old_api": null,
          "new_api": "height",
          "old_text": null,
          "new_text": "input_desc.height()",
          "old_line_content": "      dims.in_depth,                       // in_depths",
          "new_line_content": "      {{input_desc.height(),               // in_rows",
          "content_same": false
        },
        {
          "line": 615,
          "old_api": null,
          "new_api": "OP_REQUIRES_OK",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom))",
          "old_line_content": "    }",
          "new_line_content": "    OP_REQUIRES_OK(",
          "content_same": false
        },
        {
          "line": 617,
          "old_api": null,
          "new_api": "GetWindowedOutputSizeVerbose",
          "old_text": null,
          "new_text": "GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom)",
          "old_line_content": "        context,",
          "new_line_content": "        GetWindowedOutputSizeVerbose(",
          "content_same": false
        },
        {
          "line": 1130,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "filter_shape.dim_size(2)",
          "old_line_content": "        dims.spatial_dims[1].filter_size,  // filter_cols",
          "new_line_content": "        filter_shape.dim_size(2)}},        // filter_depths",
          "content_same": false
        },
        {
          "line": 621,
          "old_api": null,
          "new_api": "OP_REQUIRES_OK",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right))",
          "old_line_content": "            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom));",
          "new_line_content": "    OP_REQUIRES_OK(",
          "content_same": false
        },
        {
          "line": 623,
          "old_api": null,
          "new_api": "GetWindowedOutputSizeVerbose",
          "old_text": null,
          "new_text": "GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right)",
          "old_line_content": "        context,",
          "new_line_content": "        GetWindowedOutputSizeVerbose(",
          "content_same": false
        },
        {
          "line": 1139,
          "old_api": null,
          "new_api": "group_count",
          "old_text": null,
          "new_text": "conv_desc.group_count()",
          "old_line_content": "      device_id,                           // device_id",
          "new_line_content": "      conv_desc.group_count()              // group_count",
          "content_same": false
        },
        {
          "line": 119,
          "old_api": null,
          "new_api": "GetTensorDimIndex",
          "old_text": null,
          "new_text": "GetTensorDimIndex(data_format, 'H')",
          "old_line_content": "",
          "new_line_content": "    auto input_h = GetTensorDimIndex(data_format, 'H');",
          "content_same": false
        },
        {
          "line": 1148,
          "old_api": null,
          "new_api": "Find",
          "old_text": null,
          "new_text": "AutoTuneConvBwdData::GetInstance()->Find(\n                                conv_parameters, &algorithm_config)",
          "old_line_content": "  AlgorithmConfig algorithm_config;",
          "new_line_content": "  if (cudnn_use_autotune && !AutoTuneConvBwdData::GetInstance()->Find(",
          "content_same": false
        },
        {
          "line": 126,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_backprop->shape()",
          "old_line_content": "",
          "new_line_content": "    const TensorShape& input_shape = in_backprop->shape();",
          "content_same": false
        },
        {
          "line": 1152,
          "old_api": null,
          "new_api": "device",
          "old_text": null,
          "new_text": "ctx->device()->GetAllocator({})",
          "old_line_content": "",
          "new_line_content": "    se::TfAllocatorAdapter tf_allocator_adapter(ctx->device()->GetAllocator({}),",
          "content_same": false
        },
        {
          "line": 130,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(\n        ctx, ConvBackpropComputeDimensionsV2(\n                 \"Conv2DBackpropInput\", /*num_spatial_dims=*/2, input_shape,\n                 filter_shape, out_backprop.shape(), dilations, strides,\n                 padding, explicit_paddings, data_format, &dims))",
          "old_line_content": "    ConvBackpropDimensions dims;",
          "new_line_content": "    OP_REQUIRES_OK(",
          "content_same": false
        },
        {
          "line": 1156,
          "old_api": null,
          "new_api": "se::GpuAsmOpts()",
          "old_text": null,
          "new_text": "se::GpuAsmOpts()",
          "old_line_content": "    se::RedzoneAllocator rz_allocator(stream, &tf_allocator_adapter,",
          "new_line_content": "                                      se::GpuAsmOpts());",
          "content_same": false
        },
        {
          "line": 133,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "out_backprop.shape()",
          "old_line_content": "                 \"Conv2DBackpropInput\", /*num_spatial_dims=*/2, input_shape,",
          "new_line_content": "                 filter_shape, out_backprop.shape(), dilations, strides,",
          "content_same": false
        },
        {
          "line": 1159,
          "old_api": null,
          "new_api": "WrapRedzoneBestEffort",
          "old_text": null,
          "new_text": "WrapRedzoneBestEffort(&rz_allocator, in_backprop_ptr)",
          "old_line_content": "    se::DeviceMemory<T> in_backprop_ptr_rz(",
          "new_line_content": "        WrapRedzoneBestEffort(&rz_allocator, in_backprop_ptr));",
          "content_same": false
        },
        {
          "line": 1162,
          "old_api": null,
          "new_api": "parent",
          "old_text": null,
          "new_text": "stream->parent()->GetConvolveBackwardDataAlgorithms(\n        conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(stream->parent()),\n        &algorithms)",
          "old_line_content": "    std::vector<AlgorithmDesc> algorithms;",
          "new_line_content": "    CHECK(stream->parent()->GetConvolveBackwardDataAlgorithms(",
          "content_same": false
        },
        {
          "line": 139,
          "old_api": null,
          "new_api": "GetExplicitPaddingForDim",
          "old_text": null,
          "new_text": "GetExplicitPaddingForDim(explicit_paddings, data_format, 'H',\n                               &padding_top, &padding_bottom)",
          "old_line_content": "    if (padding == EXPLICIT) {",
          "new_line_content": "      GetExplicitPaddingForDim(explicit_paddings, data_format, 'H',",
          "content_same": false
        },
        {
          "line": 652,
          "old_api": null,
          "new_api": "device",
          "old_text": null,
          "new_text": "context->device()->tensorflow_cpu_worker_threads()",
          "old_line_content": "",
          "new_line_content": "    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());",
          "content_same": false
        },
        {
          "line": 141,
          "old_api": null,
          "new_api": "GetExplicitPaddingForDim",
          "old_text": null,
          "new_text": "GetExplicitPaddingForDim(explicit_paddings, data_format, 'W',\n                               &padding_left, &padding_right)",
          "old_line_content": "                               &padding_top, &padding_bottom);",
          "new_line_content": "      GetExplicitPaddingForDim(explicit_paddings, data_format, 'W',",
          "content_same": false
        },
        {
          "line": 148,
          "old_api": null,
          "new_api": "GetWindowedOutputSizeVerboseV2",
          "old_text": null,
          "new_text": "GetWindowedOutputSizeVerboseV2(\n        dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n        row_dilation, row_stride, padding, &expected_out_rows, &padding_top,\n        &padding_bottom)",
          "old_line_content": "    // padding was valid earlier.",
          "new_line_content": "    TF_CHECK_OK(GetWindowedOutputSizeVerboseV2(",
          "content_same": false
        },
        {
          "line": 1172,
          "old_api": null,
          "new_api": "se::GpuAsmOpts()",
          "old_text": null,
          "new_text": "se::GpuAsmOpts()",
          "old_line_content": "      se::RedzoneAllocator rz_scratch_allocator(",
          "new_line_content": "          stream, &tf_allocator_adapter, se::GpuAsmOpts(),",
          "content_same": false
        },
        {
          "line": 1175,
          "old_api": null,
          "new_api": "RedzoneCheckDisabled",
          "old_text": null,
          "new_text": "RedzoneCheckDisabled()",
          "old_line_content": "      se::ScratchAllocator* allocator_used =",
          "new_line_content": "          !RedzoneCheckDisabled()",
          "content_same": false
        },
        {
          "line": 152,
          "old_api": null,
          "new_api": "DCHECK_EQ",
          "old_text": null,
          "new_text": "DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows)",
          "old_line_content": "        &padding_bottom));",
          "new_line_content": "    DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows);",
          "content_same": false
        },
        {
          "line": 154,
          "old_api": null,
          "new_api": "GetWindowedOutputSizeVerboseV2",
          "old_text": null,
          "new_text": "GetWindowedOutputSizeVerboseV2(\n        dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n        col_dilation, col_stride, padding, &expected_out_cols, &padding_left,\n        &padding_right)",
          "old_line_content": "",
          "new_line_content": "    TF_CHECK_OK(GetWindowedOutputSizeVerboseV2(",
          "content_same": false
        },
        {
          "line": 1180,
          "old_api": null,
          "new_api": "ok",
          "old_text": null,
          "new_text": "stream\n              ->ThenConvolveBackwardDataWithAlgorithm(\n                  filter_desc, filter_ptr, output_desc, out_backprop_ptr,\n                  conv_desc, input_desc, &in_backprop_ptr_rz, allocator_used,\n                  AlgorithmConfig(profile_algorithm), &profile_result)\n              .ok()",
          "old_line_content": "      bool cudnn_launch_status =",
          "new_line_content": "          stream",
          "content_same": false
        },
        {
          "line": 158,
          "old_api": null,
          "new_api": "DCHECK_EQ",
          "old_text": null,
          "new_text": "DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols)",
          "old_line_content": "        &padding_right));",
          "new_line_content": "    DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols);",
          "content_same": false
        },
        {
          "line": 1184,
          "old_api": null,
          "new_api": "AlgorithmConfig",
          "old_text": null,
          "new_text": "AlgorithmConfig(profile_algorithm)",
          "old_line_content": "                  conv_desc, input_desc, &in_backprop_ptr_rz, allocator_used,",
          "new_line_content": "                  AlgorithmConfig(profile_algorithm), &profile_result)",
          "content_same": false
        },
        {
          "line": 1186,
          "old_api": null,
          "new_api": "is_valid",
          "old_text": null,
          "new_text": "profile_result.is_valid()",
          "old_line_content": "              .ok();",
          "new_line_content": "      if (cudnn_launch_status && profile_result.is_valid()) {",
          "content_same": false
        },
        {
          "line": 677,
          "old_api": null,
          "new_api": "allocate_temp",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(context,\n                   context->allocate_temp(\n                       DataTypeToEnum<T>::value,\n                       TensorShape({static_cast<int64>(shard_size),\n                                    static_cast<int64>(output_image_size),\n                                    static_cast<int64>(filter_total_size)}),\n                       &col_buffer))",
          "old_line_content": "    Tensor col_buffer;",
          "new_line_content": "    OP_REQUIRES_OK(context,",
          "content_same": false
        },
        {
          "line": 680,
          "old_api": null,
          "new_api": "static_cast<int64>(shard_size)",
          "old_text": null,
          "new_text": "static_cast<int64>(shard_size)",
          "old_line_content": "                       DataTypeToEnum<T>::value,",
          "new_line_content": "                       TensorShape({static_cast<int64>(shard_size),",
          "content_same": false
        },
        {
          "line": 169,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "in_backprop->dim_size(0)",
          "old_line_content": "",
          "new_line_content": "      REQUIRES_32BIT(in_backprop->dim_size(0));",
          "content_same": false
        },
        {
          "line": 1199,
          "old_api": null,
          "new_api": "CheckRedzones",
          "old_text": null,
          "new_text": "CheckRedzones(rz_scratch_allocator, &result)",
          "old_line_content": "",
          "new_line_content": "        CheckRedzones(rz_scratch_allocator, &result);",
          "content_same": false
        },
        {
          "line": 176,
          "old_api": null,
          "new_api": "in_backprop->tensor<T, 4>()",
          "old_text": null,
          "new_text": "in_backprop->tensor<T, 4>()",
          "old_line_content": "",
          "new_line_content": "    auto in_backprop_t = in_backprop->tensor<T, 4>();",
          "content_same": false
        },
        {
          "line": 692,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "filter.template flat<T>().data()",
          "old_line_content": "",
          "new_line_content": "    const T* filter_data = filter.template flat<T>().data();",
          "content_same": false
        },
        {
          "line": 1206,
          "old_api": null,
          "new_api": "parent",
          "old_text": null,
          "new_text": "OP_REQUIRES(\n        ctx,\n        stream->parent()->GetMIOpenConvolveAlgorithms(\n            se::dnn::ConvolutionKind::BACKWARD_DATA,\n            se::dnn::ToDataType<T>::value, stream, input_desc, in_backprop_ptr,\n            filter_desc, filter_ptr, output_desc, out_backprop_ptr, conv_desc,\n            &scratch_allocator, &algorithms),\n        errors::Unknown(\n            \"Failed to get convolution algorithm. This is probably \"\n            \"because MIOpen failed to initialize, so try looking to \"\n            \"see if a warning log message was printed above.\"))",
          "old_line_content": "    std::vector<ProfileResult> algorithms;",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 696,
          "old_api": null,
          "new_api": "in_backprop->template flat<T>()",
          "old_text": null,
          "new_text": "in_backprop->template flat<T>()",
          "old_line_content": "",
          "new_line_content": "    auto in_backprop_flat = in_backprop->template flat<T>();",
          "content_same": false
        },
        {
          "line": 1208,
          "old_api": null,
          "new_api": "parent",
          "old_text": null,
          "new_text": "stream->parent()->GetMIOpenConvolveAlgorithms(\n            se::dnn::ConvolutionKind::BACKWARD_DATA,\n            se::dnn::ToDataType<T>::value, stream, input_desc, in_backprop_ptr,\n            filter_desc, filter_ptr, output_desc, out_backprop_ptr, conv_desc,\n            &scratch_allocator, &algorithms)",
          "old_line_content": "        ctx,",
          "new_line_content": "        stream->parent()->GetMIOpenConvolveAlgorithms(",
          "content_same": false
        },
        {
          "line": 187,
          "old_api": null,
          "new_api": "functor::SpatialConvolutionBackwardInputFunc<Device, T>()(\n          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,\n          col_stride, row_stride, col_dilation, row_dilation)",
          "old_text": null,
          "new_text": "functor::SpatialConvolutionBackwardInputFunc<Device, T>()(\n          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,\n          col_stride, row_stride, col_dilation, row_dilation)",
          "old_line_content": "      // backward input will infer correct forward paddings from input tensors.",
          "new_line_content": "      functor::SpatialConvolutionBackwardInputFunc<Device, T>()(",
          "content_same": false
        },
        {
          "line": 1213,
          "old_api": null,
          "new_api": "errors::Unknown(\n            \"Failed to get convolution algorithm. This is probably \"\n            \"because MIOpen failed to initialize, so try looking to \"\n            \"see if a warning log message was printed above.\")",
          "old_text": null,
          "new_text": "errors::Unknown(\n            \"Failed to get convolution algorithm. This is probably \"\n            \"because MIOpen failed to initialize, so try looking to \"\n            \"see if a warning log message was printed above.\")",
          "old_line_content": "            &scratch_allocator, &algorithms),",
          "new_line_content": "        errors::Unknown(",
          "content_same": false
        },
        {
          "line": 191,
          "old_api": null,
          "new_api": "dimension",
          "old_text": null,
          "new_text": "functor::SpatialConvolutionBackwardInputWithExplicitPaddingFunc<Device,\n                                                                      T>()(\n          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,\n          in_backprop_t.dimension(2) + (padding_left + padding_right),\n          in_backprop_t.dimension(1) + (padding_top + padding_bottom),\n          col_stride, row_stride, col_dilation, row_dilation, padding_top,\n          padding_left)",
          "old_line_content": "    } else {",
          "new_line_content": "      functor::SpatialConvolutionBackwardInputWithExplicitPaddingFunc<Device,",
          "content_same": false
        },
        {
          "line": 193,
          "old_api": null,
          "new_api": "ctx->eigen_device<Device>()",
          "old_text": null,
          "new_text": "ctx->eigen_device<Device>()",
          "old_line_content": "                                                                      T>()(",
          "new_line_content": "          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,",
          "content_same": false
        },
        {
          "line": 1219,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "algorithms.size()",
          "old_line_content": "    std::vector<tensorflow::AutotuneResult> results;",
          "new_line_content": "    if (algorithms.size() == 1) {",
          "content_same": false
        },
        {
          "line": 1221,
          "old_api": null,
          "new_api": "emplace_back",
          "old_text": null,
          "new_text": "results.emplace_back()",
          "old_line_content": "      auto profile_result = algorithms[0];",
          "new_line_content": "      results.emplace_back();",
          "content_same": false
        },
        {
          "line": 1228,
          "old_api": null,
          "new_api": "scratch_size",
          "old_text": null,
          "new_text": "profile_result.scratch_size()",
          "old_line_content": "",
          "new_line_content": "      result.set_scratch_bytes(profile_result.scratch_size());",
          "content_same": false
        },
        {
          "line": 1233,
          "old_api": null,
          "new_api": "algorithm",
          "old_text": null,
          "new_text": "miopen_algorithm.algorithm()",
          "old_line_content": "      for (auto miopen_algorithm : algorithms) {",
          "new_line_content": "        auto profile_algorithm = miopen_algorithm.algorithm();",
          "content_same": false
        },
        {
          "line": 722,
          "old_api": null,
          "new_api": "contract",
          "old_text": null,
          "new_text": "A.contract(B, contract_dims)",
          "old_line_content": "",
          "new_line_content": "        C.device(context->eigen_cpu_device()) = A.contract(B, contract_dims);",
          "content_same": false
        },
        {
          "line": 724,
          "old_api": null,
          "new_api": "Col2im<T>(\n            col_buffer_data, dims.in_depth, dims.spatial_dims[0].input_size,\n            dims.spatial_dims[1].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[1].filter_size, pad_top, pad_left, pad_bottom,\n            pad_right, dims.spatial_dims[0].stride, dims.spatial_dims[1].stride,\n            input_backprop_data)",
          "old_text": null,
          "new_text": "Col2im<T>(\n            col_buffer_data, dims.in_depth, dims.spatial_dims[0].input_size,\n            dims.spatial_dims[1].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[1].filter_size, pad_top, pad_left, pad_bottom,\n            pad_right, dims.spatial_dims[0].stride, dims.spatial_dims[1].stride,\n            input_backprop_data)",
          "old_line_content": "",
          "new_line_content": "        Col2im<T>(",
          "content_same": false
        },
        {
          "line": 1237,
          "old_api": null,
          "new_api": "scratch_size",
          "old_text": null,
          "new_text": "stream\n                ->ThenConvolveBackwardDataWithAlgorithm(\n                    filter_desc, filter_ptr, output_desc, out_backprop_ptr,\n                    conv_desc, input_desc, &in_backprop_ptr, &scratch_allocator,\n                    AlgorithmConfig(profile_algorithm,\n                                    miopen_algorithm.scratch_size()),\n                    &profile_result)\n                .ok()",
          "old_line_content": "        miopen_launch_status =",
          "new_line_content": "            stream",
          "content_same": false
        },
        {
          "line": 214,
          "old_api": null,
          "new_api": "launcher",
          "old_text": null,
          "new_text": "launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,\n             row_dilation, col_dilation, row_stride, col_stride, padding,\n             explicit_paddings, in_backprop, data_format)",
          "old_line_content": "    LaunchConv2DBackpropInputOpImpl<CPUDevice, T> launcher;",
          "new_line_content": "    launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,",
          "content_same": false
        },
        {
          "line": 1241,
          "old_api": null,
          "new_api": "scratch_size",
          "old_text": null,
          "new_text": "AlgorithmConfig(profile_algorithm,\n                                    miopen_algorithm.scratch_size())",
          "old_line_content": "                    conv_desc, input_desc, &in_backprop_ptr, &scratch_allocator,",
          "new_line_content": "                    AlgorithmConfig(profile_algorithm,",
          "content_same": false
        },
        {
          "line": 1246,
          "old_api": null,
          "new_api": "is_valid",
          "old_text": null,
          "new_text": "profile_result.is_valid()",
          "old_line_content": "",
          "new_line_content": "        if (miopen_launch_status && profile_result.is_valid()) {",
          "content_same": false
        },
        {
          "line": 737,
          "old_api": null,
          "new_api": "static_cast<int>(shard_size)",
          "old_text": null,
          "new_text": "static_cast<int>(shard_size)",
          "old_line_content": "        const int shard_limit =",
          "new_line_content": "            std::min(static_cast<int>(shard_size),",
          "content_same": false
        },
        {
          "line": 232,
          "old_api": null,
          "new_api": "launcher",
          "old_text": null,
          "new_text": "launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,\n             row_dilation, col_dilation, row_stride, col_stride, padding,\n             explicit_paddings, in_backprop, data_format)",
          "old_line_content": "    LaunchConv2DBackpropInputOpImpl<GPUDevice, int32> launcher;",
          "new_line_content": "    launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,",
          "content_same": false
        },
        {
          "line": 1259,
          "old_api": null,
          "new_api": "parent",
          "old_text": null,
          "new_text": "LogConvAutotuneResults(\n        se::dnn::ConvolutionKind::BACKWARD_DATA, se::dnn::ToDataType<T>::value,\n        in_backprop_ptr, filter_ptr, out_backprop_ptr, input_desc, filter_desc,\n        output_desc, conv_desc, stream->parent(), results)",
          "old_line_content": "#endif",
          "new_line_content": "    LogConvAutotuneResults(",
          "content_same": false
        },
        {
          "line": 750,
          "old_api": null,
          "new_api": "Conv2DCustomBackpropInputMatMulFunctor<T>()(\n                context, out_data, filter_data, filter_total_size,\n                output_image_size, dims.out_depth, im2col_buf)",
          "old_text": null,
          "new_text": "Conv2DCustomBackpropInputMatMulFunctor<T>()(\n                context, out_data, filter_data, filter_total_size,\n                output_image_size, dims.out_depth, im2col_buf)",
          "old_line_content": "",
          "new_line_content": "            Conv2DCustomBackpropInputMatMulFunctor<T>()(",
          "content_same": false
        },
        {
          "line": 1262,
          "old_api": null,
          "new_api": "parent",
          "old_text": null,
          "new_text": "stream->parent()",
          "old_line_content": "        in_backprop_ptr, filter_ptr, out_backprop_ptr, input_desc, filter_desc,",
          "new_line_content": "        output_desc, conv_desc, stream->parent(), results);",
          "content_same": false
        },
        {
          "line": 754,
          "old_api": null,
          "new_api": "Col2im<T>(im2col_buf, dims.in_depth,\n                      dims.spatial_dims[0].input_size,\n                      dims.spatial_dims[1].input_size,\n                      dims.spatial_dims[0].filter_size,\n                      dims.spatial_dims[1].filter_size, pad_top, pad_left,\n                      pad_bottom, pad_right, dims.spatial_dims[0].stride,\n                      dims.spatial_dims[1].stride, input_data)",
          "old_text": null,
          "new_text": "Col2im<T>(im2col_buf, dims.in_depth,\n                      dims.spatial_dims[0].input_size,\n                      dims.spatial_dims[1].input_size,\n                      dims.spatial_dims[0].filter_size,\n                      dims.spatial_dims[1].filter_size, pad_top, pad_left,\n                      pad_bottom, pad_right, dims.spatial_dims[0].stride,\n                      dims.spatial_dims[1].stride, input_data)",
          "old_line_content": "",
          "new_line_content": "            Col2im<T>(im2col_buf, dims.in_depth,",
          "content_same": false
        },
        {
          "line": 1268,
          "old_api": null,
          "new_api": "ok",
          "old_text": null,
          "new_text": "stream\n          ->ThenConvolveBackwardDataWithAlgorithm(\n              filter_desc, filter_ptr, output_desc, out_backprop_ptr, conv_desc,\n              input_desc, &in_backprop_ptr, &scratch_allocator,\n              algorithm_config, nullptr)\n          .ok()",
          "old_line_content": "  bool cudnn_launch_status =",
          "new_line_content": "      stream",
          "content_same": false
        },
        {
          "line": 763,
          "old_api": null,
          "new_api": "Shard",
          "old_text": null,
          "new_text": "Shard(worker_threads.num_threads, worker_threads.workers, shard_limit,\n              work_unit_size, shard)",
          "old_line_content": "        };",
          "new_line_content": "        Shard(worker_threads.num_threads, worker_threads.workers, shard_limit,",
          "content_same": false
        },
        {
          "line": 1276,
          "old_api": null,
          "new_api": "DebugString",
          "old_text": null,
          "new_text": "errors::Internal(\n        \"DNN Backward Data function launch failure : input shape(\",\n        input_shape.DebugString(), \") filter shape(\",\n        filter_shape.DebugString(), \")\")",
          "old_line_content": "  if (!cudnn_launch_status) {",
          "new_line_content": "    ctx->SetStatus(errors::Internal(",
          "content_same": false
        },
        {
          "line": 1278,
          "old_api": null,
          "new_api": "DebugString",
          "old_text": null,
          "new_text": "input_shape.DebugString()",
          "old_line_content": "        \"DNN Backward Data function launch failure : input shape(\",",
          "new_line_content": "        input_shape.DebugString(), \") filter shape(\",",
          "content_same": false
        },
        {
          "line": 1285,
          "old_api": null,
          "new_api": "allocate_temp",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(\n                 DataTypeToEnum<T>::value,\n                 ShapeFromFormat(compute_data_format,\n                                 GetTensorDim(input_shape, data_format, 'N'),\n                                 GetTensorDim(input_shape, data_format, 'H'),\n                                 GetTensorDim(input_shape, data_format, 'W'),\n                                 GetTensorDim(input_shape, data_format, 'C')),\n                 &in_backprop_remove_padding))",
          "old_line_content": "    Tensor in_backprop_remove_padding;",
          "new_line_content": "    OP_REQUIRES_OK(",
          "content_same": false
        },
        {
          "line": 262,
          "old_api": null,
          "new_api": "dimension",
          "old_text": null,
          "new_text": "input_backward.dimension(0)",
          "old_line_content": "                  TensorFormat data_format) const {",
          "new_line_content": "    auto batch = input_backward.dimension(0);",
          "content_same": false
        },
        {
          "line": 1288,
          "old_api": null,
          "new_api": "ShapeFromFormat",
          "old_text": null,
          "new_text": "ShapeFromFormat(compute_data_format,\n                                 GetTensorDim(input_shape, data_format, 'N'),\n                                 GetTensorDim(input_shape, data_format, 'H'),\n                                 GetTensorDim(input_shape, data_format, 'W'),\n                                 GetTensorDim(input_shape, data_format, 'C'))",
          "old_line_content": "                 DataTypeToEnum<T>::value,",
          "new_line_content": "                 ShapeFromFormat(compute_data_format,",
          "content_same": false
        },
        {
          "line": 268,
          "old_api": null,
          "new_api": "device",
          "old_text": null,
          "new_text": "context->device()->tensorflow_cpu_worker_threads()",
          "old_line_content": "    auto num_threads =",
          "new_line_content": "        context->device()->tensorflow_cpu_worker_threads()->num_threads;",
          "content_same": false
        },
        {
          "line": 1300,
          "old_api": null,
          "new_api": "functor::PadInput<GPUDevice, T, int, 4>()(\n        ctx->template eigen_device<GPUDevice>(),\n        To32Bit(const_cast<const Tensor&>(pre_transformed_in_backprop)\n                    .tensor<T, 4>()),\n        {{static_cast<int>(-input_pad_top), static_cast<int>(-input_pad_left)}},\n        {{static_cast<int>(-input_pad_bottom),\n          static_cast<int>(-input_pad_right)}},\n        To32Bit(in_backprop_remove_padding.tensor<T, 4>()),\n        compute_data_format)",
          "old_text": null,
          "new_text": "functor::PadInput<GPUDevice, T, int, 4>()(\n        ctx->template eigen_device<GPUDevice>(),\n        To32Bit(const_cast<const Tensor&>(pre_transformed_in_backprop)\n                    .tensor<T, 4>()),\n        {{static_cast<int>(-input_pad_top), static_cast<int>(-input_pad_left)}},\n        {{static_cast<int>(-input_pad_bottom),\n          static_cast<int>(-input_pad_right)}},\n        To32Bit(in_backprop_remove_padding.tensor<T, 4>()),\n        compute_data_format)",
          "old_line_content": "    const int64 input_pad_right = padding_right - common_padding_cols;",
          "new_line_content": "    functor::PadInput<GPUDevice, T, int, 4>()(",
          "content_same": false
        },
        {
          "line": 1304,
          "old_api": null,
          "new_api": "static_cast<int>(-input_pad_left)",
          "old_text": null,
          "new_text": "static_cast<int>(-input_pad_left)",
          "old_line_content": "                    .tensor<T, 4>()),",
          "new_line_content": "        {{static_cast<int>(-input_pad_top), static_cast<int>(-input_pad_left)}},",
          "content_same": false
        },
        {
          "line": 800,
          "old_api": null,
          "new_api": "TF_CALL_half",
          "old_text": null,
          "new_text": "TF_CALL_half(REGISTER_CPU_KERNELS)",
          "old_line_content": "",
          "new_line_content": "TF_CALL_half(REGISTER_CPU_KERNELS);",
          "content_same": false
        },
        {
          "line": 1314,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(4)",
          "old_line_content": "  if (data_format == FORMAT_NHWC && compute_data_format == FORMAT_NCHW) {",
          "new_line_content": "    VLOG(4) << \"Convert the output tensor back from NCHW to NHWC.\";",
          "content_same": false
        },
        {
          "line": 1316,
          "old_api": null,
          "new_api": "functor::NCHWToNHWC<GPUDevice, T, 4>()(\n        ctx->eigen_device<GPUDevice>(),\n        toConstTensor(pre_transformed_in_backprop).template tensor<T, 4>(),\n        in_backprop->tensor<T, 4>())",
          "old_text": null,
          "new_text": "functor::NCHWToNHWC<GPUDevice, T, 4>()(\n        ctx->eigen_device<GPUDevice>(),\n        toConstTensor(pre_transformed_in_backprop).template tensor<T, 4>(),\n        in_backprop->tensor<T, 4>())",
          "old_line_content": "    auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };",
          "new_line_content": "    functor::NCHWToNHWC<GPUDevice, T, 4>()(",
          "content_same": false
        },
        {
          "line": 295,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "input_backward.data()",
          "old_line_content": "    desc.datatype_in = LIBXSMM_DNN_DATATYPE_F32;",
          "new_line_content": "    auto input_ptr = input_backward.data();",
          "content_same": false
        },
        {
          "line": 299,
          "old_api": null,
          "new_api": "functor::XsmmBkwInputConv2D<CPUDevice, float>()(\n        context, desc, input_ptr, filter_ptr, output_ptr)",
          "old_text": null,
          "new_text": "functor::XsmmBkwInputConv2D<CPUDevice, float>()(\n        context, desc, input_ptr, filter_ptr, output_ptr)",
          "old_line_content": "",
          "new_line_content": "    bool success = functor::XsmmBkwInputConv2D<CPUDevice, float>()(",
          "content_same": false
        },
        {
          "line": 1343,
          "old_api": null,
          "new_api": "DECLARE_GPU_SPEC",
          "old_text": null,
          "new_text": "DECLARE_GPU_SPEC(Eigen::half)",
          "old_line_content": "DECLARE_GPU_SPEC(float);",
          "new_line_content": "DECLARE_GPU_SPEC(Eigen::half);",
          "content_same": false
        },
        {
          "line": 322,
          "old_api": null,
          "new_api": "transpose",
          "old_text": null,
          "new_text": "B.transpose()",
          "old_line_content": "",
          "new_line_content": "    C.noalias() = A * B.transpose();",
          "content_same": false
        },
        {
          "line": 837,
          "old_api": null,
          "new_api": "GetTensorDimIndex",
          "old_text": null,
          "new_text": "GetTensorDimIndex(data_format, 'H')",
          "old_line_content": "  std::vector<int32> dilations(4, 1);",
          "new_line_content": "  auto input_h = GetTensorDimIndex(data_format, 'H');",
          "content_same": false
        },
        {
          "line": 843,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_backprop->shape()",
          "old_line_content": "  dilations[input_w] = col_dilation;",
          "new_line_content": "  TensorShape input_shape = in_backprop->shape();",
          "content_same": false
        },
        {
          "line": 845,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "filter.shape()",
          "old_line_content": "",
          "new_line_content": "  const TensorShape& filter_shape = filter.shape();",
          "content_same": false
        },
        {
          "line": 847,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(\n      ctx, ConvBackpropComputeDimensionsV2(\n               \"Conv2DSlowBackpropInput\", /*num_spatial_dims=*/2, input_shape,\n               filter_shape, out_backprop.shape(), dilations, strides, padding,\n               explicit_paddings, data_format, &dims))",
          "old_line_content": "  ConvBackpropDimensions dims;",
          "new_line_content": "  OP_REQUIRES_OK(",
          "content_same": false
        },
        {
          "line": 850,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "out_backprop.shape()",
          "old_line_content": "               \"Conv2DSlowBackpropInput\", /*num_spatial_dims=*/2, input_shape,",
          "new_line_content": "               filter_shape, out_backprop.shape(), dilations, strides, padding,",
          "content_same": false
        },
        {
          "line": 856,
          "old_api": null,
          "new_api": "GetExplicitPaddingForDim",
          "old_text": null,
          "new_text": "GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &padding_top,\n                             &padding_bottom)",
          "old_line_content": "  if (padding == EXPLICIT) {",
          "new_line_content": "    GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &padding_top,",
          "content_same": false
        },
        {
          "line": 858,
          "old_api": null,
          "new_api": "GetExplicitPaddingForDim",
          "old_text": null,
          "new_text": "GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &padding_left,\n                             &padding_right)",
          "old_line_content": "                             &padding_bottom);",
          "new_line_content": "    GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &padding_left,",
          "content_same": false
        },
        {
          "line": 1370,
          "old_api": null,
          "new_api": "Device",
          "old_text": null,
          "new_text": "Name(\"Conv2DBackpropInput\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<double>(\"T\")\n                            .HostMemory(\"input_sizes\")",
          "old_line_content": "",
          "new_line_content": "REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "content_same": false
        },
        {
          "line": 1375,
          "old_api": null,
          "new_api": "Device",
          "old_text": null,
          "new_text": "Name(\"Conv2DBackpropInput\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .HostMemory(\"input_sizes\")",
          "old_line_content": "                        Conv2DBackpropInputOp<GPUDevice, double>);",
          "new_line_content": "REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "content_same": false
        },
        {
          "line": 864,
          "old_api": null,
          "new_api": "GetWindowedOutputSizeVerboseV2",
          "old_text": null,
          "new_text": "GetWindowedOutputSizeVerboseV2(\n      dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n      row_dilation, row_stride, padding, &expected_out_rows, &padding_top,\n      &padding_bottom)",
          "old_line_content": "  // padding was valid earlier.",
          "new_line_content": "  TF_CHECK_OK(GetWindowedOutputSizeVerboseV2(",
          "content_same": false
        },
        {
          "line": 868,
          "old_api": null,
          "new_api": "DCHECK_EQ",
          "old_text": null,
          "new_text": "DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows)",
          "old_line_content": "      &padding_bottom));",
          "new_line_content": "  DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows);",
          "content_same": false
        },
        {
          "line": 1380,
          "old_api": null,
          "new_api": "Device",
          "old_text": null,
          "new_text": "Name(\"Conv2DBackpropInput\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .HostMemory(\"input_sizes\")",
          "old_line_content": "                        Conv2DBackpropInputOp<GPUDevice, float>);",
          "new_line_content": "REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "content_same": false
        },
        {
          "line": 873,
          "old_api": null,
          "new_api": "DCHECK_EQ",
          "old_text": null,
          "new_text": "DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols)",
          "old_line_content": "      &padding_right));",
          "new_line_content": "  DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols);",
          "content_same": false
        },
        {
          "line": 1385,
          "old_api": null,
          "new_api": "Device",
          "old_text": null,
          "new_text": "Name(\"Conv2DBackpropInput\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"T\")\n                            .HostMemory(\"input_sizes\")",
          "old_line_content": "                        Conv2DBackpropInputOp<GPUDevice, Eigen::half>);",
          "new_line_content": "REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "content_same": false
        },
        {
          "line": 875,
          "old_api": null,
          "new_api": "op_device_context",
          "old_text": null,
          "new_text": "ctx->op_device_context()->stream()",
          "old_line_content": "",
          "new_line_content": "  auto* stream = ctx->op_device_context()->stream();",
          "content_same": false
        },
        {
          "line": 366,
          "old_api": null,
          "new_api": "ANNOTATE_MEMORY_IS_INITIALIZED",
          "old_text": null,
          "new_text": "ANNOTATE_MEMORY_IS_INITIALIZED(\n        im2col_buf, filter_total_size * output_image_size * sizeof(T))",
          "old_line_content": "    // mkldnn_sgemm code can't be instrumented with msan.",
          "new_line_content": "    ANNOTATE_MEMORY_IS_INITIALIZED(",
          "content_same": false
        },
        {
          "line": 879,
          "old_api": null,
          "new_api": "errors::Unimplemented(\n        \"Conv2DBackpropInput for GPU is not currently supported \"\n        \"without cudnn\")",
          "old_text": null,
          "new_text": "errors::Unimplemented(\n        \"Conv2DBackpropInput for GPU is not currently supported \"\n        \"without cudnn\")",
          "old_line_content": "  if (!use_cudnn) {",
          "new_line_content": "    ctx->SetStatus(errors::Unimplemented(",
          "content_same": false
        },
        {
          "line": 370,
          "old_api": null,
          "new_api": "mkldnn_sgemm",
          "old_text": null,
          "new_text": "mkldnn_sgemm(&transposeA, &transposeB, &m, &n, &k, &alpha, filter_data,\n                     &ldA, out_data, &ldB, &beta, im2col_buf, &ldC)",
          "old_line_content": "    mkldnn_status_t st =",
          "new_line_content": "        mkldnn_sgemm(&transposeA, &transposeB, &m, &n, &k, &alpha, filter_data,",
          "content_same": false
        },
        {
          "line": 373,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(\n        ctx, st == 0,\n        errors::Internal(\"Failed to call mkldnn_sgemm. Error code: \", st))",
          "old_line_content": "",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 375,
          "old_api": null,
          "new_api": "errors::Internal(\"Failed to call mkldnn_sgemm. Error code: \", st)",
          "old_text": null,
          "new_text": "errors::Internal(\"Failed to call mkldnn_sgemm. Error code: \", st)",
          "old_line_content": "        ctx, st == 0,",
          "new_line_content": "        errors::Internal(\"Failed to call mkldnn_sgemm. Error code: \", st));",
          "content_same": false
        },
        {
          "line": 889,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "filter_shape.dim_size(2)",
          "old_line_content": "  // convolution.",
          "new_line_content": "  bool is_grouped_convolution = filter_shape.dim_size(2) != dims.in_depth;",
          "content_same": false
        },
        {
          "line": 383,
          "old_api": null,
          "new_api": "explicit",
          "old_text": null,
          "new_text": "explicit",
          "old_line_content": " public:",
          "new_line_content": "  explicit Conv2DBackpropInputOp(OpKernelConstruction* context)",
          "content_same": false
        },
        {
          "line": 386,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"data_format\", &data_format)",
          "old_line_content": "    string data_format;",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));",
          "content_same": false
        },
        {
          "line": 900,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "out_backprop.template flat<T>().data()",
          "old_line_content": "",
          "new_line_content": "    auto a_ptr = AsDeviceMemory(out_backprop.template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 390,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"strides\", &strides_)",
          "old_line_content": "",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));",
          "content_same": false
        },
        {
          "line": 394,
          "old_api": null,
          "new_api": "GetTensorDim",
          "old_text": null,
          "new_text": "GetTensorDim(strides_, data_format_, 'N')",
          "old_line_content": "                                        \"specify 4 dimensions\"));",
          "new_line_content": "    int stride_n = GetTensorDim(strides_, data_format_, 'N');",
          "content_same": false
        },
        {
          "line": 911,
          "old_api": null,
          "new_api": "ok",
          "old_text": null,
          "new_text": "stream\n            ->ThenBlasGemm(transpose, no_transpose, n, m, k, 1.0f, b_ptr, k,\n                           a_ptr, k, 0.0f, &c_ptr, n)\n            .ok()",
          "old_line_content": "    bool blas_launch_status =",
          "new_line_content": "        stream",
          "content_same": false
        },
        {
          "line": 400,
          "old_api": null,
          "new_api": "errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\")",
          "old_text": null,
          "new_text": "errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\")",
          "old_line_content": "        context, (stride_n == 1 && stride_c == 1),",
          "new_line_content": "        errors::InvalidArgument(\"Current implementation does not yet support \"",
          "content_same": false
        },
        {
          "line": 402,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(context, stride_h > 0 && stride_w > 0,\n                errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\"))",
          "old_line_content": "                                \"strides in the batch and depth dimensions.\"));",
          "new_line_content": "    OP_REQUIRES(context, stride_h > 0 && stride_w > 0,",
          "content_same": false
        },
        {
          "line": 916,
          "old_api": null,
          "new_api": "errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                      \", n=\", n, \", k=\", k)",
          "old_text": null,
          "new_text": "errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                      \", n=\", n, \", k=\", k)",
          "old_line_content": "    if (!blas_launch_status) {",
          "new_line_content": "      ctx->SetStatus(errors::Internal(\"Blas SGEMM launch failed : m=\", m,",
          "content_same": false
        },
        {
          "line": 406,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"dilations\", &dilations_)",
          "old_line_content": "",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"dilations\", &dilations_));",
          "content_same": false
        },
        {
          "line": 410,
          "old_api": null,
          "new_api": "GetTensorDim",
          "old_text": null,
          "new_text": "GetTensorDim(dilations_, data_format_, 'N')",
          "old_line_content": "                                        \"specify 4 dimensions\"));",
          "new_line_content": "    int dilation_n = GetTensorDim(dilations_, data_format_, 'N');",
          "content_same": false
        },
        {
          "line": 418,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(\n        context, dilation_h > 0 && dilation_w > 0,\n        errors::InvalidArgument(\"Dilated rates should be larger than 0.\"))",
          "old_line_content": "                    \"dilations in the batch and depth dimensions.\"));",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 420,
          "old_api": null,
          "new_api": "errors::InvalidArgument(\"Dilated rates should be larger than 0.\")",
          "old_text": null,
          "new_text": "errors::InvalidArgument(\"Dilated rates should be larger than 0.\")",
          "old_line_content": "        context, dilation_h > 0 && dilation_w > 0,",
          "new_line_content": "        errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));",
          "content_same": false
        },
        {
          "line": 933,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "out_backprop.template flat<T>().data()",
          "old_line_content": "",
          "new_line_content": "    auto a_ptr = AsDeviceMemory(out_backprop.template flat<T>().data(),",
          "content_same": false
        },
        {
          "line": 422,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"padding\", &padding_)",
          "old_line_content": "",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));",
          "content_same": false
        },
        {
          "line": 428,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_)",
          "old_line_content": "",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));",
          "content_same": false
        },
        {
          "line": 944,
          "old_api": null,
          "new_api": "ok",
          "old_text": null,
          "new_text": "stream\n            ->ThenBlasGemm(transpose, no_transpose, n, m, k, 1.0f, b_ptr, k,\n                           a_ptr, k, 0.0f, &c_ptr, n)\n            .ok()",
          "old_line_content": "    bool blas_launch_status =",
          "new_line_content": "        stream",
          "content_same": false
        },
        {
          "line": 434,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\"Conv2DBackpropInputOp [CPU or GPU(int32)] \"\n                                  \"only supports NHWC data format.\"))",
          "old_line_content": "        std::is_same<T, int32>::value) {",
          "new_line_content": "      OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 436,
          "old_api": null,
          "new_api": "errors::InvalidArgument(\"Conv2DBackpropInputOp [CPU or GPU(int32)] \"\n                                  \"only supports NHWC data format.\")",
          "old_text": null,
          "new_text": "errors::InvalidArgument(\"Conv2DBackpropInputOp [CPU or GPU(int32)] \"\n                                  \"only supports NHWC data format.\")",
          "old_line_content": "          context, data_format_ == FORMAT_NHWC,",
          "new_line_content": "          errors::InvalidArgument(\"Conv2DBackpropInputOp [CPU or GPU(int32)] \"",
          "content_same": false
        },
        {
          "line": 949,
          "old_api": null,
          "new_api": "errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                      \", n=\", n, \", k=\", k)",
          "old_text": null,
          "new_text": "errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                      \", n=\", n, \", k=\", k)",
          "old_line_content": "    if (!blas_launch_status) {",
          "new_line_content": "      ctx->SetStatus(errors::Internal(\"Blas SGEMM launch failed : m=\", m,",
          "content_same": false
        },
        {
          "line": 440,
          "old_api": null,
          "new_api": "OP_REQUIRES",
          "old_text": null,
          "new_text": "OP_REQUIRES(\n          context, (dilation_h == 1 && dilation_w == 1),\n          errors::InvalidArgument(\n              \"Conv2DBackpropInputOp [CPU or GPU(int32)] not yet support \"\n              \"dilation rates larger than 1.\"))",
          "old_line_content": "      // TODO(yangzihao): Add a CPU implementation for dilated convolution.",
          "new_line_content": "      OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 442,
          "old_api": null,
          "new_api": "errors::InvalidArgument(\n              \"Conv2DBackpropInputOp [CPU or GPU(int32)] not yet support \"\n              \"dilation rates larger than 1.\")",
          "old_text": null,
          "new_text": "errors::InvalidArgument(\n              \"Conv2DBackpropInputOp [CPU or GPU(int32)] not yet support \"\n              \"dilation rates larger than 1.\")",
          "old_line_content": "          context, (dilation_h == 1 && dilation_w == 1),",
          "new_line_content": "          errors::InvalidArgument(",
          "content_same": false
        },
        {
          "line": 955,
          "old_api": null,
          "new_api": "std::min(padding_top, padding_bottom)",
          "old_text": null,
          "new_text": "std::min(padding_top, padding_bottom)",
          "old_line_content": "",
          "new_line_content": "  const int64 common_padding_rows = std::min(padding_top, padding_bottom);",
          "content_same": false
        },
        {
          "line": 449,
          "old_api": null,
          "new_api": "input",
          "old_text": null,
          "new_text": "context->input(0)",
          "old_line_content": "  void Compute(OpKernelContext* context) override {",
          "new_line_content": "    const Tensor& input_sizes = context->input(0);",
          "content_same": false
        },
        {
          "line": 962,
          "old_api": null,
          "new_api": "std::abs(padding_bottom - padding_top)",
          "old_text": null,
          "new_text": "std::abs(padding_bottom - padding_top)",
          "old_line_content": "    // as it did during the forward pass function.",
          "new_line_content": "    const int64 padding_rows_diff = std::abs(padding_bottom - padding_top);",
          "content_same": false
        },
        {
          "line": 454,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(context,\n                   Conv2DBackpropComputeInputShape(input_sizes, filter.shape(),\n                                                   out_backprop.shape(),\n                                                   data_format_, &input_shape))",
          "old_line_content": "    TensorShape input_shape;",
          "new_line_content": "    OP_REQUIRES_OK(context,",
          "content_same": false
        },
        {
          "line": 968,
          "old_api": null,
          "new_api": "ShapeFromFormat",
          "old_text": null,
          "new_text": "ShapeFromFormat(\n        data_format, dims.batch_size, new_in_rows, new_in_cols, dims.in_depth)",
          "old_line_content": "        dims.spatial_dims[1].input_size + padding_cols_diff;",
          "new_line_content": "    compatible_input_shape = ShapeFromFormat(",
          "content_same": false
        },
        {
          "line": 460,
          "old_api": null,
          "new_api": "allocate_output",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input_shape, &in_backprop))",
          "old_line_content": "    Tensor* in_backprop = nullptr;",
          "new_line_content": "    OP_REQUIRES_OK(context,",
          "content_same": false
        },
        {
          "line": 974,
          "old_api": null,
          "new_api": "CHECK",
          "old_text": null,
          "new_text": "CHECK(common_padding_rows >= 0 && common_padding_cols >= 0)",
          "old_line_content": "",
          "new_line_content": "  CHECK(common_padding_rows >= 0 && common_padding_cols >= 0)  // Crash OK",
          "content_same": false
        },
        {
          "line": 464,
          "old_api": null,
          "new_api": "num_elements",
          "old_text": null,
          "new_text": "input_shape.num_elements()",
          "old_line_content": "    // If there is nothing to compute, return.",
          "new_line_content": "    if (input_shape.num_elements() == 0) {",
          "content_same": false
        },
        {
          "line": 470,
          "old_api": null,
          "new_api": "GetTensorDim",
          "old_text": null,
          "new_text": "GetTensorDim(strides_, data_format_, 'H')",
          "old_line_content": "    // do not support striding on the batch or depth dimension).",
          "new_line_content": "    const int stride_rows = GetTensorDim(strides_, data_format_, 'H');",
          "content_same": false
        },
        {
          "line": 982,
          "old_api": null,
          "new_api": "parent",
          "old_text": null,
          "new_text": "stream->parent()",
          "old_line_content": "  const bool compute_in_nhwc =",
          "new_line_content": "      DataTypeToEnum<T>::value == DT_HALF && IsVoltaOrLater(*stream->parent());",
          "content_same": false
        },
        {
          "line": 475,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(2)",
          "old_line_content": "",
          "new_line_content": "    VLOG(2) << \"Conv2DBackpropInput:\"",
          "content_same": false
        },
        {
          "line": 991,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(3)",
          "old_line_content": "",
          "new_line_content": "  VLOG(3) << \"Compute Conv2DBackpropInput with cuDNN:\"",
          "content_same": false
        },
        {
          "line": 483,
          "old_api": null,
          "new_api": "launch",
          "old_text": null,
          "new_text": "launch(context, use_cudnn_, cudnn_use_autotune_, out_backprop, filter,\n           dilation_rows, dilation_cols, stride_rows, stride_cols, padding_,\n           explicit_paddings_, in_backprop, data_format_)",
          "old_line_content": "    LaunchConv2DBackpropInputOp<Device, T> launch;",
          "new_line_content": "    launch(context, use_cudnn_, cudnn_use_autotune_, out_backprop, filter,",
          "content_same": false
        },
        {
          "line": 995,
          "old_api": null,
          "new_api": "constexpr",
          "old_text": null,
          "new_text": "constexpr",
          "old_line_content": "",
          "new_line_content": "  constexpr auto kComputeInNHWC =",
          "content_same": false
        },
        {
          "line": 998,
          "old_api": null,
          "new_api": "constexpr",
          "old_text": null,
          "new_text": "constexpr",
          "old_line_content": "                      se::dnn::FilterLayout::kOutputYXInput);",
          "new_line_content": "  constexpr auto kComputeInNCHW =",
          "content_same": false
        },
        {
          "line": 1005,
          "old_api": null,
          "new_api": "std::tie(compute_data_layout, filter_layout)",
          "old_text": null,
          "new_text": "std::tie(compute_data_layout, filter_layout)",
          "old_line_content": "",
          "new_line_content": "  std::tie(compute_data_layout, filter_layout) =",
          "content_same": false
        },
        {
          "line": 1009,
          "old_api": null,
          "new_api": "set_count",
          "old_text": null,
          "new_text": "input_desc.set_count(dims.batch_size)\n      .set_height(GetTensorDim(compatible_input_shape, data_format, 'H'))\n      .set_width(GetTensorDim(compatible_input_shape, data_format, 'W'))\n      .set_feature_map_count(dims.in_depth)\n      .set_layout(compute_data_layout)",
          "old_line_content": "  se::dnn::BatchDescriptor input_desc;",
          "new_line_content": "  input_desc.set_count(dims.batch_size)",
          "content_same": false
        },
        {
          "line": 1015,
          "old_api": null,
          "new_api": "set_count",
          "old_text": null,
          "new_text": "output_desc.set_count(dims.batch_size)\n      .set_height(dims.spatial_dims[0].output_size)\n      .set_width(dims.spatial_dims[1].output_size)\n      .set_feature_map_count(dims.out_depth)\n      .set_layout(compute_data_layout)",
          "old_line_content": "  se::dnn::BatchDescriptor output_desc;",
          "new_line_content": "  output_desc.set_count(dims.batch_size)",
          "content_same": false
        },
        {
          "line": 505,
          "old_api": null,
          "new_api": "explicit",
          "old_text": null,
          "new_text": "explicit",
          "old_line_content": " public:",
          "new_line_content": "  explicit Conv2DCustomBackpropInputOp(OpKernelConstruction* context)",
          "content_same": false
        },
        {
          "line": 508,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"data_format\", &data_format)",
          "old_line_content": "    string data_format;",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));",
          "content_same": false
        },
        {
          "line": 1021,
          "old_api": null,
          "new_api": "set_input_filter_height",
          "old_text": null,
          "new_text": "filter_desc.set_input_filter_height(dims.spatial_dims[0].filter_size)\n      .set_input_filter_width(dims.spatial_dims[1].filter_size)\n      .set_input_feature_map_count(filter_shape.dim_size(2))\n      .set_output_feature_map_count(filter_shape.dim_size(3))\n      .set_layout(filter_layout)",
          "old_line_content": "  se::dnn::FilterDescriptor filter_desc;",
          "new_line_content": "  filter_desc.set_input_filter_height(dims.spatial_dims[0].filter_size)",
          "content_same": false
        },
        {
          "line": 1023,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "filter_shape.dim_size(2)",
          "old_line_content": "      .set_input_filter_width(dims.spatial_dims[1].filter_size)",
          "new_line_content": "      .set_input_feature_map_count(filter_shape.dim_size(2))",
          "content_same": false
        }
      ],
      "deletions": [
        {
          "line": 513,
          "old_api": "errors::InvalidArgument(\n                    \"Conv2DCustomBackpropInputOp only supports NHWC.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\n                    \"Conv2DCustomBackpropInputOp only supports NHWC.\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(",
          "new_line_content": "                    \"Conv2DCustomBackpropInputOp only supports NHWC.\"));",
          "content_same": false
        },
        {
          "line": 1025,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "filter_shape.dim_size(3)",
          "new_text": null,
          "old_line_content": "      .set_output_feature_map_count(filter_shape.dim_size(3))",
          "new_line_content": "      .set_layout(filter_layout);",
          "content_same": false
        },
        {
          "line": 1028,
          "old_api": "set_vertical_dilation_rate",
          "new_api": null,
          "old_text": "conv_desc.set_vertical_dilation_rate(dims.spatial_dims[0].dilation)\n      .set_horizontal_dilation_rate(dims.spatial_dims[1].dilation)\n      .set_vertical_filter_stride(dims.spatial_dims[0].stride)\n      .set_horizontal_filter_stride(dims.spatial_dims[1].stride)\n      .set_zero_padding_height(common_padding_rows)\n      .set_zero_padding_width(common_padding_cols)\n      .set_group_count(dims.in_depth / filter_shape.dim_size(2))",
          "new_text": null,
          "old_line_content": "  conv_desc.set_vertical_dilation_rate(dims.spatial_dims[0].dilation)",
          "new_line_content": "      .set_horizontal_dilation_rate(dims.spatial_dims[1].dilation)",
          "content_same": false
        },
        {
          "line": 517,
          "old_api": "errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(\"Sliding window strides field must \"",
          "new_line_content": "                                        \"specify 4 dimensions\"));",
          "content_same": false
        },
        {
          "line": 519,
          "old_api": "OP_REQUIRES",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n        context, (strides_[0] == 1 && strides_[3] == 1),\n        errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\"))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "        context, (strides_[0] == 1 && strides_[3] == 1),",
          "content_same": false
        },
        {
          "line": 521,
          "old_api": "errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\")",
          "new_text": null,
          "old_line_content": "        errors::InvalidArgument(\"Current implementation does not yet support \"",
          "new_line_content": "                                \"strides in the batch and depth dimensions.\"));",
          "content_same": false
        },
        {
          "line": 1034,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "filter_shape.dim_size(2)",
          "new_text": null,
          "old_line_content": "      .set_group_count(dims.in_depth / filter_shape.dim_size(2));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 524,
          "old_api": "errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(",
          "new_line_content": "                    \"Row and column strides should be larger than 0.\"));",
          "content_same": false
        },
        {
          "line": 529,
          "old_api": "errors::InvalidArgument(\"Sliding window dilations field must \"\n                                        \"specify 4 dimensions\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Sliding window dilations field must \"\n                                        \"specify 4 dimensions\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(\"Sliding window dilations field must \"",
          "new_line_content": "                                        \"specify 4 dimensions\"));",
          "content_same": false
        },
        {
          "line": 532,
          "old_api": "errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(",
          "new_line_content": "                    \"Current implementation does not yet support \"",
          "content_same": false
        },
        {
          "line": 1044,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "ToString(dst_format)",
          "new_text": null,
          "old_line_content": "            << \" to \" << ToString(dst_format);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 537,
          "old_api": "errors::InvalidArgument(\n                    \"Current libxsmm and customized CPU implementations do \"\n                    \"not yet support dilation rates larger than 1.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\n                    \"Current libxsmm and customized CPU implementations do \"\n                    \"not yet support dilation rates larger than 1.\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(",
          "new_line_content": "                    \"Current libxsmm and customized CPU implementations do \"",
          "content_same": false
        },
        {
          "line": 1051,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "filter.dim_size(2)",
          "new_text": null,
          "old_line_content": "                           filter.dim_size(1), filter.dim_size(2)});",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1053,
          "old_api": "allocate_temp",
          "new_api": null,
          "old_text": "ctx->allocate_temp(DataTypeToEnum<T>::value, dst_shape,\n                                          &transformed_filter)",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(ctx->allocate_temp(DataTypeToEnum<T>::value, dst_shape,",
          "new_line_content": "                                          &transformed_filter));",
          "content_same": false
        },
        {
          "line": 542,
          "old_api": "CheckValidPadding",
          "new_api": null,
          "old_text": "CheckValidPadding(padding_, explicit_paddings_,\n                                              /*num_dims=*/4, data_format_)",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(context, CheckValidPadding(padding_, explicit_paddings_,",
          "new_line_content": "                                              /*num_dims=*/4, data_format_));",
          "content_same": false
        },
        {
          "line": 1058,
          "old_api": "transformed_filter.tensor<T, 4>()",
          "new_api": null,
          "old_text": "transformed_filter.tensor<T, 4>()",
          "new_text": null,
          "old_line_content": "        To32Bit(transformed_filter.tensor<T, 4>()));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1060,
          "old_api": "Status::OK()",
          "new_api": null,
          "old_text": "Status::OK()",
          "new_text": null,
          "old_line_content": "    return Status::OK();",
          "new_line_content": "  };",
          "content_same": false
        },
        {
          "line": 549,
          "old_api": "input",
          "new_api": null,
          "old_text": "context->input(2)",
          "new_text": null,
          "old_line_content": "    const Tensor& out_backprop = context->input(2);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1064,
          "old_api": "transform_filter",
          "new_api": null,
          "old_text": "transform_filter(FORMAT_OIHW)",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(ctx, transform_filter(FORMAT_OIHW));",
          "new_line_content": "  } else if (compute_data_format == FORMAT_NHWC) {",
          "content_same": false
        },
        {
          "line": 554,
          "old_api": "shape",
          "new_api": null,
          "old_text": "out_backprop.shape()",
          "new_text": null,
          "old_line_content": "                                                   out_backprop.shape(),",
          "new_line_content": "                                                   data_format_, &input_shape));",
          "content_same": false
        },
        {
          "line": 1066,
          "old_api": "transform_filter",
          "new_api": null,
          "old_text": "transform_filter(FORMAT_OHWI)",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(ctx, transform_filter(FORMAT_OHWI));",
          "new_line_content": "  } else {",
          "content_same": false
        },
        {
          "line": 1069,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "ToString(compute_data_format)",
          "new_text": null,
          "old_line_content": "                                           ToString(compute_data_format)));",
          "new_line_content": "    return;",
          "content_same": false
        },
        {
          "line": 559,
          "old_api": "shape",
          "new_api": null,
          "old_text": "ConvBackpropComputeDimensionsV2(\n                       \"Conv2DCustomBackpropInput\", /*num_spatial_dims=*/2,\n                       input_shape, filter.shape(), out_backprop.shape(),\n                       /*dilations=*/{1, 1, 1, 1}, strides_, padding_,\n                       explicit_paddings_, data_format_, &dims)",
          "new_text": null,
          "old_line_content": "                   ConvBackpropComputeDimensionsV2(",
          "new_line_content": "                       \"Conv2DCustomBackpropInput\", /*num_spatial_dims=*/2,",
          "content_same": false
        },
        {
          "line": 561,
          "old_api": "shape",
          "new_api": null,
          "old_text": "out_backprop.shape()",
          "new_text": null,
          "old_line_content": "                       input_shape, filter.shape(), out_backprop.shape(),",
          "new_line_content": "                       /*dilations=*/{1, 1, 1, 1}, strides_, padding_,",
          "content_same": false
        },
        {
          "line": 1076,
          "old_api": "ShapeFromFormat",
          "new_api": null,
          "old_text": "ShapeFromFormat(\n        compute_data_format, dims.batch_size, dims.spatial_dims[0].output_size,\n        dims.spatial_dims[1].output_size, dims.out_depth)",
          "new_text": null,
          "old_line_content": "    TensorShape compute_shape = ShapeFromFormat(",
          "new_line_content": "        compute_data_format, dims.batch_size, dims.spatial_dims[0].output_size,",
          "content_same": false
        },
        {
          "line": 567,
          "old_api": "allocate_output",
          "new_api": null,
          "old_text": "context->allocate_output(0, input_shape, &in_backprop)",
          "new_text": null,
          "old_line_content": "                   context->allocate_output(0, input_shape, &in_backprop));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1081,
          "old_api": "allocate_temp",
          "new_api": null,
          "old_text": "ctx->allocate_temp(DataTypeToEnum<T>::value, compute_shape,\n                                        &transformed_out_backprop)",
          "new_text": null,
          "old_line_content": "                     ctx->allocate_temp(DataTypeToEnum<T>::value, compute_shape,",
          "new_line_content": "                                        &transformed_out_backprop));",
          "content_same": false
        },
        {
          "line": 570,
          "old_api": "num_elements",
          "new_api": null,
          "old_text": "input_shape.num_elements()",
          "new_text": null,
          "old_line_content": "    if (input_shape.num_elements() == 0) {",
          "new_line_content": "      return;",
          "content_same": false
        },
        {
          "line": 1085,
          "old_api": "transformed_out_backprop.tensor<T, 4>()",
          "new_api": null,
          "old_text": "transformed_out_backprop.tensor<T, 4>()",
          "new_text": null,
          "old_line_content": "          transformed_out_backprop.tensor<T, 4>());",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 1088,
          "old_api": "CopyFrom",
          "new_api": null,
          "old_text": "transformed_out_backprop.CopyFrom(out_backprop, compute_shape)",
          "new_text": null,
          "old_line_content": "      CHECK(transformed_out_backprop.CopyFrom(out_backprop, compute_shape));",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 580,
          "old_api": "OP_REQUIRES_OK",
          "new_api": null,
          "old_text": "OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(",
          "new_line_content": "        context,",
          "content_same": false
        },
        {
          "line": 582,
          "old_api": "GetWindowedOutputSizeVerbose",
          "new_api": null,
          "old_text": "GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom)",
          "new_text": null,
          "old_line_content": "        GetWindowedOutputSizeVerbose(",
          "new_line_content": "            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,",
          "content_same": false
        },
        {
          "line": 1096,
          "old_api": "allocate_temp",
          "new_api": null,
          "old_text": "ctx->allocate_temp(\n               DataTypeToEnum<T>::value,\n               ShapeFromFormat(\n                   compute_data_format,\n                   GetTensorDim(compatible_input_shape, data_format, 'N'),\n                   GetTensorDim(compatible_input_shape, data_format, 'H'),\n                   GetTensorDim(compatible_input_shape, data_format, 'W'),\n                   GetTensorDim(compatible_input_shape, data_format, 'C')),\n               &pre_transformed_in_backprop)",
          "new_text": null,
          "old_line_content": "      ctx, ctx->allocate_temp(",
          "new_line_content": "               DataTypeToEnum<T>::value,",
          "content_same": false
        },
        {
          "line": 586,
          "old_api": "OP_REQUIRES_OK",
          "new_api": null,
          "old_text": "OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(",
          "new_line_content": "        context,",
          "content_same": false
        },
        {
          "line": 1098,
          "old_api": "ShapeFromFormat",
          "new_api": null,
          "old_text": "ShapeFromFormat(\n                   compute_data_format,\n                   GetTensorDim(compatible_input_shape, data_format, 'N'),\n                   GetTensorDim(compatible_input_shape, data_format, 'H'),\n                   GetTensorDim(compatible_input_shape, data_format, 'W'),\n                   GetTensorDim(compatible_input_shape, data_format, 'C'))",
          "new_text": null,
          "old_line_content": "               ShapeFromFormat(",
          "new_line_content": "                   compute_data_format,",
          "content_same": false
        },
        {
          "line": 588,
          "old_api": "GetWindowedOutputSizeVerbose",
          "new_api": null,
          "old_text": "GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right)",
          "new_text": null,
          "old_line_content": "        GetWindowedOutputSizeVerbose(",
          "new_line_content": "            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,",
          "content_same": false
        },
        {
          "line": 1103,
          "old_api": "GetTensorDim",
          "new_api": null,
          "old_text": "GetTensorDim(compatible_input_shape, data_format, 'C')",
          "new_text": null,
          "old_line_content": "                   GetTensorDim(compatible_input_shape, data_format, 'C')),",
          "new_line_content": "               &pre_transformed_in_backprop));",
          "content_same": false
        },
        {
          "line": 1108,
          "old_api": "size",
          "new_api": null,
          "old_text": "transformed_out_backprop.template flat<T>().size()",
          "new_text": null,
          "old_line_content": "                     transformed_out_backprop.template flat<T>().size());",
          "new_line_content": "  auto filter_ptr =",
          "content_same": false
        },
        {
          "line": 597,
          "old_api": "out_backprop.tensor<T, 4>()",
          "new_api": null,
          "old_text": "out_backprop.tensor<T, 4>()",
          "new_text": null,
          "old_line_content": "              out_backprop.tensor<T, 4>(), dims.spatial_dims[0].input_size,",
          "new_line_content": "              dims.spatial_dims[1].input_size,",
          "content_same": false
        },
        {
          "line": 1111,
          "old_api": "size",
          "new_api": null,
          "old_text": "transformed_filter.template flat<T>().size()",
          "new_text": null,
          "old_line_content": "                     transformed_filter.template flat<T>().size());",
          "new_line_content": "  auto in_backprop_ptr =",
          "content_same": false
        },
        {
          "line": 601,
          "old_api": "static_cast<int>(pad_left)",
          "new_api": null,
          "old_text": "static_cast<int>(pad_left)",
          "new_text": null,
          "old_line_content": "              static_cast<int>(pad_top), static_cast<int>(pad_left),",
          "new_line_content": "              data_format_)) {",
          "content_same": false
        },
        {
          "line": 1114,
          "old_api": "size",
          "new_api": null,
          "old_text": "pre_transformed_in_backprop.template flat<T>().size()",
          "new_text": null,
          "old_line_content": "                     pre_transformed_in_backprop.template flat<T>().size());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1116,
          "old_api": "GetDnnWorkspaceLimit",
          "new_api": null,
          "old_text": "GetDnnWorkspaceLimit(\n      \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default\n  )",
          "new_text": null,
          "old_line_content": "  static int64 ConvolveBackwardDataScratchSize = GetDnnWorkspaceLimit(",
          "new_line_content": "      \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default",
          "content_same": false
        },
        {
          "line": 1121,
          "old_api": "dtype",
          "new_api": null,
          "old_text": "out_backprop.dtype()",
          "new_text": null,
          "old_line_content": "  DataType dtype = out_backprop.dtype();",
          "new_line_content": "  ConvParameters conv_parameters = {",
          "content_same": false
        },
        {
          "line": 1126,
          "old_api": "width",
          "new_api": null,
          "old_text": "input_desc.width()",
          "new_text": null,
          "old_line_content": "        input_desc.width()}},              // in_cols",
          "new_line_content": "      compute_data_format,                 // compute_data_format",
          "content_same": false
        },
        {
          "line": 616,
          "old_api": "OP_REQUIRES_OK",
          "new_api": null,
          "old_text": "OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(",
          "new_line_content": "        context,",
          "content_same": false
        },
        {
          "line": 618,
          "old_api": "GetWindowedOutputSizeVerbose",
          "new_api": null,
          "old_text": "GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[0].stride, padding_,\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom)",
          "new_text": null,
          "old_line_content": "        GetWindowedOutputSizeVerbose(",
          "new_line_content": "            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,",
          "content_same": false
        },
        {
          "line": 1131,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "filter_shape.dim_size(2)",
          "new_text": null,
          "old_line_content": "        filter_shape.dim_size(2)}},        // filter_depths",
          "new_line_content": "      {{dims.spatial_dims[0].dilation,     // dilation_rows",
          "content_same": false
        },
        {
          "line": 622,
          "old_api": "OP_REQUIRES_OK",
          "new_api": null,
          "old_text": "OP_REQUIRES_OK(\n        context,\n        GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(",
          "new_line_content": "        context,",
          "content_same": false
        },
        {
          "line": 624,
          "old_api": "GetWindowedOutputSizeVerbose",
          "new_api": null,
          "old_text": "GetWindowedOutputSizeVerbose(\n            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n            dims.spatial_dims[1].stride, padding_,\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right)",
          "new_text": null,
          "old_line_content": "        GetWindowedOutputSizeVerbose(",
          "new_line_content": "            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,",
          "content_same": false
        },
        {
          "line": 1140,
          "old_api": "group_count",
          "new_api": null,
          "old_text": "conv_desc.group_count()",
          "new_text": null,
          "old_line_content": "      conv_desc.group_count()              // group_count",
          "new_line_content": "  };",
          "content_same": false
        },
        {
          "line": 121,
          "old_api": "GetTensorDimIndex",
          "new_api": null,
          "old_text": "GetTensorDimIndex(data_format, 'W')",
          "new_text": null,
          "old_line_content": "    auto input_w = GetTensorDimIndex(data_format, 'W');",
          "new_line_content": "    strides[input_h] = row_stride;",
          "content_same": false
        },
        {
          "line": 1149,
          "old_api": "Find",
          "new_api": null,
          "old_text": "AutoTuneConvBwdData::GetInstance()->Find(\n                                conv_parameters, &algorithm_config)",
          "new_text": null,
          "old_line_content": "  if (cudnn_use_autotune && !AutoTuneConvBwdData::GetInstance()->Find(",
          "new_line_content": "                                conv_parameters, &algorithm_config)) {",
          "content_same": false
        },
        {
          "line": 128,
          "old_api": "shape",
          "new_api": null,
          "old_text": "filter.shape()",
          "new_text": null,
          "old_line_content": "    const TensorShape& filter_shape = filter.shape();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1153,
          "old_api": "device",
          "new_api": null,
          "old_text": "ctx->device()->GetAllocator({})",
          "new_text": null,
          "old_line_content": "    se::TfAllocatorAdapter tf_allocator_adapter(ctx->device()->GetAllocator({}),",
          "new_line_content": "                                                stream);",
          "content_same": false
        },
        {
          "line": 132,
          "old_api": "shape",
          "new_api": null,
          "old_text": "ConvBackpropComputeDimensionsV2(\n                 \"Conv2DBackpropInput\", /*num_spatial_dims=*/2, input_shape,\n                 filter_shape, out_backprop.shape(), dilations, strides,\n                 padding, explicit_paddings, data_format, &dims)",
          "new_text": null,
          "old_line_content": "        ctx, ConvBackpropComputeDimensionsV2(",
          "new_line_content": "                 \"Conv2DBackpropInput\", /*num_spatial_dims=*/2, input_shape,",
          "content_same": false
        },
        {
          "line": 1157,
          "old_api": "se::GpuAsmOpts()",
          "new_api": null,
          "old_text": "se::GpuAsmOpts()",
          "new_text": null,
          "old_line_content": "                                      se::GpuAsmOpts());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 134,
          "old_api": "shape",
          "new_api": null,
          "old_text": "out_backprop.shape()",
          "new_text": null,
          "old_line_content": "                 filter_shape, out_backprop.shape(), dilations, strides,",
          "new_line_content": "                 padding, explicit_paddings, data_format, &dims));",
          "content_same": false
        },
        {
          "line": 1160,
          "old_api": "WrapRedzoneBestEffort",
          "new_api": null,
          "old_text": "WrapRedzoneBestEffort(&rz_allocator, in_backprop_ptr)",
          "new_text": null,
          "old_line_content": "        WrapRedzoneBestEffort(&rz_allocator, in_backprop_ptr));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 140,
          "old_api": "GetExplicitPaddingForDim",
          "new_api": null,
          "old_text": "GetExplicitPaddingForDim(explicit_paddings, data_format, 'H',\n                               &padding_top, &padding_bottom)",
          "new_text": null,
          "old_line_content": "      GetExplicitPaddingForDim(explicit_paddings, data_format, 'H',",
          "new_line_content": "                               &padding_top, &padding_bottom);",
          "content_same": false
        },
        {
          "line": 653,
          "old_api": "device",
          "new_api": null,
          "old_text": "context->device()->tensorflow_cpu_worker_threads()",
          "new_text": null,
          "old_line_content": "    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 142,
          "old_api": "GetExplicitPaddingForDim",
          "new_api": null,
          "old_text": "GetExplicitPaddingForDim(explicit_paddings, data_format, 'W',\n                               &padding_left, &padding_right)",
          "new_text": null,
          "old_line_content": "      GetExplicitPaddingForDim(explicit_paddings, data_format, 'W',",
          "new_line_content": "                               &padding_left, &padding_right);",
          "content_same": false
        },
        {
          "line": 1164,
          "old_api": "parent",
          "new_api": null,
          "old_text": "stream->parent()",
          "new_text": null,
          "old_line_content": "        conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(stream->parent()),",
          "new_line_content": "        &algorithms));",
          "content_same": false
        },
        {
          "line": 149,
          "old_api": "GetWindowedOutputSizeVerboseV2",
          "new_api": null,
          "old_text": "GetWindowedOutputSizeVerboseV2(\n        dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n        row_dilation, row_stride, padding, &expected_out_rows, &padding_top,\n        &padding_bottom)",
          "new_text": null,
          "old_line_content": "    TF_CHECK_OK(GetWindowedOutputSizeVerboseV2(",
          "new_line_content": "        dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,",
          "content_same": false
        },
        {
          "line": 1173,
          "old_api": "se::GpuAsmOpts()",
          "new_api": null,
          "old_text": "se::GpuAsmOpts()",
          "new_text": null,
          "old_line_content": "          stream, &tf_allocator_adapter, se::GpuAsmOpts(),",
          "new_line_content": "          /*memory_limit=*/ConvolveBackwardDataScratchSize);",
          "content_same": false
        },
        {
          "line": 153,
          "old_api": "DCHECK_EQ",
          "new_api": null,
          "old_text": "DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows)",
          "new_text": null,
          "old_line_content": "    DCHECK_EQ(dims.spatial_dims[0].output_size, expected_out_rows);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1178,
          "old_api": "static_cast<se::ScratchAllocator*>(&scratch_allocator)",
          "new_api": null,
          "old_text": "static_cast<se::ScratchAllocator*>(&scratch_allocator)",
          "new_text": null,
          "old_line_content": "              : static_cast<se::ScratchAllocator*>(&scratch_allocator);",
          "new_line_content": "      ProfileResult profile_result;",
          "content_same": false
        },
        {
          "line": 155,
          "old_api": "GetWindowedOutputSizeVerboseV2",
          "new_api": null,
          "old_text": "GetWindowedOutputSizeVerboseV2(\n        dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n        col_dilation, col_stride, padding, &expected_out_cols, &padding_left,\n        &padding_right)",
          "new_text": null,
          "old_line_content": "    TF_CHECK_OK(GetWindowedOutputSizeVerboseV2(",
          "new_line_content": "        dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,",
          "content_same": false
        },
        {
          "line": 1181,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n              ->ThenConvolveBackwardDataWithAlgorithm(\n                  filter_desc, filter_ptr, output_desc, out_backprop_ptr,\n                  conv_desc, input_desc, &in_backprop_ptr_rz, allocator_used,\n                  AlgorithmConfig(profile_algorithm), &profile_result)\n              .ok()",
          "new_text": null,
          "old_line_content": "          stream",
          "new_line_content": "              ->ThenConvolveBackwardDataWithAlgorithm(",
          "content_same": false
        },
        {
          "line": 159,
          "old_api": "DCHECK_EQ",
          "new_api": null,
          "old_text": "DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols)",
          "new_text": null,
          "old_line_content": "    DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1185,
          "old_api": "AlgorithmConfig",
          "new_api": null,
          "old_text": "AlgorithmConfig(profile_algorithm)",
          "new_text": null,
          "old_line_content": "                  AlgorithmConfig(profile_algorithm), &profile_result)",
          "new_line_content": "              .ok();",
          "content_same": false
        },
        {
          "line": 679,
          "old_api": "allocate_temp",
          "new_api": null,
          "old_text": "context->allocate_temp(\n                       DataTypeToEnum<T>::value,\n                       TensorShape({static_cast<int64>(shard_size),\n                                    static_cast<int64>(output_image_size),\n                                    static_cast<int64>(filter_total_size)}),\n                       &col_buffer)",
          "new_text": null,
          "old_line_content": "                   context->allocate_temp(",
          "new_line_content": "                       DataTypeToEnum<T>::value,",
          "content_same": false
        },
        {
          "line": 683,
          "old_api": "static_cast<int64>(filter_total_size)",
          "new_api": null,
          "old_text": "static_cast<int64>(filter_total_size)",
          "new_text": null,
          "old_line_content": "                                    static_cast<int64>(filter_total_size)}),",
          "new_line_content": "                       &col_buffer));",
          "content_same": false
        },
        {
          "line": 173,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "in_backprop->dim_size(3)",
          "new_text": null,
          "old_line_content": "      REQUIRES_32BIT(in_backprop->dim_size(3));",
          "new_line_content": "#undef REQUIRES_32BIT",
          "content_same": false
        },
        {
          "line": 1198,
          "old_api": "elapsed_time_in_ms",
          "new_api": null,
          "old_text": "profile_result.elapsed_time_in_ms()",
          "new_text": null,
          "old_line_content": "            absl::Milliseconds(profile_result.elapsed_time_in_ms()));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1201,
          "old_api": "CheckRedzones",
          "new_api": null,
          "old_text": "CheckRedzones(rz_allocator, &result)",
          "new_text": null,
          "old_line_content": "        CheckRedzones(rz_allocator, &result);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 179,
          "old_api": "filter.tensor<T, 4>()",
          "new_api": null,
          "old_text": "filter.tensor<T, 4>()",
          "new_text": null,
          "old_line_content": "    auto filter_t = filter.tensor<T, 4>();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 695,
          "old_api": "data",
          "new_api": null,
          "old_text": "out_backprop.template flat<T>().data()",
          "new_text": null,
          "old_line_content": "    const T* out_backprop_data = out_backprop.template flat<T>().data();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1207,
          "old_api": "parent",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n        ctx,\n        stream->parent()->GetMIOpenConvolveAlgorithms(\n            se::dnn::ConvolutionKind::BACKWARD_DATA,\n            se::dnn::ToDataType<T>::value, stream, input_desc, in_backprop_ptr,\n            filter_desc, filter_ptr, output_desc, out_backprop_ptr, conv_desc,\n            &scratch_allocator, &algorithms),\n        errors::Unknown(\n            \"Failed to get convolution algorithm. This is probably \"\n            \"because MIOpen failed to initialize, so try looking to \"\n            \"see if a warning log message was printed above.\"))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "        ctx,",
          "content_same": false
        },
        {
          "line": 1209,
          "old_api": "parent",
          "new_api": null,
          "old_text": "stream->parent()->GetMIOpenConvolveAlgorithms(\n            se::dnn::ConvolutionKind::BACKWARD_DATA,\n            se::dnn::ToDataType<T>::value, stream, input_desc, in_backprop_ptr,\n            filter_desc, filter_ptr, output_desc, out_backprop_ptr, conv_desc,\n            &scratch_allocator, &algorithms)",
          "new_text": null,
          "old_line_content": "        stream->parent()->GetMIOpenConvolveAlgorithms(",
          "new_line_content": "            se::dnn::ConvolutionKind::BACKWARD_DATA,",
          "content_same": false
        },
        {
          "line": 700,
          "old_api": "T",
          "new_api": null,
          "old_text": "T(0)",
          "new_text": null,
          "old_line_content": "        in_backprop_flat.constant(T(0));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 189,
          "old_api": "ctx->eigen_device<Device>()",
          "new_api": null,
          "old_text": "ctx->eigen_device<Device>()",
          "new_text": null,
          "old_line_content": "          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,",
          "new_line_content": "          col_stride, row_stride, col_dilation, row_dilation);",
          "content_same": false
        },
        {
          "line": 1214,
          "old_api": "errors::Unknown(\n            \"Failed to get convolution algorithm. This is probably \"\n            \"because MIOpen failed to initialize, so try looking to \"\n            \"see if a warning log message was printed above.\")",
          "new_api": null,
          "old_text": "errors::Unknown(\n            \"Failed to get convolution algorithm. This is probably \"\n            \"because MIOpen failed to initialize, so try looking to \"\n            \"see if a warning log message was printed above.\")",
          "new_text": null,
          "old_line_content": "        errors::Unknown(",
          "new_line_content": "            \"Failed to get convolution algorithm. This is probably \"",
          "content_same": false
        },
        {
          "line": 192,
          "old_api": "dimension",
          "new_api": null,
          "old_text": "functor::SpatialConvolutionBackwardInputWithExplicitPaddingFunc<Device,\n                                                                      T>()(\n          ctx->eigen_device<Device>(), in_backprop_t, filter_t, out_backprop_t,\n          in_backprop_t.dimension(2) + (padding_left + padding_right),\n          in_backprop_t.dimension(1) + (padding_top + padding_bottom),\n          col_stride, row_stride, col_dilation, row_dilation, padding_top,\n          padding_left)",
          "new_text": null,
          "old_line_content": "      functor::SpatialConvolutionBackwardInputWithExplicitPaddingFunc<Device,",
          "new_line_content": "                                                                      T>()(",
          "content_same": false
        },
        {
          "line": 196,
          "old_api": "dimension",
          "new_api": null,
          "old_text": "in_backprop_t.dimension(1)",
          "new_text": null,
          "old_line_content": "          in_backprop_t.dimension(1) + (padding_top + padding_bottom),",
          "new_line_content": "          col_stride, row_stride, col_dilation, row_dilation, padding_top,",
          "content_same": false
        },
        {
          "line": 1220,
          "old_api": "size",
          "new_api": null,
          "old_text": "algorithms.size()",
          "new_text": null,
          "old_line_content": "    if (algorithms.size() == 1) {",
          "new_line_content": "      auto profile_result = algorithms[0];",
          "content_same": false
        },
        {
          "line": 1227,
          "old_api": "algorithm",
          "new_api": null,
          "old_text": "profile_result.algorithm().tensor_ops_enabled()",
          "new_text": null,
          "old_line_content": "          profile_result.algorithm().tensor_ops_enabled());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1231,
          "old_api": "elapsed_time_in_ms",
          "new_api": null,
          "old_text": "profile_result.elapsed_time_in_ms()",
          "new_text": null,
          "old_line_content": "          absl::Milliseconds(profile_result.elapsed_time_in_ms()));",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 1234,
          "old_api": "algorithm",
          "new_api": null,
          "old_text": "miopen_algorithm.algorithm()",
          "new_text": null,
          "old_line_content": "        auto profile_algorithm = miopen_algorithm.algorithm();",
          "new_line_content": "        ProfileResult profile_result;",
          "content_same": false
        },
        {
          "line": 723,
          "old_api": "contract",
          "new_api": null,
          "old_text": "A.contract(B, contract_dims)",
          "new_text": null,
          "old_line_content": "        C.device(context->eigen_cpu_device()) = A.contract(B, contract_dims);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 725,
          "old_api": "Col2im<T>(\n            col_buffer_data, dims.in_depth, dims.spatial_dims[0].input_size,\n            dims.spatial_dims[1].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[1].filter_size, pad_top, pad_left, pad_bottom,\n            pad_right, dims.spatial_dims[0].stride, dims.spatial_dims[1].stride,\n            input_backprop_data)",
          "new_api": null,
          "old_text": "Col2im<T>(\n            col_buffer_data, dims.in_depth, dims.spatial_dims[0].input_size,\n            dims.spatial_dims[1].input_size, dims.spatial_dims[0].filter_size,\n            dims.spatial_dims[1].filter_size, pad_top, pad_left, pad_bottom,\n            pad_right, dims.spatial_dims[0].stride, dims.spatial_dims[1].stride,\n            input_backprop_data)",
          "new_text": null,
          "old_line_content": "        Col2im<T>(",
          "new_line_content": "            col_buffer_data, dims.in_depth, dims.spatial_dims[0].input_size,",
          "content_same": false
        },
        {
          "line": 1238,
          "old_api": "scratch_size",
          "new_api": null,
          "old_text": "stream\n                ->ThenConvolveBackwardDataWithAlgorithm(\n                    filter_desc, filter_ptr, output_desc, out_backprop_ptr,\n                    conv_desc, input_desc, &in_backprop_ptr, &scratch_allocator,\n                    AlgorithmConfig(profile_algorithm,\n                                    miopen_algorithm.scratch_size()),\n                    &profile_result)\n                .ok()",
          "new_text": null,
          "old_line_content": "            stream",
          "new_line_content": "                ->ThenConvolveBackwardDataWithAlgorithm(",
          "content_same": false
        },
        {
          "line": 215,
          "old_api": "launcher",
          "new_api": null,
          "old_text": "launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,\n             row_dilation, col_dilation, row_stride, col_stride, padding,\n             explicit_paddings, in_backprop, data_format)",
          "new_text": null,
          "old_line_content": "    launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,",
          "new_line_content": "             row_dilation, col_dilation, row_stride, col_stride, padding,",
          "content_same": false
        },
        {
          "line": 1243,
          "old_api": "scratch_size",
          "new_api": null,
          "old_text": "miopen_algorithm.scratch_size()",
          "new_text": null,
          "old_line_content": "                                    miopen_algorithm.scratch_size()),",
          "new_line_content": "                    &profile_result)",
          "content_same": false
        },
        {
          "line": 739,
          "old_api": "static_cast<int>(dims.batch_size)",
          "new_api": null,
          "old_text": "static_cast<int>(dims.batch_size)",
          "new_text": null,
          "old_line_content": "                     static_cast<int>(dims.batch_size) - image_id);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1255,
          "old_api": "elapsed_time_in_ms",
          "new_api": null,
          "old_text": "profile_result.elapsed_time_in_ms()",
          "new_text": null,
          "old_line_content": "              absl::Milliseconds(profile_result.elapsed_time_in_ms()));",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 233,
          "old_api": "launcher",
          "new_api": null,
          "old_text": "launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,\n             row_dilation, col_dilation, row_stride, col_stride, padding,\n             explicit_paddings, in_backprop, data_format)",
          "new_text": null,
          "old_line_content": "    launcher(ctx, use_cudnn, cudnn_use_autotune, out_backprop, filter,",
          "new_line_content": "             row_dilation, col_dilation, row_stride, col_stride, padding,",
          "content_same": false
        },
        {
          "line": 1260,
          "old_api": "parent",
          "new_api": null,
          "old_text": "LogConvAutotuneResults(\n        se::dnn::ConvolutionKind::BACKWARD_DATA, se::dnn::ToDataType<T>::value,\n        in_backprop_ptr, filter_ptr, out_backprop_ptr, input_desc, filter_desc,\n        output_desc, conv_desc, stream->parent(), results)",
          "new_text": null,
          "old_line_content": "    LogConvAutotuneResults(",
          "new_line_content": "        se::dnn::ConvolutionKind::BACKWARD_DATA, se::dnn::ToDataType<T>::value,",
          "content_same": false
        },
        {
          "line": 751,
          "old_api": "Conv2DCustomBackpropInputMatMulFunctor<T>()(\n                context, out_data, filter_data, filter_total_size,\n                output_image_size, dims.out_depth, im2col_buf)",
          "new_api": null,
          "old_text": "Conv2DCustomBackpropInputMatMulFunctor<T>()(\n                context, out_data, filter_data, filter_total_size,\n                output_image_size, dims.out_depth, im2col_buf)",
          "new_text": null,
          "old_line_content": "            Conv2DCustomBackpropInputMatMulFunctor<T>()(",
          "new_line_content": "                context, out_data, filter_data, filter_total_size,",
          "content_same": false
        },
        {
          "line": 1265,
          "old_api": "Insert",
          "new_api": null,
          "old_text": "AutoTuneConvBwdData::GetInstance()->Insert(conv_parameters,\n                                               algorithm_config)",
          "new_text": null,
          "old_line_content": "    AutoTuneConvBwdData::GetInstance()->Insert(conv_parameters,",
          "new_line_content": "                                               algorithm_config);",
          "content_same": false
        },
        {
          "line": 755,
          "old_api": "Col2im<T>(im2col_buf, dims.in_depth,\n                      dims.spatial_dims[0].input_size,\n                      dims.spatial_dims[1].input_size,\n                      dims.spatial_dims[0].filter_size,\n                      dims.spatial_dims[1].filter_size, pad_top, pad_left,\n                      pad_bottom, pad_right, dims.spatial_dims[0].stride,\n                      dims.spatial_dims[1].stride, input_data)",
          "new_api": null,
          "old_text": "Col2im<T>(im2col_buf, dims.in_depth,\n                      dims.spatial_dims[0].input_size,\n                      dims.spatial_dims[1].input_size,\n                      dims.spatial_dims[0].filter_size,\n                      dims.spatial_dims[1].filter_size, pad_top, pad_left,\n                      pad_bottom, pad_right, dims.spatial_dims[0].stride,\n                      dims.spatial_dims[1].stride, input_data)",
          "new_text": null,
          "old_line_content": "            Col2im<T>(im2col_buf, dims.in_depth,",
          "new_line_content": "                      dims.spatial_dims[0].input_size,",
          "content_same": false
        },
        {
          "line": 1269,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n          ->ThenConvolveBackwardDataWithAlgorithm(\n              filter_desc, filter_ptr, output_desc, out_backprop_ptr, conv_desc,\n              input_desc, &in_backprop_ptr, &scratch_allocator,\n              algorithm_config, nullptr)\n          .ok()",
          "new_text": null,
          "old_line_content": "      stream",
          "new_line_content": "          ->ThenConvolveBackwardDataWithAlgorithm(",
          "content_same": false
        },
        {
          "line": 764,
          "old_api": "Shard",
          "new_api": null,
          "old_text": "Shard(worker_threads.num_threads, worker_threads.workers, shard_limit,\n              work_unit_size, shard)",
          "new_text": null,
          "old_line_content": "        Shard(worker_threads.num_threads, worker_threads.workers, shard_limit,",
          "new_line_content": "              work_unit_size, shard);",
          "content_same": false
        },
        {
          "line": 1277,
          "old_api": "DebugString",
          "new_api": null,
          "old_text": "errors::Internal(\n        \"DNN Backward Data function launch failure : input shape(\",\n        input_shape.DebugString(), \") filter shape(\",\n        filter_shape.DebugString(), \")\")",
          "new_text": null,
          "old_line_content": "    ctx->SetStatus(errors::Internal(",
          "new_line_content": "        \"DNN Backward Data function launch failure : input shape(\",",
          "content_same": false
        },
        {
          "line": 1280,
          "old_api": "DebugString",
          "new_api": null,
          "old_text": "filter_shape.DebugString()",
          "new_text": null,
          "old_line_content": "        filter_shape.DebugString(), \")\"));",
          "new_line_content": "    return;",
          "content_same": false
        },
        {
          "line": 1287,
          "old_api": "allocate_temp",
          "new_api": null,
          "old_text": "ctx->allocate_temp(\n                 DataTypeToEnum<T>::value,\n                 ShapeFromFormat(compute_data_format,\n                                 GetTensorDim(input_shape, data_format, 'N'),\n                                 GetTensorDim(input_shape, data_format, 'H'),\n                                 GetTensorDim(input_shape, data_format, 'W'),\n                                 GetTensorDim(input_shape, data_format, 'C')),\n                 &in_backprop_remove_padding)",
          "new_text": null,
          "old_line_content": "        ctx, ctx->allocate_temp(",
          "new_line_content": "                 DataTypeToEnum<T>::value,",
          "content_same": false
        },
        {
          "line": 267,
          "old_api": "dimension",
          "new_api": null,
          "old_text": "kernel.dimension(1)",
          "new_text": null,
          "old_line_content": "    auto filter_cols = kernel.dimension(1);",
          "new_line_content": "    auto num_threads =",
          "content_same": false
        },
        {
          "line": 269,
          "old_api": "device",
          "new_api": null,
          "old_text": "context->device()->tensorflow_cpu_worker_threads()",
          "new_text": null,
          "old_line_content": "        context->device()->tensorflow_cpu_worker_threads()->num_threads;",
          "new_line_content": "    // See libxsmm_dnn.h for this struct definition.",
          "content_same": false
        },
        {
          "line": 1293,
          "old_api": "GetTensorDim",
          "new_api": null,
          "old_text": "GetTensorDim(input_shape, data_format, 'C')",
          "new_text": null,
          "old_line_content": "                                 GetTensorDim(input_shape, data_format, 'C')),",
          "new_line_content": "                 &in_backprop_remove_padding));",
          "content_same": false
        },
        {
          "line": 1303,
          "old_api": "const_cast<const Tensor&>(pre_transformed_in_backprop)\n                    .tensor<T, 4>()",
          "new_api": null,
          "old_text": "const_cast<const Tensor&>(pre_transformed_in_backprop)\n                    .tensor<T, 4>()",
          "new_text": null,
          "old_line_content": "        To32Bit(const_cast<const Tensor&>(pre_transformed_in_backprop)",
          "new_line_content": "                    .tensor<T, 4>()),",
          "content_same": false
        },
        {
          "line": 1308,
          "old_api": "in_backprop_remove_padding.tensor<T, 4>()",
          "new_api": null,
          "old_text": "in_backprop_remove_padding.tensor<T, 4>()",
          "new_text": null,
          "old_line_content": "        To32Bit(in_backprop_remove_padding.tensor<T, 4>()),",
          "new_line_content": "        compute_data_format);",
          "content_same": false
        },
        {
          "line": 1315,
          "old_api": "VLOG",
          "new_api": null,
          "old_text": "VLOG(4)",
          "new_text": null,
          "old_line_content": "    VLOG(4) << \"Convert the output tensor back from NCHW to NHWC.\";",
          "new_line_content": "    auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };",
          "content_same": false
        },
        {
          "line": 804,
          "old_api": "TF_CALL_int32",
          "new_api": null,
          "old_text": "TF_CALL_int32(REGISTER_CPU_KERNELS)",
          "new_text": null,
          "old_line_content": "TF_CALL_int32(REGISTER_CPU_KERNELS);",
          "new_line_content": "#undef REGISTER_CPU_KERNELS",
          "content_same": false
        },
        {
          "line": 1320,
          "old_api": "in_backprop->tensor<T, 4>()",
          "new_api": null,
          "old_text": "in_backprop->tensor<T, 4>()",
          "new_text": null,
          "old_line_content": "        in_backprop->tensor<T, 4>());",
          "new_line_content": "  } else {",
          "content_same": false
        },
        {
          "line": 298,
          "old_api": "data",
          "new_api": null,
          "old_text": "output_backward.data()",
          "new_text": null,
          "old_line_content": "    auto output_ptr = output_backward.data();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 300,
          "old_api": "functor::XsmmBkwInputConv2D<CPUDevice, float>()(\n        context, desc, input_ptr, filter_ptr, output_ptr)",
          "new_api": null,
          "old_text": "functor::XsmmBkwInputConv2D<CPUDevice, float>()(\n        context, desc, input_ptr, filter_ptr, output_ptr)",
          "new_text": null,
          "old_line_content": "    bool success = functor::XsmmBkwInputConv2D<CPUDevice, float>()(",
          "new_line_content": "        context, desc, input_ptr, filter_ptr, output_ptr);",
          "content_same": false
        },
        {
          "line": 1344,
          "old_api": "DECLARE_GPU_SPEC",
          "new_api": null,
          "old_text": "DECLARE_GPU_SPEC(Eigen::half)",
          "new_text": null,
          "old_line_content": "DECLARE_GPU_SPEC(Eigen::half);",
          "new_line_content": "DECLARE_GPU_SPEC(double);",
          "content_same": false
        },
        {
          "line": 323,
          "old_api": "transpose",
          "new_api": null,
          "old_text": "B.transpose()",
          "new_text": null,
          "old_line_content": "    C.noalias() = A * B.transpose();",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 839,
          "old_api": "GetTensorDimIndex",
          "new_api": null,
          "old_text": "GetTensorDimIndex(data_format, 'W')",
          "new_text": null,
          "old_line_content": "  auto input_w = GetTensorDimIndex(data_format, 'W');",
          "new_line_content": "  strides[input_h] = row_stride;",
          "content_same": false
        },
        {
          "line": 844,
          "old_api": "shape",
          "new_api": null,
          "old_text": "in_backprop->shape()",
          "new_text": null,
          "old_line_content": "  TensorShape input_shape = in_backprop->shape();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 846,
          "old_api": "shape",
          "new_api": null,
          "old_text": "filter.shape()",
          "new_text": null,
          "old_line_content": "  const TensorShape& filter_shape = filter.shape();",
          "new_line_content": "  ConvBackpropDimensions dims;",
          "content_same": false
        },
        {
          "line": 849,
          "old_api": "shape",
          "new_api": null,
          "old_text": "ConvBackpropComputeDimensionsV2(\n               \"Conv2DSlowBackpropInput\", /*num_spatial_dims=*/2, input_shape,\n               filter_shape, out_backprop.shape(), dilations, strides, padding,\n               explicit_paddings, data_format, &dims)",
          "new_text": null,
          "old_line_content": "      ctx, ConvBackpropComputeDimensionsV2(",
          "new_line_content": "               \"Conv2DSlowBackpropInput\", /*num_spatial_dims=*/2, input_shape,",
          "content_same": false
        },
        {
          "line": 851,
          "old_api": "shape",
          "new_api": null,
          "old_text": "out_backprop.shape()",
          "new_text": null,
          "old_line_content": "               filter_shape, out_backprop.shape(), dilations, strides, padding,",
          "new_line_content": "               explicit_paddings, data_format, &dims));",
          "content_same": false
        },
        {
          "line": 857,
          "old_api": "GetExplicitPaddingForDim",
          "new_api": null,
          "old_text": "GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &padding_top,\n                             &padding_bottom)",
          "new_text": null,
          "old_line_content": "    GetExplicitPaddingForDim(explicit_paddings, data_format, 'H', &padding_top,",
          "new_line_content": "                             &padding_bottom);",
          "content_same": false
        },
        {
          "line": 859,
          "old_api": "GetExplicitPaddingForDim",
          "new_api": null,
          "old_text": "GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &padding_left,\n                             &padding_right)",
          "new_text": null,
          "old_line_content": "    GetExplicitPaddingForDim(explicit_paddings, data_format, 'W', &padding_left,",
          "new_line_content": "                             &padding_right);",
          "content_same": false
        },
        {
          "line": 1371,
          "old_api": "Device",
          "new_api": null,
          "old_text": "Name(\"Conv2DBackpropInput\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<double>(\"T\")\n                            .HostMemory(\"input_sizes\")",
          "new_text": null,
          "old_line_content": "REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "new_line_content": "                            .Device(DEVICE_GPU)",
          "content_same": false
        },
        {
          "line": 1376,
          "old_api": "Device",
          "new_api": null,
          "old_text": "Name(\"Conv2DBackpropInput\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\")\n                            .HostMemory(\"input_sizes\")",
          "new_text": null,
          "old_line_content": "REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "new_line_content": "                            .Device(DEVICE_GPU)",
          "content_same": false
        },
        {
          "line": 865,
          "old_api": "GetWindowedOutputSizeVerboseV2",
          "new_api": null,
          "old_text": "GetWindowedOutputSizeVerboseV2(\n      dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\n      row_dilation, row_stride, padding, &expected_out_rows, &padding_top,\n      &padding_bottom)",
          "new_text": null,
          "old_line_content": "  TF_CHECK_OK(GetWindowedOutputSizeVerboseV2(",
          "new_line_content": "      dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,",
          "content_same": false
        },
        {
          "line": 1381,
          "old_api": "Device",
          "new_api": null,
          "old_text": "Name(\"Conv2DBackpropInput\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\")\n                            .HostMemory(\"input_sizes\")",
          "new_text": null,
          "old_line_content": "REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "new_line_content": "                            .Device(DEVICE_GPU)",
          "content_same": false
        },
        {
          "line": 870,
          "old_api": "GetWindowedOutputSizeVerboseV2",
          "new_api": null,
          "old_text": "GetWindowedOutputSizeVerboseV2(\n      dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\n      col_dilation, col_stride, padding, &expected_out_cols, &padding_left,\n      &padding_right)",
          "new_text": null,
          "old_line_content": "  TF_CHECK_OK(GetWindowedOutputSizeVerboseV2(",
          "new_line_content": "      dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,",
          "content_same": false
        },
        {
          "line": 874,
          "old_api": "DCHECK_EQ",
          "new_api": null,
          "old_text": "DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols)",
          "new_text": null,
          "old_line_content": "  DCHECK_EQ(dims.spatial_dims[1].output_size, expected_out_cols);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1386,
          "old_api": "Device",
          "new_api": null,
          "old_text": "Name(\"Conv2DBackpropInput\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"T\")\n                            .HostMemory(\"input_sizes\")",
          "new_text": null,
          "old_line_content": "REGISTER_KERNEL_BUILDER(Name(\"Conv2DBackpropInput\")",
          "new_line_content": "                            .Device(DEVICE_GPU)",
          "content_same": false
        },
        {
          "line": 877,
          "old_api": "errors::Internal(\"No GPU stream available.\")",
          "new_api": null,
          "old_text": "errors::Internal(\"No GPU stream available.\")",
          "new_text": null,
          "old_line_content": "  OP_REQUIRES(ctx, stream, errors::Internal(\"No GPU stream available.\"));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 367,
          "old_api": "ANNOTATE_MEMORY_IS_INITIALIZED",
          "new_api": null,
          "old_text": "ANNOTATE_MEMORY_IS_INITIALIZED(\n        im2col_buf, filter_total_size * output_image_size * sizeof(T))",
          "new_text": null,
          "old_line_content": "    ANNOTATE_MEMORY_IS_INITIALIZED(",
          "new_line_content": "        im2col_buf, filter_total_size * output_image_size * sizeof(T));",
          "content_same": false
        },
        {
          "line": 880,
          "old_api": "errors::Unimplemented(\n        \"Conv2DBackpropInput for GPU is not currently supported \"\n        \"without cudnn\")",
          "new_api": null,
          "old_text": "errors::Unimplemented(\n        \"Conv2DBackpropInput for GPU is not currently supported \"\n        \"without cudnn\")",
          "new_text": null,
          "old_line_content": "    ctx->SetStatus(errors::Unimplemented(",
          "new_line_content": "        \"Conv2DBackpropInput for GPU is not currently supported \"",
          "content_same": false
        },
        {
          "line": 371,
          "old_api": "mkldnn_sgemm",
          "new_api": null,
          "old_text": "mkldnn_sgemm(&transposeA, &transposeB, &m, &n, &k, &alpha, filter_data,\n                     &ldA, out_data, &ldB, &beta, im2col_buf, &ldC)",
          "new_text": null,
          "old_line_content": "        mkldnn_sgemm(&transposeA, &transposeB, &m, &n, &k, &alpha, filter_data,",
          "new_line_content": "                     &ldA, out_data, &ldB, &beta, im2col_buf, &ldC);",
          "content_same": false
        },
        {
          "line": 374,
          "old_api": "OP_REQUIRES",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n        ctx, st == 0,\n        errors::Internal(\"Failed to call mkldnn_sgemm. Error code: \", st))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "        ctx, st == 0,",
          "content_same": false
        },
        {
          "line": 376,
          "old_api": "errors::Internal(\"Failed to call mkldnn_sgemm. Error code: \", st)",
          "new_api": null,
          "old_text": "errors::Internal(\"Failed to call mkldnn_sgemm. Error code: \", st)",
          "new_text": null,
          "old_line_content": "        errors::Internal(\"Failed to call mkldnn_sgemm. Error code: \", st));",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 890,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "filter_shape.dim_size(2)",
          "new_text": null,
          "old_line_content": "  bool is_grouped_convolution = filter_shape.dim_size(2) != dims.in_depth;",
          "new_line_content": "  if (dims.spatial_dims[0].filter_size == 1 &&",
          "content_same": false
        },
        {
          "line": 384,
          "old_api": "explicit",
          "new_api": null,
          "old_text": "explicit",
          "new_text": null,
          "old_line_content": "  explicit Conv2DBackpropInputOp(OpKernelConstruction* context)",
          "new_line_content": "      : OpKernel(context) {",
          "content_same": false
        },
        {
          "line": 389,
          "old_api": "errors::InvalidArgument(\"Invalid data format\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Invalid data format\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(\"Invalid data format\"));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 393,
          "old_api": "errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(\"Sliding window strides field must \"",
          "new_line_content": "                                        \"specify 4 dimensions\"));",
          "content_same": false
        },
        {
          "line": 906,
          "old_api": "size",
          "new_api": null,
          "old_text": "in_backprop->template flat<T>().size()",
          "new_text": null,
          "old_line_content": "                                in_backprop->template flat<T>().size());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 399,
          "old_api": "OP_REQUIRES",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n        context, (stride_n == 1 && stride_c == 1),\n        errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\"))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "        context, (stride_n == 1 && stride_c == 1),",
          "content_same": false
        },
        {
          "line": 912,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n            ->ThenBlasGemm(transpose, no_transpose, n, m, k, 1.0f, b_ptr, k,\n                           a_ptr, k, 0.0f, &c_ptr, n)\n            .ok()",
          "new_text": null,
          "old_line_content": "        stream",
          "new_line_content": "            ->ThenBlasGemm(transpose, no_transpose, n, m, k, 1.0f, b_ptr, k,",
          "content_same": false
        },
        {
          "line": 401,
          "old_api": "errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\")",
          "new_text": null,
          "old_line_content": "        errors::InvalidArgument(\"Current implementation does not yet support \"",
          "new_line_content": "                                \"strides in the batch and depth dimensions.\"));",
          "content_same": false
        },
        {
          "line": 404,
          "old_api": "errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\n                    \"Row and column strides should be larger than 0.\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(",
          "new_line_content": "                    \"Row and column strides should be larger than 0.\"));",
          "content_same": false
        },
        {
          "line": 917,
          "old_api": "errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                      \", n=\", n, \", k=\", k)",
          "new_api": null,
          "old_text": "errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                      \", n=\", n, \", k=\", k)",
          "new_text": null,
          "old_line_content": "      ctx->SetStatus(errors::Internal(\"Blas SGEMM launch failed : m=\", m,",
          "new_line_content": "                                      \", n=\", n, \", k=\", k));",
          "content_same": false
        },
        {
          "line": 409,
          "old_api": "errors::InvalidArgument(\"Sliding window dilations field must \"\n                                        \"specify 4 dimensions\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Sliding window dilations field must \"\n                                        \"specify 4 dimensions\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(\"Sliding window dilations field must \"",
          "new_line_content": "                                        \"specify 4 dimensions\"));",
          "content_same": false
        },
        {
          "line": 416,
          "old_api": "errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\n                    \"Current implementation does not yet support \"\n                    \"dilations in the batch and depth dimensions.\")",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(",
          "new_line_content": "                    \"Current implementation does not yet support \"",
          "content_same": false
        },
        {
          "line": 419,
          "old_api": "OP_REQUIRES",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n        context, dilation_h > 0 && dilation_w > 0,\n        errors::InvalidArgument(\"Dilated rates should be larger than 0.\"))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "        context, dilation_h > 0 && dilation_w > 0,",
          "content_same": false
        },
        {
          "line": 421,
          "old_api": "errors::InvalidArgument(\"Dilated rates should be larger than 0.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Dilated rates should be larger than 0.\")",
          "new_text": null,
          "old_line_content": "        errors::InvalidArgument(\"Dilated rates should be larger than 0.\"));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 426,
          "old_api": "CheckValidPadding",
          "new_api": null,
          "old_text": "CheckValidPadding(padding_, explicit_paddings_,\n                                              /*num_dims=*/4, data_format_)",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(context, CheckValidPadding(padding_, explicit_paddings_,",
          "new_line_content": "                                              /*num_dims=*/4, data_format_));",
          "content_same": false
        },
        {
          "line": 939,
          "old_api": "size",
          "new_api": null,
          "old_text": "in_backprop->template flat<T>().size()",
          "new_text": null,
          "old_line_content": "                                in_backprop->template flat<T>().size());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 431,
          "old_api": "CudnnUseAutotune",
          "new_api": null,
          "old_text": "CudnnUseAutotune()",
          "new_text": null,
          "old_line_content": "    cudnn_use_autotune_ = CudnnUseAutotune();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 945,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n            ->ThenBlasGemm(transpose, no_transpose, n, m, k, 1.0f, b_ptr, k,\n                           a_ptr, k, 0.0f, &c_ptr, n)\n            .ok()",
          "new_text": null,
          "old_line_content": "        stream",
          "new_line_content": "            ->ThenBlasGemm(transpose, no_transpose, n, m, k, 1.0f, b_ptr, k,",
          "content_same": false
        },
        {
          "line": 435,
          "old_api": "OP_REQUIRES",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\"Conv2DBackpropInputOp [CPU or GPU(int32)] \"\n                                  \"only supports NHWC data format.\"))",
          "new_text": null,
          "old_line_content": "      OP_REQUIRES(",
          "new_line_content": "          context, data_format_ == FORMAT_NHWC,",
          "content_same": false
        },
        {
          "line": 437,
          "old_api": "errors::InvalidArgument(\"Conv2DBackpropInputOp [CPU or GPU(int32)] \"\n                                  \"only supports NHWC data format.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"Conv2DBackpropInputOp [CPU or GPU(int32)] \"\n                                  \"only supports NHWC data format.\")",
          "new_text": null,
          "old_line_content": "          errors::InvalidArgument(\"Conv2DBackpropInputOp [CPU or GPU(int32)] \"",
          "new_line_content": "                                  \"only supports NHWC data format.\"));",
          "content_same": false
        },
        {
          "line": 950,
          "old_api": "errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                      \", n=\", n, \", k=\", k)",
          "new_api": null,
          "old_text": "errors::Internal(\"Blas SGEMM launch failed : m=\", m,\n                                      \", n=\", n, \", k=\", k)",
          "new_text": null,
          "old_line_content": "      ctx->SetStatus(errors::Internal(\"Blas SGEMM launch failed : m=\", m,",
          "new_line_content": "                                      \", n=\", n, \", k=\", k));",
          "content_same": false
        },
        {
          "line": 441,
          "old_api": "OP_REQUIRES",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n          context, (dilation_h == 1 && dilation_w == 1),\n          errors::InvalidArgument(\n              \"Conv2DBackpropInputOp [CPU or GPU(int32)] not yet support \"\n              \"dilation rates larger than 1.\"))",
          "new_text": null,
          "old_line_content": "      OP_REQUIRES(",
          "new_line_content": "          context, (dilation_h == 1 && dilation_w == 1),",
          "content_same": false
        },
        {
          "line": 443,
          "old_api": "errors::InvalidArgument(\n              \"Conv2DBackpropInputOp [CPU or GPU(int32)] not yet support \"\n              \"dilation rates larger than 1.\")",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\n              \"Conv2DBackpropInputOp [CPU or GPU(int32)] not yet support \"\n              \"dilation rates larger than 1.\")",
          "new_text": null,
          "old_line_content": "          errors::InvalidArgument(",
          "new_line_content": "              \"Conv2DBackpropInputOp [CPU or GPU(int32)] not yet support \"",
          "content_same": false
        },
        {
          "line": 957,
          "old_api": "std::min(padding_left, padding_right)",
          "new_api": null,
          "old_text": "std::min(padding_left, padding_right)",
          "new_text": null,
          "old_line_content": "  const int64 common_padding_cols = std::min(padding_left, padding_right);",
          "new_line_content": "  TensorShape compatible_input_shape;",
          "content_same": false
        },
        {
          "line": 452,
          "old_api": "input",
          "new_api": null,
          "old_text": "context->input(2)",
          "new_text": null,
          "old_line_content": "    const Tensor& out_backprop = context->input(2);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 964,
          "old_api": "std::abs(padding_right - padding_left)",
          "new_api": null,
          "old_text": "std::abs(padding_right - padding_left)",
          "new_text": null,
          "old_line_content": "    const int64 padding_cols_diff = std::abs(padding_right - padding_left);",
          "new_line_content": "    const int64 new_in_rows =",
          "content_same": false
        },
        {
          "line": 457,
          "old_api": "shape",
          "new_api": null,
          "old_text": "out_backprop.shape()",
          "new_text": null,
          "old_line_content": "                                                   out_backprop.shape(),",
          "new_line_content": "                                                   data_format_, &input_shape));",
          "content_same": false
        },
        {
          "line": 969,
          "old_api": "ShapeFromFormat",
          "new_api": null,
          "old_text": "ShapeFromFormat(\n        data_format, dims.batch_size, new_in_rows, new_in_cols, dims.in_depth)",
          "new_text": null,
          "old_line_content": "    compatible_input_shape = ShapeFromFormat(",
          "new_line_content": "        data_format, dims.batch_size, new_in_rows, new_in_cols, dims.in_depth);",
          "content_same": false
        },
        {
          "line": 462,
          "old_api": "allocate_output",
          "new_api": null,
          "old_text": "context->allocate_output(0, input_shape, &in_backprop)",
          "new_text": null,
          "old_line_content": "                   context->allocate_output(0, input_shape, &in_backprop));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 975,
          "old_api": "CHECK",
          "new_api": null,
          "old_text": "CHECK(common_padding_rows >= 0 && common_padding_cols >= 0)",
          "new_text": null,
          "old_line_content": "  CHECK(common_padding_rows >= 0 && common_padding_cols >= 0)  // Crash OK",
          "new_line_content": "      << \"Negative row or col paddings: (\" << common_padding_rows << \", \"",
          "content_same": false
        },
        {
          "line": 465,
          "old_api": "num_elements",
          "new_api": null,
          "old_text": "input_shape.num_elements()",
          "new_text": null,
          "old_line_content": "    if (input_shape.num_elements() == 0) {",
          "new_line_content": "      return;",
          "content_same": false
        },
        {
          "line": 983,
          "old_api": "parent",
          "new_api": null,
          "old_text": "stream->parent()",
          "new_text": null,
          "old_line_content": "      DataTypeToEnum<T>::value == DT_HALF && IsVoltaOrLater(*stream->parent());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 474,
          "old_api": "GetTensorDim",
          "new_api": null,
          "old_text": "GetTensorDim(dilations_, data_format_, 'W')",
          "new_text": null,
          "old_line_content": "    const int dilation_cols = GetTensorDim(dilations_, data_format_, 'W');",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 479,
          "old_api": "shape",
          "new_api": null,
          "old_text": "out_backprop.shape().DebugString()",
          "new_text": null,
          "old_line_content": "            << \" out_backprop: \" << out_backprop.shape().DebugString()",
          "new_line_content": "            << \" strides: [\" << stride_rows << \", \" << stride_cols << \"]\"",
          "content_same": false
        },
        {
          "line": 994,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "ToString(compute_data_format)",
          "new_text": null,
          "old_line_content": "          << \" compute_data_format=\" << ToString(compute_data_format);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 484,
          "old_api": "launch",
          "new_api": null,
          "old_text": "launch(context, use_cudnn_, cudnn_use_autotune_, out_backprop, filter,\n           dilation_rows, dilation_cols, stride_rows, stride_cols, padding_,\n           explicit_paddings_, in_backprop, data_format_)",
          "new_text": null,
          "old_line_content": "    launch(context, use_cudnn_, cudnn_use_autotune_, out_backprop, filter,",
          "new_line_content": "           dilation_rows, dilation_cols, stride_rows, stride_cols, padding_,",
          "content_same": false
        },
        {
          "line": 997,
          "old_api": "std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,\n                      se::dnn::FilterLayout::kOutputYXInput)",
          "new_api": null,
          "old_text": "std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,\n                      se::dnn::FilterLayout::kOutputYXInput)",
          "new_text": null,
          "old_line_content": "      std::make_tuple(se::dnn::DataLayout::kBatchYXDepth,",
          "new_line_content": "                      se::dnn::FilterLayout::kOutputYXInput);",
          "content_same": false
        },
        {
          "line": 1000,
          "old_api": "std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,\n                      se::dnn::FilterLayout::kOutputInputYX)",
          "new_api": null,
          "old_text": "std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,\n                      se::dnn::FilterLayout::kOutputInputYX)",
          "new_text": null,
          "old_line_content": "      std::make_tuple(se::dnn::DataLayout::kBatchDepthYX,",
          "new_line_content": "                      se::dnn::FilterLayout::kOutputInputYX);",
          "content_same": false
        },
        {
          "line": 1006,
          "old_api": "std::tie(compute_data_layout, filter_layout)",
          "new_api": null,
          "old_text": "std::tie(compute_data_layout, filter_layout)",
          "new_text": null,
          "old_line_content": "  std::tie(compute_data_layout, filter_layout) =",
          "new_line_content": "      compute_data_format == FORMAT_NHWC ? kComputeInNHWC : kComputeInNCHW;",
          "content_same": false
        },
        {
          "line": 1012,
          "old_api": "GetTensorDim",
          "new_api": null,
          "old_text": "GetTensorDim(compatible_input_shape, data_format, 'W')",
          "new_text": null,
          "old_line_content": "      .set_width(GetTensorDim(compatible_input_shape, data_format, 'W'))",
          "new_line_content": "      .set_feature_map_count(dims.in_depth)",
          "content_same": false
        },
        {
          "line": 1016,
          "old_api": "set_count",
          "new_api": null,
          "old_text": "output_desc.set_count(dims.batch_size)\n      .set_height(dims.spatial_dims[0].output_size)\n      .set_width(dims.spatial_dims[1].output_size)\n      .set_feature_map_count(dims.out_depth)\n      .set_layout(compute_data_layout)",
          "new_text": null,
          "old_line_content": "  output_desc.set_count(dims.batch_size)",
          "new_line_content": "      .set_height(dims.spatial_dims[0].output_size)",
          "content_same": false
        },
        {
          "line": 506,
          "old_api": "explicit",
          "new_api": null,
          "old_text": "explicit",
          "new_text": null,
          "old_line_content": "  explicit Conv2DCustomBackpropInputOp(OpKernelConstruction* context)",
          "new_line_content": "      : OpKernel(context) {",
          "content_same": false
        },
        {
          "line": 1022,
          "old_api": "set_input_filter_height",
          "new_api": null,
          "old_text": "filter_desc.set_input_filter_height(dims.spatial_dims[0].filter_size)\n      .set_input_filter_width(dims.spatial_dims[1].filter_size)\n      .set_input_feature_map_count(filter_shape.dim_size(2))\n      .set_output_feature_map_count(filter_shape.dim_size(3))\n      .set_layout(filter_layout)",
          "new_text": null,
          "old_line_content": "  filter_desc.set_input_filter_height(dims.spatial_dims[0].filter_size)",
          "new_line_content": "      .set_input_filter_width(dims.spatial_dims[1].filter_size)",
          "content_same": false
        }
      ]
    },
    "api_summary": {
      "total_replacements": 111,
      "total_additions": 181,
      "total_deletions": 181,
      "total_api_changes": 473
    },
    "non_api_changes": {
      "has_non_api_changes": true,
      "evidence": {
        "total_diff_lines": 2,
        "api_related_lines": 473,
        "non_api_lines": 2,
        "non_api_line_numbers": [
          90,
          79
        ]
      }
    },
    "api_calls_before": 531,
    "api_calls_after": 531,
    "diff_info": {
      "added_lines": 1,
      "removed_lines": 2,
      "total_diff_lines": 22
    }
  }
}