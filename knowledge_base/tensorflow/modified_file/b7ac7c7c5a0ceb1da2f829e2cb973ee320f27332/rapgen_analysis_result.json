{
  "status": "completed",
  "commit_dir": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/b7ac7c7c5a0ceb1da2f829e2cb973ee320f27332",
  "file_info": {
    "before_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/b7ac7c7c5a0ceb1da2f829e2cb973ee320f27332/before.cc",
    "after_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/b7ac7c7c5a0ceb1da2f829e2cb973ee320f27332/after.cc",
    "diff_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/b7ac7c7c5a0ceb1da2f829e2cb973ee320f27332/diff.txt"
  },
  "semgrep_analysis": {
    "is_api_change": false,
    "api_changes": {
      "replacements": [
        {
          "line": 4786,
          "old_api": "getFusionResults",
          "new_api": "size",
          "old_text": "fusion.getFusionResults()",
          "new_text": "fusion_results.size()",
          "old_line_content": "  auto fusion_results = ToStdVector(fusion.getFusionResults());",
          "new_line_content": "  if (fusion_results.size() == 1) {",
          "content_same": false
        },
        {
          "line": 4788,
          "old_api": "size",
          "new_api": "getDefiningOp",
          "old_text": "fusion_results.size()",
          "new_text": "fusion_results[0].getDefiningOp()",
          "old_line_content": "  if (fusion_results.size() == 1) {",
          "new_line_content": "            fusion_results[0].getDefiningOp(), layout_analysis)) {",
          "content_same": false
        },
        {
          "line": 4835,
          "old_api": "getOperand",
          "new_api": "GetReductionKindAndContiguousComponents",
          "old_text": "first_reduce->getOperand(0)",
          "new_text": "GetReductionKindAndContiguousComponents(first_reduce)",
          "old_line_content": "  Shape input_shape = layout_analysis.GetShape(first_reduce->getOperand(0));",
          "new_line_content": "      GetReductionKindAndContiguousComponents(first_reduce);",
          "content_same": false
        },
        {
          "line": 4849,
          "old_api": "getOperand",
          "new_api": "GetHloOperands",
          "old_text": "first_reduce->getOperand(0)",
          "new_text": "GetHloOperands(unnested_hlo)",
          "old_line_content": "  int smallest_input_dtype_bits = get_dtype_bits(first_reduce->getOperand(0));",
          "new_line_content": "  for (mlir::Value operand : GetHloOperands(unnested_hlo)) {",
          "content_same": false
        },
        {
          "line": 4851,
          "old_api": "GetHloOperands",
          "new_api": "get_dtype_bits",
          "old_text": "GetHloOperands(unnested_hlo)",
          "new_text": "get_dtype_bits(operand)",
          "old_line_content": "  for (mlir::Value operand : GetHloOperands(unnested_hlo)) {",
          "new_line_content": "        std::min(get_dtype_bits(operand), smallest_input_dtype_bits);",
          "content_same": false
        },
        {
          "line": 4871,
          "old_api": "std::max(kMinThreadsXRowReduction,\n                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)))",
          "new_api": "std::min(\n          max_block_size,\n          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],\n                                       reduction_tiling[2]),\n                           kWarpSize))",
          "old_text": "std::max(kMinThreadsXRowReduction,\n                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)))",
          "new_text": "std::min(\n          max_block_size,\n          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],\n                                       reduction_tiling[2]),\n                           kWarpSize))",
          "old_line_content": "          std::max(kMinThreadsXRowReduction,",
          "new_line_content": "      return std::min(",
          "content_same": false
        },
        {
          "line": 4873,
          "old_api": "std::min(\n          max_block_size,\n          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],\n                                       reduction_tiling[2]),\n                           kWarpSize))",
          "new_api": "CeilOfRatio",
          "old_text": "std::min(\n          max_block_size,\n          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],\n                                       reduction_tiling[2]),\n                           kWarpSize))",
          "new_text": "CeilOfRatio(reduction_dimensions.dimensions[2],\n                                       reduction_tiling[2])",
          "old_line_content": "      return std::min(",
          "new_line_content": "          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],",
          "content_same": false
        },
        {
          "line": 4944,
          "old_api": "mlir::dyn_cast<mlir::mhlo::ReduceOp>(reduce)",
          "new_api": "opcode",
          "old_text": "mlir::dyn_cast<mlir::mhlo::ReduceOp>(reduce)",
          "new_text": "root->opcode()",
          "old_line_content": "    if (auto nested_reduce = mlir::dyn_cast<mlir::mhlo::ReduceOp>(reduce)) {",
          "new_line_content": "      if (root->opcode() == HloOpcode::kTuple) {",
          "content_same": false
        },
        {
          "line": 4945,
          "old_api": "root_instruction",
          "new_api": "mutable_operand",
          "old_text": "fused_computation->root_instruction()",
          "new_text": "root->mutable_operand(index)",
          "old_line_content": "      HloInstruction* root = fused_computation->root_instruction();",
          "new_line_content": "        root = root->mutable_operand(index);",
          "content_same": false
        },
        {
          "line": 4947,
          "old_api": "mutable_operand",
          "new_api": "CHECK_EQ",
          "old_text": "root->mutable_operand(index)",
          "new_text": "CHECK_EQ(0, index)",
          "old_line_content": "        root = root->mutable_operand(index);",
          "new_line_content": "        CHECK_EQ(0, index);",
          "content_same": false
        },
        {
          "line": 4949,
          "old_api": "CHECK_EQ",
          "new_api": "to_apply",
          "old_text": "CHECK_EQ(0, index)",
          "new_text": "root->to_apply()",
          "old_line_content": "        CHECK_EQ(0, index);",
          "new_line_content": "      reducers.push_back(root->to_apply());",
          "content_same": false
        },
        {
          "line": 4951,
          "old_api": "to_apply",
          "new_api": "MlirToString",
          "old_text": "root->to_apply()",
          "new_text": "MlirToString(reduce)",
          "old_line_content": "      reducers.push_back(root->to_apply());",
          "new_line_content": "      LOG(FATAL) << \"Unexpected reduce op: \" << MlirToString(reduce);",
          "content_same": false
        },
        {
          "line": 4959,
          "old_api": "GetKernelMappingScheme",
          "new_api": "GetThreadsPerBlock",
          "old_text": "reduction_info->GetKernelMappingScheme()",
          "new_text": "mapping_scheme.GetThreadsPerBlock()",
          "old_line_content": "      reduction_info->GetKernelMappingScheme();",
          "new_line_content": "                                     mapping_scheme.GetThreadsPerBlock());",
          "content_same": false
        },
        {
          "line": 4960,
          "old_api": "GetNumberOfBlocks",
          "new_api": "launch_bound",
          "old_text": "mapping_scheme.GetNumberOfBlocks()",
          "new_text": "GetIndexTypeForKernel(\n      unnested_hlo, launch_dimensions.launch_bound(), &b_)",
          "old_line_content": "  LaunchDimensions launch_dimensions(mapping_scheme.GetNumberOfBlocks(),",
          "new_line_content": "  llvm::Type* index_ty = GetIndexTypeForKernel(",
          "content_same": false
        },
        {
          "line": 4961,
          "old_api": "GetThreadsPerBlock",
          "new_api": "launch_bound",
          "old_text": "mapping_scheme.GetThreadsPerBlock()",
          "new_text": "launch_dimensions.launch_bound()",
          "old_line_content": "                                     mapping_scheme.GetThreadsPerBlock());",
          "new_line_content": "      unnested_hlo, launch_dimensions.launch_bound(), &b_);",
          "content_same": false
        },
        {
          "line": 4962,
          "old_api": "launch_bound",
          "new_api": "EmitPrologueForReduction",
          "old_text": "GetIndexTypeForKernel(\n      unnested_hlo, launch_dimensions.launch_bound(), &b_)",
          "new_text": "EmitPrologueForReduction(unnested_hlo, instr_index_group, fused_computation,\n                           fused_emitter, result_ir_arrays, reduction_info,\n                           layout_analysis)",
          "old_line_content": "  llvm::Type* index_ty = GetIndexTypeForKernel(",
          "new_line_content": "  EmitPrologueForReduction(unnested_hlo, instr_index_group, fused_computation,",
          "content_same": false
        },
        {
          "line": 4996,
          "old_api": "shape",
          "new_api": "operand",
          "old_text": "instr.shape()",
          "new_text": "instr.operand(0)->IsConstant()",
          "old_line_content": "  return instr.IsConstant() || ShapeUtil::IsScalar(instr.shape()) ||",
          "new_line_content": "          (instr.operand(0)->IsConstant() ||",
          "content_same": false
        },
        {
          "line": 4997,
          "old_api": "opcode",
          "new_api": "operand",
          "old_text": "instr.opcode()",
          "new_text": "instr.operand(0)->shape()",
          "old_line_content": "         (HloOpcode::kBroadcast == instr.opcode() &&",
          "new_line_content": "           ShapeUtil::IsScalar(instr.operand(0)->shape())));",
          "content_same": false
        },
        {
          "line": 5029,
          "old_api": "root_instruction",
          "new_api": "IsBroadcastedConstantOrScalar",
          "old_text": "fused_computation->root_instruction()->mutable_operand(oid)",
          "new_text": "IsBroadcastedConstantOrScalar(*instr)",
          "old_line_content": "          fused_computation->root_instruction()->mutable_operand(oid);",
          "new_line_content": "          (IsBroadcastedConstantOrScalar(*instr))) {",
          "content_same": false
        },
        {
          "line": 5038,
          "old_api": "IsReachable",
          "new_api": "ToString",
          "old_text": "reachability_map->IsReachable(instr, reduce)",
          "new_text": "instr->ToString()",
          "old_line_content": "      if (reachability_map->IsReachable(instr, reduce)) {",
          "new_line_content": "                << instr->ToString();",
          "content_same": false
        },
        {
          "line": 5039,
          "old_api": "ToString",
          "new_api": "push_back",
          "old_text": "reduce->ToString()",
          "new_text": "reached_output_ids.push_back(oid)",
          "old_line_content": "        VLOG(3) << \"Reaching \" << reduce->ToString() << \" from \"",
          "new_line_content": "        reached_output_ids.push_back(oid);",
          "content_same": false
        },
        {
          "line": 5066,
          "old_api": "mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "new_api": "getFusionResults",
          "old_text": "mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "new_text": "fusion.getFusionResults().size()",
          "old_line_content": "  auto fusion = mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo);",
          "new_line_content": "  int num_reduces = fusion.getFusionResults().size();",
          "content_same": false
        },
        {
          "line": 5075,
          "old_api": "IsReductionFromOrToContiguousDimensions",
          "new_api": "GetReduceFromUnnestedMlir",
          "old_text": "IsReductionFromOrToContiguousDimensions(output_instruction,\n                                                layout_analysis)",
          "new_text": "GetReduceFromUnnestedMlir(unnested_hlo, i)",
          "old_line_content": "    if (IsReductionFromOrToContiguousDimensions(output_instruction,",
          "new_line_content": "      first_reduce = GetReduceFromUnnestedMlir(unnested_hlo, i);",
          "content_same": false
        },
        {
          "line": 5085,
          "old_api": "GetReduceFromUnnestedMlir",
          "new_api": "IsFusedReductionOutputConsistent",
          "old_text": "GetReduceFromUnnestedMlir(unnested_hlo, i)",
          "new_text": "IsFusedReductionOutputConsistent(\n              candidate, mlir::cast<mlir::mhlo::ReduceOp>(first_reduce),\n              layout_analysis)",
          "old_line_content": "          GetReduceFromUnnestedMlir(unnested_hlo, i));",
          "new_line_content": "          !IsFusedReductionOutputConsistent(",
          "content_same": false
        },
        {
          "line": 5088,
          "old_api": "mlir::cast<mlir::mhlo::ReduceOp>(first_reduce)",
          "new_api": "InternalError",
          "old_text": "mlir::cast<mlir::mhlo::ReduceOp>(first_reduce)",
          "new_text": "InternalError(\"Inconsistent reduction fusion outputs\")",
          "old_line_content": "              candidate, mlir::cast<mlir::mhlo::ReduceOp>(first_reduce),",
          "new_line_content": "        return InternalError(\"Inconsistent reduction fusion outputs\");",
          "content_same": false
        },
        {
          "line": 5097,
          "old_api": "has_layout",
          "new_api": "MlirToString",
          "old_text": "input_shape.has_layout()",
          "new_text": "MlirToString(first_reduce)",
          "old_line_content": "  CHECK(input_shape.has_layout()) << \"LayoutAssignment or InstructionFusion \"",
          "new_line_content": "                                  << MlirToString(first_reduce);",
          "content_same": false
        },
        {
          "line": 5108,
          "old_api": "GroupDisjointReductions",
          "new_api": "size",
          "old_text": "GroupDisjointReductions(fused_computation, num_reduces)",
          "new_text": "instr_index_groups.size()",
          "old_line_content": "      GroupDisjointReductions(fused_computation, num_reduces);",
          "new_line_content": "  VLOG(2) << StrCat(\"Generate in \", instr_index_groups.size(), \" groups for \",",
          "content_same": false
        },
        {
          "line": 5114,
          "old_api": "ComputeReductionCodegenInfo",
          "new_api": "GetKernelMappingScheme",
          "old_text": "ComputeReductionCodegenInfo(unnested_hlo, first_reduce, layout_analysis)",
          "new_text": "reduction_info.GetKernelMappingScheme()",
          "old_line_content": "      ComputeReductionCodegenInfo(unnested_hlo, first_reduce, layout_analysis);",
          "new_line_content": "      reduction_info.GetKernelMappingScheme();",
          "content_same": false
        },
        {
          "line": 5122,
          "old_api": "size",
          "new_api": "GetThreadsPerBlock",
          "old_text": "instr_index_groups.size()",
          "new_text": "mapping_scheme.GetThreadsPerBlock()",
          "old_line_content": "       /*y=*/static_cast<int64_t>(instr_index_groups.size()),",
          "new_line_content": "      {/*x=*/mapping_scheme.GetThreadsPerBlock(), /*y=*/1, /*z=*/1});",
          "content_same": false
        },
        {
          "line": 5124,
          "old_api": "GetThreadsPerBlock",
          "new_api": "getLoc",
          "old_text": "mapping_scheme.GetThreadsPerBlock()",
          "new_text": "unnested_hlo->getLoc()",
          "old_line_content": "      {/*x=*/mapping_scheme.GetThreadsPerBlock(), /*y=*/1, /*z=*/1});",
          "new_line_content": "          << mlir::GetNameFromLoc(unnested_hlo->getLoc())",
          "content_same": false
        },
        {
          "line": 5125,
          "old_api": "VLOG",
          "new_api": "GetNumberOfBlocks",
          "old_text": "VLOG(3)",
          "new_text": "mapping_scheme.GetNumberOfBlocks()",
          "old_line_content": "  VLOG(3) << \"Launch dimensions of \"",
          "new_line_content": "          << \": number of blocks: \" << mapping_scheme.GetNumberOfBlocks()",
          "content_same": false
        },
        {
          "line": 5126,
          "old_api": "getLoc",
          "new_api": "GetThreadsPerBlock",
          "old_text": "unnested_hlo->getLoc()",
          "new_text": "mapping_scheme.GetThreadsPerBlock()",
          "old_line_content": "          << mlir::GetNameFromLoc(unnested_hlo->getLoc())",
          "new_line_content": "          << \" - threads per block: \" << mapping_scheme.GetThreadsPerBlock();",
          "content_same": false
        },
        {
          "line": 5141,
          "old_api": "num_parameters",
          "new_api": "parameter_instruction",
          "old_text": "fused_computation->num_parameters()",
          "new_text": "fused_computation->parameter_instruction(i)",
          "old_line_content": "  for (int i = 0; i < fused_computation->num_parameters(); i++) {",
          "new_line_content": "    HloInstruction* fused_operand = fused_computation->parameter_instruction(i);",
          "content_same": false
        },
        {
          "line": 5171,
          "old_api": "gpu::EmitCallToTargetIntrinsic(\n        gpu::TargetIntrinsicID::kBlockIdy, {}, {}, &b_)",
          "new_api": "size",
          "old_text": "gpu::EmitCallToTargetIntrinsic(\n        gpu::TargetIntrinsicID::kBlockIdy, {}, {}, &b_)",
          "new_text": "instr_index_groups.size()",
          "old_line_content": "    llvm::CallInst* raw_block_id_y = gpu::EmitCallToTargetIntrinsic(",
          "new_line_content": "    llvm_ir::AddRangeMetadata(0, instr_index_groups.size(),",
          "content_same": false
        },
        {
          "line": 5173,
          "old_api": "size",
          "new_api": "StrCat",
          "old_text": "instr_index_groups.size()",
          "new_text": "StrCat(\"reduce-group-\", i)",
          "old_line_content": "    llvm_ir::AddRangeMetadata(0, instr_index_groups.size(),",
          "new_line_content": "    ksl.If(StrCat(\"reduce-group-\", i),",
          "content_same": false
        },
        {
          "line": 5174,
          "old_api": "llvm::cast<llvm::Instruction>(raw_block_id_y)",
          "new_api": "getInt32",
          "old_text": "llvm::cast<llvm::Instruction>(raw_block_id_y)",
          "new_text": "b_.getInt32(i)",
          "old_line_content": "                              llvm::cast<llvm::Instruction>(raw_block_id_y));",
          "new_line_content": "           b_.CreateICmpEQ(raw_block_id_y, b_.getInt32(i)), [&] {",
          "content_same": false
        },
        {
          "line": 5175,
          "old_api": "StrCat",
          "new_api": "EmitIRForReduction",
          "old_text": "StrCat(\"reduce-group-\", i)",
          "new_text": "EmitIRForReduction(unnested_hlo, instr_index_groups[i],\n                                fused_computation, &fused_emitter,\n                                result_ir_arrays, &reduction_info, input_shape,\n                                layout_analysis)",
          "old_line_content": "    ksl.If(StrCat(\"reduce-group-\", i),",
          "new_line_content": "             EmitIRForReduction(unnested_hlo, instr_index_groups[i],",
          "content_same": false
        },
        {
          "line": 5184,
          "old_api": "debug_options",
          "new_api": "InternalError",
          "old_text": "hlo_module_config_.debug_options().xla_gpu_deterministic_reductions()",
          "new_text": "InternalError(\n        \"All reductions should be race-free if deterministic reductions are \"\n        \"enabled\")",
          "old_line_content": "  if (hlo_module_config_.debug_options().xla_gpu_deterministic_reductions() &&",
          "new_line_content": "    return InternalError(",
          "content_same": false
        },
        {
          "line": 5205,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": "std::move(initializer_thunk)",
          "old_text": "TF_ASSIGN_OR_RETURN(std::unique_ptr<Thunk> initializer_thunk,\n                        BuildFusedInitializerThunk(fusion, i))",
          "new_text": "std::move(initializer_thunk)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(std::unique_ptr<Thunk> initializer_thunk,",
          "new_line_content": "    thunks.push_back(std::move(initializer_thunk));",
          "content_same": false
        },
        {
          "line": 5210,
          "old_api": "std::move(kernel_thunk)",
          "new_api": "std::move(thunks)",
          "old_text": "std::move(kernel_thunk)",
          "new_text": "std::move(thunks)",
          "old_line_content": "  thunks.push_back(std::move(kernel_thunk));",
          "new_line_content": "      GetThunkInfo(unnested_hlo), std::move(thunks));",
          "content_same": false
        },
        {
          "line": 5211,
          "old_api": "absl::make_unique<SequentialThunk>(\n      GetThunkInfo(unnested_hlo), std::move(thunks))",
          "new_api": "std::move(sequential_thunk)",
          "old_text": "absl::make_unique<SequentialThunk>(\n      GetThunkInfo(unnested_hlo), std::move(thunks))",
          "new_text": "std::move(sequential_thunk)",
          "old_line_content": "  auto sequential_thunk = absl::make_unique<SequentialThunk>(",
          "new_line_content": "  AddThunkToThunkSequence(std::move(sequential_thunk));",
          "content_same": false
        },
        {
          "line": 5213,
          "old_api": "std::move(sequential_thunk)",
          "new_api": "Status::OK()",
          "old_text": "std::move(sequential_thunk)",
          "new_text": "Status::OK()",
          "old_line_content": "  AddThunkToThunkSequence(std::move(sequential_thunk));",
          "new_line_content": "  return Status::OK();",
          "content_same": false
        },
        {
          "line": 5240,
          "old_api": "ToString",
          "new_api": "root_instruction",
          "old_text": "fused_computation->ToString()",
          "new_text": "fused_computation->root_instruction()",
          "old_line_content": "           << fused_computation->ToString();",
          "new_line_content": "  HloInstruction* slice_or_tuple = fused_computation->root_instruction();",
          "content_same": false
        },
        {
          "line": 5242,
          "old_api": "root_instruction",
          "new_api": "opcode",
          "old_text": "fused_computation->root_instruction()",
          "new_text": "slice_or_tuple->opcode()",
          "old_line_content": "  HloInstruction* slice_or_tuple = fused_computation->root_instruction();",
          "new_line_content": "    if (slice_or_tuple->opcode() == HloOpcode::kSlice) {",
          "content_same": false
        },
        {
          "line": 5243,
          "old_api": "opcode",
          "new_api": "absl::Span<HloInstruction* const>(&slice_or_tuple, 1)",
          "old_text": "() -> absl::Span<HloInstruction* const> {\n    if (slice_or_tuple->opcode() == HloOpcode::kSlice) {\n      return absl::Span<HloInstruction* const>(&slice_or_tuple, 1);\n    }\n    CHECK_EQ(slice_or_tuple->opcode(), HloOpcode::kTuple);\n    return slice_or_tuple->operands();\n  }()",
          "new_text": "absl::Span<HloInstruction* const>(&slice_or_tuple, 1)",
          "old_line_content": "  auto slice_instructions = [&]() -> absl::Span<HloInstruction* const> {",
          "new_line_content": "      return absl::Span<HloInstruction* const>(&slice_or_tuple, 1);",
          "content_same": false
        },
        {
          "line": 5245,
          "old_api": "absl::Span<HloInstruction* const>(&slice_or_tuple, 1)",
          "new_api": "opcode",
          "old_text": "absl::Span<HloInstruction* const>(&slice_or_tuple, 1)",
          "new_text": "slice_or_tuple->opcode()",
          "old_line_content": "      return absl::Span<HloInstruction* const>(&slice_or_tuple, 1);",
          "new_line_content": "    CHECK_EQ(slice_or_tuple->opcode(), HloOpcode::kTuple);",
          "content_same": false
        },
        {
          "line": 5254,
          "old_api": "GetNestedComputer",
          "new_api": "num_parameters",
          "old_text": "GetNestedComputer()",
          "new_text": "fused_computation->num_parameters()",
          "old_line_content": "                                     GetNestedComputer());",
          "new_line_content": "  for (int i = 0; i < fused_computation->num_parameters(); i++) {",
          "content_same": false
        },
        {
          "line": 5256,
          "old_api": "num_parameters",
          "new_api": "parameter_instruction",
          "old_text": "fused_computation->num_parameters()",
          "new_text": "fused_computation->parameter_instruction(i)",
          "old_line_content": "  for (int i = 0; i < fused_computation->num_parameters(); i++) {",
          "new_line_content": "        fused_computation->parameter_instruction(i),",
          "content_same": false
        },
        {
          "line": 5258,
          "old_api": "parameter_instruction",
          "new_api": "EmitReadArrayElement",
          "old_text": "fused_computation->parameter_instruction(i)",
          "new_text": "ir_arrays[i].EmitReadArrayElement(index, &b_)",
          "old_line_content": "        fused_computation->parameter_instruction(i),",
          "new_line_content": "          return ir_arrays[i].EmitReadArrayElement(index, &b_);",
          "content_same": false
        },
        {
          "line": 5275,
          "old_api": "size",
          "new_api": "CreateICmpSGE",
          "old_text": "slice->slice_starts().size()",
          "new_text": "b_.CreateICmpSGE(\n          index.multidim()[dim],\n          index.GetConstantWithIndexType(slice->slice_starts(dim)))",
          "old_line_content": "    for (size_t dim = 0; dim < slice->slice_starts().size(); ++dim) {",
          "new_line_content": "      auto larger_or_equal_than_start = b_.CreateICmpSGE(",
          "content_same": false
        },
        {
          "line": 5276,
          "old_api": "slice_strides",
          "new_api": "multidim",
          "old_text": "slice->slice_strides(dim)",
          "new_text": "index.multidim()",
          "old_line_content": "      CHECK_EQ(slice->slice_strides(dim), 1);",
          "new_line_content": "          index.multidim()[dim],",
          "content_same": false
        },
        {
          "line": 5277,
          "old_api": "CreateICmpSGE",
          "new_api": "slice_starts",
          "old_text": "b_.CreateICmpSGE(\n          index.multidim()[dim],\n          index.GetConstantWithIndexType(slice->slice_starts(dim)))",
          "new_text": "slice->slice_starts(dim)",
          "old_line_content": "      auto larger_or_equal_than_start = b_.CreateICmpSGE(",
          "new_line_content": "          index.GetConstantWithIndexType(slice->slice_starts(dim)));",
          "content_same": false
        },
        {
          "line": 5278,
          "old_api": "multidim",
          "new_api": "CreateICmpSLT",
          "old_text": "index.multidim()",
          "new_text": "b_.CreateICmpSLT(\n          index.multidim()[dim],\n          index.GetConstantWithIndexType(slice->slice_limits(dim)))",
          "old_line_content": "          index.multidim()[dim],",
          "new_line_content": "      llvm::Value* smaller_than_limit = b_.CreateICmpSLT(",
          "content_same": false
        },
        {
          "line": 5279,
          "old_api": "slice_starts",
          "new_api": "multidim",
          "old_text": "slice->slice_starts(dim)",
          "new_text": "index.multidim()",
          "old_line_content": "          index.GetConstantWithIndexType(slice->slice_starts(dim)));",
          "new_line_content": "          index.multidim()[dim],",
          "content_same": false
        },
        {
          "line": 5280,
          "old_api": "CreateICmpSLT",
          "new_api": "slice_limits",
          "old_text": "b_.CreateICmpSLT(\n          index.multidim()[dim],\n          index.GetConstantWithIndexType(slice->slice_limits(dim)))",
          "new_text": "slice->slice_limits(dim)",
          "old_line_content": "      llvm::Value* smaller_than_limit = b_.CreateICmpSLT(",
          "new_line_content": "          index.GetConstantWithIndexType(slice->slice_limits(dim)));",
          "content_same": false
        },
        {
          "line": 5282,
          "old_api": "slice_limits",
          "new_api": "CreateAnd",
          "old_text": "slice->slice_limits(dim)",
          "new_text": "b_.CreateAnd(larger_or_equal_than_start, smaller_than_limit)",
          "old_line_content": "          index.GetConstantWithIndexType(slice->slice_limits(dim)));",
          "new_line_content": "          b_.CreateAnd(larger_or_equal_than_start, smaller_than_limit);",
          "content_same": false
        },
        {
          "line": 5285,
          "old_api": "push_back",
          "new_api": "CreateAnd",
          "old_text": "index_within_ranges.push_back(within_range)",
          "new_text": "b_.CreateAnd(index_within_ranges)",
          "old_line_content": "      index_within_ranges.push_back(within_range);",
          "new_line_content": "    llvm::Value* guarding_cond = b_.CreateAnd(index_within_ranges);",
          "content_same": false
        },
        {
          "line": 5290,
          "old_api": "multidim",
          "new_api": "size",
          "old_text": "index.multidim()",
          "new_text": "src_multidim.size()",
          "old_line_content": "      const std::vector<llvm::Value*>& src_multidim = index.multidim();",
          "new_line_content": "      for (size_t dim = 0; dim < src_multidim.size(); ++dim) {",
          "content_same": false
        },
        {
          "line": 5292,
          "old_api": "size",
          "new_api": "GetConstantWithIndexType",
          "old_text": "src_multidim.size()",
          "new_text": "Sub(src_multidim[dim],\n                index.GetConstantWithIndexType(slice->slice_starts(dim)))",
          "old_line_content": "      for (size_t dim = 0; dim < src_multidim.size(); ++dim) {",
          "new_line_content": "            Sub(src_multidim[dim],",
          "content_same": false
        },
        {
          "line": 5298,
          "old_api": "num_parameters",
          "new_api": "GetType",
          "old_text": "fused_computation->num_parameters()",
          "new_text": "index.GetType()",
          "old_line_content": "          ir_arrays[fused_computation->num_parameters() + i];",
          "new_line_content": "                                     index.GetType());",
          "content_same": false
        },
        {
          "line": 5299,
          "old_api": "shape",
          "new_api": "EmitWriteArrayElement",
          "old_text": "slice->shape()",
          "new_text": "src_ir_array.EmitWriteArrayElement(slice_dst_index, input_ir_values[i],\n                                         &b_)",
          "old_line_content": "      IrArray::Index slice_dst_index(dst_multidim, slice->shape(),",
          "new_line_content": "      src_ir_array.EmitWriteArrayElement(slice_dst_index, input_ir_values[i],",
          "content_same": false
        },
        {
          "line": 5305,
          "old_api": "StrCat",
          "new_api": "Status::OK()",
          "old_text": "StrCat(\"slice\", i)",
          "new_text": "Status::OK()",
          "old_line_content": "    ksl.If(StrCat(\"slice\", i), guarding_cond, emit_slice_elem_func);",
          "new_line_content": "  return Status::OK();",
          "content_same": false
        },
        {
          "line": 5312,
          "old_api": "mlir::cast<mlir::lmhlo::FusionOp>(op)",
          "new_api": "constexpr",
          "old_text": "mlir::cast<mlir::lmhlo::FusionOp>(op)",
          "new_text": "constexpr",
          "old_line_content": "  auto fusion = mlir::cast<mlir::lmhlo::FusionOp>(op);",
          "new_line_content": "  constexpr int unroll_factor = 1;",
          "content_same": false
        },
        {
          "line": 5333,
          "old_api": "EmitLoop",
          "new_api": "EmitElementForInputFusibleSlices",
          "old_text": "ParallelLoopEmitter(\n          [&](const llvm_ir::IrArray::Index index) -> Status {\n            return EmitElementForInputFusibleSlices(fused_computation,\n                                                    ir_arrays, index);\n          },\n          element_shape, launch_dimensions, &b_)\n          .EmitLoop(IrName(mlir::GetNameFromLoc(fusion.getLoc())),\n                    GetIndexTypeForKernel(\n                        fusion, launch_dimensions.launch_bound(), &b_))",
          "new_text": "EmitElementForInputFusibleSlices(fused_computation,\n                                                    ir_arrays, index)",
          "old_line_content": "      ParallelLoopEmitter(",
          "new_line_content": "            return EmitElementForInputFusibleSlices(fused_computation,",
          "content_same": false
        },
        {
          "line": 5339,
          "old_api": "getLoc",
          "new_api": "launch_bound",
          "old_text": "fusion.getLoc()",
          "new_text": "launch_dimensions.launch_bound()",
          "old_line_content": "          .EmitLoop(IrName(mlir::GetNameFromLoc(fusion.getLoc())),",
          "new_line_content": "                        fusion, launch_dimensions.launch_bound(), &b_));",
          "content_same": false
        },
        {
          "line": 5341,
          "old_api": "launch_bound",
          "new_api": "std::move(kernel_thunk)",
          "old_text": "launch_dimensions.launch_bound()",
          "new_text": "std::move(kernel_thunk)",
          "old_line_content": "                        fusion, launch_dimensions.launch_bound(), &b_));",
          "new_line_content": "  thunk_sequence_.emplace_back(std::move(kernel_thunk));",
          "content_same": false
        },
        {
          "line": 5359,
          "old_api": "mlir::dyn_cast<mlir::lmhlo::CustomCallOp>(op)",
          "new_api": "EmitPadToStatic",
          "old_text": "mlir::dyn_cast<mlir::lmhlo::CustomCallOp>(op)",
          "new_text": "EmitPadToStatic(op)",
          "old_line_content": "  if (auto call = mlir::dyn_cast<mlir::lmhlo::CustomCallOp>(op)) {",
          "new_line_content": "      return EmitPadToStatic(op);",
          "content_same": false
        },
        {
          "line": 5361,
          "old_api": "EmitPadToStatic",
          "new_api": "call_target_name",
          "old_text": "EmitPadToStatic(op)",
          "new_text": "call.call_target_name()",
          "old_line_content": "      return EmitPadToStatic(op);",
          "new_line_content": "    if (call.call_target_name() == \"SliceToDynamic\") {",
          "content_same": false
        },
        {
          "line": 5364,
          "old_api": "EmitSliceToDynamic",
          "new_api": "EmitCustomCallThunk",
          "old_text": "EmitSliceToDynamic(op)",
          "new_text": "EmitCustomCallThunk(op)",
          "old_line_content": "      return EmitSliceToDynamic(op);",
          "new_line_content": "    return EmitCustomCallThunk(op);",
          "content_same": false
        },
        {
          "line": 5482,
          "old_api": "EmitOp",
          "new_api": "Status::OK()",
          "old_text": "EmitOp(&op)",
          "new_text": "Status::OK()",
          "old_line_content": "    TF_RETURN_IF_ERROR(EmitOp(&op));",
          "new_line_content": "  return Status::OK();",
          "content_same": false
        },
        {
          "line": 5488,
          "old_api": "op->getParentOfType<mlir::ModuleOp>()",
          "new_api": "getLoc",
          "old_text": "op->getParentOfType<mlir::ModuleOp>()",
          "new_text": "absl::StrFormat(\n      \"Thunk:#hlo_op=%s,hlo_module=%s#\", mlir::GetNameFromLoc(op->getLoc()),\n      mlir::GetNameFromLoc(module->getLoc()))",
          "old_line_content": "  auto module = op->getParentOfType<mlir::ModuleOp>();",
          "new_line_content": "  thunk_info.profile_annotation = absl::StrFormat(",
          "content_same": false
        },
        {
          "line": 5497,
          "old_api": "getLoc",
          "new_api": "GetHloOperands",
          "old_text": "op->getLoc()",
          "new_text": "GetHloOperands(op)",
          "old_line_content": "  this->name = mlir::GetNameFromLoc(op->getLoc());",
          "new_line_content": "  auto operands = GetHloOperands(op);",
          "content_same": false
        },
        {
          "line": 5500,
          "old_api": "GetHloOutputs",
          "new_api": "GetShape",
          "old_text": "GetHloOutputs(op)",
          "new_text": "GetShape(operand)",
          "old_line_content": "  auto outputs = GetHloOutputs(op);",
          "new_line_content": "    operand_shapes.push_back(GetShape(operand));",
          "content_same": false
        }
      ],
      "additions": [
        {
          "line": 5120,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "instr_index_groups.size()",
          "old_line_content": "  LaunchDimensions launch_dimensions(",
          "new_line_content": "       /*y=*/static_cast<int64_t>(instr_index_groups.size()),",
          "content_same": false
        },
        {
          "line": 5123,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(3)",
          "old_line_content": "       /*z=*/1},",
          "new_line_content": "  VLOG(3) << \"Launch dimensions of \"",
          "content_same": false
        },
        {
          "line": 5129,
          "old_api": null,
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": null,
          "new_text": "TF_ASSIGN_OR_RETURN(std::unique_ptr<KernelThunk> kernel_thunk,\n                      BuildKernelThunk(unnested_hlo, Thunk::ThunkInfo(),\n                                       &ir_arrays, launch_dimensions))",
          "old_line_content": "",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(std::unique_ptr<KernelThunk> kernel_thunk,",
          "content_same": false
        },
        {
          "line": 5130,
          "old_api": null,
          "new_api": "Thunk::ThunkInfo()",
          "old_text": null,
          "new_text": "Thunk::ThunkInfo()",
          "old_line_content": "  std::vector<llvm_ir::IrArray> ir_arrays;",
          "new_line_content": "                      BuildKernelThunk(unnested_hlo, Thunk::ThunkInfo(),",
          "content_same": false
        },
        {
          "line": 5134,
          "old_api": null,
          "new_api": "llvm_module",
          "old_text": null,
          "new_text": "ir_emitter_context_->llvm_module()",
          "old_line_content": "",
          "new_line_content": "                                          ir_emitter_context_->llvm_module(),",
          "content_same": false
        },
        {
          "line": 5135,
          "old_api": null,
          "new_api": "GetNestedComputer",
          "old_text": null,
          "new_text": "GetNestedComputer()",
          "old_line_content": "  GpuElementalIrEmitter elemental_emitter(hlo_module_config_,",
          "new_line_content": "                                          &b_, GetNestedComputer());",
          "content_same": false
        },
        {
          "line": 5138,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "ir_arrays.size()",
          "old_line_content": "  FusedIrEmitter fused_emitter(&elemental_emitter);",
          "new_line_content": "  CHECK_LT(fused_computation->num_parameters(), ir_arrays.size());",
          "content_same": false
        },
        {
          "line": 5139,
          "old_api": null,
          "new_api": "num_parameters",
          "old_text": null,
          "new_text": "fused_computation->num_parameters()",
          "old_line_content": "",
          "new_line_content": "  for (int i = 0; i < fused_computation->num_parameters(); i++) {",
          "content_same": false
        },
        {
          "line": 5142,
          "old_api": null,
          "new_api": "BindGenerator",
          "old_text": null,
          "new_text": "fused_emitter.BindGenerator(\n        fused_operand,\n        [this, ir_array, fused_operand](const llvm_ir::IrArray::Index& index) {\n          return ir_array.EmitReadArrayElement(index, &b_,\n                                               fused_operand->name());\n        })",
          "old_line_content": "    llvm_ir::IrArray ir_array = ir_arrays[i];",
          "new_line_content": "    fused_emitter.BindGenerator(",
          "content_same": false
        },
        {
          "line": 5145,
          "old_api": null,
          "new_api": "EmitReadArrayElement",
          "old_text": null,
          "new_text": "ir_array.EmitReadArrayElement(index, &b_,\n                                               fused_operand->name())",
          "old_line_content": "        fused_operand,",
          "new_line_content": "          return ir_array.EmitReadArrayElement(index, &b_,",
          "content_same": false
        },
        {
          "line": 5146,
          "old_api": null,
          "new_api": "name",
          "old_text": null,
          "new_text": "fused_operand->name()",
          "old_line_content": "        [this, ir_array, fused_operand](const llvm_ir::IrArray::Index& index) {",
          "new_line_content": "                                               fused_operand->name());",
          "content_same": false
        },
        {
          "line": 5150,
          "old_api": null,
          "new_api": "num_parameters",
          "old_text": null,
          "new_text": "fused_computation->num_parameters()",
          "old_line_content": "  }",
          "new_line_content": "      absl::MakeSpan(ir_arrays).subspan(fused_computation->num_parameters(),",
          "content_same": false
        },
        {
          "line": 5157,
          "old_api": null,
          "new_api": "ComputeReductionCodegenInfo",
          "old_text": null,
          "new_text": "ComputeReductionCodegenInfo(unnested_hlo, first_reduce, layout_analysis)",
          "old_line_content": "  // same shape and layout as verified by `IsFusedReductionOutputConsistent()`.",
          "new_line_content": "      ComputeReductionCodegenInfo(unnested_hlo, first_reduce, layout_analysis);",
          "content_same": false
        },
        {
          "line": 5160,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "instr_index_groups.size()",
          "old_line_content": "",
          "new_line_content": "  for (size_t i = 0; i < instr_index_groups.size(); ++i) {",
          "content_same": false
        },
        {
          "line": 5164,
          "old_api": null,
          "new_api": "ReductionCodegenState",
          "old_text": null,
          "new_text": "ReductionCodegenState(reduction_codegen_info)",
          "old_line_content": "    // code generation per reduction group.",
          "new_line_content": "        ReductionCodegenState(reduction_codegen_info);",
          "content_same": false
        },
        {
          "line": 5169,
          "old_api": null,
          "new_api": "gpu::EmitCallToTargetIntrinsic(\n        gpu::TargetIntrinsicID::kBlockIdy, {}, {}, &b_)",
          "old_text": null,
          "new_text": "gpu::EmitCallToTargetIntrinsic(\n        gpu::TargetIntrinsicID::kBlockIdy, {}, {}, &b_)",
          "old_line_content": "    // for reduction code generation as the block_id_y is orthogonal to",
          "new_line_content": "    llvm::CallInst* raw_block_id_y = gpu::EmitCallToTargetIntrinsic(",
          "content_same": false
        },
        {
          "line": 5172,
          "old_api": null,
          "new_api": "llvm::cast<llvm::Instruction>(raw_block_id_y)",
          "old_text": null,
          "new_text": "llvm::cast<llvm::Instruction>(raw_block_id_y)",
          "old_line_content": "        gpu::TargetIntrinsicID::kBlockIdy, {}, {}, &b_);",
          "new_line_content": "                              llvm::cast<llvm::Instruction>(raw_block_id_y));",
          "content_same": false
        },
        {
          "line": 5182,
          "old_api": null,
          "new_api": "debug_options",
          "old_text": null,
          "new_text": "hlo_module_config_.debug_options().xla_gpu_deterministic_reductions()",
          "old_line_content": "  }",
          "new_line_content": "  if (hlo_module_config_.debug_options().xla_gpu_deterministic_reductions() &&",
          "content_same": false
        },
        {
          "line": 5183,
          "old_api": null,
          "new_api": "IsRaceFree",
          "old_text": null,
          "new_text": "reduction_codegen_info.IsRaceFree()",
          "old_line_content": "",
          "new_line_content": "      !reduction_codegen_info.IsRaceFree()) {",
          "content_same": false
        },
        {
          "line": 5193,
          "old_api": null,
          "new_api": "GetReduceFromUnnestedMlir",
          "old_text": null,
          "new_text": "GetReduceFromUnnestedMlir(unnested_hlo, i)",
          "old_line_content": "  for (int i = 0; i < num_reduces; ++i) {",
          "new_line_content": "        GetReduceFromUnnestedMlir(unnested_hlo, i);",
          "content_same": false
        },
        {
          "line": 5194,
          "old_api": null,
          "new_api": "IsReductionFromOrToContiguousDimensions",
          "old_text": null,
          "new_text": "IsReductionFromOrToContiguousDimensions(output_instruction,\n                                                 layout_analysis)",
          "old_line_content": "    mlir::Operation* output_instruction =",
          "new_line_content": "    if (!IsReductionFromOrToContiguousDimensions(output_instruction,",
          "content_same": false
        },
        {
          "line": 5198,
          "old_api": null,
          "new_api": "IsRaceFree",
          "old_text": null,
          "new_text": "reduction_codegen_info.IsRaceFree()",
          "old_line_content": "      // Elemental IR emitter is used.",
          "new_line_content": "    } else if (reduction_codegen_info.IsRaceFree()) {",
          "content_same": false
        },
        {
          "line": 5199,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(5)",
          "old_line_content": "      continue;",
          "new_line_content": "      VLOG(5) << \"We do not need initialization: using tree reductions\";",
          "content_same": false
        },
        {
          "line": 5203,
          "old_api": null,
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": null,
          "new_text": "TF_ASSIGN_OR_RETURN(std::unique_ptr<Thunk> initializer_thunk,\n                        BuildFusedInitializerThunk(fusion, i))",
          "old_line_content": "    }",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(std::unique_ptr<Thunk> initializer_thunk,",
          "content_same": false
        },
        {
          "line": 5204,
          "old_api": null,
          "new_api": "BuildFusedInitializerThunk",
          "old_text": null,
          "new_text": "BuildFusedInitializerThunk(fusion, i)",
          "old_line_content": "",
          "new_line_content": "                        BuildFusedInitializerThunk(fusion, i));",
          "content_same": false
        },
        {
          "line": 5208,
          "old_api": null,
          "new_api": "std::move(kernel_thunk)",
          "old_text": null,
          "new_text": "std::move(kernel_thunk)",
          "old_line_content": "  }",
          "new_line_content": "  thunks.push_back(std::move(kernel_thunk));",
          "content_same": false
        },
        {
          "line": 5209,
          "old_api": null,
          "new_api": "absl::make_unique<SequentialThunk>(\n      GetThunkInfo(unnested_hlo), std::move(thunks))",
          "old_text": null,
          "new_text": "absl::make_unique<SequentialThunk>(\n      GetThunkInfo(unnested_hlo), std::move(thunks))",
          "old_line_content": "",
          "new_line_content": "  auto sequential_thunk = absl::make_unique<SequentialThunk>(",
          "content_same": false
        },
        {
          "line": 5237,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(10)",
          "old_line_content": "    absl::Span<const llvm_ir::IrArray> ir_arrays,",
          "new_line_content": "  VLOG(10) << \"Emitting slice input fusion for \"",
          "content_same": false
        },
        {
          "line": 5238,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "fused_computation->ToString()",
          "old_line_content": "    const llvm_ir::IrArray::Index& index) {",
          "new_line_content": "           << fused_computation->ToString();",
          "content_same": false
        },
        {
          "line": 5241,
          "old_api": null,
          "new_api": "opcode",
          "old_text": null,
          "new_text": "() -> absl::Span<HloInstruction* const> {\n    if (slice_or_tuple->opcode() == HloOpcode::kSlice) {\n      return absl::Span<HloInstruction* const>(&slice_or_tuple, 1);\n    }\n    CHECK_EQ(slice_or_tuple->opcode(), HloOpcode::kTuple);\n    return slice_or_tuple->operands();\n  }()",
          "old_line_content": "",
          "new_line_content": "  auto slice_instructions = [&]() -> absl::Span<HloInstruction* const> {",
          "content_same": false
        },
        {
          "line": 5246,
          "old_api": null,
          "new_api": "operands",
          "old_text": null,
          "new_text": "slice_or_tuple->operands()",
          "old_line_content": "    }",
          "new_line_content": "    return slice_or_tuple->operands();",
          "content_same": false
        },
        {
          "line": 5252,
          "old_api": null,
          "new_api": "GetNestedComputer",
          "old_text": null,
          "new_text": "GetNestedComputer()",
          "old_line_content": "  std::vector<llvm::Value*> input_ir_values;",
          "new_line_content": "                                     GetNestedComputer());",
          "content_same": false
        },
        {
          "line": 5255,
          "old_api": null,
          "new_api": "BindGenerator",
          "old_text": null,
          "new_text": "fused_emitter.BindGenerator(\n        fused_computation->parameter_instruction(i),\n        [this, &ir_arrays, i](llvm_ir::IrArray::Index index) {\n          return ir_arrays[i].EmitReadArrayElement(index, &b_);\n        })",
          "old_line_content": "  FusedIrEmitter fused_emitter(&elem_emitter);",
          "new_line_content": "    fused_emitter.BindGenerator(",
          "content_same": false
        },
        {
          "line": 5262,
          "old_api": null,
          "new_api": "operand",
          "old_text": null,
          "new_text": "slice->operand(0)",
          "old_line_content": "  }",
          "new_line_content": "    auto input_generator = *fused_emitter.GetGenerator(slice->operand(0));",
          "content_same": false
        },
        {
          "line": 5263,
          "old_api": null,
          "new_api": "ValueOrDie",
          "old_text": null,
          "new_text": "input_generator(index).ValueOrDie()",
          "old_line_content": "  for (const HloInstruction* slice : slice_instructions) {",
          "new_line_content": "    input_ir_values.push_back(input_generator(index).ValueOrDie());",
          "content_same": false
        },
        {
          "line": 5268,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "slice_instructions.size()",
          "old_line_content": "  // Emit for slice_instructions.",
          "new_line_content": "  for (int64_t i = 0; i < slice_instructions.size(); ++i) {",
          "content_same": false
        },
        {
          "line": 5273,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "slice->slice_starts().size()",
          "old_line_content": "    // guarding_cond := index >= start && index < limit, for each dim.",
          "new_line_content": "    for (size_t dim = 0; dim < slice->slice_starts().size(); ++dim) {",
          "content_same": false
        },
        {
          "line": 5274,
          "old_api": null,
          "new_api": "slice_strides",
          "old_text": null,
          "new_text": "slice->slice_strides(dim)",
          "old_line_content": "    std::vector<llvm::Value*> index_within_ranges;",
          "new_line_content": "      CHECK_EQ(slice->slice_strides(dim), 1);",
          "content_same": false
        },
        {
          "line": 5283,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "index_within_ranges.push_back(within_range)",
          "old_line_content": "      llvm::Value* within_range =",
          "new_line_content": "      index_within_ranges.push_back(within_range);",
          "content_same": false
        },
        {
          "line": 5288,
          "old_api": null,
          "new_api": "multidim",
          "old_text": null,
          "new_text": "index.multidim()",
          "old_line_content": "",
          "new_line_content": "      const std::vector<llvm::Value*>& src_multidim = index.multidim();",
          "content_same": false
        },
        {
          "line": 4777,
          "old_api": null,
          "new_api": "IsReductionFromOrToContiguousDimensions",
          "old_text": null,
          "new_text": "IsReductionFromOrToContiguousDimensions(unnested_hlo, layout_analysis)",
          "old_line_content": "  }",
          "new_line_content": "  if (IsReductionFromOrToContiguousDimensions(unnested_hlo, layout_analysis)) {",
          "content_same": false
        },
        {
          "line": 5289,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "src_multidim.size()",
          "old_line_content": "    auto emit_slice_elem_func = [&] {",
          "new_line_content": "      std::vector<llvm::Value*> dst_multidim(src_multidim.size());",
          "content_same": false
        },
        {
          "line": 4781,
          "old_api": null,
          "new_api": "mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "old_text": null,
          "new_text": "mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "old_line_content": "  }",
          "new_line_content": "  auto fusion = mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo);",
          "content_same": false
        },
        {
          "line": 5293,
          "old_api": null,
          "new_api": "slice_starts",
          "old_text": null,
          "new_text": "slice->slice_starts(dim)",
          "old_line_content": "        dst_multidim[dim] =",
          "new_line_content": "                index.GetConstantWithIndexType(slice->slice_starts(dim)));",
          "content_same": false
        },
        {
          "line": 4784,
          "old_api": null,
          "new_api": "getFusionResults",
          "old_text": null,
          "new_text": "fusion.getFusionResults()",
          "old_line_content": "  int64_t can_be_vectorized = 0;",
          "new_line_content": "  auto fusion_results = ToStdVector(fusion.getFusionResults());",
          "content_same": false
        },
        {
          "line": 5296,
          "old_api": null,
          "new_api": "num_parameters",
          "old_text": null,
          "new_text": "fused_computation->num_parameters()",
          "old_line_content": "      }",
          "new_line_content": "          ir_arrays[fused_computation->num_parameters() + i];",
          "content_same": false
        },
        {
          "line": 5297,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "slice->shape()",
          "old_line_content": "      llvm_ir::IrArray src_ir_array =",
          "new_line_content": "      IrArray::Index slice_dst_index(dst_multidim, slice->shape(),",
          "content_same": false
        },
        {
          "line": 4787,
          "old_api": null,
          "new_api": "getDefiningOp",
          "old_text": null,
          "new_text": "IsReductionFromOrToContiguousDimensions(\n            fusion_results[0].getDefiningOp(), layout_analysis)",
          "old_line_content": "  absl::flat_hash_set<mlir::Operation*> use_chain_endings;",
          "new_line_content": "    if (IsReductionFromOrToContiguousDimensions(",
          "content_same": false
        },
        {
          "line": 5303,
          "old_api": null,
          "new_api": "StrCat",
          "old_text": null,
          "new_text": "StrCat(\"slice\", i)",
          "old_line_content": "    };",
          "new_line_content": "    ksl.If(StrCat(\"slice\", i), guarding_cond, emit_slice_elem_func);",
          "content_same": false
        },
        {
          "line": 4795,
          "old_api": null,
          "new_api": "getDefiningOp",
          "old_text": null,
          "new_text": "result.getDefiningOp()",
          "old_line_content": "  } else {",
          "new_line_content": "      if (IsReductionFromOrToContiguousDimensions(result.getDefiningOp(),",
          "content_same": false
        },
        {
          "line": 5310,
          "old_api": null,
          "new_api": "mlir::cast<mlir::lmhlo::FusionOp>(op)",
          "old_text": null,
          "new_text": "mlir::cast<mlir::lmhlo::FusionOp>(op)",
          "old_line_content": "Status IrEmitterUnnested::EmitInputFusibleNonStridedSlices(",
          "new_line_content": "  auto fusion = mlir::cast<mlir::lmhlo::FusionOp>(op);",
          "content_same": false
        },
        {
          "line": 4803,
          "old_api": null,
          "new_api": "getDefiningOp",
          "old_text": null,
          "new_text": "result.getDefiningOp()",
          "old_line_content": "        can_be_vectorized++;",
          "new_line_content": "      use_chain_endings.insert(result.getDefiningOp());",
          "content_same": false
        },
        {
          "line": 4808,
          "old_api": null,
          "new_api": "NumInputsInvolveInOnlyElementwiseOps",
          "old_text": null,
          "new_text": "NumInputsInvolveInOnlyElementwiseOps(fusion, input_shape,\n                                                            use_chain_endings)",
          "old_line_content": "  // Fusion inputs that have the same dimension as the reduce input and",
          "new_line_content": "  can_be_vectorized += NumInputsInvolveInOnlyElementwiseOps(fusion, input_shape,",
          "content_same": false
        },
        {
          "line": 4815,
          "old_api": null,
          "new_api": "NumInputsWithMoreElementsThan",
          "old_text": null,
          "new_text": "NumInputsWithMoreElementsThan(fusion, input_shape)",
          "old_line_content": "  // unrolled even with such an assumption,  and the accesses to those inputs",
          "new_line_content": "  cannot_be_vectorized += NumInputsWithMoreElementsThan(fusion, input_shape);",
          "content_same": false
        },
        {
          "line": 5331,
          "old_api": null,
          "new_api": "EmitLoop",
          "old_text": null,
          "new_text": "ParallelLoopEmitter(\n          [&](const llvm_ir::IrArray::Index index) -> Status {\n            return EmitElementForInputFusibleSlices(fused_computation,\n                                                    ir_arrays, index);\n          },\n          element_shape, launch_dimensions, &b_)\n          .EmitLoop(IrName(mlir::GetNameFromLoc(fusion.getLoc())),\n                    GetIndexTypeForKernel(\n                        fusion, launch_dimensions.launch_bound(), &b_))",
          "old_line_content": "",
          "new_line_content": "      ParallelLoopEmitter(",
          "content_same": false
        },
        {
          "line": 4823,
          "old_api": null,
          "new_api": "tensorflow::NextPowerOfTwo64(v)",
          "old_text": null,
          "new_text": "tensorflow::NextPowerOfTwo64(v)",
          "old_line_content": "    return 0;",
          "new_line_content": "  int64_t upper = tensorflow::NextPowerOfTwo64(v);",
          "content_same": false
        },
        {
          "line": 5337,
          "old_api": null,
          "new_api": "getLoc",
          "old_text": null,
          "new_text": "fusion.getLoc()",
          "old_line_content": "          },",
          "new_line_content": "          .EmitLoop(IrName(mlir::GetNameFromLoc(fusion.getLoc())),",
          "content_same": false
        },
        {
          "line": 5338,
          "old_api": null,
          "new_api": "launch_bound",
          "old_text": null,
          "new_text": "GetIndexTypeForKernel(\n                        fusion, launch_dimensions.launch_bound(), &b_)",
          "old_line_content": "          element_shape, launch_dimensions, &b_)",
          "new_line_content": "                    GetIndexTypeForKernel(",
          "content_same": false
        },
        {
          "line": 4833,
          "old_api": null,
          "new_api": "getOperand",
          "old_text": null,
          "new_text": "first_reduce->getOperand(0)",
          "old_line_content": "    mlir::Operation* unnested_hlo, mlir::Operation* first_reduce,",
          "new_line_content": "  Shape input_shape = layout_analysis.GetShape(first_reduce->getOperand(0));",
          "content_same": false
        },
        {
          "line": 5347,
          "old_api": null,
          "new_api": "mlir::isa<mlir::ConstantOp, mlir::memref::ViewOp,\n                mlir::memref::ReinterpretCastOp, mlir::ReturnOp,\n                mlir::lmhlo::TerminatorOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::ConstantOp, mlir::memref::ViewOp,\n                mlir::memref::ReinterpretCastOp, mlir::ReturnOp,\n                mlir::lmhlo::TerminatorOp>(op)",
          "old_line_content": "",
          "new_line_content": "  if (mlir::isa<mlir::ConstantOp, mlir::memref::ViewOp,",
          "content_same": false
        },
        {
          "line": 4836,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(10)",
          "old_line_content": "  ReductionDimensions reduction_dimensions =",
          "new_line_content": "  VLOG(10) << \"is_row_reduction \" << reduction_dimensions.is_row_reduction",
          "content_same": false
        },
        {
          "line": 5350,
          "old_api": null,
          "new_api": "Status::OK()",
          "old_text": null,
          "new_text": "Status::OK()",
          "old_line_content": "                mlir::memref::ReinterpretCastOp, mlir::ReturnOp,",
          "new_line_content": "    return Status::OK();",
          "content_same": false
        },
        {
          "line": 5353,
          "old_api": null,
          "new_api": "mlir::isa<mlir::memref::GetGlobalOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::memref::GetGlobalOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::memref::GetGlobalOp>(op)) {",
          "content_same": false
        },
        {
          "line": 4842,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "GetShape(i).element_type()",
          "old_line_content": "  auto get_dtype_bits = [](mlir::Value i) {",
          "new_line_content": "    return primitive_util::BitWidth(GetShape(i).element_type());",
          "content_same": false
        },
        {
          "line": 5354,
          "old_api": null,
          "new_api": "EmitConstant",
          "old_text": null,
          "new_text": "EmitConstant(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitConstant(op);",
          "content_same": false
        },
        {
          "line": 5357,
          "old_api": null,
          "new_api": "mlir::dyn_cast<mlir::lmhlo::CustomCallOp>(op)",
          "old_text": null,
          "new_text": "mlir::dyn_cast<mlir::lmhlo::CustomCallOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (auto call = mlir::dyn_cast<mlir::lmhlo::CustomCallOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5358,
          "old_api": null,
          "new_api": "call_target_name",
          "old_text": null,
          "new_text": "call.call_target_name()",
          "old_line_content": "",
          "new_line_content": "    if (call.call_target_name() == \"PadToStatic\") {",
          "content_same": false
        },
        {
          "line": 4847,
          "old_api": null,
          "new_api": "getOperand",
          "old_text": null,
          "new_text": "first_reduce->getOperand(0)",
          "old_line_content": "  // For fusion with multiple inputs, use the smallest input dtype to",
          "new_line_content": "  int smallest_input_dtype_bits = get_dtype_bits(first_reduce->getOperand(0));",
          "content_same": false
        },
        {
          "line": 5362,
          "old_api": null,
          "new_api": "EmitSliceToDynamic",
          "old_text": null,
          "new_text": "EmitSliceToDynamic(op)",
          "old_line_content": "    }",
          "new_line_content": "      return EmitSliceToDynamic(op);",
          "content_same": false
        },
        {
          "line": 4854,
          "old_api": null,
          "new_api": "cuda_compute_capability",
          "old_text": null,
          "new_text": "GetReductionTiling(reduction_dimensions, smallest_input_dtype_bits,\n                         ir_emitter_context_->cuda_compute_capability())",
          "old_line_content": "  }",
          "new_line_content": "      GetReductionTiling(reduction_dimensions, smallest_input_dtype_bits,",
          "content_same": false
        },
        {
          "line": 4855,
          "old_api": null,
          "new_api": "cuda_compute_capability",
          "old_text": null,
          "new_text": "ir_emitter_context_->cuda_compute_capability()",
          "old_line_content": "  std::array<int64_t, 3> reduction_tiling =",
          "new_line_content": "                         ir_emitter_context_->cuda_compute_capability());",
          "content_same": false
        },
        {
          "line": 5367,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo_gpu::GEMMOp, mlir::lmhlo_gpu::GEMM_BiasOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo_gpu::GEMMOp, mlir::lmhlo_gpu::GEMM_BiasOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::GEMMOp, mlir::lmhlo_gpu::GEMM_BiasOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5368,
          "old_api": null,
          "new_api": "EmitGemmThunk",
          "old_text": null,
          "new_text": "EmitGemmThunk(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitGemmThunk(op);",
          "content_same": false
        },
        {
          "line": 4858,
          "old_api": null,
          "new_api": "getFusionResults",
          "old_text": null,
          "new_text": "[&] {\n    if (reduction_dimensions.is_row_reduction) {\n      // Use 512 as default block size (threads per block) for row reductions.\n      // For multi-output fusions, reduce the block size further to decrease\n      // register pressure when multiple outputs are computed by each thread.\n      int64_t fan_out = 1;\n      if (auto fusion = mlir::dyn_cast<mlir::lmhlo::FusionOp>(unnested_hlo)) {\n        fan_out = fusion.getFusionResults().size();\n      }\n\n      int64_t max_block_size =\n          std::max(kMinThreadsXRowReduction,\n                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)));\n      return std::min(\n          max_block_size,\n          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],\n                                       reduction_tiling[2]),\n                           kWarpSize));\n    }\n    return kWarpSize;\n  }()",
          "old_line_content": "",
          "new_line_content": "  int64_t num_threads_x = [&] {",
          "content_same": false
        },
        {
          "line": 5371,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo_gpu::ConvForwardOp,\n                mlir::lmhlo_gpu::ConvForwardFusedOp,\n                mlir::lmhlo_gpu::ConvForwardFusedSideInputOp,\n                mlir::lmhlo_gpu::ConvBackwardFilterOp,\n                mlir::lmhlo_gpu::ConvBackwardInputOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo_gpu::ConvForwardOp,\n                mlir::lmhlo_gpu::ConvForwardFusedOp,\n                mlir::lmhlo_gpu::ConvForwardFusedSideInputOp,\n                mlir::lmhlo_gpu::ConvBackwardFilterOp,\n                mlir::lmhlo_gpu::ConvBackwardInputOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::ConvForwardOp,",
          "content_same": false
        },
        {
          "line": 4864,
          "old_api": null,
          "new_api": "mlir::dyn_cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "old_text": null,
          "new_text": "mlir::dyn_cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "old_line_content": "      // register pressure when multiple outputs are computed by each thread.",
          "new_line_content": "      if (auto fusion = mlir::dyn_cast<mlir::lmhlo::FusionOp>(unnested_hlo)) {",
          "content_same": false
        },
        {
          "line": 4865,
          "old_api": null,
          "new_api": "getFusionResults",
          "old_text": null,
          "new_text": "fusion.getFusionResults().size()",
          "old_line_content": "      int64_t fan_out = 1;",
          "new_line_content": "        fan_out = fusion.getFusionResults().size();",
          "content_same": false
        },
        {
          "line": 5376,
          "old_api": null,
          "new_api": "EmitConvolutionThunk",
          "old_text": null,
          "new_text": "EmitConvolutionThunk(op)",
          "old_line_content": "                mlir::lmhlo_gpu::ConvBackwardFilterOp,",
          "new_line_content": "    return EmitConvolutionThunk(op);",
          "content_same": false
        },
        {
          "line": 5379,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo_gpu::BatchNormTrainingOp,\n                mlir::lmhlo_gpu::BatchNormInferenceOp,\n                mlir::lmhlo_gpu::BatchNormGradOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo_gpu::BatchNormTrainingOp,\n                mlir::lmhlo_gpu::BatchNormInferenceOp,\n                mlir::lmhlo_gpu::BatchNormGradOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::BatchNormTrainingOp,",
          "content_same": false
        },
        {
          "line": 4869,
          "old_api": null,
          "new_api": "std::max(kMinThreadsXRowReduction,\n                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)))",
          "old_text": null,
          "new_text": "std::max(kMinThreadsXRowReduction,\n                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)))",
          "old_line_content": "",
          "new_line_content": "          std::max(kMinThreadsXRowReduction,",
          "content_same": false
        },
        {
          "line": 4870,
          "old_api": null,
          "new_api": "NearestPowerOfTwo",
          "old_text": null,
          "new_text": "NearestPowerOfTwo(fan_out)",
          "old_line_content": "      int64_t max_block_size =",
          "new_line_content": "                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)));",
          "content_same": false
        },
        {
          "line": 5382,
          "old_api": null,
          "new_api": "EmitBatchNormThunk",
          "old_text": null,
          "new_text": "EmitBatchNormThunk(op)",
          "old_line_content": "                mlir::lmhlo_gpu::BatchNormInferenceOp,",
          "new_line_content": "    return EmitBatchNormThunk(op);",
          "content_same": false
        },
        {
          "line": 5386,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo_gpu::CholeskyOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo_gpu::CholeskyOp>(op)",
          "old_line_content": "",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::CholeskyOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5387,
          "old_api": null,
          "new_api": "EmitCholeskyThunk",
          "old_text": null,
          "new_text": "EmitCholeskyThunk(op)",
          "old_line_content": "#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM",
          "new_line_content": "    return EmitCholeskyThunk(op);",
          "content_same": false
        },
        {
          "line": 5391,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::FftOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::FftOp>(op)",
          "old_line_content": "#endif  // GOOGLE_CUDA",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::FftOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5392,
          "old_api": null,
          "new_api": "EmitFftThunk",
          "old_text": null,
          "new_text": "EmitFftThunk(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitFftThunk(op);",
          "content_same": false
        },
        {
          "line": 4883,
          "old_api": null,
          "new_api": "cuda_compute_capability",
          "old_text": null,
          "new_text": "ir_emitter_context_->cuda_compute_capability()",
          "old_line_content": "                      (reduction_tiling[2] * num_threads_x) ==",
          "new_line_content": "  se::CudaComputeCapability cc = ir_emitter_context_->cuda_compute_capability();",
          "content_same": false
        },
        {
          "line": 5395,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::TriangularSolveOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::TriangularSolveOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::TriangularSolveOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5396,
          "old_api": null,
          "new_api": "EmitTriangularSolve",
          "old_text": null,
          "new_text": "EmitTriangularSolve(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitTriangularSolve(op);",
          "content_same": false
        },
        {
          "line": 4886,
          "old_api": null,
          "new_api": "[&]() {\n    if (reduction_dimensions.is_row_reduction &&\n        // P100, only try to vectorize+coales memory access when the\n        // tile size fits exactly and dtypes <= 32 bits\n        ((cc.major == 6 && smallest_input_dtype_bits <= 32 && tile_fit) ||\n         // On V100, only try to vectorize+coales memory access for\n         // rows of even size.  For odd row sizes, every other row\n         // isn't aligned, so it can't be vectorized.\n         (cc.major >= 7 && reduction_dimensions.dimensions[2] % 2 == 0))) {\n      return kStridedLinearIndexingX;\n    } else if (!reduction_dimensions.is_row_reduction &&\n               IsUnrollingColumnReductionBeneficial(\n                   unnested_hlo, input_shape,\n                   reduction_dimensions.dimensions[2], layout_analysis)) {\n      num_partial_results = 2;\n      reduction_tiling[2] *= num_partial_results;\n      return kLinearIndexingX;\n    } else {\n      return kStridedIndexingX;\n    }\n  }()",
          "old_text": null,
          "new_text": "[&]() {\n    if (reduction_dimensions.is_row_reduction &&\n        // P100, only try to vectorize+coales memory access when the\n        // tile size fits exactly and dtypes <= 32 bits\n        ((cc.major == 6 && smallest_input_dtype_bits <= 32 && tile_fit) ||\n         // On V100, only try to vectorize+coales memory access for\n         // rows of even size.  For odd row sizes, every other row\n         // isn't aligned, so it can't be vectorized.\n         (cc.major >= 7 && reduction_dimensions.dimensions[2] % 2 == 0))) {\n      return kStridedLinearIndexingX;\n    } else if (!reduction_dimensions.is_row_reduction &&\n               IsUnrollingColumnReductionBeneficial(\n                   unnested_hlo, input_shape,\n                   reduction_dimensions.dimensions[2], layout_analysis)) {\n      num_partial_results = 2;\n      reduction_tiling[2] *= num_partial_results;\n      return kLinearIndexingX;\n    } else {\n      return kStridedIndexingX;\n    }\n  }()",
          "old_line_content": "",
          "new_line_content": "  KernelMappingScheme::IndexingOrder indexing_order = [&]() {",
          "content_same": false
        },
        {
          "line": 5399,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::FusionOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::FusionOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::FusionOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5400,
          "old_api": null,
          "new_api": "EmitFusion",
          "old_text": null,
          "new_text": "EmitFusion(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitFusion(op);",
          "content_same": false
        },
        {
          "line": 5403,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::SelectAndScatterOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::SelectAndScatterOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::SelectAndScatterOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5404,
          "old_api": null,
          "new_api": "EmitSelectAndScatter",
          "old_text": null,
          "new_text": "EmitSelectAndScatter(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitSelectAndScatter(op);",
          "content_same": false
        },
        {
          "line": 5407,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::RngGetAndUpdateStateOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::RngGetAndUpdateStateOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::RngGetAndUpdateStateOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5408,
          "old_api": null,
          "new_api": "EmitRngGetAndUpdateState",
          "old_text": null,
          "new_text": "EmitRngGetAndUpdateState(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitRngGetAndUpdateState(op);",
          "content_same": false
        },
        {
          "line": 4897,
          "old_api": null,
          "new_api": "IsUnrollingColumnReductionBeneficial",
          "old_text": null,
          "new_text": "IsUnrollingColumnReductionBeneficial(\n                   unnested_hlo, input_shape,\n                   reduction_dimensions.dimensions[2], layout_analysis)",
          "old_line_content": "      return kStridedLinearIndexingX;",
          "new_line_content": "               IsUnrollingColumnReductionBeneficial(",
          "content_same": false
        },
        {
          "line": 5411,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::ScatterOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::ScatterOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::ScatterOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5412,
          "old_api": null,
          "new_api": "EmitScatter",
          "old_text": null,
          "new_text": "EmitScatter(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitScatter(op);",
          "content_same": false
        },
        {
          "line": 5415,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::SortOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::SortOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::SortOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5416,
          "old_api": null,
          "new_api": "EmitSort",
          "old_text": null,
          "new_text": "EmitSort(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitSort(op);",
          "content_same": false
        },
        {
          "line": 5419,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::ReplicaIdOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::ReplicaIdOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::ReplicaIdOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5420,
          "old_api": null,
          "new_api": "EmitReplicaOrPartitionId<ReplicaIdThunk, mlir::lmhlo::ReplicaIdOp>(\n        op)",
          "old_text": null,
          "new_text": "EmitReplicaOrPartitionId<ReplicaIdThunk, mlir::lmhlo::ReplicaIdOp>(\n        op)",
          "old_line_content": "",
          "new_line_content": "    return EmitReplicaOrPartitionId<ReplicaIdThunk, mlir::lmhlo::ReplicaIdOp>(",
          "content_same": false
        },
        {
          "line": 5424,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::PartitionIdOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::PartitionIdOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::PartitionIdOp>(op)) {",
          "content_same": false
        },
        {
          "line": 4913,
          "old_api": null,
          "new_api": "MayPreventVectorization",
          "old_text": null,
          "new_text": "MayPreventVectorization(unnested_hlo)",
          "old_line_content": "    // disable the unroll for the cases that LLVM doesn't vectorize.",
          "new_line_content": "        !MayPreventVectorization(unnested_hlo)) {",
          "content_same": false
        },
        {
          "line": 5425,
          "old_api": null,
          "new_api": "EmitReplicaOrPartitionId<PartitionIdThunk,\n                                    mlir::lmhlo::PartitionIdOp>(op)",
          "old_text": null,
          "new_text": "EmitReplicaOrPartitionId<PartitionIdThunk,\n                                    mlir::lmhlo::PartitionIdOp>(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitReplicaOrPartitionId<PartitionIdThunk,",
          "content_same": false
        },
        {
          "line": 5429,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::CollectivePermuteOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::CollectivePermuteOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::CollectivePermuteOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5430,
          "old_api": null,
          "new_api": "EmitCollectivePermute",
          "old_text": null,
          "new_text": "EmitCollectivePermute(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitCollectivePermute(op);",
          "content_same": false
        },
        {
          "line": 5433,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::AllGatherOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::AllGatherOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::AllGatherOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5434,
          "old_api": null,
          "new_api": "EmitNcclThunk<NcclAllGatherThunk, mlir::lmhlo::AllGatherOp>(op)",
          "old_text": null,
          "new_text": "EmitNcclThunk<NcclAllGatherThunk, mlir::lmhlo::AllGatherOp>(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitNcclThunk<NcclAllGatherThunk, mlir::lmhlo::AllGatherOp>(op);",
          "content_same": false
        },
        {
          "line": 4924,
          "old_api": null,
          "new_api": "ReductionCodegenInfo",
          "old_text": null,
          "new_text": "ReductionCodegenInfo(\n      mapping_scheme, num_partial_results,\n      reduction_dimensions.is_row_reduction,\n      ReductionIsRaceFree(reduction_dimensions, reduction_tiling))",
          "old_line_content": "       reduction_tiling[2] * num_threads_x},",
          "new_line_content": "  return ReductionCodegenInfo(",
          "content_same": false
        },
        {
          "line": 5437,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::AllReduceOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::AllReduceOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::AllReduceOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5438,
          "old_api": null,
          "new_api": "EmitNcclThunk<NcclAllReduceThunk, mlir::lmhlo::AllReduceOp>(op)",
          "old_text": null,
          "new_text": "EmitNcclThunk<NcclAllReduceThunk, mlir::lmhlo::AllReduceOp>(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitNcclThunk<NcclAllReduceThunk, mlir::lmhlo::AllReduceOp>(op);",
          "content_same": false
        },
        {
          "line": 4927,
          "old_api": null,
          "new_api": "ReductionIsRaceFree",
          "old_text": null,
          "new_text": "ReductionIsRaceFree(reduction_dimensions, reduction_tiling)",
          "old_line_content": "      mapping_scheme, num_partial_results,",
          "new_line_content": "      ReductionIsRaceFree(reduction_dimensions, reduction_tiling));",
          "content_same": false
        },
        {
          "line": 5441,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo_gpu::AllReduceStartOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo_gpu::AllReduceStartOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::AllReduceStartOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5442,
          "old_api": null,
          "new_api": "EmitNcclThunk<NcclAllReduceStartThunk,\n                         mlir::lmhlo_gpu::AllReduceStartOp>(op)",
          "old_text": null,
          "new_text": "EmitNcclThunk<NcclAllReduceStartThunk,\n                         mlir::lmhlo_gpu::AllReduceStartOp>(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitNcclThunk<NcclAllReduceStartThunk,",
          "content_same": false
        },
        {
          "line": 5446,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo_gpu::AllReduceDoneOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo_gpu::AllReduceDoneOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::AllReduceDoneOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5447,
          "old_api": null,
          "new_api": "EmitAllReduceDone",
          "old_text": null,
          "new_text": "EmitAllReduceDone(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitAllReduceDone(op);",
          "content_same": false
        },
        {
          "line": 4938,
          "old_api": null,
          "new_api": "GetReduceFromUnnestedMlir",
          "old_text": null,
          "new_text": "GetReduceFromUnnestedMlir(unnested_hlo, index)",
          "old_line_content": "  std::vector<HloComputation*> reducers;",
          "new_line_content": "    mlir::Operation* reduce = GetReduceFromUnnestedMlir(unnested_hlo, index);",
          "content_same": false
        },
        {
          "line": 4939,
          "old_api": null,
          "new_api": "IsReductionFromOrToContiguousDimensions",
          "old_text": null,
          "new_text": "IsReductionFromOrToContiguousDimensions(reduce, layout_analysis)",
          "old_line_content": "  for (int index : instr_index_group) {",
          "new_line_content": "    if (!IsReductionFromOrToContiguousDimensions(reduce, layout_analysis)) {",
          "content_same": false
        },
        {
          "line": 5450,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::ReduceScatterOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::ReduceScatterOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::ReduceScatterOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5451,
          "old_api": null,
          "new_api": "EmitNcclThunk<NcclReduceScatterThunk, mlir::lmhlo::ReduceScatterOp>(\n        op)",
          "old_text": null,
          "new_text": "EmitNcclThunk<NcclReduceScatterThunk, mlir::lmhlo::ReduceScatterOp>(\n        op)",
          "old_line_content": "",
          "new_line_content": "    return EmitNcclThunk<NcclReduceScatterThunk, mlir::lmhlo::ReduceScatterOp>(",
          "content_same": false
        },
        {
          "line": 4942,
          "old_api": null,
          "new_api": "mlir::dyn_cast<mlir::mhlo::ReduceOp>(reduce)",
          "old_text": null,
          "new_text": "mlir::dyn_cast<mlir::mhlo::ReduceOp>(reduce)",
          "old_line_content": "      continue;",
          "new_line_content": "    if (auto nested_reduce = mlir::dyn_cast<mlir::mhlo::ReduceOp>(reduce)) {",
          "content_same": false
        },
        {
          "line": 4943,
          "old_api": null,
          "new_api": "root_instruction",
          "old_text": null,
          "new_text": "fused_computation->root_instruction()",
          "old_line_content": "    }",
          "new_line_content": "      HloInstruction* root = fused_computation->root_instruction();",
          "content_same": false
        },
        {
          "line": 5455,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::AllToAllOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::AllToAllOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::AllToAllOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5456,
          "old_api": null,
          "new_api": "EmitNcclThunk<NcclAllToAllThunk, mlir::lmhlo::AllToAllOp>(op)",
          "old_text": null,
          "new_text": "EmitNcclThunk<NcclAllToAllThunk, mlir::lmhlo::AllToAllOp>(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitNcclThunk<NcclAllToAllThunk, mlir::lmhlo::AllToAllOp>(op);",
          "content_same": false
        },
        {
          "line": 5459,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::InfeedOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::InfeedOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::InfeedOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5460,
          "old_api": null,
          "new_api": "EmitInfeed",
          "old_text": null,
          "new_text": "EmitInfeed(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitInfeed(op);",
          "content_same": false
        },
        {
          "line": 5463,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::OutfeedOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::OutfeedOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::OutfeedOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5464,
          "old_api": null,
          "new_api": "EmitOutfeed",
          "old_text": null,
          "new_text": "EmitOutfeed(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitOutfeed(op);",
          "content_same": false
        },
        {
          "line": 4954,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "reducers.empty()",
          "old_line_content": "    }",
          "new_line_content": "  CHECK(!reducers.empty()) << \" expect at least one reduce instructions.\";",
          "content_same": false
        },
        {
          "line": 5467,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::CaseOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::CaseOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::CaseOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5468,
          "old_api": null,
          "new_api": "EmitConditional",
          "old_text": null,
          "new_text": "EmitConditional(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitConditional(op);",
          "content_same": false
        },
        {
          "line": 4957,
          "old_api": null,
          "new_api": "GetKernelMappingScheme",
          "old_text": null,
          "new_text": "reduction_info->GetKernelMappingScheme()",
          "old_line_content": "",
          "new_line_content": "      reduction_info->GetKernelMappingScheme();",
          "content_same": false
        },
        {
          "line": 4958,
          "old_api": null,
          "new_api": "GetNumberOfBlocks",
          "old_text": null,
          "new_text": "mapping_scheme.GetNumberOfBlocks()",
          "old_line_content": "  const KernelMappingScheme& mapping_scheme =",
          "new_line_content": "  LaunchDimensions launch_dimensions(mapping_scheme.GetNumberOfBlocks(),",
          "content_same": false
        },
        {
          "line": 5471,
          "old_api": null,
          "new_api": "mlir::isa<mlir::lmhlo::WhileOp>(op)",
          "old_text": null,
          "new_text": "mlir::isa<mlir::lmhlo::WhileOp>(op)",
          "old_line_content": "  }",
          "new_line_content": "  if (mlir::isa<mlir::lmhlo::WhileOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5472,
          "old_api": null,
          "new_api": "EmitWhile",
          "old_text": null,
          "new_text": "EmitWhile(op)",
          "old_line_content": "",
          "new_line_content": "    return EmitWhile(op);",
          "content_same": false
        },
        {
          "line": 5475,
          "old_api": null,
          "new_api": "MlirToString",
          "old_text": null,
          "new_text": "MlirToString(op)",
          "old_line_content": "  }",
          "new_line_content": "  return InternalError(\"Unrecognized op: %s\", MlirToString(op));",
          "content_same": false
        },
        {
          "line": 5479,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "region->front()",
          "old_line_content": "",
          "new_line_content": "  for (mlir::Operation& op : llvm::make_early_inc_range(region->front())) {",
          "content_same": false
        },
        {
          "line": 5480,
          "old_api": null,
          "new_api": "EmitOp",
          "old_text": null,
          "new_text": "EmitOp(&op)",
          "old_line_content": "Status IrEmitterUnnested::EmitLmhloRegion(mlir::Region* region) {",
          "new_line_content": "    TF_RETURN_IF_ERROR(EmitOp(&op));",
          "content_same": false
        },
        {
          "line": 4969,
          "old_api": null,
          "new_api": "EmitTileElementForReduction",
          "old_text": null,
          "new_text": "EmitTileElementForReduction(\n            unnested_hlo, input_shape, instr_index_group, fused_computation,\n            fused_emitter, result_ir_arrays, reducers, index, *reduction_info,\n            x_iter_num, layout_analysis)",
          "old_line_content": "      [&](const llvm_ir::IrArray::Index& index, llvm::Value* y_loc,",
          "new_line_content": "        EmitTileElementForReduction(",
          "content_same": false
        },
        {
          "line": 5486,
          "old_api": null,
          "new_api": "op->getParentOfType<mlir::ModuleOp>()",
          "old_text": null,
          "new_text": "op->getParentOfType<mlir::ModuleOp>()",
          "old_line_content": "",
          "new_line_content": "  auto module = op->getParentOfType<mlir::ModuleOp>();",
          "content_same": false
        },
        {
          "line": 4975,
          "old_api": null,
          "new_api": "GetKernelMappingScheme",
          "old_text": null,
          "new_text": "EmitTilingKernel(\n      mapping_scheme, index_ty,\n      [&](const ThreadIdInfo& thread_id_info, const IrArray::Index& index,\n          const string& loop_name, llvm::Value* tile_height,\n          llvm::Value* tile_width, KernelSupportLibrary* ksl) {\n        EmitTile(reduction_info->GetKernelMappingScheme(), index, loop_name,\n                 ksl, thread_id_info, tile_height, tile_width,\n                 emit_reduction_tile);\n      })",
          "old_line_content": "      };",
          "new_line_content": "  TilingKernelInfo tiling_kernel_info = EmitTilingKernel(",
          "content_same": false
        },
        {
          "line": 5489,
          "old_api": null,
          "new_api": "getLoc",
          "old_text": null,
          "new_text": "op->getLoc()",
          "old_line_content": "  Thunk::ThunkInfo thunk_info;",
          "new_line_content": "      \"Thunk:#hlo_op=%s,hlo_module=%s#\", mlir::GetNameFromLoc(op->getLoc()),",
          "content_same": false
        },
        {
          "line": 4980,
          "old_api": null,
          "new_api": "GetKernelMappingScheme",
          "old_text": null,
          "new_text": "reduction_info->GetKernelMappingScheme()",
          "old_line_content": "          const string& loop_name, llvm::Value* tile_height,",
          "new_line_content": "        EmitTile(reduction_info->GetKernelMappingScheme(), index, loop_name,",
          "content_same": false
        },
        {
          "line": 5495,
          "old_api": null,
          "new_api": "getLoc",
          "old_text": null,
          "new_text": "op->getLoc()",
          "old_line_content": "",
          "new_line_content": "  this->name = mlir::GetNameFromLoc(op->getLoc());",
          "content_same": false
        },
        {
          "line": 4984,
          "old_api": null,
          "new_api": "EmitEpilogueForReduction",
          "old_text": null,
          "new_text": "EmitEpilogueForReduction(index_ty, unnested_hlo, instr_index_group,\n                           result_ir_arrays, reducers, *reduction_info,\n                           tiling_kernel_info, layout_analysis)",
          "old_line_content": "                 emit_reduction_tile);",
          "new_line_content": "  EmitEpilogueForReduction(index_ty, unnested_hlo, instr_index_group,",
          "content_same": false
        },
        {
          "line": 5498,
          "old_api": null,
          "new_api": "GetHloOutputs",
          "old_text": null,
          "new_text": "GetHloOutputs(op)",
          "old_line_content": "",
          "new_line_content": "  auto outputs = GetHloOutputs(op);",
          "content_same": false
        },
        {
          "line": 5503,
          "old_api": null,
          "new_api": "GetShape",
          "old_text": null,
          "new_text": "GetShape(output)",
          "old_line_content": "  }",
          "new_line_content": "    output_shapes.push_back(GetShape(output));",
          "content_same": false
        },
        {
          "line": 4994,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "instr.shape()",
          "old_line_content": "// broadcasted constant/scalar.",
          "new_line_content": "  return instr.IsConstant() || ShapeUtil::IsScalar(instr.shape()) ||",
          "content_same": false
        },
        {
          "line": 4995,
          "old_api": null,
          "new_api": "opcode",
          "old_text": null,
          "new_text": "instr.opcode()",
          "old_line_content": "bool IsBroadcastedConstantOrScalar(const HloInstruction& instr) {",
          "new_line_content": "         (HloOpcode::kBroadcast == instr.opcode() &&",
          "content_same": false
        },
        {
          "line": 5009,
          "old_api": null,
          "new_api": "CHECK_NE",
          "old_text": null,
          "new_text": "CHECK_NE(0, num_reduces)",
          "old_line_content": "std::vector<std::vector<int>> GroupDisjointReductions(",
          "new_line_content": "  CHECK_NE(0, num_reduces);",
          "content_same": false
        },
        {
          "line": 5017,
          "old_api": null,
          "new_api": "Get",
          "old_text": null,
          "new_text": "disjoint_sets[i].Get()",
          "old_line_content": "      num_reduces);",
          "new_line_content": "    disjoint_sets[i].Get() =",
          "content_same": false
        },
        {
          "line": 5018,
          "old_api": null,
          "new_api": "root_instruction",
          "old_text": null,
          "new_text": "fused_computation->root_instruction()->mutable_operand(i)",
          "old_line_content": "  for (size_t i = 0; i < num_reduces; ++i) {",
          "new_line_content": "        fused_computation->root_instruction()->mutable_operand(i);",
          "content_same": false
        },
        {
          "line": 5022,
          "old_api": null,
          "new_api": "HloReachabilityMap::Build(fused_computation)",
          "old_text": null,
          "new_text": "HloReachabilityMap::Build(fused_computation)",
          "old_line_content": "",
          "new_line_content": "      HloReachabilityMap::Build(fused_computation);",
          "content_same": false
        },
        {
          "line": 5023,
          "old_api": null,
          "new_api": "instructions",
          "old_text": null,
          "new_text": "fused_computation->instructions()",
          "old_line_content": "  std::unique_ptr<HloReachabilityMap> reachability_map =",
          "new_line_content": "  for (HloInstruction* instr : fused_computation->instructions()) {",
          "content_same": false
        },
        {
          "line": 5027,
          "old_api": null,
          "new_api": "root_instruction",
          "old_text": null,
          "new_text": "fused_computation->root_instruction()->mutable_operand(oid)",
          "old_line_content": "    for (size_t oid = 0; oid < num_reduces; ++oid) {",
          "new_line_content": "          fused_computation->root_instruction()->mutable_operand(oid);",
          "content_same": false
        },
        {
          "line": 5028,
          "old_api": null,
          "new_api": "opcode",
          "old_text": null,
          "new_text": "reduce->opcode()",
          "old_line_content": "      HloInstruction* reduce =",
          "new_line_content": "      if (HloOpcode::kReduce == reduce->opcode() &&",
          "content_same": false
        },
        {
          "line": 5032,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "instr->ToString()",
          "old_line_content": "        // Do not group output reduce instructions through broadcasted",
          "new_line_content": "        VLOG(3) << \"Skip broadcasted constant or scalar \" << instr->ToString();",
          "content_same": false
        },
        {
          "line": 5036,
          "old_api": null,
          "new_api": "IsReachable",
          "old_text": null,
          "new_text": "reachability_map->IsReachable(instr, reduce)",
          "old_line_content": "      }",
          "new_line_content": "      if (reachability_map->IsReachable(instr, reduce)) {",
          "content_same": false
        },
        {
          "line": 5037,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "reduce->ToString()",
          "old_line_content": "      // Now group output instructions if they have common predecessors.",
          "new_line_content": "        VLOG(3) << \"Reaching \" << reduce->ToString() << \" from \"",
          "content_same": false
        },
        {
          "line": 5042,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "reached_output_ids.size()",
          "old_line_content": "      }",
          "new_line_content": "    for (size_t j = 1; j < reached_output_ids.size(); ++j) {",
          "content_same": false
        },
        {
          "line": 5043,
          "old_api": null,
          "new_api": "Merge",
          "old_text": null,
          "new_text": "disjoint_sets[reached_output_ids[0]].Merge(\n          &disjoint_sets[reached_output_ids[j]])",
          "old_line_content": "    }",
          "new_line_content": "      disjoint_sets[reached_output_ids[0]].Merge(",
          "content_same": false
        },
        {
          "line": 5050,
          "old_api": null,
          "new_api": "Get",
          "old_text": null,
          "new_text": "disjoint_sets[oid].Get()",
          "old_line_content": "  HloInstructionMap<std::vector<int>> groups;",
          "new_line_content": "    groups[disjoint_sets[oid].Get()].push_back(oid);",
          "content_same": false
        },
        {
          "line": 5054,
          "old_api": null,
          "new_api": "emplace_back",
          "old_text": null,
          "new_text": "absl::c_for_each(\n      groups, [&](auto& iter) { ret.emplace_back(std::move(iter.second)); })",
          "old_line_content": "",
          "new_line_content": "  absl::c_for_each(",
          "content_same": false
        },
        {
          "line": 5055,
          "old_api": null,
          "new_api": "std::move(iter.second)",
          "old_text": null,
          "new_text": "std::move(iter.second)",
          "old_line_content": "  std::vector<std::vector<int>> ret;",
          "new_line_content": "      groups, [&](auto& iter) { ret.emplace_back(std::move(iter.second)); });",
          "content_same": false
        },
        {
          "line": 5064,
          "old_api": null,
          "new_api": "mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "old_text": null,
          "new_text": "mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "old_line_content": "    mlir::Operation* unnested_hlo,",
          "new_line_content": "  auto fusion = mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo);",
          "content_same": false
        },
        {
          "line": 5072,
          "old_api": null,
          "new_api": "GetReduceFromUnnestedMlir",
          "old_text": null,
          "new_text": "GetReduceFromUnnestedMlir(unnested_hlo, i)",
          "old_line_content": "  for (int i = 0; i < num_reduces; ++i) {",
          "new_line_content": "        GetReduceFromUnnestedMlir(unnested_hlo, i);",
          "content_same": false
        },
        {
          "line": 5073,
          "old_api": null,
          "new_api": "IsReductionFromOrToContiguousDimensions",
          "old_text": null,
          "new_text": "IsReductionFromOrToContiguousDimensions(output_instruction,\n                                                layout_analysis)",
          "old_line_content": "    mlir::Operation* output_instruction =",
          "new_line_content": "    if (IsReductionFromOrToContiguousDimensions(output_instruction,",
          "content_same": false
        },
        {
          "line": 5079,
          "old_api": null,
          "new_api": "MlirToString",
          "old_text": null,
          "new_text": "MlirToString(unnested_hlo)",
          "old_line_content": "    }",
          "new_line_content": "  CHECK(first_reduce) << MlirToString(unnested_hlo);",
          "content_same": false
        },
        {
          "line": 5082,
          "old_api": null,
          "new_api": "mlir::dyn_cast<mlir::mhlo::ReduceOp>(\n          GetReduceFromUnnestedMlir(unnested_hlo, i))",
          "old_text": null,
          "new_text": "mlir::dyn_cast<mlir::mhlo::ReduceOp>(\n          GetReduceFromUnnestedMlir(unnested_hlo, i))",
          "old_line_content": "  if (num_reduces > 1) {",
          "new_line_content": "      auto candidate = mlir::dyn_cast<mlir::mhlo::ReduceOp>(",
          "content_same": false
        },
        {
          "line": 5083,
          "old_api": null,
          "new_api": "GetReduceFromUnnestedMlir",
          "old_text": null,
          "new_text": "GetReduceFromUnnestedMlir(unnested_hlo, i)",
          "old_line_content": "    for (int i = 0; i < num_reduces; i++) {",
          "new_line_content": "          GetReduceFromUnnestedMlir(unnested_hlo, i));",
          "content_same": false
        },
        {
          "line": 5086,
          "old_api": null,
          "new_api": "mlir::cast<mlir::mhlo::ReduceOp>(first_reduce)",
          "old_text": null,
          "new_text": "mlir::cast<mlir::mhlo::ReduceOp>(first_reduce)",
          "old_line_content": "      if (candidate &&",
          "new_line_content": "              candidate, mlir::cast<mlir::mhlo::ReduceOp>(first_reduce),",
          "content_same": false
        },
        {
          "line": 5092,
          "old_api": null,
          "new_api": "getOperand",
          "old_text": null,
          "new_text": "first_reduce->getOperand(0)",
          "old_line_content": "    }",
          "new_line_content": "  Shape input_shape = GetShape(first_reduce->getOperand(0));",
          "content_same": false
        },
        {
          "line": 5095,
          "old_api": null,
          "new_api": "has_layout",
          "old_text": null,
          "new_text": "input_shape.has_layout()",
          "old_line_content": "  // The layout of a reduction input is either set by LayoutAssignment for",
          "new_line_content": "  CHECK(input_shape.has_layout()) << \"LayoutAssignment or InstructionFusion \"",
          "content_same": false
        },
        {
          "line": 5100,
          "old_api": null,
          "new_api": "region",
          "old_text": null,
          "new_text": "TF_ASSIGN_OR_RETURN(fused_computation,\n                      GetOrCreateSubComputationFromRegion(&fusion.region(),\n                                                          /*is_fusion=*/true))",
          "old_line_content": "",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(fused_computation,",
          "content_same": false
        },
        {
          "line": 5101,
          "old_api": null,
          "new_api": "region",
          "old_text": null,
          "new_text": "fusion.region()",
          "old_line_content": "  HloComputation* fused_computation = nullptr;",
          "new_line_content": "                      GetOrCreateSubComputationFromRegion(&fusion.region(),",
          "content_same": false
        },
        {
          "line": 5106,
          "old_api": null,
          "new_api": "GroupDisjointReductions",
          "old_text": null,
          "new_text": "GroupDisjointReductions(fused_computation, num_reduces)",
          "old_line_content": "  // Group disjoint reductions in groups, to be executed in parallel.",
          "new_line_content": "      GroupDisjointReductions(fused_computation, num_reduces);",
          "content_same": false
        },
        {
          "line": 5109,
          "old_api": null,
          "new_api": "MlirToString",
          "old_text": null,
          "new_text": "MlirToString(unnested_hlo)",
          "old_line_content": "",
          "new_line_content": "                    MlirToString(unnested_hlo));",
          "content_same": false
        },
        {
          "line": 5112,
          "old_api": null,
          "new_api": "ComputeReductionCodegenInfo",
          "old_text": null,
          "new_text": "ComputeReductionCodegenInfo(unnested_hlo, first_reduce, layout_analysis)",
          "old_line_content": "",
          "new_line_content": "      ComputeReductionCodegenInfo(unnested_hlo, first_reduce, layout_analysis);",
          "content_same": false
        },
        {
          "line": 5119,
          "old_api": null,
          "new_api": "GetNumberOfBlocks",
          "old_text": null,
          "new_text": "mapping_scheme.GetNumberOfBlocks()",
          "old_line_content": "  // group can be run in parallel by a different BlockIdy.",
          "new_line_content": "      {/*x=*/mapping_scheme.GetNumberOfBlocks(),",
          "content_same": false
        }
      ],
      "deletions": [
        {
          "line": 5121,
          "old_api": "GetNumberOfBlocks",
          "new_api": null,
          "old_text": "mapping_scheme.GetNumberOfBlocks()",
          "new_text": null,
          "old_line_content": "      {/*x=*/mapping_scheme.GetNumberOfBlocks(),",
          "new_line_content": "       /*z=*/1},",
          "content_same": false
        },
        {
          "line": 5127,
          "old_api": "GetNumberOfBlocks",
          "new_api": null,
          "old_text": "mapping_scheme.GetNumberOfBlocks()",
          "new_text": null,
          "old_line_content": "          << \": number of blocks: \" << mapping_scheme.GetNumberOfBlocks()",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5128,
          "old_api": "GetThreadsPerBlock",
          "new_api": null,
          "old_text": "mapping_scheme.GetThreadsPerBlock()",
          "new_text": null,
          "old_line_content": "          << \" - threads per block: \" << mapping_scheme.GetThreadsPerBlock();",
          "new_line_content": "  std::vector<llvm_ir::IrArray> ir_arrays;",
          "content_same": false
        },
        {
          "line": 5131,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": null,
          "old_text": "TF_ASSIGN_OR_RETURN(std::unique_ptr<KernelThunk> kernel_thunk,\n                      BuildKernelThunk(unnested_hlo, Thunk::ThunkInfo(),\n                                       &ir_arrays, launch_dimensions))",
          "new_text": null,
          "old_line_content": "  TF_ASSIGN_OR_RETURN(std::unique_ptr<KernelThunk> kernel_thunk,",
          "new_line_content": "                                       &ir_arrays, launch_dimensions));",
          "content_same": false
        },
        {
          "line": 5132,
          "old_api": "Thunk::ThunkInfo()",
          "new_api": null,
          "old_text": "Thunk::ThunkInfo()",
          "new_text": null,
          "old_line_content": "                      BuildKernelThunk(unnested_hlo, Thunk::ThunkInfo(),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5136,
          "old_api": "llvm_module",
          "new_api": null,
          "old_text": "ir_emitter_context_->llvm_module()",
          "new_text": null,
          "old_line_content": "                                          ir_emitter_context_->llvm_module(),",
          "new_line_content": "  FusedIrEmitter fused_emitter(&elemental_emitter);",
          "content_same": false
        },
        {
          "line": 5137,
          "old_api": "GetNestedComputer",
          "new_api": null,
          "old_text": "GetNestedComputer()",
          "new_text": null,
          "old_line_content": "                                          &b_, GetNestedComputer());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5140,
          "old_api": "size",
          "new_api": null,
          "old_text": "ir_arrays.size()",
          "new_text": null,
          "old_line_content": "  CHECK_LT(fused_computation->num_parameters(), ir_arrays.size());",
          "new_line_content": "    llvm_ir::IrArray ir_array = ir_arrays[i];",
          "content_same": false
        },
        {
          "line": 5143,
          "old_api": "parameter_instruction",
          "new_api": null,
          "old_text": "fused_computation->parameter_instruction(i)",
          "new_text": null,
          "old_line_content": "    HloInstruction* fused_operand = fused_computation->parameter_instruction(i);",
          "new_line_content": "        fused_operand,",
          "content_same": false
        },
        {
          "line": 5144,
          "old_api": "BindGenerator",
          "new_api": null,
          "old_text": "fused_emitter.BindGenerator(\n        fused_operand,\n        [this, ir_array, fused_operand](const llvm_ir::IrArray::Index& index) {\n          return ir_array.EmitReadArrayElement(index, &b_,\n                                               fused_operand->name());\n        })",
          "new_text": null,
          "old_line_content": "    fused_emitter.BindGenerator(",
          "new_line_content": "        [this, ir_array, fused_operand](const llvm_ir::IrArray::Index& index) {",
          "content_same": false
        },
        {
          "line": 5147,
          "old_api": "EmitReadArrayElement",
          "new_api": null,
          "old_text": "ir_array.EmitReadArrayElement(index, &b_,\n                                               fused_operand->name())",
          "new_text": null,
          "old_line_content": "          return ir_array.EmitReadArrayElement(index, &b_,",
          "new_line_content": "        });",
          "content_same": false
        },
        {
          "line": 5148,
          "old_api": "name",
          "new_api": null,
          "old_text": "fused_operand->name()",
          "new_text": null,
          "old_line_content": "                                               fused_operand->name());",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5152,
          "old_api": "num_parameters",
          "new_api": null,
          "old_text": "fused_computation->num_parameters()",
          "new_text": null,
          "old_line_content": "      absl::MakeSpan(ir_arrays).subspan(fused_computation->num_parameters(),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5159,
          "old_api": "ComputeReductionCodegenInfo",
          "new_api": null,
          "old_text": "ComputeReductionCodegenInfo(unnested_hlo, first_reduce, layout_analysis)",
          "new_text": null,
          "old_line_content": "      ComputeReductionCodegenInfo(unnested_hlo, first_reduce, layout_analysis);",
          "new_line_content": "  KernelSupportLibrary ksl(&b_, llvm_ir::UnrollMode::kDefaultUnroll);",
          "content_same": false
        },
        {
          "line": 5162,
          "old_api": "size",
          "new_api": null,
          "old_text": "instr_index_groups.size()",
          "new_text": null,
          "old_line_content": "  for (size_t i = 0; i < instr_index_groups.size(); ++i) {",
          "new_line_content": "    // code generation per reduction group.",
          "content_same": false
        },
        {
          "line": 5166,
          "old_api": "ReductionCodegenState",
          "new_api": null,
          "old_text": "ReductionCodegenState(reduction_codegen_info)",
          "new_text": null,
          "old_line_content": "        ReductionCodegenState(reduction_codegen_info);",
          "new_line_content": "    // block_id_y instead of block_id_x simplifies the index calculation",
          "content_same": false
        },
        {
          "line": 5176,
          "old_api": "getInt32",
          "new_api": null,
          "old_text": "b_.getInt32(i)",
          "new_text": null,
          "old_line_content": "           b_.CreateICmpEQ(raw_block_id_y, b_.getInt32(i)), [&] {",
          "new_line_content": "                                fused_computation, &fused_emitter,",
          "content_same": false
        },
        {
          "line": 5177,
          "old_api": "EmitIRForReduction",
          "new_api": null,
          "old_text": "EmitIRForReduction(unnested_hlo, instr_index_groups[i],\n                                fused_computation, &fused_emitter,\n                                result_ir_arrays, &reduction_info, input_shape,\n                                layout_analysis)",
          "new_text": null,
          "old_line_content": "             EmitIRForReduction(unnested_hlo, instr_index_groups[i],",
          "new_line_content": "                                result_ir_arrays, &reduction_info, input_shape,",
          "content_same": false
        },
        {
          "line": 5185,
          "old_api": "IsRaceFree",
          "new_api": null,
          "old_text": "reduction_codegen_info.IsRaceFree()",
          "new_text": null,
          "old_line_content": "      !reduction_codegen_info.IsRaceFree()) {",
          "new_line_content": "        \"All reductions should be race-free if deterministic reductions are \"",
          "content_same": false
        },
        {
          "line": 5186,
          "old_api": "InternalError",
          "new_api": null,
          "old_text": "InternalError(\n        \"All reductions should be race-free if deterministic reductions are \"\n        \"enabled\")",
          "new_text": null,
          "old_line_content": "    return InternalError(",
          "new_line_content": "        \"enabled\");",
          "content_same": false
        },
        {
          "line": 5195,
          "old_api": "GetReduceFromUnnestedMlir",
          "new_api": null,
          "old_text": "GetReduceFromUnnestedMlir(unnested_hlo, i)",
          "new_text": null,
          "old_line_content": "        GetReduceFromUnnestedMlir(unnested_hlo, i);",
          "new_line_content": "                                                 layout_analysis)) {",
          "content_same": false
        },
        {
          "line": 5196,
          "old_api": "IsReductionFromOrToContiguousDimensions",
          "new_api": null,
          "old_text": "IsReductionFromOrToContiguousDimensions(output_instruction,\n                                                 layout_analysis)",
          "new_text": null,
          "old_line_content": "    if (!IsReductionFromOrToContiguousDimensions(output_instruction,",
          "new_line_content": "      // Elemental IR emitter is used.",
          "content_same": false
        },
        {
          "line": 5200,
          "old_api": "IsRaceFree",
          "new_api": null,
          "old_text": "reduction_codegen_info.IsRaceFree()",
          "new_text": null,
          "old_line_content": "    } else if (reduction_codegen_info.IsRaceFree()) {",
          "new_line_content": "      continue;",
          "content_same": false
        },
        {
          "line": 5201,
          "old_api": "VLOG",
          "new_api": null,
          "old_text": "VLOG(5)",
          "new_text": null,
          "old_line_content": "      VLOG(5) << \"We do not need initialization: using tree reductions\";",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5206,
          "old_api": "BuildFusedInitializerThunk",
          "new_api": null,
          "old_text": "BuildFusedInitializerThunk(fusion, i)",
          "new_text": null,
          "old_line_content": "                        BuildFusedInitializerThunk(fusion, i));",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5207,
          "old_api": "std::move(initializer_thunk)",
          "new_api": null,
          "old_text": "std::move(initializer_thunk)",
          "new_text": null,
          "old_line_content": "    thunks.push_back(std::move(initializer_thunk));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5212,
          "old_api": "std::move(thunks)",
          "new_api": null,
          "old_text": "std::move(thunks)",
          "new_text": null,
          "old_line_content": "      GetThunkInfo(unnested_hlo), std::move(thunks));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5215,
          "old_api": "Status::OK()",
          "new_api": null,
          "old_text": "Status::OK()",
          "new_text": null,
          "old_line_content": "  return Status::OK();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5239,
          "old_api": "VLOG",
          "new_api": null,
          "old_text": "VLOG(10)",
          "new_text": null,
          "old_line_content": "  VLOG(10) << \"Emitting slice input fusion for \"",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5244,
          "old_api": "opcode",
          "new_api": null,
          "old_text": "slice_or_tuple->opcode()",
          "new_text": null,
          "old_line_content": "    if (slice_or_tuple->opcode() == HloOpcode::kSlice) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5247,
          "old_api": "opcode",
          "new_api": null,
          "old_text": "slice_or_tuple->opcode()",
          "new_text": null,
          "old_line_content": "    CHECK_EQ(slice_or_tuple->opcode(), HloOpcode::kTuple);",
          "new_line_content": "  }();",
          "content_same": false
        },
        {
          "line": 5248,
          "old_api": "operands",
          "new_api": null,
          "old_text": "slice_or_tuple->operands()",
          "new_text": null,
          "old_line_content": "    return slice_or_tuple->operands();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5257,
          "old_api": "BindGenerator",
          "new_api": null,
          "old_text": "fused_emitter.BindGenerator(\n        fused_computation->parameter_instruction(i),\n        [this, &ir_arrays, i](llvm_ir::IrArray::Index index) {\n          return ir_arrays[i].EmitReadArrayElement(index, &b_);\n        })",
          "new_text": null,
          "old_line_content": "    fused_emitter.BindGenerator(",
          "new_line_content": "        [this, &ir_arrays, i](llvm_ir::IrArray::Index index) {",
          "content_same": false
        },
        {
          "line": 5260,
          "old_api": "EmitReadArrayElement",
          "new_api": null,
          "old_text": "ir_arrays[i].EmitReadArrayElement(index, &b_)",
          "new_text": null,
          "old_line_content": "          return ir_arrays[i].EmitReadArrayElement(index, &b_);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5264,
          "old_api": "operand",
          "new_api": null,
          "old_text": "slice->operand(0)",
          "new_text": null,
          "old_line_content": "    auto input_generator = *fused_emitter.GetGenerator(slice->operand(0));",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5265,
          "old_api": "ValueOrDie",
          "new_api": null,
          "old_text": "input_generator(index).ValueOrDie()",
          "new_text": null,
          "old_line_content": "    input_ir_values.push_back(input_generator(index).ValueOrDie());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5270,
          "old_api": "size",
          "new_api": null,
          "old_text": "slice_instructions.size()",
          "new_text": null,
          "old_line_content": "  for (int64_t i = 0; i < slice_instructions.size(); ++i) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5281,
          "old_api": "multidim",
          "new_api": null,
          "old_text": "index.multidim()",
          "new_text": null,
          "old_line_content": "          index.multidim()[dim],",
          "new_line_content": "      llvm::Value* within_range =",
          "content_same": false
        },
        {
          "line": 5284,
          "old_api": "CreateAnd",
          "new_api": null,
          "old_text": "b_.CreateAnd(larger_or_equal_than_start, smaller_than_limit)",
          "new_text": null,
          "old_line_content": "          b_.CreateAnd(larger_or_equal_than_start, smaller_than_limit);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4775,
          "old_api": "static_cast<uint64_t>(num_kept_minor)",
          "new_api": null,
          "old_text": "static_cast<uint64_t>(num_kept_minor)",
          "new_text": null,
          "old_line_content": "  if (!IsPowerOfTwo(static_cast<uint64_t>(num_kept_minor))) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5287,
          "old_api": "CreateAnd",
          "new_api": null,
          "old_text": "b_.CreateAnd(index_within_ranges)",
          "new_text": null,
          "old_line_content": "    llvm::Value* guarding_cond = b_.CreateAnd(index_within_ranges);",
          "new_line_content": "    auto emit_slice_elem_func = [&] {",
          "content_same": false
        },
        {
          "line": 4779,
          "old_api": "IsReductionFromOrToContiguousDimensions",
          "new_api": null,
          "old_text": "IsReductionFromOrToContiguousDimensions(unnested_hlo, layout_analysis)",
          "new_text": null,
          "old_line_content": "  if (IsReductionFromOrToContiguousDimensions(unnested_hlo, layout_analysis)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5291,
          "old_api": "size",
          "new_api": null,
          "old_text": "src_multidim.size()",
          "new_text": null,
          "old_line_content": "      std::vector<llvm::Value*> dst_multidim(src_multidim.size());",
          "new_line_content": "        dst_multidim[dim] =",
          "content_same": false
        },
        {
          "line": 5294,
          "old_api": "GetConstantWithIndexType",
          "new_api": null,
          "old_text": "Sub(src_multidim[dim],\n                index.GetConstantWithIndexType(slice->slice_starts(dim)))",
          "new_text": null,
          "old_line_content": "            Sub(src_multidim[dim],",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 4783,
          "old_api": "mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "new_api": null,
          "old_text": "mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "new_text": null,
          "old_line_content": "  auto fusion = mlir::cast<mlir::lmhlo::FusionOp>(unnested_hlo);",
          "new_line_content": "  int64_t cannot_be_vectorized = 0;",
          "content_same": false
        },
        {
          "line": 5295,
          "old_api": "slice_starts",
          "new_api": null,
          "old_text": "slice->slice_starts(dim)",
          "new_text": null,
          "old_line_content": "                index.GetConstantWithIndexType(slice->slice_starts(dim)));",
          "new_line_content": "      llvm_ir::IrArray src_ir_array =",
          "content_same": false
        },
        {
          "line": 5300,
          "old_api": "GetType",
          "new_api": null,
          "old_text": "index.GetType()",
          "new_text": null,
          "old_line_content": "                                     index.GetType());",
          "new_line_content": "                                         &b_);",
          "content_same": false
        },
        {
          "line": 5301,
          "old_api": "EmitWriteArrayElement",
          "new_api": null,
          "old_text": "src_ir_array.EmitWriteArrayElement(slice_dst_index, input_ir_values[i],\n                                         &b_)",
          "new_text": null,
          "old_line_content": "      src_ir_array.EmitWriteArrayElement(slice_dst_index, input_ir_values[i],",
          "new_line_content": "    };",
          "content_same": false
        },
        {
          "line": 4790,
          "old_api": "getDefiningOp",
          "new_api": null,
          "old_text": "fusion_results[0].getDefiningOp()",
          "new_text": null,
          "old_line_content": "            fusion_results[0].getDefiningOp(), layout_analysis)) {",
          "new_line_content": "      // Atomic.add of the reduction result can't be vectorized.",
          "content_same": false
        },
        {
          "line": 4791,
          "old_api": "getDefiningOp",
          "new_api": null,
          "old_text": "fusion_results[0].getDefiningOp()",
          "new_text": null,
          "old_line_content": "      use_chain_endings.insert(fusion_results[0].getDefiningOp());",
          "new_line_content": "      cannot_be_vectorized++;",
          "content_same": false
        },
        {
          "line": 5307,
          "old_api": "Status::OK()",
          "new_api": null,
          "old_text": "Status::OK()",
          "new_text": null,
          "old_line_content": "  return Status::OK();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4797,
          "old_api": "getDefiningOp",
          "new_api": null,
          "old_text": "result.getDefiningOp()",
          "new_text": null,
          "old_line_content": "      if (IsReductionFromOrToContiguousDimensions(result.getDefiningOp(),",
          "new_line_content": "        // Atomic.add of the reduction result can't be vectorized.",
          "content_same": false
        },
        {
          "line": 5314,
          "old_api": "constexpr",
          "new_api": null,
          "old_text": "constexpr",
          "new_text": null,
          "old_line_content": "  constexpr int unroll_factor = 1;",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(const HloComputation* fused_computation,",
          "content_same": false
        },
        {
          "line": 4805,
          "old_api": "getDefiningOp",
          "new_api": null,
          "old_text": "result.getDefiningOp()",
          "new_text": null,
          "old_line_content": "      use_chain_endings.insert(result.getDefiningOp());",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4810,
          "old_api": "NumInputsInvolveInOnlyElementwiseOps",
          "new_api": null,
          "old_text": "NumInputsInvolveInOnlyElementwiseOps(fusion, input_shape,\n                                                            use_chain_endings)",
          "new_text": null,
          "old_line_content": "  can_be_vectorized += NumInputsInvolveInOnlyElementwiseOps(fusion, input_shape,",
          "new_line_content": "  // Fusion inputs with more elements than the reduce op input must participate",
          "content_same": false
        },
        {
          "line": 4817,
          "old_api": "NumInputsWithMoreElementsThan",
          "new_api": null,
          "old_text": "NumInputsWithMoreElementsThan(fusion, input_shape)",
          "new_text": null,
          "old_line_content": "  cannot_be_vectorized += NumInputsWithMoreElementsThan(fusion, input_shape);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5335,
          "old_api": "EmitElementForInputFusibleSlices",
          "new_api": null,
          "old_text": "EmitElementForInputFusibleSlices(fused_computation,\n                                                    ir_arrays, index)",
          "new_text": null,
          "old_line_content": "            return EmitElementForInputFusibleSlices(fused_computation,",
          "new_line_content": "          },",
          "content_same": false
        },
        {
          "line": 4825,
          "old_api": "tensorflow::NextPowerOfTwo64(v)",
          "new_api": null,
          "old_text": "tensorflow::NextPowerOfTwo64(v)",
          "new_text": null,
          "old_line_content": "  int64_t upper = tensorflow::NextPowerOfTwo64(v);",
          "new_line_content": "  return upper - v < v - lower ? upper : lower;",
          "content_same": false
        },
        {
          "line": 5340,
          "old_api": "launch_bound",
          "new_api": null,
          "old_text": "GetIndexTypeForKernel(\n                        fusion, launch_dimensions.launch_bound(), &b_)",
          "new_text": null,
          "old_line_content": "                    GetIndexTypeForKernel(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5343,
          "old_api": "std::move(kernel_thunk)",
          "new_api": null,
          "old_text": "std::move(kernel_thunk)",
          "new_text": null,
          "old_line_content": "  thunk_sequence_.emplace_back(std::move(kernel_thunk));",
          "new_line_content": "  return emit_status;",
          "content_same": false
        },
        {
          "line": 4837,
          "old_api": "GetReductionKindAndContiguousComponents",
          "new_api": null,
          "old_text": "GetReductionKindAndContiguousComponents(first_reduce)",
          "new_text": null,
          "old_line_content": "      GetReductionKindAndContiguousComponents(first_reduce);",
          "new_line_content": "           << \" \" << reduction_dimensions.dimensions[0] << \" \"",
          "content_same": false
        },
        {
          "line": 4838,
          "old_api": "VLOG",
          "new_api": null,
          "old_text": "VLOG(10)",
          "new_text": null,
          "old_line_content": "  VLOG(10) << \"is_row_reduction \" << reduction_dimensions.is_row_reduction",
          "new_line_content": "           << reduction_dimensions.dimensions[1] << \" \"",
          "content_same": false
        },
        {
          "line": 5349,
          "old_api": "mlir::isa<mlir::ConstantOp, mlir::memref::ViewOp,\n                mlir::memref::ReinterpretCastOp, mlir::ReturnOp,\n                mlir::lmhlo::TerminatorOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::ConstantOp, mlir::memref::ViewOp,\n                mlir::memref::ReinterpretCastOp, mlir::ReturnOp,\n                mlir::lmhlo::TerminatorOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::ConstantOp, mlir::memref::ViewOp,",
          "new_line_content": "                mlir::lmhlo::TerminatorOp>(op)) {",
          "content_same": false
        },
        {
          "line": 5352,
          "old_api": "Status::OK()",
          "new_api": null,
          "old_text": "Status::OK()",
          "new_text": null,
          "old_line_content": "    return Status::OK();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5355,
          "old_api": "mlir::isa<mlir::memref::GetGlobalOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::memref::GetGlobalOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::memref::GetGlobalOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4844,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "GetShape(i).element_type()",
          "new_text": null,
          "old_line_content": "    return primitive_util::BitWidth(GetShape(i).element_type());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5356,
          "old_api": "EmitConstant",
          "new_api": null,
          "old_text": "EmitConstant(op)",
          "new_text": null,
          "old_line_content": "    return EmitConstant(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5360,
          "old_api": "call_target_name",
          "new_api": null,
          "old_text": "call.call_target_name()",
          "new_text": null,
          "old_line_content": "    if (call.call_target_name() == \"PadToStatic\") {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5363,
          "old_api": "call_target_name",
          "new_api": null,
          "old_text": "call.call_target_name()",
          "new_text": null,
          "old_line_content": "    if (call.call_target_name() == \"SliceToDynamic\") {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4853,
          "old_api": "get_dtype_bits",
          "new_api": null,
          "old_text": "get_dtype_bits(operand)",
          "new_text": null,
          "old_line_content": "        std::min(get_dtype_bits(operand), smallest_input_dtype_bits);",
          "new_line_content": "  std::array<int64_t, 3> reduction_tiling =",
          "content_same": false
        },
        {
          "line": 5366,
          "old_api": "EmitCustomCallThunk",
          "new_api": null,
          "old_text": "EmitCustomCallThunk(op)",
          "new_text": null,
          "old_line_content": "    return EmitCustomCallThunk(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4856,
          "old_api": "cuda_compute_capability",
          "new_api": null,
          "old_text": "GetReductionTiling(reduction_dimensions, smallest_input_dtype_bits,\n                         ir_emitter_context_->cuda_compute_capability())",
          "new_text": null,
          "old_line_content": "      GetReductionTiling(reduction_dimensions, smallest_input_dtype_bits,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4857,
          "old_api": "cuda_compute_capability",
          "new_api": null,
          "old_text": "ir_emitter_context_->cuda_compute_capability()",
          "new_text": null,
          "old_line_content": "                         ir_emitter_context_->cuda_compute_capability());",
          "new_line_content": "  int64_t num_threads_y = reduction_dimensions.is_row_reduction ? 1 : kWarpSize;",
          "content_same": false
        },
        {
          "line": 5369,
          "old_api": "mlir::isa<mlir::lmhlo_gpu::GEMMOp, mlir::lmhlo_gpu::GEMM_BiasOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo_gpu::GEMMOp, mlir::lmhlo_gpu::GEMM_BiasOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::GEMMOp, mlir::lmhlo_gpu::GEMM_BiasOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5370,
          "old_api": "EmitGemmThunk",
          "new_api": null,
          "old_text": "EmitGemmThunk(op)",
          "new_text": null,
          "old_line_content": "    return EmitGemmThunk(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4860,
          "old_api": "getFusionResults",
          "new_api": null,
          "old_text": "[&] {\n    if (reduction_dimensions.is_row_reduction) {\n      // Use 512 as default block size (threads per block) for row reductions.\n      // For multi-output fusions, reduce the block size further to decrease\n      // register pressure when multiple outputs are computed by each thread.\n      int64_t fan_out = 1;\n      if (auto fusion = mlir::dyn_cast<mlir::lmhlo::FusionOp>(unnested_hlo)) {\n        fan_out = fusion.getFusionResults().size();\n      }\n\n      int64_t max_block_size =\n          std::max(kMinThreadsXRowReduction,\n                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)));\n      return std::min(\n          max_block_size,\n          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],\n                                       reduction_tiling[2]),\n                           kWarpSize));\n    }\n    return kWarpSize;\n  }()",
          "new_text": null,
          "old_line_content": "  int64_t num_threads_x = [&] {",
          "new_line_content": "      // Use 512 as default block size (threads per block) for row reductions.",
          "content_same": false
        },
        {
          "line": 5373,
          "old_api": "mlir::isa<mlir::lmhlo_gpu::ConvForwardOp,\n                mlir::lmhlo_gpu::ConvForwardFusedOp,\n                mlir::lmhlo_gpu::ConvForwardFusedSideInputOp,\n                mlir::lmhlo_gpu::ConvBackwardFilterOp,\n                mlir::lmhlo_gpu::ConvBackwardInputOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo_gpu::ConvForwardOp,\n                mlir::lmhlo_gpu::ConvForwardFusedOp,\n                mlir::lmhlo_gpu::ConvForwardFusedSideInputOp,\n                mlir::lmhlo_gpu::ConvBackwardFilterOp,\n                mlir::lmhlo_gpu::ConvBackwardInputOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::ConvForwardOp,",
          "new_line_content": "                mlir::lmhlo_gpu::ConvForwardFusedSideInputOp,",
          "content_same": false
        },
        {
          "line": 4866,
          "old_api": "mlir::dyn_cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "new_api": null,
          "old_text": "mlir::dyn_cast<mlir::lmhlo::FusionOp>(unnested_hlo)",
          "new_text": null,
          "old_line_content": "      if (auto fusion = mlir::dyn_cast<mlir::lmhlo::FusionOp>(unnested_hlo)) {",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 4867,
          "old_api": "getFusionResults",
          "new_api": null,
          "old_text": "fusion.getFusionResults().size()",
          "new_text": null,
          "old_line_content": "        fan_out = fusion.getFusionResults().size();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5378,
          "old_api": "EmitConvolutionThunk",
          "new_api": null,
          "old_text": "EmitConvolutionThunk(op)",
          "new_text": null,
          "old_line_content": "    return EmitConvolutionThunk(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5381,
          "old_api": "mlir::isa<mlir::lmhlo_gpu::BatchNormTrainingOp,\n                mlir::lmhlo_gpu::BatchNormInferenceOp,\n                mlir::lmhlo_gpu::BatchNormGradOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo_gpu::BatchNormTrainingOp,\n                mlir::lmhlo_gpu::BatchNormInferenceOp,\n                mlir::lmhlo_gpu::BatchNormGradOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::BatchNormTrainingOp,",
          "new_line_content": "                mlir::lmhlo_gpu::BatchNormGradOp>(op)) {",
          "content_same": false
        },
        {
          "line": 4872,
          "old_api": "NearestPowerOfTwo",
          "new_api": null,
          "old_text": "NearestPowerOfTwo(fan_out)",
          "new_text": null,
          "old_line_content": "                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)));",
          "new_line_content": "          max_block_size,",
          "content_same": false
        },
        {
          "line": 5384,
          "old_api": "EmitBatchNormThunk",
          "new_api": null,
          "old_text": "EmitBatchNormThunk(op)",
          "new_text": null,
          "old_line_content": "    return EmitBatchNormThunk(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4875,
          "old_api": "CeilOfRatio",
          "new_api": null,
          "old_text": "CeilOfRatio(reduction_dimensions.dimensions[2],\n                                       reduction_tiling[2])",
          "new_text": null,
          "old_line_content": "          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],",
          "new_line_content": "                           kWarpSize));",
          "content_same": false
        },
        {
          "line": 5388,
          "old_api": "mlir::isa<mlir::lmhlo_gpu::CholeskyOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo_gpu::CholeskyOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::CholeskyOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5389,
          "old_api": "EmitCholeskyThunk",
          "new_api": null,
          "old_text": "EmitCholeskyThunk(op)",
          "new_text": null,
          "old_line_content": "    return EmitCholeskyThunk(op);",
          "new_line_content": "#endif  // GOOGLE_CUDA",
          "content_same": false
        },
        {
          "line": 5393,
          "old_api": "mlir::isa<mlir::lmhlo::FftOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::FftOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::FftOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5394,
          "old_api": "EmitFftThunk",
          "new_api": null,
          "old_text": "EmitFftThunk(op)",
          "new_text": null,
          "old_line_content": "    return EmitFftThunk(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4885,
          "old_api": "cuda_compute_capability",
          "new_api": null,
          "old_text": "ir_emitter_context_->cuda_compute_capability()",
          "new_text": null,
          "old_line_content": "  se::CudaComputeCapability cc = ir_emitter_context_->cuda_compute_capability();",
          "new_line_content": "  int num_partial_results = 1;",
          "content_same": false
        },
        {
          "line": 5397,
          "old_api": "mlir::isa<mlir::lmhlo::TriangularSolveOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::TriangularSolveOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::TriangularSolveOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5398,
          "old_api": "EmitTriangularSolve",
          "new_api": null,
          "old_text": "EmitTriangularSolve(op)",
          "new_text": null,
          "old_line_content": "    return EmitTriangularSolve(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4888,
          "old_api": "[&]() {\n    if (reduction_dimensions.is_row_reduction &&\n        // P100, only try to vectorize+coales memory access when the\n        // tile size fits exactly and dtypes <= 32 bits\n        ((cc.major == 6 && smallest_input_dtype_bits <= 32 && tile_fit) ||\n         // On V100, only try to vectorize+coales memory access for\n         // rows of even size.  For odd row sizes, every other row\n         // isn't aligned, so it can't be vectorized.\n         (cc.major >= 7 && reduction_dimensions.dimensions[2] % 2 == 0))) {\n      return kStridedLinearIndexingX;\n    } else if (!reduction_dimensions.is_row_reduction &&\n               IsUnrollingColumnReductionBeneficial(\n                   unnested_hlo, input_shape,\n                   reduction_dimensions.dimensions[2], layout_analysis)) {\n      num_partial_results = 2;\n      reduction_tiling[2] *= num_partial_results;\n      return kLinearIndexingX;\n    } else {\n      return kStridedIndexingX;\n    }\n  }()",
          "new_api": null,
          "old_text": "[&]() {\n    if (reduction_dimensions.is_row_reduction &&\n        // P100, only try to vectorize+coales memory access when the\n        // tile size fits exactly and dtypes <= 32 bits\n        ((cc.major == 6 && smallest_input_dtype_bits <= 32 && tile_fit) ||\n         // On V100, only try to vectorize+coales memory access for\n         // rows of even size.  For odd row sizes, every other row\n         // isn't aligned, so it can't be vectorized.\n         (cc.major >= 7 && reduction_dimensions.dimensions[2] % 2 == 0))) {\n      return kStridedLinearIndexingX;\n    } else if (!reduction_dimensions.is_row_reduction &&\n               IsUnrollingColumnReductionBeneficial(\n                   unnested_hlo, input_shape,\n                   reduction_dimensions.dimensions[2], layout_analysis)) {\n      num_partial_results = 2;\n      reduction_tiling[2] *= num_partial_results;\n      return kLinearIndexingX;\n    } else {\n      return kStridedIndexingX;\n    }\n  }()",
          "new_text": null,
          "old_line_content": "  KernelMappingScheme::IndexingOrder indexing_order = [&]() {",
          "new_line_content": "        // P100, only try to vectorize+coales memory access when the",
          "content_same": false
        },
        {
          "line": 5401,
          "old_api": "mlir::isa<mlir::lmhlo::FusionOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::FusionOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::FusionOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5402,
          "old_api": "EmitFusion",
          "new_api": null,
          "old_text": "EmitFusion(op)",
          "new_text": null,
          "old_line_content": "    return EmitFusion(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5405,
          "old_api": "mlir::isa<mlir::lmhlo::SelectAndScatterOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::SelectAndScatterOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::SelectAndScatterOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5406,
          "old_api": "EmitSelectAndScatter",
          "new_api": null,
          "old_text": "EmitSelectAndScatter(op)",
          "new_text": null,
          "old_line_content": "    return EmitSelectAndScatter(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5409,
          "old_api": "mlir::isa<mlir::lmhlo::RngGetAndUpdateStateOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::RngGetAndUpdateStateOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::RngGetAndUpdateStateOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5410,
          "old_api": "EmitRngGetAndUpdateState",
          "new_api": null,
          "old_text": "EmitRngGetAndUpdateState(op)",
          "new_text": null,
          "old_line_content": "    return EmitRngGetAndUpdateState(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4899,
          "old_api": "IsUnrollingColumnReductionBeneficial",
          "new_api": null,
          "old_text": "IsUnrollingColumnReductionBeneficial(\n                   unnested_hlo, input_shape,\n                   reduction_dimensions.dimensions[2], layout_analysis)",
          "new_text": null,
          "old_line_content": "               IsUnrollingColumnReductionBeneficial(",
          "new_line_content": "                   reduction_dimensions.dimensions[2], layout_analysis)) {",
          "content_same": false
        },
        {
          "line": 5413,
          "old_api": "mlir::isa<mlir::lmhlo::ScatterOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::ScatterOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::ScatterOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5414,
          "old_api": "EmitScatter",
          "new_api": null,
          "old_text": "EmitScatter(op)",
          "new_text": null,
          "old_line_content": "    return EmitScatter(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5417,
          "old_api": "mlir::isa<mlir::lmhlo::SortOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::SortOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::SortOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5418,
          "old_api": "EmitSort",
          "new_api": null,
          "old_text": "EmitSort(op)",
          "new_text": null,
          "old_line_content": "    return EmitSort(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5421,
          "old_api": "mlir::isa<mlir::lmhlo::ReplicaIdOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::ReplicaIdOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::ReplicaIdOp>(op)) {",
          "new_line_content": "        op);",
          "content_same": false
        },
        {
          "line": 5422,
          "old_api": "EmitReplicaOrPartitionId<ReplicaIdThunk, mlir::lmhlo::ReplicaIdOp>(\n        op)",
          "new_api": null,
          "old_text": "EmitReplicaOrPartitionId<ReplicaIdThunk, mlir::lmhlo::ReplicaIdOp>(\n        op)",
          "new_text": null,
          "old_line_content": "    return EmitReplicaOrPartitionId<ReplicaIdThunk, mlir::lmhlo::ReplicaIdOp>(",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5426,
          "old_api": "mlir::isa<mlir::lmhlo::PartitionIdOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::PartitionIdOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::PartitionIdOp>(op)) {",
          "new_line_content": "                                    mlir::lmhlo::PartitionIdOp>(op);",
          "content_same": false
        },
        {
          "line": 4915,
          "old_api": "MayPreventVectorization",
          "new_api": null,
          "old_text": "MayPreventVectorization(unnested_hlo)",
          "new_text": null,
          "old_line_content": "        !MayPreventVectorization(unnested_hlo)) {",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 5427,
          "old_api": "EmitReplicaOrPartitionId<PartitionIdThunk,\n                                    mlir::lmhlo::PartitionIdOp>(op)",
          "new_api": null,
          "old_text": "EmitReplicaOrPartitionId<PartitionIdThunk,\n                                    mlir::lmhlo::PartitionIdOp>(op)",
          "new_text": null,
          "old_line_content": "    return EmitReplicaOrPartitionId<PartitionIdThunk,",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5431,
          "old_api": "mlir::isa<mlir::lmhlo::CollectivePermuteOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::CollectivePermuteOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::CollectivePermuteOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5432,
          "old_api": "EmitCollectivePermute",
          "new_api": null,
          "old_text": "EmitCollectivePermute(op)",
          "new_text": null,
          "old_line_content": "    return EmitCollectivePermute(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5435,
          "old_api": "mlir::isa<mlir::lmhlo::AllGatherOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::AllGatherOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::AllGatherOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5436,
          "old_api": "EmitNcclThunk<NcclAllGatherThunk, mlir::lmhlo::AllGatherOp>(op)",
          "new_api": null,
          "old_text": "EmitNcclThunk<NcclAllGatherThunk, mlir::lmhlo::AllGatherOp>(op)",
          "new_text": null,
          "old_line_content": "    return EmitNcclThunk<NcclAllGatherThunk, mlir::lmhlo::AllGatherOp>(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4926,
          "old_api": "ReductionCodegenInfo",
          "new_api": null,
          "old_text": "ReductionCodegenInfo(\n      mapping_scheme, num_partial_results,\n      reduction_dimensions.is_row_reduction,\n      ReductionIsRaceFree(reduction_dimensions, reduction_tiling))",
          "new_text": null,
          "old_line_content": "  return ReductionCodegenInfo(",
          "new_line_content": "      reduction_dimensions.is_row_reduction,",
          "content_same": false
        },
        {
          "line": 5439,
          "old_api": "mlir::isa<mlir::lmhlo::AllReduceOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::AllReduceOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::AllReduceOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5440,
          "old_api": "EmitNcclThunk<NcclAllReduceThunk, mlir::lmhlo::AllReduceOp>(op)",
          "new_api": null,
          "old_text": "EmitNcclThunk<NcclAllReduceThunk, mlir::lmhlo::AllReduceOp>(op)",
          "new_text": null,
          "old_line_content": "    return EmitNcclThunk<NcclAllReduceThunk, mlir::lmhlo::AllReduceOp>(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4929,
          "old_api": "ReductionIsRaceFree",
          "new_api": null,
          "old_text": "ReductionIsRaceFree(reduction_dimensions, reduction_tiling)",
          "new_text": null,
          "old_line_content": "      ReductionIsRaceFree(reduction_dimensions, reduction_tiling));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5443,
          "old_api": "mlir::isa<mlir::lmhlo_gpu::AllReduceStartOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo_gpu::AllReduceStartOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::AllReduceStartOp>(op)) {",
          "new_line_content": "                         mlir::lmhlo_gpu::AllReduceStartOp>(op);",
          "content_same": false
        },
        {
          "line": 5444,
          "old_api": "EmitNcclThunk<NcclAllReduceStartThunk,\n                         mlir::lmhlo_gpu::AllReduceStartOp>(op)",
          "new_api": null,
          "old_text": "EmitNcclThunk<NcclAllReduceStartThunk,\n                         mlir::lmhlo_gpu::AllReduceStartOp>(op)",
          "new_text": null,
          "old_line_content": "    return EmitNcclThunk<NcclAllReduceStartThunk,",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5448,
          "old_api": "mlir::isa<mlir::lmhlo_gpu::AllReduceDoneOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo_gpu::AllReduceDoneOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo_gpu::AllReduceDoneOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5449,
          "old_api": "EmitAllReduceDone",
          "new_api": null,
          "old_text": "EmitAllReduceDone(op)",
          "new_text": null,
          "old_line_content": "    return EmitAllReduceDone(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4940,
          "old_api": "GetReduceFromUnnestedMlir",
          "new_api": null,
          "old_text": "GetReduceFromUnnestedMlir(unnested_hlo, index)",
          "new_text": null,
          "old_line_content": "    mlir::Operation* reduce = GetReduceFromUnnestedMlir(unnested_hlo, index);",
          "new_line_content": "      continue;",
          "content_same": false
        },
        {
          "line": 4941,
          "old_api": "IsReductionFromOrToContiguousDimensions",
          "new_api": null,
          "old_text": "IsReductionFromOrToContiguousDimensions(reduce, layout_analysis)",
          "new_text": null,
          "old_line_content": "    if (!IsReductionFromOrToContiguousDimensions(reduce, layout_analysis)) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5452,
          "old_api": "mlir::isa<mlir::lmhlo::ReduceScatterOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::ReduceScatterOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::ReduceScatterOp>(op)) {",
          "new_line_content": "        op);",
          "content_same": false
        },
        {
          "line": 5453,
          "old_api": "EmitNcclThunk<NcclReduceScatterThunk, mlir::lmhlo::ReduceScatterOp>(\n        op)",
          "new_api": null,
          "old_text": "EmitNcclThunk<NcclReduceScatterThunk, mlir::lmhlo::ReduceScatterOp>(\n        op)",
          "new_text": null,
          "old_line_content": "    return EmitNcclThunk<NcclReduceScatterThunk, mlir::lmhlo::ReduceScatterOp>(",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5457,
          "old_api": "mlir::isa<mlir::lmhlo::AllToAllOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::AllToAllOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::AllToAllOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4946,
          "old_api": "opcode",
          "new_api": null,
          "old_text": "root->opcode()",
          "new_text": null,
          "old_line_content": "      if (root->opcode() == HloOpcode::kTuple) {",
          "new_line_content": "      } else {",
          "content_same": false
        },
        {
          "line": 5458,
          "old_api": "EmitNcclThunk<NcclAllToAllThunk, mlir::lmhlo::AllToAllOp>(op)",
          "new_api": null,
          "old_text": "EmitNcclThunk<NcclAllToAllThunk, mlir::lmhlo::AllToAllOp>(op)",
          "new_text": null,
          "old_line_content": "    return EmitNcclThunk<NcclAllToAllThunk, mlir::lmhlo::AllToAllOp>(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5461,
          "old_api": "mlir::isa<mlir::lmhlo::InfeedOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::InfeedOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::InfeedOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5462,
          "old_api": "EmitInfeed",
          "new_api": null,
          "old_text": "EmitInfeed(op)",
          "new_text": null,
          "old_line_content": "    return EmitInfeed(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4953,
          "old_api": "MlirToString",
          "new_api": null,
          "old_text": "MlirToString(reduce)",
          "new_text": null,
          "old_line_content": "      LOG(FATAL) << \"Unexpected reduce op: \" << MlirToString(reduce);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5465,
          "old_api": "mlir::isa<mlir::lmhlo::OutfeedOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::OutfeedOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::OutfeedOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5466,
          "old_api": "EmitOutfeed",
          "new_api": null,
          "old_text": "EmitOutfeed(op)",
          "new_text": null,
          "old_line_content": "    return EmitOutfeed(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4956,
          "old_api": "empty",
          "new_api": null,
          "old_text": "reducers.empty()",
          "new_text": null,
          "old_line_content": "  CHECK(!reducers.empty()) << \" expect at least one reduce instructions.\";",
          "new_line_content": "  const KernelMappingScheme& mapping_scheme =",
          "content_same": false
        },
        {
          "line": 5469,
          "old_api": "mlir::isa<mlir::lmhlo::CaseOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::CaseOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::CaseOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5470,
          "old_api": "EmitConditional",
          "new_api": null,
          "old_text": "EmitConditional(op)",
          "new_text": null,
          "old_line_content": "    return EmitConditional(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5473,
          "old_api": "mlir::isa<mlir::lmhlo::WhileOp>(op)",
          "new_api": null,
          "old_text": "mlir::isa<mlir::lmhlo::WhileOp>(op)",
          "new_text": null,
          "old_line_content": "  if (mlir::isa<mlir::lmhlo::WhileOp>(op)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5474,
          "old_api": "EmitWhile",
          "new_api": null,
          "old_text": "EmitWhile(op)",
          "new_text": null,
          "old_line_content": "    return EmitWhile(op);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4963,
          "old_api": "launch_bound",
          "new_api": null,
          "old_text": "launch_dimensions.launch_bound()",
          "new_text": null,
          "old_line_content": "      unnested_hlo, launch_dimensions.launch_bound(), &b_);",
          "new_line_content": "                           fused_emitter, result_ir_arrays, reduction_info,",
          "content_same": false
        },
        {
          "line": 4964,
          "old_api": "EmitPrologueForReduction",
          "new_api": null,
          "old_text": "EmitPrologueForReduction(unnested_hlo, instr_index_group, fused_computation,\n                           fused_emitter, result_ir_arrays, reduction_info,\n                           layout_analysis)",
          "new_text": null,
          "old_line_content": "  EmitPrologueForReduction(unnested_hlo, instr_index_group, fused_computation,",
          "new_line_content": "                           layout_analysis);",
          "content_same": false
        },
        {
          "line": 5477,
          "old_api": "MlirToString",
          "new_api": null,
          "old_text": "MlirToString(op)",
          "new_text": null,
          "old_line_content": "  return InternalError(\"Unrecognized op: %s\", MlirToString(op));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5481,
          "old_api": "front",
          "new_api": null,
          "old_text": "region->front()",
          "new_text": null,
          "old_line_content": "  for (mlir::Operation& op : llvm::make_early_inc_range(region->front())) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4971,
          "old_api": "EmitTileElementForReduction",
          "new_api": null,
          "old_text": "EmitTileElementForReduction(\n            unnested_hlo, input_shape, instr_index_group, fused_computation,\n            fused_emitter, result_ir_arrays, reducers, index, *reduction_info,\n            x_iter_num, layout_analysis)",
          "new_text": null,
          "old_line_content": "        EmitTileElementForReduction(",
          "new_line_content": "            fused_emitter, result_ir_arrays, reducers, index, *reduction_info,",
          "content_same": false
        },
        {
          "line": 5484,
          "old_api": "Status::OK()",
          "new_api": null,
          "old_text": "Status::OK()",
          "new_text": null,
          "old_line_content": "  return Status::OK();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4977,
          "old_api": "GetKernelMappingScheme",
          "new_api": null,
          "old_text": "EmitTilingKernel(\n      mapping_scheme, index_ty,\n      [&](const ThreadIdInfo& thread_id_info, const IrArray::Index& index,\n          const string& loop_name, llvm::Value* tile_height,\n          llvm::Value* tile_width, KernelSupportLibrary* ksl) {\n        EmitTile(reduction_info->GetKernelMappingScheme(), index, loop_name,\n                 ksl, thread_id_info, tile_height, tile_width,\n                 emit_reduction_tile);\n      })",
          "new_text": null,
          "old_line_content": "  TilingKernelInfo tiling_kernel_info = EmitTilingKernel(",
          "new_line_content": "      [&](const ThreadIdInfo& thread_id_info, const IrArray::Index& index,",
          "content_same": false
        },
        {
          "line": 5491,
          "old_api": "getLoc",
          "new_api": null,
          "old_text": "op->getLoc()",
          "new_text": null,
          "old_line_content": "      \"Thunk:#hlo_op=%s,hlo_module=%s#\", mlir::GetNameFromLoc(op->getLoc()),",
          "new_line_content": "  return thunk_info;",
          "content_same": false
        },
        {
          "line": 5492,
          "old_api": "getLoc",
          "new_api": null,
          "old_text": "module->getLoc()",
          "new_text": null,
          "old_line_content": "      mlir::GetNameFromLoc(module->getLoc()));",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 4982,
          "old_api": "GetKernelMappingScheme",
          "new_api": null,
          "old_text": "reduction_info->GetKernelMappingScheme()",
          "new_text": null,
          "old_line_content": "        EmitTile(reduction_info->GetKernelMappingScheme(), index, loop_name,",
          "new_line_content": "                 emit_reduction_tile);",
          "content_same": false
        },
        {
          "line": 4986,
          "old_api": "EmitEpilogueForReduction",
          "new_api": null,
          "old_text": "EmitEpilogueForReduction(index_ty, unnested_hlo, instr_index_group,\n                           result_ir_arrays, reducers, *reduction_info,\n                           tiling_kernel_info, layout_analysis)",
          "new_text": null,
          "old_line_content": "  EmitEpilogueForReduction(index_ty, unnested_hlo, instr_index_group,",
          "new_line_content": "                           tiling_kernel_info, layout_analysis);",
          "content_same": false
        },
        {
          "line": 5499,
          "old_api": "GetHloOperands",
          "new_api": null,
          "old_text": "GetHloOperands(op)",
          "new_text": null,
          "old_line_content": "  auto operands = GetHloOperands(op);",
          "new_line_content": "  for (auto operand : operands) {",
          "content_same": false
        },
        {
          "line": 5502,
          "old_api": "GetShape",
          "new_api": null,
          "old_text": "GetShape(operand)",
          "new_text": null,
          "old_line_content": "    operand_shapes.push_back(GetShape(operand));",
          "new_line_content": "  for (auto output : outputs) {",
          "content_same": false
        },
        {
          "line": 5505,
          "old_api": "GetShape",
          "new_api": null,
          "old_text": "GetShape(output)",
          "new_text": null,
          "old_line_content": "    output_shapes.push_back(GetShape(output));",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 4998,
          "old_api": "operand",
          "new_api": null,
          "old_text": "instr.operand(0)->IsConstant()",
          "new_text": null,
          "old_line_content": "          (instr.operand(0)->IsConstant() ||",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 4999,
          "old_api": "operand",
          "new_api": null,
          "old_text": "instr.operand(0)->shape()",
          "new_text": null,
          "old_line_content": "           ShapeUtil::IsScalar(instr.operand(0)->shape())));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5011,
          "old_api": "CHECK_NE",
          "new_api": null,
          "old_text": "CHECK_NE(0, num_reduces)",
          "new_text": null,
          "old_line_content": "  CHECK_NE(0, num_reduces);",
          "new_line_content": "    return {{0}};",
          "content_same": false
        },
        {
          "line": 5019,
          "old_api": "Get",
          "new_api": null,
          "old_text": "disjoint_sets[i].Get()",
          "new_text": null,
          "old_line_content": "    disjoint_sets[i].Get() =",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5020,
          "old_api": "root_instruction",
          "new_api": null,
          "old_text": "fused_computation->root_instruction()->mutable_operand(i)",
          "new_text": null,
          "old_line_content": "        fused_computation->root_instruction()->mutable_operand(i);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5024,
          "old_api": "HloReachabilityMap::Build(fused_computation)",
          "new_api": null,
          "old_text": "HloReachabilityMap::Build(fused_computation)",
          "new_text": null,
          "old_line_content": "      HloReachabilityMap::Build(fused_computation);",
          "new_line_content": "    std::vector<int64_t> reached_output_ids;",
          "content_same": false
        },
        {
          "line": 5025,
          "old_api": "instructions",
          "new_api": null,
          "old_text": "fused_computation->instructions()",
          "new_text": null,
          "old_line_content": "  for (HloInstruction* instr : fused_computation->instructions()) {",
          "new_line_content": "    for (size_t oid = 0; oid < num_reduces; ++oid) {",
          "content_same": false
        },
        {
          "line": 5030,
          "old_api": "opcode",
          "new_api": null,
          "old_text": "reduce->opcode()",
          "new_text": null,
          "old_line_content": "      if (HloOpcode::kReduce == reduce->opcode() &&",
          "new_line_content": "        // Do not group output reduce instructions through broadcasted",
          "content_same": false
        },
        {
          "line": 5031,
          "old_api": "IsBroadcastedConstantOrScalar",
          "new_api": null,
          "old_text": "IsBroadcastedConstantOrScalar(*instr)",
          "new_text": null,
          "old_line_content": "          (IsBroadcastedConstantOrScalar(*instr))) {",
          "new_line_content": "        // constants or scalars, as the recomputation should be acceptable.",
          "content_same": false
        },
        {
          "line": 5034,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "instr->ToString()",
          "new_text": null,
          "old_line_content": "        VLOG(3) << \"Skip broadcasted constant or scalar \" << instr->ToString();",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 5040,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "instr->ToString()",
          "new_text": null,
          "old_line_content": "                << instr->ToString();",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 5041,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "reached_output_ids.push_back(oid)",
          "new_text": null,
          "old_line_content": "        reached_output_ids.push_back(oid);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5044,
          "old_api": "size",
          "new_api": null,
          "old_text": "reached_output_ids.size()",
          "new_text": null,
          "old_line_content": "    for (size_t j = 1; j < reached_output_ids.size(); ++j) {",
          "new_line_content": "          &disjoint_sets[reached_output_ids[j]]);",
          "content_same": false
        },
        {
          "line": 5045,
          "old_api": "Merge",
          "new_api": null,
          "old_text": "disjoint_sets[reached_output_ids[0]].Merge(\n          &disjoint_sets[reached_output_ids[j]])",
          "new_text": null,
          "old_line_content": "      disjoint_sets[reached_output_ids[0]].Merge(",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5052,
          "old_api": "Get",
          "new_api": null,
          "old_text": "disjoint_sets[oid].Get()",
          "new_text": null,
          "old_line_content": "    groups[disjoint_sets[oid].Get()].push_back(oid);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5056,
          "old_api": "emplace_back",
          "new_api": null,
          "old_text": "absl::c_for_each(\n      groups, [&](auto& iter) { ret.emplace_back(std::move(iter.second)); })",
          "new_text": null,
          "old_line_content": "  absl::c_for_each(",
          "new_line_content": "  return ret;",
          "content_same": false
        },
        {
          "line": 5057,
          "old_api": "std::move(iter.second)",
          "new_api": null,
          "old_text": "std::move(iter.second)",
          "new_text": null,
          "old_line_content": "      groups, [&](auto& iter) { ret.emplace_back(std::move(iter.second)); });",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5068,
          "old_api": "getFusionResults",
          "new_api": null,
          "old_text": "fusion.getFusionResults().size()",
          "new_text": null,
          "old_line_content": "  int num_reduces = fusion.getFusionResults().size();",
          "new_line_content": "  // Build a kernel thunk to compute all the outputs.",
          "content_same": false
        },
        {
          "line": 5074,
          "old_api": "GetReduceFromUnnestedMlir",
          "new_api": null,
          "old_text": "GetReduceFromUnnestedMlir(unnested_hlo, i)",
          "new_text": null,
          "old_line_content": "        GetReduceFromUnnestedMlir(unnested_hlo, i);",
          "new_line_content": "                                                layout_analysis)) {",
          "content_same": false
        },
        {
          "line": 5077,
          "old_api": "GetReduceFromUnnestedMlir",
          "new_api": null,
          "old_text": "GetReduceFromUnnestedMlir(unnested_hlo, i)",
          "new_text": null,
          "old_line_content": "      first_reduce = GetReduceFromUnnestedMlir(unnested_hlo, i);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5081,
          "old_api": "MlirToString",
          "new_api": null,
          "old_text": "MlirToString(unnested_hlo)",
          "new_text": null,
          "old_line_content": "  CHECK(first_reduce) << MlirToString(unnested_hlo);",
          "new_line_content": "    for (int i = 0; i < num_reduces; i++) {",
          "content_same": false
        },
        {
          "line": 5084,
          "old_api": "mlir::dyn_cast<mlir::mhlo::ReduceOp>(\n          GetReduceFromUnnestedMlir(unnested_hlo, i))",
          "new_api": null,
          "old_text": "mlir::dyn_cast<mlir::mhlo::ReduceOp>(\n          GetReduceFromUnnestedMlir(unnested_hlo, i))",
          "new_text": null,
          "old_line_content": "      auto candidate = mlir::dyn_cast<mlir::mhlo::ReduceOp>(",
          "new_line_content": "      if (candidate &&",
          "content_same": false
        },
        {
          "line": 5087,
          "old_api": "IsFusedReductionOutputConsistent",
          "new_api": null,
          "old_text": "IsFusedReductionOutputConsistent(\n              candidate, mlir::cast<mlir::mhlo::ReduceOp>(first_reduce),\n              layout_analysis)",
          "new_text": null,
          "old_line_content": "          !IsFusedReductionOutputConsistent(",
          "new_line_content": "              layout_analysis)) {",
          "content_same": false
        },
        {
          "line": 5090,
          "old_api": "InternalError",
          "new_api": null,
          "old_text": "InternalError(\"Inconsistent reduction fusion outputs\")",
          "new_text": null,
          "old_line_content": "        return InternalError(\"Inconsistent reduction fusion outputs\");",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5094,
          "old_api": "getOperand",
          "new_api": null,
          "old_text": "first_reduce->getOperand(0)",
          "new_text": null,
          "old_line_content": "  Shape input_shape = GetShape(first_reduce->getOperand(0));",
          "new_line_content": "  // unnested kReduce or by InstructionFusion for fused kReduce.",
          "content_same": false
        },
        {
          "line": 5099,
          "old_api": "MlirToString",
          "new_api": null,
          "old_text": "MlirToString(first_reduce)",
          "new_text": null,
          "old_line_content": "                                  << MlirToString(first_reduce);",
          "new_line_content": "  HloComputation* fused_computation = nullptr;",
          "content_same": false
        },
        {
          "line": 5102,
          "old_api": "region",
          "new_api": null,
          "old_text": "TF_ASSIGN_OR_RETURN(fused_computation,\n                      GetOrCreateSubComputationFromRegion(&fusion.region(),\n                                                          /*is_fusion=*/true))",
          "new_text": null,
          "old_line_content": "  TF_ASSIGN_OR_RETURN(fused_computation,",
          "new_line_content": "                                                          /*is_fusion=*/true));",
          "content_same": false
        },
        {
          "line": 5103,
          "old_api": "region",
          "new_api": null,
          "old_text": "fusion.region()",
          "new_text": null,
          "old_line_content": "                      GetOrCreateSubComputationFromRegion(&fusion.region(),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5110,
          "old_api": "size",
          "new_api": null,
          "old_text": "instr_index_groups.size()",
          "new_text": null,
          "old_line_content": "  VLOG(2) << StrCat(\"Generate in \", instr_index_groups.size(), \" groups for \",",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5111,
          "old_api": "MlirToString",
          "new_api": null,
          "old_text": "MlirToString(unnested_hlo)",
          "new_text": null,
          "old_line_content": "                    MlirToString(unnested_hlo));",
          "new_line_content": "  ReductionCodegenInfo reduction_info =",
          "content_same": false
        },
        {
          "line": 5116,
          "old_api": "GetKernelMappingScheme",
          "new_api": null,
          "old_text": "reduction_info.GetKernelMappingScheme()",
          "new_text": null,
          "old_line_content": "      reduction_info.GetKernelMappingScheme();",
          "new_line_content": "  // block_y_count is set to instr_index_groups.size(), so that each reduction",
          "content_same": false
        }
      ]
    },
    "api_summary": {
      "total_replacements": 73,
      "total_additions": 181,
      "total_deletions": 182,
      "total_api_changes": 436
    },
    "non_api_changes": {
      "has_non_api_changes": true,
      "evidence": {
        "total_diff_lines": 3,
        "api_related_lines": 436,
        "non_api_lines": 2,
        "non_api_line_numbers": [
          4773,
          4774
        ]
      }
    },
    "api_calls_before": 2533,
    "api_calls_after": 2531,
    "diff_info": {
      "added_lines": 1,
      "removed_lines": 3,
      "total_diff_lines": 16
    }
  }
}