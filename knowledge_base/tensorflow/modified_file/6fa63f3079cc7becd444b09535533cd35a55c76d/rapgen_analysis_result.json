{
  "status": "completed",
  "commit_dir": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/6fa63f3079cc7becd444b09535533cd35a55c76d",
  "file_info": {
    "before_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/6fa63f3079cc7becd444b09535533cd35a55c76d/before.cc",
    "after_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/6fa63f3079cc7becd444b09535533cd35a55c76d/after.cc",
    "diff_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/6fa63f3079cc7becd444b09535533cd35a55c76d/diff.txt"
  },
  "semgrep_analysis": {
    "is_api_change": false,
    "api_changes": {
      "replacements": [
        {
          "line": 1211,
          "old_api": "HloOpcodeString",
          "new_api": "InvalidArgument",
          "old_text": "HloOpcodeString(binop)",
          "new_text": "InvalidArgument(\n          \"A comparison direction is provided for a non-compare opcode: %s.\",\n          HloOpcodeString(binop))",
          "old_line_content": "          HloOpcodeString(binop));",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 1213,
          "old_api": "BinaryOpNoBroadcast",
          "new_api": "HloOpcodeString",
          "old_text": "BinaryOpNoBroadcast(binop, shape, updated_lhs, updated_rhs)",
          "new_text": "HloOpcodeString(binop)",
          "old_line_content": "    return BinaryOpNoBroadcast(binop, shape, updated_lhs, updated_rhs);",
          "new_line_content": "          HloOpcodeString(binop));",
          "content_same": false
        },
        {
          "line": 1221,
          "old_api": "ToProto",
          "new_api": "mutable_shape",
          "old_text": "shape.ToProto()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    return AddInstruction(std::move(instr), binop, {lhs, rhs});\n  })",
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1240,
          "old_api": "ToProto",
          "new_api": "ComparisonDirectionToString",
          "old_text": "shape.ToProto()",
          "new_text": "ComparisonDirectionToString(direction)",
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "  instr.set_comparison_direction(ComparisonDirectionToString(direction));",
          "content_same": false
        },
        {
          "line": 1241,
          "old_api": "std::move(instr)",
          "new_api": "ComparisonTypeToString",
          "old_text": "std::move(instr)",
          "new_text": "ComparisonTypeToString(type)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kCompare, {lhs, rhs});",
          "new_line_content": "  instr.set_comparison_type(ComparisonTypeToString(type));",
          "content_same": false
        },
        {
          "line": 1252,
          "old_api": "element_type",
          "new_api": "is_unbounded_dynamic",
          "old_text": "scalar_shape->element_type()",
          "new_text": "output_shape->is_unbounded_dynamic()",
          "old_line_content": "    output_shape_copy.set_element_type(scalar_shape->element_type());",
          "new_line_content": "  if (output_shape->is_unbounded_dynamic()) {",
          "content_same": false
        },
        {
          "line": 1254,
          "old_api": "BroadcastScalarToOutputShapeWithUnbounded",
          "new_api": "element_type",
          "old_text": "BroadcastScalarToOutputShapeWithUnbounded(\n                            this, scalar, output, output_shape_copy)",
          "new_text": "scalar_shape->element_type()",
          "old_line_content": "                        BroadcastScalarToOutputShapeWithUnbounded(",
          "new_line_content": "    output_shape_copy.set_element_type(scalar_shape->element_type());",
          "content_same": false
        },
        {
          "line": 1279,
          "old_api": "ShapeInference::InferScalarBroadcastShape(\n              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape}))",
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": "ShapeInference::InferScalarBroadcastShape(\n              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape}))",
          "new_text": "TF_ASSIGN_OR_RETURN(\n          std::optional<Shape> output_shape,\n          ShapeInference::InferScalarBroadcastShape(\n              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})))",
          "old_line_content": "          ShapeInference::InferScalarBroadcastShape(",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(",
          "content_same": false
        },
        {
          "line": 1285,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": "has_value",
          "old_text": "TF_ASSIGN_OR_RETURN(\n              updated_lhs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/lhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *rhs_shape) ? rhs : ehs))",
          "new_text": "output_shape.has_value()",
          "old_line_content": "          TF_ASSIGN_OR_RETURN(",
          "new_line_content": "      if (output_shape.has_value()) {",
          "content_same": false
        },
        {
          "line": 1287,
          "old_api": "BroadcastScalarToOutputShape",
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": "BroadcastScalarToOutputShape(\n                  /*scalar=*/lhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *rhs_shape) ? rhs : ehs)",
          "new_text": "TF_ASSIGN_OR_RETURN(\n              updated_lhs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/lhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *rhs_shape) ? rhs : ehs))",
          "old_line_content": "              BroadcastScalarToOutputShape(",
          "new_line_content": "          TF_ASSIGN_OR_RETURN(",
          "content_same": false
        },
        {
          "line": 1292,
          "old_api": "ShapeUtil::IsScalar(*rhs_shape)",
          "new_api": "ShapeUtil::Equal(*output_shape, *rhs_shape)",
          "old_text": "ShapeUtil::IsScalar(*rhs_shape)",
          "new_text": "ShapeUtil::Equal(*output_shape, *rhs_shape)",
          "old_line_content": "        if (ShapeUtil::IsScalar(*rhs_shape)) {",
          "new_line_content": "                  ShapeUtil::Equal(*output_shape, *rhs_shape) ? rhs : ehs));",
          "content_same": false
        },
        {
          "line": 1295,
          "old_api": "BroadcastScalarToOutputShape",
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": "BroadcastScalarToOutputShape(\n                  /*scalar=*/rhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : ehs)",
          "new_text": "TF_ASSIGN_OR_RETURN(\n              updated_rhs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/rhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : ehs))",
          "old_line_content": "              BroadcastScalarToOutputShape(",
          "new_line_content": "          TF_ASSIGN_OR_RETURN(",
          "content_same": false
        },
        {
          "line": 1300,
          "old_api": "ShapeUtil::IsScalar(*ehs_shape)",
          "new_api": "ShapeUtil::Equal(*output_shape, *lhs_shape)",
          "old_text": "ShapeUtil::IsScalar(*ehs_shape)",
          "new_text": "ShapeUtil::Equal(*output_shape, *lhs_shape)",
          "old_line_content": "        if (ShapeUtil::IsScalar(*ehs_shape)) {",
          "new_line_content": "                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : ehs));",
          "content_same": false
        },
        {
          "line": 1303,
          "old_api": "BroadcastScalarToOutputShape",
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": "BroadcastScalarToOutputShape(\n                  /*scalar=*/ehs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : rhs)",
          "new_text": "TF_ASSIGN_OR_RETURN(\n              updated_ehs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/ehs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : rhs))",
          "old_line_content": "              BroadcastScalarToOutputShape(",
          "new_line_content": "          TF_ASSIGN_OR_RETURN(",
          "content_same": false
        },
        {
          "line": 1326,
          "old_api": "IsAllFirst",
          "new_api": "shape",
          "old_text": "literal.IsAllFirst()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (literal.shape().IsArray() && literal.element_count() > 1 &&\n        literal.IsAllFirst()) {\n      Literal scalar = LiteralUtil::GetFirstScalarLiteral(literal);\n      HloInstructionProto instr;\n      *instr.mutable_shape() = scalar.shape().ToProto();\n      *instr.mutable_literal() = scalar.ToProto();\n      TF_ASSIGN_OR_RETURN(\n          XlaOp scalar_op,\n          AddInstruction(std::move(instr), HloOpcode::kConstant));\n      return Broadcast(scalar_op, literal.shape().dimensions());\n    } else {\n      HloInstructionProto instr;\n      *instr.mutable_shape() = literal.shape().ToProto();\n      *instr.mutable_literal() = literal.ToProto();\n      return AddInstruction(std::move(instr), HloOpcode::kConstant);\n    }\n  })",
          "old_line_content": "        literal.IsAllFirst()) {",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1327,
          "old_api": "LiteralUtil::GetFirstScalarLiteral(literal)",
          "new_api": "element_count",
          "old_text": "LiteralUtil::GetFirstScalarLiteral(literal)",
          "new_text": "literal.element_count()",
          "old_line_content": "      Literal scalar = LiteralUtil::GetFirstScalarLiteral(literal);",
          "new_line_content": "    if (literal.shape().IsArray() && literal.element_count() > 1 &&",
          "content_same": false
        },
        {
          "line": 1329,
          "old_api": "shape",
          "new_api": "LiteralUtil::GetFirstScalarLiteral(literal)",
          "old_text": "scalar.shape().ToProto()",
          "new_text": "LiteralUtil::GetFirstScalarLiteral(literal)",
          "old_line_content": "      *instr.mutable_shape() = scalar.shape().ToProto();",
          "new_line_content": "      Literal scalar = LiteralUtil::GetFirstScalarLiteral(literal);",
          "content_same": false
        },
        {
          "line": 1339,
          "old_api": "std::move(instr)",
          "new_api": "shape",
          "old_text": "std::move(instr)",
          "new_text": "literal.shape().ToProto()",
          "old_line_content": "      return AddInstruction(std::move(instr), HloOpcode::kConstant);",
          "new_line_content": "      *instr.mutable_shape() = literal.shape().ToProto();",
          "content_same": false
        },
        {
          "line": 1347,
          "old_api": "ToString",
          "new_api": "is_static",
          "old_text": "InvalidArgument(\n          \"The output of iota must not have dynamic dimensions: %s\",\n          shape.ToString())",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!shape.is_static()) {\n      return InvalidArgument(\n          \"The output of iota must not have dynamic dimensions: %s\",\n          shape.ToString());\n    }\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    instr.add_dimensions(iota_dimension);\n    return AddInstruction(std::move(instr), HloOpcode::kIota);\n  })",
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1354,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kIota);",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1377,
          "old_api": "AddCalledComputation",
          "new_api": "ToProto",
          "old_text": "AddCalledComputation(computation, &instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    AddCalledComputation(computation, &instr);",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1379,
          "old_api": "std::move(instr)",
          "new_api": "AddCalledComputation",
          "old_text": "std::move(instr)",
          "new_text": "AddCalledComputation(computation, &instr)",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCall, operands);",
          "new_line_content": "    AddCalledComputation(computation, &instr);",
          "content_same": false
        },
        {
          "line": 1394,
          "old_api": "ToProto",
          "new_api": "set_parameter_number",
          "old_text": "shape.ToProto()",
          "new_text": "instr.set_parameter_number(parameter_number)",
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    instr.set_parameter_number(parameter_number);",
          "content_same": false
        },
        {
          "line": 1395,
          "old_api": "empty",
          "new_api": "set_name",
          "old_text": "replicated_at_leaf_buffers.empty()",
          "new_text": "instr.set_name(name)",
          "old_line_content": "    if (!replicated_at_leaf_buffers.empty()) {",
          "new_line_content": "    instr.set_name(name);",
          "content_same": false
        },
        {
          "line": 1396,
          "old_api": "mutable_parameter_replication",
          "new_api": "ToProto",
          "old_text": "instr.mutable_parameter_replication()",
          "new_text": "shape.ToProto()",
          "old_line_content": "      auto replication = instr.mutable_parameter_replication();",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1398,
          "old_api": "add_replicated_at_leaf_buffers",
          "new_api": "mutable_parameter_replication",
          "old_text": "replication->add_replicated_at_leaf_buffers(replicated)",
          "new_text": "instr.mutable_parameter_replication()",
          "old_line_content": "        replication->add_replicated_at_leaf_buffers(replicated);",
          "new_line_content": "      auto replication = instr.mutable_parameter_replication();",
          "content_same": false
        },
        {
          "line": 1425,
          "old_api": "InDimBroadcast",
          "new_api": "rank",
          "old_text": "InDimBroadcast(shape, operand, dimensions)",
          "new_text": "shape.rank()",
          "old_line_content": "    return InDimBroadcast(shape, operand, dimensions);",
          "new_line_content": "      dimensions[i] = i + shape.rank() - operand_rank;",
          "content_same": false
        },
        {
          "line": 1441,
          "old_api": "ToString",
          "new_api": "is_unbounded_dynamic",
          "old_text": "output_shape.ToString()",
          "new_text": "output_shape.is_unbounded_dynamic()",
          "old_line_content": "        << output_shape.ToString();",
          "new_line_content": "    TF_RET_CHECK(!output_shape.is_unbounded_dynamic())",
          "content_same": false
        },
        {
          "line": 1443,
          "old_api": "rank",
          "new_api": "ToString",
          "old_text": "operand_shape->rank()",
          "new_text": "output_shape.ToString()",
          "old_line_content": "    if (operand_shape->rank() != broadcast_rank) {",
          "new_line_content": "        << output_shape.ToString();",
          "content_same": false
        },
        {
          "line": 1452,
          "old_api": "InvalidArgument",
          "new_api": "size",
          "old_text": "InvalidArgument(\"Broadcast dimension %lld is out of bound\",\n                               broadcast_dimensions[i])",
          "new_text": "out_dim_size.size()",
          "old_line_content": "        return InvalidArgument(\"Broadcast dimension %lld is out of bound\",",
          "new_line_content": "      const int64_t num_dims = out_dim_size.size();",
          "content_same": false
        },
        {
          "line": 1457,
          "old_api": "is_bounded_dynamic_dimension",
          "new_api": "set_dynamic_dimension",
          "old_text": "operand_shape->is_bounded_dynamic_dimension(i)",
          "new_text": "output_shape.set_dynamic_dimension(\n          broadcast_dimensions[i],\n          operand_shape->is_bounded_dynamic_dimension(i))",
          "old_line_content": "          operand_shape->is_bounded_dynamic_dimension(i));",
          "new_line_content": "      output_shape.set_dynamic_dimension(",
          "content_same": false
        },
        {
          "line": 1469,
          "old_api": "dimensions",
          "new_api": "is_unbounded_dynamic_dimension",
          "old_text": "operand_shape->dimensions(i)",
          "new_text": "operand_shape->is_unbounded_dynamic_dimension(i)",
          "old_line_content": "              : operand_shape->dimensions(i);",
          "new_line_content": "          (operand_shape->is_unbounded_dynamic_dimension(i))",
          "content_same": false
        },
        {
          "line": 1471,
          "old_api": "is_bounded_dynamic_dimension",
          "new_api": "dimensions",
          "old_text": "operand_shape->is_bounded_dynamic_dimension(i)",
          "new_text": "operand_shape->dimensions(i)",
          "old_line_content": "          operand_shape->is_bounded_dynamic_dimension(i);",
          "new_line_content": "              : operand_shape->dimensions(i);",
          "content_same": false
        },
        {
          "line": 1473,
          "old_api": "element_type",
          "new_api": "is_bounded_dynamic_dimension",
          "old_text": "ShapeUtil::MakeShape(\n        operand_shape->element_type(), in_dim_size, in_dim_dynamic)",
          "new_text": "operand_shape->is_bounded_dynamic_dimension(i)",
          "old_line_content": "    const auto& in_dim_shape = ShapeUtil::MakeShape(",
          "new_line_content": "          operand_shape->is_bounded_dynamic_dimension(i);",
          "content_same": false
        },
        {
          "line": 1493,
          "old_api": "InvalidArgument",
          "new_api": "TF_RETURN_IF_ERROR",
          "old_text": "InvalidArgument(\n        \"Reshaping with unbounded result shape is not supported.\")",
          "new_text": "TF_RETURN_IF_ERROR(first_error_)",
          "old_line_content": "    return InvalidArgument(",
          "new_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "content_same": false
        },
        {
          "line": 1500,
          "old_api": "add_dimensions",
          "new_api": "ToProto",
          "old_text": "instr.add_dimensions(inferred_dimension)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    instr.add_dimensions(inferred_dimension);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1502,
          "old_api": "std::move(instr)",
          "new_api": "add_dimensions",
          "old_text": "std::move(instr)",
          "new_text": "instr.add_dimensions(inferred_dimension)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kReshape, {operand});",
          "new_line_content": "    instr.add_dimensions(inferred_dimension);",
          "content_same": false
        },
        {
          "line": 1524,
          "old_api": "add_slice_dimensions",
          "new_api": "ToProto",
          "old_text": "instr.add_slice_dimensions()",
          "new_text": "shape.ToProto()",
          "old_line_content": "    auto* slice_config = instr.add_slice_dimensions();",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1525,
          "old_api": "set_start",
          "new_api": "size",
          "old_text": "slice_config->set_start(start_indices[i])",
          "new_text": "start_indices.size()",
          "old_line_content": "    slice_config->set_start(start_indices[i]);",
          "new_line_content": "  for (int i = 0, end = start_indices.size(); i < end; i++) {",
          "content_same": false
        },
        {
          "line": 1526,
          "old_api": "set_limit",
          "new_api": "add_slice_dimensions",
          "old_text": "slice_config->set_limit(limit_indices[i])",
          "new_text": "instr.add_slice_dimensions()",
          "old_line_content": "    slice_config->set_limit(limit_indices[i]);",
          "new_line_content": "    auto* slice_config = instr.add_slice_dimensions();",
          "content_same": false
        },
        {
          "line": 1527,
          "old_api": "set_stride",
          "new_api": "set_start",
          "old_text": "slice_config->set_stride(strides[i])",
          "new_text": "slice_config->set_start(start_indices[i])",
          "old_line_content": "    slice_config->set_stride(strides[i]);",
          "new_line_content": "    slice_config->set_start(start_indices[i]);",
          "content_same": false
        },
        {
          "line": 1529,
          "old_api": "std::move(instr)",
          "new_api": "set_stride",
          "old_text": "std::move(instr)",
          "new_text": "slice_config->set_stride(strides[i])",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kSlice, {operand});",
          "new_line_content": "    slice_config->set_stride(strides[i]);",
          "content_same": false
        },
        {
          "line": 1537,
          "old_api": "rank",
          "new_api": "begin",
          "old_text": "shape->rank()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));\n    std::vector<int64_t> starts(shape->rank(), 0);\n    std::vector<int64_t> limits(shape->dimensions().begin(),\n                                shape->dimensions().end());\n    std::vector<int64_t> strides(shape->rank(), 1);\n    starts[dimno] = start_index;\n    limits[dimno] = limit_index;\n    strides[dimno] = stride;\n    return Slice(operand, starts, limits, strides);\n  })",
          "old_line_content": "    std::vector<int64_t> starts(shape->rank(), 0);",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1539,
          "old_api": "end",
          "new_api": "rank",
          "old_text": "shape->dimensions().end()",
          "new_text": "shape->rank()",
          "old_line_content": "                                shape->dimensions().end());",
          "new_line_content": "    std::vector<int64_t> starts(shape->rank(), 0);",
          "content_same": false
        },
        {
          "line": 1540,
          "old_api": "rank",
          "new_api": "begin",
          "old_text": "shape->rank()",
          "new_text": "shape->dimensions().begin()",
          "old_line_content": "    std::vector<int64_t> strides(shape->rank(), 1);",
          "new_line_content": "    std::vector<int64_t> limits(shape->dimensions().begin(),",
          "content_same": false
        },
        {
          "line": 1629,
          "old_api": "add_dimensions",
          "new_api": "ToProto",
          "old_text": "instr.add_dimensions(dimension)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  instr.add_dimensions(dimension);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1631,
          "old_api": "std::move(instr)",
          "new_api": "add_dimensions",
          "old_text": "std::move(instr)",
          "new_text": "instr.add_dimensions(dimension)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kConcatenate, operands);",
          "new_line_content": "  instr.add_dimensions(dimension);",
          "content_same": false
        },
        {
          "line": 1651,
          "old_api": "rank",
          "new_api": "mutable_dimensions",
          "old_text": "shape->rank()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));\n    PaddingConfig padding_config = MakeNoPaddingConfig(shape->rank());\n    auto* dims = padding_config.mutable_dimensions(dimno);\n    dims->set_edge_padding_low(pad_lo);\n    dims->set_edge_padding_high(pad_hi);\n    return Pad(operand, padding_value, padding_config);\n  })",
          "old_line_content": "    PaddingConfig padding_config = MakeNoPaddingConfig(shape->rank());",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1653,
          "old_api": "set_edge_padding_low",
          "new_api": "rank",
          "old_text": "dims->set_edge_padding_low(pad_lo)",
          "new_text": "shape->rank()",
          "old_line_content": "    dims->set_edge_padding_low(pad_lo);",
          "new_line_content": "    PaddingConfig padding_config = MakeNoPaddingConfig(shape->rank());",
          "content_same": false
        },
        {
          "line": 1654,
          "old_api": "set_edge_padding_high",
          "new_api": "mutable_dimensions",
          "old_text": "dims->set_edge_padding_high(pad_hi)",
          "new_text": "padding_config.mutable_dimensions(dimno)",
          "old_line_content": "    dims->set_edge_padding_high(pad_hi);",
          "new_line_content": "    auto* dims = padding_config.mutable_dimensions(dimno);",
          "content_same": false
        },
        {
          "line": 1655,
          "old_api": "Pad",
          "new_api": "set_edge_padding_low",
          "old_text": "Pad(operand, padding_value, padding_config)",
          "new_text": "dims->set_edge_padding_low(pad_lo)",
          "old_line_content": "    return Pad(operand, padding_value, padding_config);",
          "new_line_content": "    dims->set_edge_padding_low(pad_lo);",
          "content_same": false
        },
        {
          "line": 1665,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kPad,",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1679,
          "old_api": "Transpose",
          "new_api": "IsIdentityPermutation",
          "old_text": "Transpose(operand, dimensions)",
          "new_text": "IsIdentityPermutation(dimensions)",
          "old_line_content": "                           : Transpose(operand, dimensions);",
          "new_line_content": "    XlaOp transposed = IsIdentityPermutation(dimensions)",
          "content_same": false
        },
        {
          "line": 1688,
          "old_api": "dimensions_size",
          "new_api": "begin",
          "old_text": "shape->dimensions_size()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));\n    std::vector<int64_t> dimensions(shape->dimensions_size());\n    std::iota(dimensions.begin(), dimensions.end(), 0);\n    return Reshape(operand, dimensions, new_sizes, inferred_dimension);\n  })",
          "old_line_content": "    std::vector<int64_t> dimensions(shape->dimensions_size());",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1690,
          "old_api": "Reshape",
          "new_api": "dimensions_size",
          "old_text": "Reshape(operand, dimensions, new_sizes, inferred_dimension)",
          "new_text": "shape->dimensions_size()",
          "old_line_content": "    return Reshape(operand, dimensions, new_sizes, inferred_dimension);",
          "new_line_content": "    std::vector<int64_t> dimensions(shape->dimensions_size());",
          "content_same": false
        },
        {
          "line": 1719,
          "old_api": "size",
          "new_api": "TF_RETURN_IF_ERROR",
          "old_text": "dim_sizes.size()",
          "new_text": "TF_RETURN_IF_ERROR(first_error_)",
          "old_line_content": "    operands.reserve(1 + dim_sizes.size());",
          "new_line_content": "    TF_RETURN_IF_ERROR(first_error_);",
          "content_same": false
        },
        {
          "line": 1760,
          "old_api": "dimensions",
          "new_api": "rank",
          "old_text": "original_shape->dimensions(i)",
          "new_text": "original_shape->rank()",
          "old_line_content": "        new_sizes.push_back(original_shape->dimensions(i));",
          "new_line_content": "    for (int i = 0; i < original_shape->rank(); ++i) {",
          "content_same": false
        },
        {
          "line": 1768,
          "old_api": "Reshape",
          "new_api": "absl::StrJoin(new_sizes, \",\")",
          "old_text": "Reshape(operand, new_sizes)",
          "new_text": "absl::StrJoin(new_sizes, \",\")",
          "old_line_content": "    return Reshape(operand, new_sizes);",
          "new_line_content": "    VLOG(3) << \"new sizes: [\" << absl::StrJoin(new_sizes, \",\") << \"]\";",
          "content_same": false
        },
        {
          "line": 1822,
          "old_api": "ShapeUtil::HumanString(*tuple_shape)",
          "new_api": "InvalidArgument",
          "old_text": "ShapeUtil::HumanString(*tuple_shape)",
          "new_text": "InvalidArgument(\n          \"Operand to GetTupleElement() is not a tuple; got %s\",\n          ShapeUtil::HumanString(*tuple_shape))",
          "old_line_content": "          ShapeUtil::HumanString(*tuple_shape));",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 1824,
          "old_api": "ShapeUtil::TupleElementCount(*tuple_shape)",
          "new_api": "ShapeUtil::HumanString(*tuple_shape)",
          "old_text": "ShapeUtil::TupleElementCount(*tuple_shape)",
          "new_text": "ShapeUtil::HumanString(*tuple_shape)",
          "old_line_content": "    if (index < 0 || index >= ShapeUtil::TupleElementCount(*tuple_shape)) {",
          "new_line_content": "          ShapeUtil::HumanString(*tuple_shape));",
          "content_same": false
        },
        {
          "line": 1827,
          "old_api": "ShapeUtil::HumanString(*tuple_shape)",
          "new_api": "InvalidArgument",
          "old_text": "ShapeUtil::HumanString(*tuple_shape)",
          "new_text": "InvalidArgument(\n          \"GetTupleElement() index (%d) out of range for tuple shape %s\", index,\n          ShapeUtil::HumanString(*tuple_shape))",
          "old_line_content": "          ShapeUtil::HumanString(*tuple_shape));",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 1829,
          "old_api": "GetTupleElementInternal",
          "new_api": "ShapeUtil::HumanString(*tuple_shape)",
          "old_text": "GetTupleElementInternal(\n        ShapeUtil::GetTupleElementShape(*tuple_shape, index), tuple_data,\n        index)",
          "new_text": "ShapeUtil::HumanString(*tuple_shape)",
          "old_line_content": "    return GetTupleElementInternal(",
          "new_line_content": "          ShapeUtil::HumanString(*tuple_shape));",
          "content_same": false
        },
        {
          "line": 1841,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kGetTupleElement,",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1854,
          "old_api": "add_rhs_contracting_dimensions",
          "new_api": "add_lhs_contracting_dimensions",
          "old_text": "dimension_numbers.add_rhs_contracting_dimensions(0)",
          "new_text": "dimension_numbers.add_lhs_contracting_dimensions(\n        lhs_shape->dimensions_size() == 1 ? 0 : 1)",
          "old_line_content": "    dimension_numbers.add_rhs_contracting_dimensions(0);",
          "new_line_content": "    dimension_numbers.add_lhs_contracting_dimensions(",
          "content_same": false
        },
        {
          "line": 1855,
          "old_api": "DotGeneral",
          "new_api": "dimensions_size",
          "old_text": "DotGeneral(lhs, rhs, dimension_numbers, precision_config,\n                      preferred_element_type)",
          "new_text": "lhs_shape->dimensions_size()",
          "old_line_content": "    return DotGeneral(lhs, rhs, dimension_numbers, precision_config,",
          "new_line_content": "        lhs_shape->dimensions_size() == 1 ? 0 : 1);",
          "content_same": false
        },
        {
          "line": 1884,
          "old_api": "mutable_precision_config",
          "new_api": "mutable_dot_dimension_numbers",
          "old_text": "instr.mutable_precision_config()",
          "new_text": "instr.mutable_dot_dimension_numbers()",
          "old_line_content": "    *instr.mutable_precision_config() = *precision_config;",
          "new_line_content": "  *instr.mutable_dot_dimension_numbers() = dimension_numbers;",
          "content_same": false
        },
        {
          "line": 1886,
          "old_api": "std::move(instr)",
          "new_api": "mutable_precision_config",
          "old_text": "std::move(instr)",
          "new_text": "instr.mutable_precision_config()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kDot, {lhs, rhs});",
          "new_line_content": "    *instr.mutable_precision_config() = *precision_config;",
          "content_same": false
        },
        {
          "line": 1909,
          "old_api": "mutable_precision_config",
          "new_api": "mutable_dot_dimension_numbers",
          "old_text": "instr.mutable_precision_config()",
          "new_text": "instr.mutable_dot_dimension_numbers()",
          "old_line_content": "      *instr.mutable_precision_config() = *precision_config;",
          "new_line_content": "    *instr.mutable_dot_dimension_numbers() = dimension_numbers;",
          "content_same": false
        },
        {
          "line": 1914,
          "old_api": "std::move(instr)",
          "new_api": "add_dot_sparsity",
          "old_text": "std::move(instr)",
          "new_text": "instr.add_dot_sparsity()",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kDot, operands);",
          "new_line_content": "      *instr.add_dot_sparsity() = descriptor;",
          "content_same": false
        },
        {
          "line": 1927,
          "old_api": "rank",
          "new_api": "ShapeUtil::HumanString(rhs_shape)",
          "old_text": "lhs_shape.rank()",
          "new_text": "ShapeUtil::HumanString(rhs_shape)",
          "old_line_content": "  int num_dims = lhs_shape.rank();",
          "new_line_content": "        ShapeUtil::HumanString(lhs_shape), ShapeUtil::HumanString(rhs_shape));",
          "content_same": false
        },
        {
          "line": 1929,
          "old_api": "InvalidArgument",
          "new_api": "rank",
          "old_text": "InvalidArgument(\n        \"Convolution expects argument arrays with >= 3 dimensions. \"\n        \"Got: %s and %s\",\n        ShapeUtil::HumanString(lhs_shape), ShapeUtil::HumanString(rhs_shape))",
          "new_text": "lhs_shape.rank()",
          "old_line_content": "    return InvalidArgument(",
          "new_line_content": "  int num_dims = lhs_shape.rank();",
          "content_same": false
        },
        {
          "line": 1944,
          "old_api": "InvalidArgument",
          "new_api": "size",
          "old_text": "InvalidArgument(\"Convolution %s[%d] is out of bounds: %d\",\n                               field_name, i, numbers[i])",
          "new_text": "numbers.size()",
          "old_line_content": "        return InvalidArgument(\"Convolution %s[%d] is out of bounds: %d\",",
          "new_line_content": "    for (int i = 0; i < numbers.size(); ++i) {",
          "content_same": false
        },
        {
          "line": 1950,
          "old_api": "input_spatial_dimensions",
          "new_api": "OkStatus",
          "old_text": "TF_RETURN_IF_ERROR(\n      check_spatial_dimensions(\"input_spatial_dimensions\",\n                               dimension_numbers.input_spatial_dimensions()))",
          "new_text": "OkStatus()",
          "old_line_content": "  TF_RETURN_IF_ERROR(",
          "new_line_content": "    return OkStatus();",
          "content_same": false
        },
        {
          "line": 1953,
          "old_api": "kernel_spatial_dimensions",
          "new_api": "input_spatial_dimensions",
          "old_text": "TF_RETURN_IF_ERROR(\n      check_spatial_dimensions(\"kernel_spatial_dimensions\",\n                               dimension_numbers.kernel_spatial_dimensions()))",
          "new_text": "check_spatial_dimensions(\"input_spatial_dimensions\",\n                               dimension_numbers.input_spatial_dimensions())",
          "old_line_content": "  TF_RETURN_IF_ERROR(",
          "new_line_content": "      check_spatial_dimensions(\"input_spatial_dimensions\",",
          "content_same": false
        },
        {
          "line": 1954,
          "old_api": "kernel_spatial_dimensions",
          "new_api": "input_spatial_dimensions",
          "old_text": "check_spatial_dimensions(\"kernel_spatial_dimensions\",\n                               dimension_numbers.kernel_spatial_dimensions())",
          "new_text": "dimension_numbers.input_spatial_dimensions()",
          "old_line_content": "      check_spatial_dimensions(\"kernel_spatial_dimensions\",",
          "new_line_content": "                               dimension_numbers.input_spatial_dimensions()));",
          "content_same": false
        },
        {
          "line": 1956,
          "old_api": "output_spatial_dimensions",
          "new_api": "kernel_spatial_dimensions",
          "old_text": "check_spatial_dimensions(\n      \"output_spatial_dimensions\",\n      dimension_numbers.output_spatial_dimensions())",
          "new_text": "check_spatial_dimensions(\"kernel_spatial_dimensions\",\n                               dimension_numbers.kernel_spatial_dimensions())",
          "old_line_content": "  return check_spatial_dimensions(",
          "new_line_content": "      check_spatial_dimensions(\"kernel_spatial_dimensions\",",
          "content_same": false
        },
        {
          "line": 2111,
          "old_api": "mutable_window",
          "new_api": "ToProto",
          "old_text": "instr.mutable_window()",
          "new_text": "shape.ToProto()",
          "old_line_content": "  *instr.mutable_window() = window;",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2113,
          "old_api": "set_feature_group_count",
          "new_api": "mutable_window",
          "old_text": "instr.set_feature_group_count(feature_group_count)",
          "new_text": "instr.mutable_window()",
          "old_line_content": "  instr.set_feature_group_count(feature_group_count);",
          "new_line_content": "  *instr.mutable_window() = window;",
          "content_same": false
        },
        {
          "line": 2114,
          "old_api": "set_batch_group_count",
          "new_api": "mutable_convolution_dimension_numbers",
          "old_text": "instr.set_batch_group_count(batch_group_count)",
          "new_text": "instr.mutable_convolution_dimension_numbers()",
          "old_line_content": "  instr.set_batch_group_count(batch_group_count);",
          "new_line_content": "  *instr.mutable_convolution_dimension_numbers() = dimension_numbers;",
          "content_same": false
        },
        {
          "line": 2115,
          "old_api": "set_padding_type",
          "new_api": "set_feature_group_count",
          "old_text": "instr.set_padding_type(padding_type)",
          "new_text": "instr.set_feature_group_count(feature_group_count)",
          "old_line_content": "  instr.set_padding_type(padding_type);",
          "new_line_content": "  instr.set_feature_group_count(feature_group_count);",
          "content_same": false
        },
        {
          "line": 2120,
          "old_api": "std::move(instr)",
          "new_api": "mutable_precision_config",
          "old_text": "std::move(instr)",
          "new_text": "instr.mutable_precision_config()",
          "old_line_content": "  return std::move(instr);",
          "new_line_content": "    *instr.mutable_precision_config() = *precision_config;",
          "content_same": false
        },
        {
          "line": 2143,
          "old_api": "std::move(instr)",
          "new_api": "set_custom_call_target",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_custom_call_target(\"DynamicConvolutionInputGrad\")",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCustomCall,",
          "new_line_content": "    instr.set_custom_call_target(\"DynamicConvolutionInputGrad\");",
          "content_same": false
        },
        {
          "line": 2194,
          "old_api": "std::move(instr)",
          "new_api": "set_custom_call_target",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_custom_call_target(\"DynamicConvolutionForward\")",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCustomCall, {lhs, rhs});",
          "new_line_content": "    instr.set_custom_call_target(\"DynamicConvolutionForward\");",
          "content_same": false
        },
        {
          "line": 2210,
          "old_api": "mutable_window",
          "new_api": "ToProto",
          "old_text": "instr.mutable_window()",
          "new_text": "shape.ToProto()",
          "old_line_content": "  *instr.mutable_window() = window;",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2212,
          "old_api": "set_feature_group_count",
          "new_api": "mutable_window",
          "old_text": "instr.set_feature_group_count(feature_group_count)",
          "new_text": "instr.mutable_window()",
          "old_line_content": "  instr.set_feature_group_count(feature_group_count);",
          "new_line_content": "  *instr.mutable_window() = window;",
          "content_same": false
        },
        {
          "line": 2213,
          "old_api": "set_batch_group_count",
          "new_api": "mutable_convolution_dimension_numbers",
          "old_text": "instr.set_batch_group_count(batch_group_count)",
          "new_text": "instr.mutable_convolution_dimension_numbers()",
          "old_line_content": "  instr.set_batch_group_count(batch_group_count);",
          "new_line_content": "  *instr.mutable_convolution_dimension_numbers() = dimension_numbers;",
          "content_same": false
        },
        {
          "line": 2239,
          "old_api": "add_fft_length",
          "new_api": "set_fft_type",
          "old_text": "instr.add_fft_length(i)",
          "new_text": "instr.set_fft_type(fft_type)",
          "old_line_content": "    instr.add_fft_length(i);",
          "new_line_content": "  instr.set_fft_type(fft_type);",
          "content_same": false
        },
        {
          "line": 2251,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kTriangularSolve, {a, b});",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2259,
          "old_api": "ToProto",
          "new_api": "mutable_cholesky_options",
          "old_text": "shape.ToProto()",
          "new_text": "instr.mutable_cholesky_options()",
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "  CholeskyOptions& options = *instr.mutable_cholesky_options();",
          "content_same": false
        },
        {
          "line": 2261,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kCholesky, {a});",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2267,
          "old_api": "LayoutUtil::HasLayout(shape)",
          "new_api": "mutable_shape",
          "old_text": "LayoutUtil::HasLayout(shape)",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    if (!LayoutUtil::HasLayout(shape)) {\n      return InvalidArgument(\"Given shape to Infeed must have a layout\");\n    }\n    const Shape infeed_instruction_shape =\n        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()});\n    *instr.mutable_shape() = infeed_instruction_shape.ToProto();\n    instr.set_infeed_config(config);\n\n    if (shape.IsArray() && sharding() &&\n        sharding()->type() == OpSharding::OTHER) {\n      // TODO(b/110793772): Support tiled array-shaped infeeds.\n      return InvalidArgument(\n          \"Tiled sharding is not yet supported for array-shaped infeeds\");\n    }\n\n    if (sharding() && sharding()->type() == OpSharding::REPLICATED) {\n      return InvalidArgument(\n          \"Replicated sharding is not yet supported for infeeds\");\n    }\n\n    // Infeed takes a single token operand. Generate the token to pass to the\n    // infeed.\n    XlaOp token;\n    auto make_token = [&]() {\n      HloInstructionProto token_instr;\n      *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n      return AddInstruction(std::move(token_instr), HloOpcode::kAfterAll, {});\n    };\n    if (sharding()) {\n      // Arbitrarily assign token to device 0.\n      OpSharding sharding = sharding_builder::AssignDevice(0);\n      XlaScopedShardingAssignment scoped_sharding(this, sharding);\n      TF_ASSIGN_OR_RETURN(token, make_token());\n    } else {\n      TF_ASSIGN_OR_RETURN(token, make_token());\n    }\n\n    // The sharding is set by the client according to the data tuple shape.\n    // However, the shape of the infeed instruction is a tuple containing the\n    // data and a token. For tuple sharding type, the sharding must be changed\n    // to accommodate the token.\n    XlaOp infeed;\n    if (sharding() && sharding()->type() == OpSharding::TUPLE) {\n      // TODO(b/80000000): Remove this when clients have been updated to handle\n      // tokens.\n      OpSharding infeed_instruction_sharding = *sharding();\n      // Arbitrarily assign the token to device 0.\n      *infeed_instruction_sharding.add_tuple_shardings() =\n          sharding_builder::AssignDevice(0);\n      XlaScopedShardingAssignment scoped_sharding(this,\n                                                  infeed_instruction_sharding);\n      TF_ASSIGN_OR_RETURN(infeed, AddInstruction(std::move(instr),\n                                                 HloOpcode::kInfeed, {token}));\n    } else {\n      TF_ASSIGN_OR_RETURN(infeed, AddInstruction(std::move(instr),\n                                                 HloOpcode::kInfeed, {token}));\n    }\n\n    // The infeed instruction produces a tuple of the infed data and a token\n    // type. Return XLA op containing the data.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto infeed_data;\n    *infeed_data.mutable_shape() = shape.ToProto();\n    infeed_data.set_tuple_index(0);\n    return AddInstruction(std::move(infeed_data), HloOpcode::kGetTupleElement,\n                          {infeed});\n  })",
          "old_line_content": "    if (!LayoutUtil::HasLayout(shape)) {",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2273,
          "old_api": "set_infeed_config",
          "new_api": "ShapeUtil::MakeTokenShape()",
          "old_text": "instr.set_infeed_config(config)",
          "new_text": "ShapeUtil::MakeTokenShape()",
          "old_line_content": "    instr.set_infeed_config(config);",
          "new_line_content": "        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()});",
          "content_same": false
        },
        {
          "line": 2275,
          "old_api": "sharding",
          "new_api": "set_infeed_config",
          "old_text": "sharding()",
          "new_text": "instr.set_infeed_config(config)",
          "old_line_content": "    if (shape.IsArray() && sharding() &&",
          "new_line_content": "    instr.set_infeed_config(config);",
          "content_same": false
        },
        {
          "line": 2278,
          "old_api": "InvalidArgument",
          "new_api": "type",
          "old_text": "InvalidArgument(\n          \"Tiled sharding is not yet supported for array-shaped infeeds\")",
          "new_text": "sharding()->type()",
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "        sharding()->type() == OpSharding::OTHER) {",
          "content_same": false
        },
        {
          "line": 2295,
          "old_api": "sharding",
          "new_api": "std::move(token_instr)",
          "old_text": "sharding()",
          "new_text": "std::move(token_instr)",
          "old_line_content": "    if (sharding()) {",
          "new_line_content": "      return AddInstruction(std::move(token_instr), HloOpcode::kAfterAll, {});",
          "content_same": false
        },
        {
          "line": 2297,
          "old_api": "sharding_builder::AssignDevice(0)",
          "new_api": "sharding",
          "old_text": "sharding_builder::AssignDevice(0)",
          "new_text": "sharding()",
          "old_line_content": "      OpSharding sharding = sharding_builder::AssignDevice(0);",
          "new_line_content": "    if (sharding()) {",
          "content_same": false
        },
        {
          "line": 2299,
          "old_api": "make_token",
          "new_api": "sharding_builder::AssignDevice(0)",
          "old_text": "make_token()",
          "new_text": "sharding_builder::AssignDevice(0)",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(token, make_token());",
          "new_line_content": "      OpSharding sharding = sharding_builder::AssignDevice(0);",
          "content_same": false
        },
        {
          "line": 2314,
          "old_api": "add_tuple_shardings",
          "new_api": "sharding",
          "old_text": "infeed_instruction_sharding.add_tuple_shardings()",
          "new_text": "sharding()",
          "old_line_content": "      *infeed_instruction_sharding.add_tuple_shardings() =",
          "new_line_content": "      OpSharding infeed_instruction_sharding = *sharding();",
          "content_same": false
        },
        {
          "line": 2332,
          "old_api": "std::move(infeed_data)",
          "new_api": "ToProto",
          "old_text": "std::move(infeed_data)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    return AddInstruction(std::move(infeed_data), HloOpcode::kGetTupleElement,",
          "new_line_content": "    *infeed_data.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2341,
          "old_api": "InvalidArgument",
          "new_api": "IsArray",
          "old_text": "InvalidArgument(\"Given shape to Infeed must have a layout\")",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!LayoutUtil::HasLayout(shape)) {\n      return InvalidArgument(\"Given shape to Infeed must have a layout\");\n    }\n    const Shape infeed_instruction_shape =\n        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()});\n\n    if (shape.IsArray() && sharding() &&\n        sharding()->type() == OpSharding::OTHER) {\n      // TODO(b/110793772): Support tiled array-shaped infeeds.\n      return InvalidArgument(\n          \"Tiled sharding is not yet supported for array-shaped infeeds\");\n    }\n\n    if (sharding() && sharding()->type() == OpSharding::REPLICATED) {\n      return InvalidArgument(\n          \"Replicated sharding is not yet supported for infeeds\");\n    }\n    return InfeedWithTokenInternal(infeed_instruction_shape, token, config);\n  })",
          "old_line_content": "      return InvalidArgument(\"Given shape to Infeed must have a layout\");",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2346,
          "old_api": "sharding",
          "new_api": "ShapeUtil::MakeTokenShape()",
          "old_text": "sharding()",
          "new_text": "ShapeUtil::MakeTokenShape()",
          "old_line_content": "    if (shape.IsArray() && sharding() &&",
          "new_line_content": "        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()});",
          "content_same": false
        },
        {
          "line": 2349,
          "old_api": "InvalidArgument",
          "new_api": "type",
          "old_text": "InvalidArgument(\n          \"Tiled sharding is not yet supported for array-shaped infeeds\")",
          "new_text": "sharding()->type()",
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "        sharding()->type() == OpSharding::OTHER) {",
          "content_same": false
        },
        {
          "line": 2367,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "infeed_instruction_shape.ToProto()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kInfeed, {token});",
          "new_line_content": "  *instr.mutable_shape() = infeed_instruction_shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2385,
          "old_api": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "new_api": "InvalidArgument",
          "old_text": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "new_text": "InvalidArgument(\n          \"Outfeed shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape))",
          "old_line_content": "          ShapeUtil::HumanStringWithLayout(shape_with_layout),",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 2388,
          "old_api": "ToProto",
          "new_api": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "old_text": "shape_with_layout.ToProto()",
          "new_text": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "old_line_content": "    *instr.mutable_outfeed_shape() = shape_with_layout.ToProto();",
          "new_line_content": "          ShapeUtil::HumanStringWithLayout(*operand_shape));",
          "content_same": false
        },
        {
          "line": 2390,
          "old_api": "set_outfeed_config",
          "new_api": "ToProto",
          "old_text": "instr.set_outfeed_config(outfeed_config)",
          "new_text": "shape_with_layout.ToProto()",
          "old_line_content": "    instr.set_outfeed_config(outfeed_config);",
          "new_line_content": "    *instr.mutable_outfeed_shape() = shape_with_layout.ToProto();",
          "content_same": false
        },
        {
          "line": 2406,
          "old_api": "sharding_builder::AssignDevice(0)",
          "new_api": "sharding",
          "old_text": "sharding_builder::AssignDevice(0)",
          "new_text": "sharding()",
          "old_line_content": "          this, sharding_builder::AssignDevice(0));",
          "new_line_content": "    if (sharding()) {",
          "content_same": false
        },
        {
          "line": 2411,
          "old_api": "sharding",
          "new_api": "make_token",
          "old_text": "sharding()",
          "new_text": "make_token()",
          "old_line_content": "    if (sharding()) {",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(token, make_token());",
          "content_same": false
        },
        {
          "line": 2413,
          "old_api": "type",
          "new_api": "sharding",
          "old_text": "tuple_sharding.type()",
          "new_text": "sharding()",
          "old_line_content": "      if (tuple_sharding.type() != OpSharding::TUPLE) {",
          "new_line_content": "    if (sharding()) {",
          "content_same": false
        },
        {
          "line": 2414,
          "old_api": "sharding_builder::Tuple({})",
          "new_api": "sharding",
          "old_text": "sharding_builder::Tuple({})",
          "new_text": "sharding()",
          "old_line_content": "        tuple_sharding = sharding_builder::Tuple({});",
          "new_line_content": "      OpSharding tuple_sharding = *sharding();",
          "content_same": false
        },
        {
          "line": 2415,
          "old_api": "sharding",
          "new_api": "type",
          "old_text": "sharding()",
          "new_text": "tuple_sharding.type()",
          "old_line_content": "        *tuple_sharding.add_tuple_shardings() = *sharding();",
          "new_line_content": "      if (tuple_sharding.type() != OpSharding::TUPLE) {",
          "content_same": false
        },
        {
          "line": 2417,
          "old_api": "sharding_builder::AssignDevice(0)",
          "new_api": "sharding",
          "old_text": "sharding_builder::AssignDevice(0)",
          "new_text": "sharding()",
          "old_line_content": "      *tuple_sharding.add_tuple_shardings() = sharding_builder::AssignDevice(0);",
          "new_line_content": "        *tuple_sharding.add_tuple_shardings() = *sharding();",
          "content_same": false
        },
        {
          "line": 2419,
          "old_api": "status",
          "new_api": "sharding_builder::AssignDevice(0)",
          "old_text": "make_outfeed(token).status()",
          "new_text": "sharding_builder::AssignDevice(0)",
          "old_line_content": "      TF_RETURN_IF_ERROR(make_outfeed(token).status());",
          "new_line_content": "      *tuple_sharding.add_tuple_shardings() = sharding_builder::AssignDevice(0);",
          "content_same": false
        },
        {
          "line": 2447,
          "old_api": "LayoutUtil::HasLayout(shape_with_layout)",
          "new_api": "ReportErrorOrReturn",
          "old_text": "LayoutUtil::HasLayout(shape_with_layout)",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Check and set outfeed shape.\n    if (!LayoutUtil::HasLayout(shape_with_layout)) {\n      return InvalidArgument(\"Given shape to Outfeed must have a layout\");\n    }\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {\n      return InvalidArgument(\n          \"Outfeed shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape));\n    }\n    return OutfeedWithTokenInternal(operand, token, shape_with_layout,\n                                    outfeed_config);\n  })",
          "old_line_content": "    if (!LayoutUtil::HasLayout(shape_with_layout)) {",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2454,
          "old_api": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "new_api": "InvalidArgument",
          "old_text": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "new_text": "InvalidArgument(\n          \"Outfeed shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape))",
          "old_line_content": "          ShapeUtil::HumanStringWithLayout(shape_with_layout),",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 2457,
          "old_api": "OutfeedWithTokenInternal",
          "new_api": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "old_text": "OutfeedWithTokenInternal(operand, token, shape_with_layout,\n                                    outfeed_config)",
          "new_text": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "old_line_content": "    return OutfeedWithTokenInternal(operand, token, shape_with_layout,",
          "new_line_content": "          ShapeUtil::HumanStringWithLayout(*operand_shape));",
          "content_same": false
        },
        {
          "line": 2468,
          "old_api": "set_outfeed_config",
          "new_api": "ToProto",
          "old_text": "instr.set_outfeed_config(outfeed_config)",
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "  instr.set_outfeed_config(outfeed_config);",
          "new_line_content": "  *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 2469,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "shape_with_layout.ToProto()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kOutfeed,",
          "new_line_content": "  *instr.mutable_outfeed_shape() = shape_with_layout.ToProto();",
          "content_same": false
        },
        {
          "line": 2476,
          "old_api": "ToProto",
          "new_api": "mutable_shape",
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kAfterAll);\n  })",
          "old_line_content": "    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2484,
          "old_api": "InvalidArgument",
          "new_api": "empty",
          "old_text": "InvalidArgument(\"AfterAll requires at least one operand\")",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (tokens.empty()) {\n      return InvalidArgument(\"AfterAll requires at least one operand\");\n    }\n    for (int i = 0, end = tokens.size(); i < end; ++i) {\n      XlaOp operand = tokens[i];\n      TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n      if (!operand_shape->IsToken()) {\n        return InvalidArgument(\n            \"All operands to AfterAll must be tokens; operand %d has shape %s\",\n            i, ShapeUtil::HumanString(*operand_shape));\n      }\n    }\n    HloInstructionProto instr;\n    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kAfterAll, tokens);\n  })",
          "old_line_content": "      return InvalidArgument(\"AfterAll requires at least one operand\");",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2486,
          "old_api": "size",
          "new_api": "InvalidArgument",
          "old_text": "tokens.size()",
          "new_text": "InvalidArgument(\"AfterAll requires at least one operand\")",
          "old_line_content": "    for (int i = 0, end = tokens.size(); i < end; ++i) {",
          "new_line_content": "      return InvalidArgument(\"AfterAll requires at least one operand\");",
          "content_same": false
        },
        {
          "line": 2492,
          "old_api": "ShapeUtil::HumanString(*operand_shape)",
          "new_api": "InvalidArgument",
          "old_text": "ShapeUtil::HumanString(*operand_shape)",
          "new_text": "InvalidArgument(\n            \"All operands to AfterAll must be tokens; operand %d has shape %s\",\n            i, ShapeUtil::HumanString(*operand_shape))",
          "old_line_content": "            i, ShapeUtil::HumanString(*operand_shape));",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 2513,
          "old_api": "InvalidArgument",
          "new_api": "has_value",
          "old_text": "InvalidArgument(\n          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"\n          \"are reserved for internal use.\",\n          call_target_name)",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (absl::StartsWith(call_target_name, \"$\")) {\n      return InvalidArgument(\n          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"\n          \"are reserved for internal use.\",\n          call_target_name);\n    }\n    if (operand_shapes_with_layout.has_value()) {\n      if (!LayoutUtil::HasLayout(shape)) {\n        return InvalidArgument(\n            \"Result shape must have layout for custom call with constrained \"\n            \"layout.\");\n      }\n      if (operands.size() != operand_shapes_with_layout->size()) {\n        return InvalidArgument(\n            \"Must specify a shape with layout for each operand for custom call \"\n            \"with constrained layout; given %d shapes, expected %d\",\n            operand_shapes_with_layout->size(), operands.size());\n      }\n      int64_t operand_num = 0;\n      for (const Shape& operand_shape : *operand_shapes_with_layout) {\n        if (!LayoutUtil::HasLayout(operand_shape)) {\n          return InvalidArgument(\n              \"No layout specified for operand %d for custom call with \"\n              \"constrained layout.\",\n              operand_num);\n        }\n        ++operand_num;\n      }\n    }\n    return CustomCallInternal(\n        call_target_name, operands, /*computation=*/nullptr, shape, opaque,\n        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,\n        literal, window, dnums, schedule, api_version);\n  })",
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2520,
          "old_api": "InvalidArgument",
          "new_api": "has_value",
          "old_text": "InvalidArgument(\n            \"Result shape must have layout for custom call with constrained \"\n            \"layout.\")",
          "new_text": "operand_shapes_with_layout.has_value()",
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "    if (operand_shapes_with_layout.has_value()) {",
          "content_same": false
        },
        {
          "line": 2572,
          "old_api": "ToProto",
          "new_api": "set_name",
          "old_text": "shape.ToProto()",
          "new_text": "instr.set_name(\"cudnn-conv-bias-activation\")",
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    instr.set_name(\"cudnn-conv-bias-activation\");",
          "content_same": false
        },
        {
          "line": 2574,
          "old_api": "set_backend_config",
          "new_api": "ToProto",
          "old_text": "instr.set_backend_config(opaque)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  instr.set_backend_config(opaque);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2575,
          "old_api": "has_value",
          "new_api": "set_custom_call_target",
          "old_text": "operand_shapes_with_layout.has_value()",
          "new_text": "instr.set_custom_call_target(call_target_name)",
          "old_line_content": "  if (operand_shapes_with_layout.has_value()) {",
          "new_line_content": "  instr.set_custom_call_target(call_target_name);",
          "content_same": false
        },
        {
          "line": 2576,
          "old_api": "set_constrain_layout",
          "new_api": "set_backend_config",
          "old_text": "instr.set_constrain_layout(true)",
          "new_text": "instr.set_backend_config(opaque)",
          "old_line_content": "    instr.set_constrain_layout(true);",
          "new_line_content": "  instr.set_backend_config(opaque);",
          "content_same": false
        },
        {
          "line": 2578,
          "old_api": "ToProto",
          "new_api": "set_constrain_layout",
          "old_text": "operand_shape.ToProto()",
          "new_text": "instr.set_constrain_layout(true)",
          "old_line_content": "      *instr.add_operand_shapes_with_layout() = operand_shape.ToProto();",
          "new_line_content": "    instr.set_constrain_layout(true);",
          "content_same": false
        },
        {
          "line": 2584,
          "old_api": "set_custom_call_has_side_effect",
          "new_api": "ToProto",
          "old_text": "instr.set_custom_call_has_side_effect(has_side_effect)",
          "new_text": "literal->ToProto()",
          "old_line_content": "  instr.set_custom_call_has_side_effect(has_side_effect);",
          "new_line_content": "    *instr.mutable_literal() = literal->ToProto();",
          "content_same": false
        },
        {
          "line": 2586,
          "old_api": "AddCalledComputation",
          "new_api": "set_custom_call_has_side_effect",
          "old_text": "AddCalledComputation(*computation, &instr)",
          "new_text": "instr.set_custom_call_has_side_effect(has_side_effect)",
          "old_line_content": "    AddCalledComputation(*computation, &instr);",
          "new_line_content": "  instr.set_custom_call_has_side_effect(has_side_effect);",
          "content_same": false
        },
        {
          "line": 2592,
          "old_api": "add_operand_shape_index",
          "new_api": "set_operand_index",
          "old_text": "aliasing->add_operand_shape_index(index)",
          "new_text": "aliasing->set_operand_index(pair.second.first)",
          "old_line_content": "      aliasing->add_operand_shape_index(index);",
          "new_line_content": "    aliasing->set_operand_index(pair.second.first);",
          "content_same": false
        },
        {
          "line": 2601,
          "old_api": "has_value",
          "new_api": "mutable_window",
          "old_text": "dnums.has_value()",
          "new_text": "instr.mutable_window()",
          "old_line_content": "  if (dnums.has_value()) {",
          "new_line_content": "    *instr.mutable_window() = *window;",
          "content_same": false
        },
        {
          "line": 2604,
          "old_api": "set_custom_call_schedule",
          "new_api": "mutable_convolution_dimension_numbers",
          "old_text": "instr.set_custom_call_schedule(schedule)",
          "new_text": "instr.mutable_convolution_dimension_numbers()",
          "old_line_content": "  instr.set_custom_call_schedule(schedule);",
          "new_line_content": "    *instr.mutable_convolution_dimension_numbers() = *dnums;",
          "content_same": false
        },
        {
          "line": 2606,
          "old_api": "std::move(instr)",
          "new_api": "set_custom_call_schedule",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_custom_call_schedule(schedule)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kCustomCall, operands);",
          "new_line_content": "  instr.set_custom_call_schedule(schedule);",
          "content_same": false
        },
        {
          "line": 2621,
          "old_api": "InvalidArgument",
          "new_api": "has_value",
          "old_text": "InvalidArgument(\n          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"\n          \"are reserved for internal use.\",\n          call_target_name)",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (absl::StartsWith(call_target_name, \"$\")) {\n      return InvalidArgument(\n          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"\n          \"are reserved for internal use.\",\n          call_target_name);\n    }\n    if (operand_shapes_with_layout.has_value()) {\n      if (!LayoutUtil::HasLayout(shape)) {\n        return InvalidArgument(\n            \"Result shape must have layout for custom call with constrained \"\n            \"layout.\");\n      }\n      if (operands.size() != operand_shapes_with_layout->size()) {\n        return InvalidArgument(\n            \"Must specify a shape with layout for each operand for custom call \"\n            \"with constrained layout; given %d shapes, expected %d\",\n            operand_shapes_with_layout->size(), operands.size());\n      }\n      int64_t operand_num = 0;\n      for (const Shape& operand_shape : *operand_shapes_with_layout) {\n        if (!LayoutUtil::HasLayout(operand_shape)) {\n          return InvalidArgument(\n              \"No layout specified for operand %d for custom call with \"\n              \"constrained layout.\",\n              operand_num);\n        }\n        ++operand_num;\n      }\n    }\n    return CustomCallInternal(\n        call_target_name, operands, &computation, shape, opaque,\n        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,\n        literal, /*window=*/{}, /*dnums=*/{}, schedule, api_version);\n  })",
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2628,
          "old_api": "InvalidArgument",
          "new_api": "has_value",
          "old_text": "InvalidArgument(\n            \"Result shape must have layout for custom call with constrained \"\n            \"layout.\")",
          "new_text": "operand_shapes_with_layout.has_value()",
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "    if (operand_shapes_with_layout.has_value()) {",
          "content_same": false
        },
        {
          "line": 2682,
          "old_api": "add_dimensions",
          "new_api": "ToProto",
          "old_text": "instr.add_dimensions(dim)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    instr.add_dimensions(dim);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2684,
          "old_api": "std::move(instr)",
          "new_api": "add_dimensions",
          "old_text": "std::move(instr)",
          "new_text": "instr.add_dimensions(dim)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kTranspose, {operand});",
          "new_line_content": "    instr.add_dimensions(dim);",
          "content_same": false
        },
        {
          "line": 2701,
          "old_api": "add_dimensions",
          "new_api": "ToProto",
          "old_text": "instr.add_dimensions(dim)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    instr.add_dimensions(dim);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2703,
          "old_api": "std::move(instr)",
          "new_api": "add_dimensions",
          "old_text": "std::move(instr)",
          "new_text": "instr.add_dimensions(dim)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kReverse, {operand});",
          "new_line_content": "    instr.add_dimensions(dim);",
          "content_same": false
        },
        {
          "line": 2711,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": "ReportErrorOrReturn",
          "old_text": "TF_ASSIGN_OR_RETURN(std::vector<Shape> operand_shapes,\n                        GetOperandShapes(operands))",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(std::vector<Shape> operand_shapes,\n                        GetOperandShapes(operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferVariadicOpShape(\n                                         HloOpcode::kSort, operand_shape_ptrs));\n    return SortInternal(shape, operands, comparator, dimension, is_stable);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(std::vector<Shape> operand_shapes,",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2713,
          "old_api": "std::back_inserter(operand_shape_ptrs)",
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": "std::back_inserter(operand_shape_ptrs)",
          "new_text": "TF_ASSIGN_OR_RETURN(std::vector<Shape> operand_shapes,\n                        GetOperandShapes(operands))",
          "old_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(std::vector<Shape> operand_shapes,",
          "content_same": false
        },
        {
          "line": 2732,
          "old_api": "add_dimensions",
          "new_api": "rank",
          "old_text": "instr.add_dimensions(dimension)",
          "new_text": "keys_shape->rank()",
          "old_line_content": "  instr.add_dimensions(dimension);",
          "new_line_content": "    dimension = keys_shape->rank() - 1;",
          "content_same": false
        },
        {
          "line": 2734,
          "old_api": "std::move(instr)",
          "new_api": "add_dimensions",
          "old_text": "std::move(instr)",
          "new_text": "instr.add_dimensions(dimension)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kSort, operands);",
          "new_line_content": "  instr.add_dimensions(dimension);",
          "content_same": false
        },
        {
          "line": 2752,
          "old_api": "set_largest",
          "new_api": "ToProto",
          "old_text": "instr.set_largest(largest)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  instr.set_largest(largest);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2753,
          "old_api": "std::move(instr)",
          "new_api": "set_k",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_k(k)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kTopK, {operand});",
          "new_line_content": "  instr.set_k(k);",
          "content_same": false
        },
        {
          "line": 2764,
          "old_api": "Real",
          "new_api": "element_type",
          "old_text": "Real(operand)",
          "new_text": "operand_shape->element_type()",
          "old_line_content": "      operand = Real(operand);",
          "new_line_content": "    if (primitive_util::IsComplexType(operand_shape->element_type()) &&",
          "content_same": false
        },
        {
          "line": 2766,
          "old_api": "AddOpWithShape",
          "new_api": "Real",
          "old_text": "AddOpWithShape(HloOpcode::kConvert, shape, {operand})",
          "new_text": "Real(operand)",
          "old_line_content": "    return AddOpWithShape(HloOpcode::kConvert, shape, {operand});",
          "new_line_content": "      operand = Real(operand);",
          "content_same": false
        },
        {
          "line": 2811,
          "old_api": "Unimplemented",
          "new_api": "empty",
          "old_text": "Unimplemented(\"static_operands is not supported in Map\")",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!static_operands.empty()) {\n      return Unimplemented(\"static_operands is not supported in Map\");\n    }\n\n    HloInstructionProto instr;\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(const ProgramShape& called_program_shape,\n                        computation.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(\n        Shape shape, ShapeInference::InferMapShape(\n                         operand_shape_ptrs, called_program_shape, dimensions));\n    *instr.mutable_shape() = shape.ToProto();\n\n    Shape output_shape(instr.shape());\n    const int64_t output_rank = output_shape.rank();\n    AddCalledComputation(computation, &instr);\n    std::vector<XlaOp> new_operands(operands.begin(), operands.end());\n    for (XlaOp& new_operand : new_operands) {\n      TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(new_operand));\n      const int64_t rank = shape->rank();\n      if (rank != output_rank) {\n        TF_ASSIGN_OR_RETURN(new_operand,\n                            InDimBroadcast(output_shape, new_operand, {}));\n        TF_ASSIGN_OR_RETURN(shape, GetShapePtr(new_operand));\n      }\n      if (!ShapeUtil::SameDimensions(output_shape, *shape)) {\n        TF_ASSIGN_OR_RETURN(new_operand,\n                            AddBroadcastSequence(output_shape, new_operand));\n      }\n    }\n\n    return AddInstruction(std::move(instr), HloOpcode::kMap, new_operands);\n  })",
          "old_line_content": "      return Unimplemented(\"static_operands is not supported in Map\");",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2826,
          "old_api": "shape",
          "new_api": "ToProto",
          "old_text": "instr.shape()",
          "new_text": "shape.ToProto()",
          "old_line_content": "    Shape output_shape(instr.shape());",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2828,
          "old_api": "AddCalledComputation",
          "new_api": "shape",
          "old_text": "AddCalledComputation(computation, &instr)",
          "new_text": "instr.shape()",
          "old_line_content": "    AddCalledComputation(computation, &instr);",
          "new_line_content": "    Shape output_shape(instr.shape());",
          "content_same": false
        },
        {
          "line": 2829,
          "old_api": "end",
          "new_api": "rank",
          "old_text": "operands.end()",
          "new_text": "output_shape.rank()",
          "old_line_content": "    std::vector<XlaOp> new_operands(operands.begin(), operands.end());",
          "new_line_content": "    const int64_t output_rank = output_shape.rank();",
          "content_same": false
        },
        {
          "line": 2834,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": "rank",
          "old_text": "TF_ASSIGN_OR_RETURN(new_operand,\n                            InDimBroadcast(output_shape, new_operand, {}))",
          "new_text": "shape->rank()",
          "old_line_content": "        TF_ASSIGN_OR_RETURN(new_operand,",
          "new_line_content": "      const int64_t rank = shape->rank();",
          "content_same": false
        },
        {
          "line": 2836,
          "old_api": "GetShapePtr",
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": "GetShapePtr(new_operand)",
          "new_text": "TF_ASSIGN_OR_RETURN(new_operand,\n                            InDimBroadcast(output_shape, new_operand, {}))",
          "old_line_content": "        TF_ASSIGN_OR_RETURN(shape, GetShapePtr(new_operand));",
          "new_line_content": "        TF_ASSIGN_OR_RETURN(new_operand,",
          "content_same": false
        },
        {
          "line": 2838,
          "old_api": "ShapeUtil::SameDimensions(output_shape, *shape)",
          "new_api": "GetShapePtr",
          "old_text": "ShapeUtil::SameDimensions(output_shape, *shape)",
          "new_text": "GetShapePtr(new_operand)",
          "old_line_content": "      if (!ShapeUtil::SameDimensions(output_shape, *shape)) {",
          "new_line_content": "        TF_ASSIGN_OR_RETURN(shape, GetShapePtr(new_operand));",
          "content_same": false
        },
        {
          "line": 2840,
          "old_api": "AddBroadcastSequence",
          "new_api": "ShapeUtil::SameDimensions(output_shape, *shape)",
          "old_text": "AddBroadcastSequence(output_shape, new_operand)",
          "new_text": "ShapeUtil::SameDimensions(output_shape, *shape)",
          "old_line_content": "                            AddBroadcastSequence(output_shape, new_operand));",
          "new_line_content": "      if (!ShapeUtil::SameDimensions(output_shape, *shape)) {",
          "content_same": false
        },
        {
          "line": 2878,
          "old_api": "std::move(instr)",
          "new_api": "set_distribution",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_distribution(distribution)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kRng, parameters);",
          "new_line_content": "  instr.set_distribution(distribution);",
          "content_same": false
        },
        {
          "line": 2899,
          "old_api": "element_type",
          "new_api": "set_element_type",
          "old_text": "shape.element_type()",
          "new_text": "output_shape.set_element_type(\n          primitive_util::UnsignedIntegralTypeForBitWidth(\n              primitive_util::BitWidth(shape.element_type())))",
          "old_line_content": "              primitive_util::BitWidth(shape.element_type())));",
          "new_line_content": "      output_shape.set_element_type(",
          "content_same": false
        },
        {
          "line": 2905,
          "old_api": "RngBitGeneratorInternal",
          "new_api": "element_type",
          "old_text": "RngBitGeneratorInternal(\n        ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape}),\n        algorithm, initial_state)",
          "new_text": "shape.element_type()",
          "old_line_content": "    return RngBitGeneratorInternal(",
          "new_line_content": "                             PrimitiveType_Name(shape.element_type()));",
          "content_same": false
        },
        {
          "line": 2917,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "full_result_shape.ToProto()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kRngBitGenerator,",
          "new_line_content": "  *instr.mutable_shape() = full_result_shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2943,
          "old_api": "AddCalledComputation",
          "new_api": "ToProto",
          "old_text": "AddCalledComputation(body, &instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  AddCalledComputation(body, &instr);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2945,
          "old_api": "std::move(instr)",
          "new_api": "AddCalledComputation",
          "old_text": "std::move(instr)",
          "new_text": "AddCalledComputation(body, &instr)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kWhile, {init});",
          "new_line_content": "  AddCalledComputation(body, &instr);",
          "content_same": false
        },
        {
          "line": 2971,
          "old_api": "mutable_gather_dimension_numbers",
          "new_api": "set_indices_are_sorted",
          "old_text": "instr.mutable_gather_dimension_numbers()",
          "new_text": "instr.set_indices_are_sorted(indices_are_sorted)",
          "old_line_content": "  *instr.mutable_gather_dimension_numbers() = dimension_numbers;",
          "new_line_content": "  instr.set_indices_are_sorted(indices_are_sorted);",
          "content_same": false
        },
        {
          "line": 2973,
          "old_api": "add_gather_slice_sizes",
          "new_api": "mutable_gather_dimension_numbers",
          "old_text": "instr.add_gather_slice_sizes(bound)",
          "new_text": "instr.mutable_gather_dimension_numbers()",
          "old_line_content": "    instr.add_gather_slice_sizes(bound);",
          "new_line_content": "  *instr.mutable_gather_dimension_numbers() = dimension_numbers;",
          "content_same": false
        },
        {
          "line": 2996,
          "old_api": "InvalidArgument",
          "new_api": "empty",
          "old_text": "InvalidArgument(\"Scatter inputs cannot be empty.\")",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (inputs.empty()) {\n      return InvalidArgument(\"Scatter inputs cannot be empty.\");\n    }\n    if (inputs.size() != updates.size()) {\n      return InvalidArgument(\n          \"Scatter should have same number of inputs and updates: %d vs %d.\",\n          inputs.size(), updates.size());\n    }\n    absl::InlinedVector<const Shape*, 3> operand_shapes;\n    operand_shapes.reserve(inputs.size() + 1 + updates.size());\n    for (const XlaOp& input : inputs) {\n      TF_ASSIGN_OR_RETURN(const Shape* input_shape, GetShapePtr(input));\n      operand_shapes.push_back(input_shape);\n    }\n    TF_ASSIGN_OR_RETURN(const Shape* scatter_indices_shape,\n                        GetShapePtr(scatter_indices));\n    operand_shapes.push_back(scatter_indices_shape);\n    for (const XlaOp& update : updates) {\n      TF_ASSIGN_OR_RETURN(const Shape* update_shape, GetShapePtr(update));\n      operand_shapes.push_back(update_shape);\n    }\n    TF_ASSIGN_OR_RETURN(const ProgramShape& to_apply_shape,\n                        update_computation.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferScatterShape(\n                            operand_shapes, to_apply_shape, dimension_numbers));\n    return ScatterInternal(shape, inputs, scatter_indices, updates,\n                           update_computation, dimension_numbers,\n                           indices_are_sorted, unique_indices);\n  })",
          "old_line_content": "      return InvalidArgument(\"Scatter inputs cannot be empty.\");",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2998,
          "old_api": "size",
          "new_api": "InvalidArgument",
          "old_text": "updates.size()",
          "new_text": "InvalidArgument(\"Scatter inputs cannot be empty.\")",
          "old_line_content": "    if (inputs.size() != updates.size()) {",
          "new_line_content": "      return InvalidArgument(\"Scatter inputs cannot be empty.\");",
          "content_same": false
        },
        {
          "line": 3036,
          "old_api": "ToProto",
          "new_api": "set_indices_are_sorted",
          "old_text": "shape.ToProto()",
          "new_text": "instr.set_indices_are_sorted(indices_are_sorted)",
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    instr.set_indices_are_sorted(indices_are_sorted);",
          "content_same": false
        },
        {
          "line": 3037,
          "old_api": "mutable_scatter_dimension_numbers",
          "new_api": "set_unique_indices",
          "old_text": "instr.mutable_scatter_dimension_numbers()",
          "new_text": "instr.set_unique_indices(unique_indices)",
          "old_line_content": "    *instr.mutable_scatter_dimension_numbers() = dimension_numbers;",
          "new_line_content": "    instr.set_unique_indices(unique_indices);",
          "content_same": false
        },
        {
          "line": 3039,
          "old_api": "AddCalledComputation",
          "new_api": "mutable_scatter_dimension_numbers",
          "old_text": "AddCalledComputation(update_computation, &instr)",
          "new_text": "instr.mutable_scatter_dimension_numbers()",
          "old_line_content": "    AddCalledComputation(update_computation, &instr);",
          "new_line_content": "    *instr.mutable_scatter_dimension_numbers() = dimension_numbers;",
          "content_same": false
        },
        {
          "line": 3041,
          "old_api": "size",
          "new_api": "AddCalledComputation",
          "old_text": "updates.size()",
          "new_text": "AddCalledComputation(update_computation, &instr)",
          "old_line_content": "    operands.reserve(inputs.size() + 1 + updates.size());",
          "new_line_content": "    AddCalledComputation(update_computation, &instr);",
          "content_same": false
        },
        {
          "line": 3043,
          "old_api": "push_back",
          "new_api": "size",
          "old_text": "operands.push_back(scatter_indices)",
          "new_text": "updates.size()",
          "old_line_content": "    operands.push_back(scatter_indices);",
          "new_line_content": "    operands.reserve(inputs.size() + 1 + updates.size());",
          "content_same": false
        },
        {
          "line": 3045,
          "old_api": "std::move(instr)",
          "new_api": "push_back",
          "old_text": "std::move(instr)",
          "new_text": "operands.push_back(scatter_indices)",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kScatter, operands);",
          "new_line_content": "    operands.push_back(scatter_indices);",
          "content_same": false
        },
        {
          "line": 3079,
          "old_api": "ShapeUtil::HumanString(*shape)",
          "new_api": "InvalidArgument",
          "old_text": "ShapeUtil::HumanString(*shape)",
          "new_text": "InvalidArgument(\n          \"Argument to indexed-Conditional is not a scalar of S32 type (%s).\",\n          ShapeUtil::HumanString(*shape))",
          "old_line_content": "          ShapeUtil::HumanString(*shape));",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 3081,
          "old_api": "ConditionalImpl",
          "new_api": "ShapeUtil::HumanString(*shape)",
          "old_text": "ConditionalImpl(branch_index, branch_computations, branch_operands)",
          "new_text": "ShapeUtil::HumanString(*shape)",
          "old_line_content": "    return ConditionalImpl(branch_index, branch_computations, branch_operands);",
          "new_line_content": "          ShapeUtil::HumanString(*shape));",
          "content_same": false
        },
        {
          "line": 3099,
          "old_api": "Unimplemented",
          "new_api": "IsTuple",
          "old_text": "Unimplemented(\"0 element tuple AllReduce is not supported\")",
          "new_text": "operand_shape->IsTuple()",
          "old_line_content": "        return Unimplemented(\"0 element tuple AllReduce is not supported\");",
          "new_line_content": "    if (operand_shape->IsTuple()) {",
          "content_same": false
        },
        {
          "line": 3101,
          "old_api": "tuple_shapes_size",
          "new_api": "Unimplemented",
          "old_text": "operand_shape->tuple_shapes_size()",
          "new_text": "Unimplemented(\"0 element tuple AllReduce is not supported\")",
          "old_line_content": "      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {",
          "new_line_content": "        return Unimplemented(\"0 element tuple AllReduce is not supported\");",
          "content_same": false
        },
        {
          "line": 3103,
          "old_api": "element_type",
          "new_api": "tuple_shapes_size",
          "old_text": "operand_shape->tuple_shapes(0).element_type()",
          "new_text": "operand_shape->tuple_shapes_size()",
          "old_line_content": "            operand_shape->tuple_shapes(0).element_type()) {",
          "new_line_content": "      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {",
          "content_same": false
        },
        {
          "line": 3104,
          "old_api": "Unimplemented",
          "new_api": "element_type",
          "old_text": "Unimplemented(\n              \"All the shapes of a tuple input of AllReduce must have the same \"\n              \"element type\")",
          "new_text": "operand_shape->tuple_shapes(i).element_type()",
          "old_line_content": "          return Unimplemented(",
          "new_line_content": "        if (operand_shape->tuple_shapes(i).element_type() !=",
          "content_same": false
        },
        {
          "line": 3121,
          "old_api": "ToString",
          "new_api": "LayoutUtil::HasLayout(*layout)",
          "old_text": "layout->ToString()",
          "new_text": "LayoutUtil::HasLayout(*layout)",
          "old_line_content": "                               layout->ToString());",
          "new_line_content": "      if (!LayoutUtil::HasLayout(*layout)) {",
          "content_same": false
        },
        {
          "line": 3123,
          "old_api": "ShapeUtil::Compatible(*layout, *operand_shape)",
          "new_api": "ToString",
          "old_text": "ShapeUtil::Compatible(*layout, *operand_shape)",
          "new_text": "layout->ToString()",
          "old_line_content": "      if (!ShapeUtil::Compatible(*layout, *operand_shape)) {",
          "new_line_content": "                               layout->ToString());",
          "content_same": false
        },
        {
          "line": 3129,
          "old_api": "set_constrain_layout",
          "new_api": "ToString",
          "old_text": "instr.set_constrain_layout(true)",
          "new_text": "operand_shape->ToString()",
          "old_line_content": "      instr.set_constrain_layout(true);",
          "new_line_content": "            layout->ToString(), operand_shape->ToString());",
          "content_same": false
        },
        {
          "line": 3132,
          "old_api": "tuple_shapes_size",
          "new_api": "IsTuple",
          "old_text": "layout->tuple_shapes_size()",
          "new_text": "inferred_shape.IsTuple()",
          "old_line_content": "        TF_RET_CHECK(layout->tuple_shapes_size() == 1);",
          "new_line_content": "      if (operand_shape->IsTuple() && !inferred_shape.IsTuple()) {",
          "content_same": false
        },
        {
          "line": 3164,
          "old_api": "Tuple",
          "new_api": "size",
          "old_text": "Tuple({all_reduce})",
          "new_text": "operand_shapes.size()",
          "old_line_content": "      return Tuple({all_reduce});",
          "new_line_content": "      TF_RET_CHECK(operand_shapes.size() == 1);",
          "content_same": false
        },
        {
          "line": 3186,
          "old_api": "Unimplemented",
          "new_api": "IsTuple",
          "old_text": "Unimplemented(\"0 element tuple AllGather is not supported\")",
          "new_text": "operand_shape->IsTuple()",
          "old_line_content": "        return Unimplemented(\"0 element tuple AllGather is not supported\");",
          "new_line_content": "    if (operand_shape->IsTuple()) {",
          "content_same": false
        },
        {
          "line": 3188,
          "old_api": "tuple_shapes_size",
          "new_api": "Unimplemented",
          "old_text": "operand_shape->tuple_shapes_size()",
          "new_text": "Unimplemented(\"0 element tuple AllGather is not supported\")",
          "old_line_content": "      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {",
          "new_line_content": "        return Unimplemented(\"0 element tuple AllGather is not supported\");",
          "content_same": false
        },
        {
          "line": 3190,
          "old_api": "GetTupleElement",
          "new_api": "tuple_shapes_size",
          "old_text": "GetTupleElement(operand, i)",
          "new_text": "operand_shape->tuple_shapes_size()",
          "old_line_content": "        operands.push_back(GetTupleElement(operand, i));",
          "new_line_content": "      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {",
          "content_same": false
        },
        {
          "line": 3204,
          "old_api": "ToProto",
          "new_api": "set_constrain_layout",
          "old_text": "inferred_shape.ToProto()",
          "new_text": "instr.set_constrain_layout(true)",
          "old_line_content": "    *instr.mutable_shape() = inferred_shape.ToProto();",
          "new_line_content": "      instr.set_constrain_layout(true);",
          "content_same": false
        },
        {
          "line": 3206,
          "old_api": "add_dimensions",
          "new_api": "ToProto",
          "old_text": "instr.add_dimensions(all_gather_dimension)",
          "new_text": "inferred_shape.ToProto()",
          "old_line_content": "    instr.add_dimensions(all_gather_dimension);",
          "new_line_content": "    *instr.mutable_shape() = inferred_shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3208,
          "old_api": "add_replica_groups",
          "new_api": "add_dimensions",
          "old_text": "instr.add_replica_groups()",
          "new_text": "instr.add_dimensions(all_gather_dimension)",
          "old_line_content": "      *instr.add_replica_groups() = group;",
          "new_line_content": "    instr.add_dimensions(all_gather_dimension);",
          "content_same": false
        },
        {
          "line": 3210,
          "old_api": "has_value",
          "new_api": "add_replica_groups",
          "old_text": "channel_id.has_value()",
          "new_text": "instr.add_replica_groups()",
          "old_line_content": "    if (channel_id.has_value()) {",
          "new_line_content": "      *instr.add_replica_groups() = group;",
          "content_same": false
        },
        {
          "line": 3213,
          "old_api": "has_value",
          "new_api": "handle",
          "old_text": "use_global_device_ids.has_value()",
          "new_text": "channel_id->handle()",
          "old_line_content": "    if (use_global_device_ids.has_value()) {",
          "new_line_content": "      instr.set_channel_id(channel_id->handle());",
          "content_same": false
        },
        {
          "line": 3239,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": "size",
          "old_text": "TF_ASSIGN_OR_RETURN(branch_operand_shapes[j],\n                          GetShape(branch_operands[j]))",
          "new_text": "branch_computations.size()",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(branch_operand_shapes[j],",
          "new_line_content": "        branch_computations.size());",
          "content_same": false
        },
        {
          "line": 3240,
          "old_api": "GetShape",
          "new_api": "size",
          "old_text": "GetShape(branch_operands[j])",
          "new_text": "branch_operands.size()",
          "old_line_content": "                          GetShape(branch_operands[j]));",
          "new_line_content": "    for (int j = 0, end = branch_operands.size(); j < end; ++j) {",
          "content_same": false
        },
        {
          "line": 3241,
          "old_api": "GetProgramShape",
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": "TF_ASSIGN_OR_RETURN(branch_computation_shapes[j],\n                          branch_computations[j]->GetProgramShape())",
          "new_text": "TF_ASSIGN_OR_RETURN(branch_operand_shapes[j],\n                          GetShape(branch_operands[j]))",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(branch_computation_shapes[j],",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(branch_operand_shapes[j],",
          "content_same": false
        },
        {
          "line": 3242,
          "old_api": "GetProgramShape",
          "new_api": "GetShape",
          "old_text": "branch_computations[j]->GetProgramShape()",
          "new_text": "GetShape(branch_operands[j])",
          "old_line_content": "                          branch_computations[j]->GetProgramShape());",
          "new_line_content": "                          GetShape(branch_operands[j]));",
          "content_same": false
        },
        {
          "line": 3258,
          "old_api": "std::move(instr)",
          "new_api": "emplace_back",
          "old_text": "std::move(instr)",
          "new_text": "operands.emplace_back(branch_operand)",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kConditional,",
          "new_line_content": "      operands.emplace_back(branch_operand);",
          "content_same": false
        },
        {
          "line": 3270,
          "old_api": "OkStatus",
          "new_api": "name",
          "old_text": "OkStatus()",
          "new_text": "name()",
          "old_line_content": "  return OkStatus();",
          "new_line_content": "        op.handle(), op.builder()->name(), name());",
          "content_same": false
        },
        {
          "line": 3315,
          "old_api": "ToProto",
          "new_api": "mutable_shape",
          "old_text": "shape.ToProto()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n\n    for (int64_t dim : dimensions_to_reduce) {\n      instr.add_dimensions(dim);\n    }\n\n    AddCalledComputation(computation, &instr);\n    return AddInstruction(std::move(instr), HloOpcode::kReduce, all_operands);\n  })",
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3330,
          "old_api": "rank",
          "new_api": "begin",
          "old_text": "operand_shape->rank()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<int64_t> all_dimnos(operand_shape->rank());\n    std::iota(all_dimnos.begin(), all_dimnos.end(), 0);\n    return Reduce(operand, init_value, computation, all_dimnos);\n  })",
          "old_line_content": "    std::vector<int64_t> all_dimnos(operand_shape->rank());",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3332,
          "old_api": "Reduce",
          "new_api": "rank",
          "old_text": "Reduce(operand, init_value, computation, all_dimnos)",
          "new_text": "operand_shape->rank()",
          "old_line_content": "    return Reduce(operand, init_value, computation, all_dimnos);",
          "new_line_content": "    std::vector<int64_t> all_dimnos(operand_shape->rank());",
          "content_same": false
        },
        {
          "line": 3357,
          "old_api": "dimensions",
          "new_api": "GetShapePtr",
          "old_text": "operand_shape->dimensions()",
          "new_text": "GetShapePtr(operand)",
          "old_line_content": "          operand_shape->dimensions(), window_dimensions, window_strides));",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(operand_shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 3359,
          "old_api": "CHECK",
          "new_api": "dimensions",
          "old_text": "CHECK(operand_shape != nullptr)",
          "new_text": "operand_shape->dimensions()",
          "old_line_content": "    CHECK(operand_shape != nullptr);",
          "new_line_content": "          operand_shape->dimensions(), window_dimensions, window_strides));",
          "content_same": false
        },
        {
          "line": 3361,
          "old_api": "dimensions",
          "new_api": "CHECK",
          "old_text": "operand_shape->dimensions()",
          "new_text": "CHECK(operand_shape != nullptr)",
          "old_line_content": "        MakePadding(operand_shape->dimensions(), window_dimensions,",
          "new_line_content": "    CHECK(operand_shape != nullptr);",
          "content_same": false
        },
        {
          "line": 3371,
          "old_api": "dimensions",
          "new_api": "rank",
          "old_text": "window.dimensions(i)",
          "new_text": "operand_shape->rank()",
          "old_line_content": "          !window_util::IsTrivialWindowDimension(window.dimensions(i)) &&",
          "new_line_content": "    for (int64_t i = 0; i < operand_shape->rank(); ++i) {",
          "content_same": false
        },
        {
          "line": 3387,
          "old_api": "end",
          "new_api": "set_custom_call_target",
          "old_text": "operands.end()",
          "new_text": "instr.set_custom_call_target(\"DynamicReduceWindowSamePadding\")",
          "old_line_content": "      args.insert(args.end(), operands.begin(), operands.end());",
          "new_line_content": "      instr.set_custom_call_target(\"DynamicReduceWindowSamePadding\");",
          "content_same": false
        },
        {
          "line": 3389,
          "old_api": "std::move(instr)",
          "new_api": "end",
          "old_text": "std::move(instr)",
          "new_text": "operands.end()",
          "old_line_content": "      return AddInstruction(std::move(instr), HloOpcode::kCustomCall, args);",
          "new_line_content": "      args.insert(args.end(), operands.begin(), operands.end());",
          "content_same": false
        },
        {
          "line": 3391,
          "old_api": "ReduceWindowWithGeneralPadding",
          "new_api": "std::move(instr)",
          "old_text": "ReduceWindowWithGeneralPadding(\n        operands, init_values, computation, window_dimensions, window_strides,\n        /*base_dilations=*/{}, /*window_dilations=*/{}, padding_values)",
          "new_text": "std::move(instr)",
          "old_line_content": "    return ReduceWindowWithGeneralPadding(",
          "new_line_content": "      return AddInstruction(std::move(instr), HloOpcode::kCustomCall, args);",
          "content_same": false
        },
        {
          "line": 3438,
          "old_api": "std::move(instr)",
          "new_api": "end",
          "old_text": "std::move(instr)",
          "new_text": "operands.end()",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kReduceWindow, args);",
          "new_line_content": "    args.insert(args.end(), operands.begin(), operands.end());",
          "content_same": false
        },
        {
          "line": 3473,
          "old_api": "AddCalledComputation",
          "new_api": "ToProto",
          "old_text": "AddCalledComputation(computation, &instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  AddCalledComputation(computation, &instr);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3484,
          "old_api": "AddCalledComputation",
          "new_api": "std::move(window)",
          "old_text": "AddCalledComputation(computation, &instr)",
          "new_text": "std::move(window)",
          "old_line_content": "  AddCalledComputation(computation, &instr);",
          "new_line_content": "  *instr.mutable_window() = std::move(window);",
          "content_same": false
        },
        {
          "line": 3503,
          "old_api": "set_epsilon",
          "new_api": "ToProto",
          "old_text": "instr.set_epsilon(epsilon)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    instr.set_epsilon(epsilon);",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3506,
          "old_api": "std::move(instr)",
          "new_api": "set_feature_index",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_feature_index(feature_index)",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kBatchNormTraining,",
          "new_line_content": "    instr.set_feature_index(feature_index);",
          "content_same": false
        },
        {
          "line": 3528,
          "old_api": "set_epsilon",
          "new_api": "ToProto",
          "old_text": "instr.set_epsilon(epsilon)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    instr.set_epsilon(epsilon);",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3531,
          "old_api": "std::move(instr)",
          "new_api": "set_feature_index",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_feature_index(feature_index)",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kBatchNormInference,",
          "new_line_content": "    instr.set_feature_index(feature_index);",
          "content_same": false
        },
        {
          "line": 3554,
          "old_api": "set_epsilon",
          "new_api": "ToProto",
          "old_text": "instr.set_epsilon(epsilon)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    instr.set_epsilon(epsilon);",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3557,
          "old_api": "std::move(instr)",
          "new_api": "set_feature_index",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_feature_index(feature_index)",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kBatchNormGrad,",
          "new_line_content": "    instr.set_feature_index(feature_index);",
          "content_same": false
        },
        {
          "line": 3580,
          "old_api": "Unimplemented",
          "new_api": "IsTuple",
          "old_text": "Unimplemented(\n            \"0 element tuple CrossReplicaSum is not supported\")",
          "new_text": "shape->IsTuple()",
          "old_line_content": "        return Unimplemented(",
          "new_line_content": "    if (shape->IsTuple()) {",
          "content_same": false
        },
        {
          "line": 3590,
          "old_api": "Parameter",
          "new_api": "element_type",
          "old_text": "b->Parameter(/*parameter_number=*/0, scalar_shape, \"x\")",
          "new_text": "element_shape->element_type()",
          "old_line_content": "    auto x = b->Parameter(/*parameter_number=*/0, scalar_shape, \"x\");",
          "new_line_content": "        ShapeUtil::MakeShape(element_shape->element_type(), {});",
          "content_same": false
        },
        {
          "line": 3591,
          "old_api": "Parameter",
          "new_api": "CreateSubBuilder",
          "old_text": "b->Parameter(/*parameter_number=*/1, scalar_shape, \"y\")",
          "new_text": "CreateSubBuilder(\"sum\")",
          "old_line_content": "    auto y = b->Parameter(/*parameter_number=*/1, scalar_shape, \"y\");",
          "new_line_content": "    auto b = CreateSubBuilder(\"sum\");",
          "content_same": false
        },
        {
          "line": 3592,
          "old_api": "element_type",
          "new_api": "Parameter",
          "old_text": "scalar_shape.element_type()",
          "new_text": "b->Parameter(/*parameter_number=*/0, scalar_shape, \"x\")",
          "old_line_content": "    if (scalar_shape.element_type() == PRED) {",
          "new_line_content": "    auto x = b->Parameter(/*parameter_number=*/0, scalar_shape, \"x\");",
          "content_same": false
        },
        {
          "line": 3593,
          "old_api": "Or",
          "new_api": "Parameter",
          "old_text": "Or(x, y)",
          "new_text": "b->Parameter(/*parameter_number=*/1, scalar_shape, \"y\")",
          "old_line_content": "      Or(x, y);",
          "new_line_content": "    auto y = b->Parameter(/*parameter_number=*/1, scalar_shape, \"y\");",
          "content_same": false
        },
        {
          "line": 3595,
          "old_api": "Add",
          "new_api": "Or",
          "old_text": "Add(x, y)",
          "new_text": "Or(x, y)",
          "old_line_content": "      Add(x, y);",
          "new_line_content": "      Or(x, y);",
          "content_same": false
        },
        {
          "line": 3626,
          "old_api": "Unimplemented",
          "new_api": "IsTuple",
          "old_text": "Unimplemented(\"0 element tuple ReduceScatter is not supported\")",
          "new_text": "operand_shape->IsTuple()",
          "old_line_content": "        return Unimplemented(\"0 element tuple ReduceScatter is not supported\");",
          "new_line_content": "    if (operand_shape->IsTuple()) {",
          "content_same": false
        },
        {
          "line": 3628,
          "old_api": "tuple_shapes_size",
          "new_api": "Unimplemented",
          "old_text": "operand_shape->tuple_shapes_size()",
          "new_text": "Unimplemented(\"0 element tuple ReduceScatter is not supported\")",
          "old_line_content": "      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {",
          "new_line_content": "        return Unimplemented(\"0 element tuple ReduceScatter is not supported\");",
          "content_same": false
        },
        {
          "line": 3630,
          "old_api": "element_type",
          "new_api": "tuple_shapes_size",
          "old_text": "operand_shape->tuple_shapes(0).element_type()",
          "new_text": "operand_shape->tuple_shapes_size()",
          "old_line_content": "            operand_shape->tuple_shapes(0).element_type()) {",
          "new_line_content": "      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {",
          "content_same": false
        },
        {
          "line": 3631,
          "old_api": "Unimplemented",
          "new_api": "element_type",
          "old_text": "Unimplemented(\n              \"All the shapes of a tuple input of ReduceScatter must have \"\n              \"the same element type\")",
          "new_text": "operand_shape->tuple_shapes(i).element_type()",
          "old_line_content": "          return Unimplemented(",
          "new_line_content": "        if (operand_shape->tuple_shapes(i).element_type() !=",
          "content_same": false
        },
        {
          "line": 3650,
          "old_api": "ToProto",
          "new_api": "set_constrain_layout",
          "old_text": "inferred_shape.ToProto()",
          "new_text": "instr.set_constrain_layout(true)",
          "old_line_content": "    *instr.mutable_shape() = inferred_shape.ToProto();",
          "new_line_content": "      instr.set_constrain_layout(true);",
          "content_same": false
        },
        {
          "line": 3652,
          "old_api": "AddCalledComputation",
          "new_api": "ToProto",
          "old_text": "AddCalledComputation(computation, &instr)",
          "new_text": "inferred_shape.ToProto()",
          "old_line_content": "    AddCalledComputation(computation, &instr);",
          "new_line_content": "    *instr.mutable_shape() = inferred_shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3654,
          "old_api": "add_dimensions",
          "new_api": "AddCalledComputation",
          "old_text": "instr.add_dimensions(scatter_dimension)",
          "new_text": "AddCalledComputation(computation, &instr)",
          "old_line_content": "    instr.add_dimensions(scatter_dimension);",
          "new_line_content": "    AddCalledComputation(computation, &instr);",
          "content_same": false
        },
        {
          "line": 3656,
          "old_api": "add_replica_groups",
          "new_api": "add_dimensions",
          "old_text": "instr.add_replica_groups()",
          "new_text": "instr.add_dimensions(scatter_dimension)",
          "old_line_content": "      *instr.add_replica_groups() = group;",
          "new_line_content": "    instr.add_dimensions(scatter_dimension);",
          "content_same": false
        },
        {
          "line": 3658,
          "old_api": "has_value",
          "new_api": "add_replica_groups",
          "old_text": "channel_id.has_value()",
          "new_text": "instr.add_replica_groups()",
          "old_line_content": "    if (channel_id.has_value()) {",
          "new_line_content": "      *instr.add_replica_groups() = group;",
          "content_same": false
        },
        {
          "line": 3661,
          "old_api": "has_value",
          "new_api": "handle",
          "old_text": "use_global_device_ids.has_value()",
          "new_text": "channel_id->handle()",
          "old_line_content": "    if (use_global_device_ids.has_value()) {",
          "new_line_content": "      instr.set_channel_id(channel_id->handle());",
          "content_same": false
        },
        {
          "line": 3700,
          "old_api": "add_replica_groups",
          "new_api": "ToProto",
          "old_text": "instr.add_replica_groups()",
          "new_text": "operand_shape->ToProto()",
          "old_line_content": "      auto* group = instr.add_replica_groups();",
          "new_line_content": "    *instr.mutable_shape() = operand_shape->ToProto();",
          "content_same": false
        },
        {
          "line": 3702,
          "old_api": "add_replica_ids",
          "new_api": "add_replica_groups",
          "old_text": "group->add_replica_ids(i)",
          "new_text": "instr.add_replica_groups()",
          "old_line_content": "        group->add_replica_ids(i);",
          "new_line_content": "      auto* group = instr.add_replica_groups();",
          "content_same": false
        },
        {
          "line": 3711,
          "old_api": "handle",
          "new_api": "add_dimensions",
          "old_text": "channel_id->handle()",
          "new_text": "instr.add_dimensions(split_dimension)",
          "old_line_content": "      instr.set_channel_id(channel_id->handle());",
          "new_line_content": "    instr.add_dimensions(split_dimension);",
          "content_same": false
        },
        {
          "line": 3722,
          "old_api": "dimensions",
          "new_api": "rank",
          "old_text": "operand_shape->dimensions(i)",
          "new_text": "operand_shape->rank()",
          "old_line_content": "        sizes.push_back(operand_shape->dimensions(i));",
          "new_line_content": "    for (int64_t i = 0; i < operand_shape->rank(); ++i) {",
          "content_same": false
        },
        {
          "line": 3728,
          "old_api": "Reshape",
          "new_api": "dimensions",
          "old_text": "Reshape(all_to_all, sizes)",
          "new_text": "operand_shape->dimensions(i)",
          "old_line_content": "    all_to_all = Reshape(all_to_all, sizes);",
          "new_line_content": "      sizes.push_back(operand_shape->dimensions(i) / split_count);",
          "content_same": false
        },
        {
          "line": 3740,
          "old_api": "Transpose",
          "new_api": "push_back",
          "old_text": "Transpose(all_to_all, permutation)",
          "new_text": "permutation.push_back(dim_after_reshape)",
          "old_line_content": "    all_to_all = Transpose(all_to_all, permutation);",
          "new_line_content": "      permutation.push_back(dim_after_reshape);",
          "content_same": false
        },
        {
          "line": 3764,
          "old_api": "size",
          "new_api": "ShapeUtil::TupleElementCount(shape)",
          "old_text": "layout->minor_to_major().size()",
          "new_text": "ShapeUtil::TupleElementCount(shape)",
          "old_line_content": "            layout->minor_to_major().size();",
          "new_line_content": "      for (int64_t i = 0; i < ShapeUtil::TupleElementCount(shape); ++i) {",
          "content_same": false
        },
        {
          "line": 3766,
          "old_api": "tuple_shapes",
          "new_api": "size",
          "old_text": "InvalidArgument(\n              \"Provided layout must be compatible with the operands' shape. \"\n              \"The layout is %s, but operand %d has shape %s.\",\n              layout->ToString(), i, shape.tuple_shapes(i).ToString())",
          "new_text": "layout->minor_to_major().size()",
          "old_line_content": "          return InvalidArgument(",
          "new_line_content": "            layout->minor_to_major().size();",
          "content_same": false
        },
        {
          "line": 3771,
          "old_api": "mutable_tuple_shapes",
          "new_api": "tuple_shapes",
          "old_text": "shape.mutable_tuple_shapes(i)->mutable_layout()",
          "new_text": "shape.tuple_shapes(i).ToString()",
          "old_line_content": "        *(shape.mutable_tuple_shapes(i)->mutable_layout()) = *layout;",
          "new_line_content": "              layout->ToString(), i, shape.tuple_shapes(i).ToString());",
          "content_same": false
        },
        {
          "line": 3773,
          "old_api": "set_constrain_layout",
          "new_api": "mutable_tuple_shapes",
          "old_text": "instr.set_constrain_layout(true)",
          "new_text": "shape.mutable_tuple_shapes(i)->mutable_layout()",
          "old_line_content": "      instr.set_constrain_layout(true);",
          "new_line_content": "        *(shape.mutable_tuple_shapes(i)->mutable_layout()) = *layout;",
          "content_same": false
        },
        {
          "line": 3775,
          "old_api": "ToProto",
          "new_api": "set_constrain_layout",
          "old_text": "shape.ToProto()",
          "new_text": "instr.set_constrain_layout(true)",
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "      instr.set_constrain_layout(true);",
          "content_same": false
        },
        {
          "line": 3780,
          "old_api": "has_value",
          "new_api": "add_replica_groups",
          "old_text": "channel_id.has_value()",
          "new_text": "instr.add_replica_groups()",
          "old_line_content": "    if (channel_id.has_value()) {",
          "new_line_content": "      *instr.add_replica_groups() = group;",
          "content_same": false
        },
        {
          "line": 3812,
          "old_api": "dimensions",
          "new_api": "reserve",
          "old_text": "operand_shape->dimensions(split_dimension)",
          "new_text": "slices.reserve(split_count)",
          "old_line_content": "        operand_shape->dimensions(split_dimension) / split_count;",
          "new_line_content": "    slices.reserve(split_count);",
          "content_same": false
        },
        {
          "line": 3814,
          "old_api": "SliceInDim",
          "new_api": "dimensions",
          "old_text": "SliceInDim(operand, /*start_index=*/i * block_size,\n                                  /*limit_index=*/(i + 1) * block_size,\n                                  /*stride=*/1, /*dimno=*/split_dimension)",
          "new_text": "operand_shape->dimensions(split_dimension)",
          "old_line_content": "      slices.push_back(SliceInDim(operand, /*start_index=*/i * block_size,",
          "new_line_content": "        operand_shape->dimensions(split_dimension) / split_count;",
          "content_same": false
        },
        {
          "line": 3827,
          "old_api": "GetTupleElement",
          "new_api": "reserve",
          "old_text": "this->GetTupleElement(alltoall, i)",
          "new_text": "received.reserve(split_count)",
          "old_line_content": "      received.push_back(this->GetTupleElement(alltoall, i));",
          "new_line_content": "    received.reserve(split_count);",
          "content_same": false
        },
        {
          "line": 3829,
          "old_api": "ConcatInDim",
          "new_api": "GetTupleElement",
          "old_text": "this->ConcatInDim(received, concat_dimension)",
          "new_text": "this->GetTupleElement(alltoall, i)",
          "old_line_content": "    return this->ConcatInDim(received, concat_dimension);",
          "new_line_content": "      received.push_back(this->GetTupleElement(alltoall, i));",
          "content_same": false
        },
        {
          "line": 3850,
          "old_api": "add_replica_groups",
          "new_api": "ToProto",
          "old_text": "instr.add_replica_groups()",
          "new_text": "shape.ToProto()",
          "old_line_content": "      *instr.add_replica_groups() = group;",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3852,
          "old_api": "has_value",
          "new_api": "add_replica_groups",
          "old_text": "channel_id.has_value()",
          "new_text": "instr.add_replica_groups()",
          "old_line_content": "    if (channel_id.has_value()) {",
          "new_line_content": "      *instr.add_replica_groups() = group;",
          "content_same": false
        },
        {
          "line": 3884,
          "old_api": "set_target",
          "new_api": "add_source_target_pairs",
          "old_text": "proto_pair->set_target(pair.second)",
          "new_text": "instr.add_source_target_pairs()",
          "old_line_content": "      proto_pair->set_target(pair.second);",
          "new_line_content": "      auto* proto_pair = instr.add_source_target_pairs();",
          "content_same": false
        },
        {
          "line": 3886,
          "old_api": "has_value",
          "new_api": "set_target",
          "old_text": "channel_id.has_value()",
          "new_text": "proto_pair->set_target(pair.second)",
          "old_line_content": "    if (channel_id.has_value()) {",
          "new_line_content": "      proto_pair->set_target(pair.second);",
          "content_same": false
        },
        {
          "line": 3900,
          "old_api": "ToProto",
          "new_api": "mutable_shape",
          "old_text": "ShapeUtil::MakeShape(U32, {}).ToProto()",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    *instr.mutable_shape() = ShapeUtil::MakeShape(U32, {}).ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kReplicaId, {});\n  })",
          "old_line_content": "    *instr.mutable_shape() = ShapeUtil::MakeShape(U32, {}).ToProto();",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3926,
          "old_api": "dimensions",
          "new_api": "rank",
          "old_text": "window.dimensions(i)",
          "new_text": "operand_shape->rank()",
          "old_line_content": "          !window_util::IsTrivialWindowDimension(window.dimensions(i)) &&",
          "new_line_content": "    for (int64_t i = 0; i < operand_shape->rank(); ++i) {",
          "content_same": false
        },
        {
          "line": 3975,
          "old_api": "AddCalledComputation",
          "new_api": "ToProto",
          "old_text": "AddCalledComputation(select, &instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  AddCalledComputation(select, &instr);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4016,
          "old_api": "set_mantissa_bits",
          "new_api": "ToProto",
          "old_text": "instr.set_mantissa_bits(mantissa_bits)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  instr.set_mantissa_bits(mantissa_bits);",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4017,
          "old_api": "std::move(instr)",
          "new_api": "set_exponent_bits",
          "old_text": "std::move(instr)",
          "new_text": "instr.set_exponent_bits(exponent_bits)",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kReducePrecision,",
          "new_line_content": "  instr.set_exponent_bits(exponent_bits);",
          "content_same": false
        },
        {
          "line": 4040,
          "old_api": "InvalidArgument",
          "new_api": "type",
          "old_text": "InvalidArgument(\"Send must use a device-to-device channel\")",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (handle.type() != ChannelHandle::DEVICE_TO_DEVICE) {\n      return InvalidArgument(\"Send must use a device-to-device channel\");\n    }\n\n    XlaOp send_op = internal::XlaBuilderFriend::BuildSend(this, operand, token,\n                                                          handle, false);\n    return internal::XlaBuilderFriend::BuildSendDone(this, send_op, handle,\n                                                     false);\n  })",
          "old_line_content": "      return InvalidArgument(\"Send must use a device-to-device channel\");",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4045,
          "old_api": "internal::XlaBuilderFriend::BuildSendDone(this, send_op, handle,\n                                                     false)",
          "new_api": "internal::XlaBuilderFriend::BuildSend(this, operand, token,\n                                                          handle, false)",
          "old_text": "internal::XlaBuilderFriend::BuildSendDone(this, send_op, handle,\n                                                     false)",
          "new_text": "internal::XlaBuilderFriend::BuildSend(this, operand, token,\n                                                          handle, false)",
          "old_line_content": "    return internal::XlaBuilderFriend::BuildSendDone(this, send_op, handle,",
          "new_line_content": "    XlaOp send_op = internal::XlaBuilderFriend::BuildSend(this, operand, token,",
          "content_same": false
        },
        {
          "line": 4070,
          "old_api": "std::move(recv_data)",
          "new_api": "ToProto",
          "old_text": "std::move(recv_data)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    return AddInstruction(std::move(recv_data), HloOpcode::kGetTupleElement,",
          "new_line_content": "    *recv_data.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4079,
          "old_api": "InvalidArgument",
          "new_api": "type",
          "old_text": "InvalidArgument(\"Recv must use a device-to-device channel\")",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (handle.type() != ChannelHandle::DEVICE_TO_DEVICE) {\n      return InvalidArgument(\"Recv must use a device-to-device channel\");\n    }\n\n    XlaOp recv_op = internal::XlaBuilderFriend::BuildRecv(this, token, shape,\n                                                          handle, false);\n    return internal::XlaBuilderFriend::BuildRecvDone(this, recv_op, shape,\n                                                     handle, false);\n  })",
          "old_line_content": "      return InvalidArgument(\"Recv must use a device-to-device channel\");",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4084,
          "old_api": "internal::XlaBuilderFriend::BuildRecvDone(this, recv_op, shape,\n                                                     handle, false)",
          "new_api": "internal::XlaBuilderFriend::BuildRecv(this, token, shape,\n                                                          handle, false)",
          "old_text": "internal::XlaBuilderFriend::BuildRecvDone(this, recv_op, shape,\n                                                     handle, false)",
          "new_text": "internal::XlaBuilderFriend::BuildRecv(this, token, shape,\n                                                          handle, false)",
          "old_line_content": "    return internal::XlaBuilderFriend::BuildRecvDone(this, recv_op, shape,",
          "new_line_content": "    XlaOp recv_op = internal::XlaBuilderFriend::BuildRecv(this, token, shape,",
          "content_same": false
        },
        {
          "line": 4094,
          "old_api": "InvalidArgument",
          "new_api": "type",
          "old_text": "InvalidArgument(\"Shape passed to SendToHost must have a layout\")",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!LayoutUtil::HasLayout(shape_with_layout)) {\n      return InvalidArgument(\"Shape passed to SendToHost must have a layout\");\n    }\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {\n      return InvalidArgument(\n          \"SendToHost shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape));\n    }\n    // TODO(b/111544877): Support tuple shapes.\n    if (!operand_shape->IsArray()) {\n      return InvalidArgument(\"SendToHost only supports array shapes, shape: %s\",\n                             ShapeUtil::HumanString(*operand_shape));\n    }\n\n    if (handle.type() != ChannelHandle::DEVICE_TO_HOST) {\n      return InvalidArgument(\"SendToHost must use a device-to-host channel\");\n    }\n\n    // Send instruction produces a tuple of {aliased operand, U32 context,\n    // token}.\n    HloInstructionProto send_instr;\n    *send_instr.mutable_shape() =\n        ShapeUtil::MakeTupleShape({shape_with_layout,\n                                   ShapeUtil::MakeShape(U32, {}),\n                                   ShapeUtil::MakeTokenShape()})\n            .ToProto();\n    send_instr.set_channel_id(handle.handle());\n    send_instr.set_is_host_transfer(true);\n    TF_ASSIGN_OR_RETURN(XlaOp send,\n                        AddInstruction(std::move(send_instr), HloOpcode::kSend,\n                                       {operand, token}));\n\n    HloInstructionProto send_done_instr;\n    *send_done_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    send_done_instr.set_channel_id(handle.handle());\n    send_done_instr.set_is_host_transfer(true);\n    TF_ASSIGN_OR_RETURN(XlaOp send_done,\n                        AddInstruction(std::move(send_done_instr),\n                                       HloOpcode::kSendDone, {send}));\n    return send_done;\n  })",
          "old_line_content": "      return InvalidArgument(\"Shape passed to SendToHost must have a layout\");",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4100,
          "old_api": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "new_api": "InvalidArgument",
          "old_text": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "new_text": "InvalidArgument(\n          \"SendToHost shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape))",
          "old_line_content": "          ShapeUtil::HumanStringWithLayout(shape_with_layout),",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4106,
          "old_api": "ShapeUtil::HumanString(*operand_shape)",
          "new_api": "IsArray",
          "old_text": "ShapeUtil::HumanString(*operand_shape)",
          "new_text": "operand_shape->IsArray()",
          "old_line_content": "                             ShapeUtil::HumanString(*operand_shape));",
          "new_line_content": "    if (!operand_shape->IsArray()) {",
          "content_same": false
        },
        {
          "line": 4118,
          "old_api": "ShapeUtil::MakeShape(U32, {})",
          "new_api": "mutable_shape",
          "old_text": "ShapeUtil::MakeShape(U32, {})",
          "new_text": "send_instr.mutable_shape()",
          "old_line_content": "                                   ShapeUtil::MakeShape(U32, {}),",
          "new_line_content": "    *send_instr.mutable_shape() =",
          "content_same": false
        },
        {
          "line": 4119,
          "old_api": "ShapeUtil::MakeTokenShape()",
          "new_api": "ToProto",
          "old_text": "ShapeUtil::MakeTokenShape()",
          "new_text": "ShapeUtil::MakeTupleShape({shape_with_layout,\n                                   ShapeUtil::MakeShape(U32, {}),\n                                   ShapeUtil::MakeTokenShape()})\n            .ToProto()",
          "old_line_content": "                                   ShapeUtil::MakeTokenShape()})",
          "new_line_content": "        ShapeUtil::MakeTupleShape({shape_with_layout,",
          "content_same": false
        },
        {
          "line": 4121,
          "old_api": "handle",
          "new_api": "ShapeUtil::MakeTokenShape()",
          "old_text": "handle.handle()",
          "new_text": "ShapeUtil::MakeTokenShape()",
          "old_line_content": "    send_instr.set_channel_id(handle.handle());",
          "new_line_content": "                                   ShapeUtil::MakeTokenShape()})",
          "content_same": false
        },
        {
          "line": 4130,
          "old_api": "set_is_host_transfer",
          "new_api": "ToProto",
          "old_text": "send_done_instr.set_is_host_transfer(true)",
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "    send_done_instr.set_is_host_transfer(true);",
          "new_line_content": "    *send_done_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 4142,
          "old_api": "InvalidArgument",
          "new_api": "IsArray",
          "old_text": "InvalidArgument(\"Shape passed to RecvFromHost must have a layout\")",
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!LayoutUtil::HasLayout(shape)) {\n      return InvalidArgument(\"Shape passed to RecvFromHost must have a layout\");\n    }\n\n    // TODO(b/111544877): Support tuple shapes.\n    if (!shape.IsArray()) {\n      return InvalidArgument(\n          \"RecvFromHost only supports array shapes, shape: %s\",\n          ShapeUtil::HumanString(shape));\n    }\n\n    if (handle.type() != ChannelHandle::HOST_TO_DEVICE) {\n      return InvalidArgument(\"RecvFromHost must use a host-to-device channel\");\n    }\n\n    // Recv instruction produces a tuple of {receive buffer, U32 context,\n    // token}.\n    HloInstructionProto recv_instr;\n    *recv_instr.mutable_shape() =\n        ShapeUtil::MakeTupleShape(\n            {shape, ShapeUtil::MakeShape(U32, {}), ShapeUtil::MakeTokenShape()})\n            .ToProto();\n    recv_instr.set_channel_id(handle.handle());\n    recv_instr.set_is_host_transfer(true);\n    TF_ASSIGN_OR_RETURN(XlaOp recv, AddInstruction(std::move(recv_instr),\n                                                   HloOpcode::kRecv, {token}));\n\n    HloInstructionProto recv_done_instr;\n    *recv_done_instr.mutable_shape() =\n        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()})\n            .ToProto();\n    recv_done_instr.set_channel_id(handle.handle());\n    recv_done_instr.set_is_host_transfer(true);\n    return AddInstruction(std::move(recv_done_instr), HloOpcode::kRecvDone,\n                          {recv});\n  })",
          "old_line_content": "      return InvalidArgument(\"Shape passed to RecvFromHost must have a layout\");",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4149,
          "old_api": "ShapeUtil::HumanString(shape)",
          "new_api": "InvalidArgument",
          "old_text": "ShapeUtil::HumanString(shape)",
          "new_text": "InvalidArgument(\n          \"RecvFromHost only supports array shapes, shape: %s\",\n          ShapeUtil::HumanString(shape))",
          "old_line_content": "          ShapeUtil::HumanString(shape));",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4161,
          "old_api": "ShapeUtil::MakeTokenShape()",
          "new_api": "mutable_shape",
          "old_text": "ShapeUtil::MakeTokenShape()",
          "new_text": "recv_instr.mutable_shape()",
          "old_line_content": "            {shape, ShapeUtil::MakeShape(U32, {}), ShapeUtil::MakeTokenShape()})",
          "new_line_content": "    *recv_instr.mutable_shape() =",
          "content_same": false
        },
        {
          "line": 4163,
          "old_api": "handle",
          "new_api": "ShapeUtil::MakeTokenShape()",
          "old_text": "handle.handle()",
          "new_text": "ShapeUtil::MakeTokenShape()",
          "old_line_content": "    recv_instr.set_channel_id(handle.handle());",
          "new_line_content": "            {shape, ShapeUtil::MakeShape(U32, {}), ShapeUtil::MakeTokenShape()})",
          "content_same": false
        },
        {
          "line": 4172,
          "old_api": "handle",
          "new_api": "ShapeUtil::MakeTokenShape()",
          "old_text": "handle.handle()",
          "new_text": "ShapeUtil::MakeTokenShape()",
          "old_line_content": "    recv_done_instr.set_channel_id(handle.handle());",
          "new_line_content": "        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()})",
          "content_same": false
        },
        {
          "line": 4174,
          "old_api": "std::move(recv_done_instr)",
          "new_api": "handle",
          "old_text": "std::move(recv_done_instr)",
          "new_text": "handle.handle()",
          "old_line_content": "    return AddInstruction(std::move(recv_done_instr), HloOpcode::kRecvDone,",
          "new_line_content": "    recv_done_instr.set_channel_id(handle.handle());",
          "content_same": false
        },
        {
          "line": 4190,
          "old_api": "ToProto",
          "new_api": "dimensions",
          "old_text": "shape.ToProto()",
          "new_text": "operand_shape->dimensions(dimension)",
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "      return ConstantR0<int32_t>(this, operand_shape->dimensions(dimension));",
          "content_same": false
        },
        {
          "line": 4192,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kGetDimensionSize,",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4239,
          "old_api": "std::move(instr)",
          "new_api": "ToProto",
          "old_text": "std::move(instr)",
          "new_text": "shape.ToProto()",
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kSetDimensionSize,",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4261,
          "old_api": "value",
          "new_api": "LookUpInstruction",
          "old_text": "op_status.value()->name()",
          "new_text": "LookUpInstruction(root_op)",
          "old_line_content": "        op_status.ok() ? op_status.value()->name() : \"<unknown operation>\";",
          "new_line_content": "    auto op_status = LookUpInstruction(root_op);",
          "content_same": false
        },
        {
          "line": 4283,
          "old_api": "mutable_program_shape",
          "new_api": "StrCat",
          "old_text": "entry.mutable_program_shape()",
          "new_text": "StrCat(name_, \"_compute_constant\")",
          "old_line_content": "  ProgramShapeProto* program_shape = entry.mutable_program_shape();",
          "new_line_content": "  SetProtoIdAndName(&entry, StrCat(name_, \"_compute_constant\"), kNameSeparator,",
          "content_same": false
        },
        {
          "line": 4284,
          "old_api": "shape",
          "new_api": "GetNextId",
          "old_text": "root->shape()",
          "new_text": "GetNextId()",
          "old_line_content": "  *program_shape->mutable_result() = root->shape();",
          "new_line_content": "                    GetNextId());",
          "content_same": false
        },
        {
          "line": 4296,
          "old_api": "empty",
          "new_api": "id",
          "old_text": "worklist.empty()",
          "new_text": "root->id()",
          "old_line_content": "  while (!worklist.empty()) {",
          "new_line_content": "  related_ops.insert(root->id());",
          "content_same": false
        },
        {
          "line": 4298,
          "old_api": "pop",
          "new_api": "empty",
          "old_text": "worklist.pop()",
          "new_text": "worklist.empty()",
          "old_line_content": "    worklist.pop();",
          "new_line_content": "  while (!worklist.empty()) {",
          "content_same": false
        },
        {
          "line": 4306,
          "old_api": "push",
          "new_api": "operand_ids",
          "old_text": "worklist.push(id)",
          "new_text": "instr_proto->operand_ids()",
          "old_line_content": "          worklist.push(id);",
          "new_line_content": "      for (int64_t id : instr_proto->operand_ids()) {",
          "content_same": false
        },
        {
          "line": 4316,
          "old_api": "InstrIsSetBound",
          "new_api": "opcode",
          "old_text": "InstrIsSetBound(instr_proto)",
          "new_text": "instr_proto->opcode()",
          "old_line_content": "        InstrIsSetBound(instr_proto)) {",
          "new_line_content": "    if (instr_proto->opcode() ==",
          "content_same": false
        },
        {
          "line": 4335,
          "old_api": "dimensions",
          "new_api": "is_dynamic_dimension",
          "old_text": "static_cast<int32_t>(\n              operand_proto->shape().dimensions(dimension))",
          "new_text": "operand_proto->shape().is_dynamic_dimension(dimension)",
          "old_line_content": "          constant_value = static_cast<int32_t>(",
          "new_line_content": "        if (!(operand_proto->shape().is_dynamic_dimension(dimension) &&",
          "content_same": false
        },
        {
          "line": 4338,
          "old_api": "LiteralUtil::CreateR0(constant_value)",
          "new_api": "dimensions",
          "old_text": "LiteralUtil::CreateR0(constant_value)",
          "new_text": "operand_proto->shape().dimensions(dimension)",
          "old_line_content": "        Literal literal = LiteralUtil::CreateR0(constant_value);",
          "new_line_content": "              operand_proto->shape().dimensions(dimension));",
          "content_same": false
        },
        {
          "line": 4340,
          "old_api": "shape",
          "new_api": "LiteralUtil::CreateR0(constant_value)",
          "old_text": "literal.shape().ToProto()",
          "new_text": "LiteralUtil::CreateR0(constant_value)",
          "old_line_content": "        *const_instr.mutable_shape() = literal.shape().ToProto();",
          "new_line_content": "        Literal literal = LiteralUtil::CreateR0(constant_value);",
          "content_same": false
        },
        {
          "line": 4348,
          "old_api": "literal",
          "new_api": "tuple_literals",
          "old_text": "instr_proto->literal()",
          "new_text": "instr_proto->literal().tuple_literals(0)",
          "old_line_content": "          *const_instr.mutable_literal() = instr_proto->literal();",
          "new_line_content": "              instr_proto->literal().tuple_literals(0);",
          "content_same": false
        },
        {
          "line": 4353,
          "old_api": "mutable_opcode",
          "new_api": "shape",
          "old_text": "const_instr.mutable_opcode()",
          "new_text": "instr_proto->shape()",
          "old_line_content": "      *const_instr.mutable_opcode() =",
          "new_line_content": "        *const_instr.mutable_shape() = instr_proto->shape();",
          "content_same": false
        },
        {
          "line": 4355,
          "old_api": "set_id",
          "new_api": "mutable_opcode",
          "old_text": "const_instr.set_id(handle)",
          "new_text": "const_instr.mutable_opcode()",
          "old_line_content": "      const_instr.set_id(handle);",
          "new_line_content": "      *const_instr.mutable_opcode() =",
          "content_same": false
        },
        {
          "line": 4356,
          "old_api": "mutable_name",
          "new_api": "HloOpcodeString",
          "old_text": "const_instr.mutable_name()",
          "new_text": "HloOpcodeString(HloOpcode::kConstant)",
          "old_line_content": "      *const_instr.mutable_name() =",
          "new_line_content": "          std::string(HloOpcodeString(HloOpcode::kConstant));",
          "content_same": false
        },
        {
          "line": 4357,
          "old_api": "id",
          "new_api": "set_id",
          "old_text": "const_instr.id()",
          "new_text": "const_instr.set_id(handle)",
          "old_line_content": "          GetFullName(const_instr.opcode(), kNameSeparator, const_instr.id());",
          "new_line_content": "      const_instr.set_id(handle);",
          "content_same": false
        },
        {
          "line": 4358,
          "old_api": "add_instructions",
          "new_api": "mutable_name",
          "old_text": "entry.add_instructions()",
          "new_text": "const_instr.mutable_name()",
          "old_line_content": "      *entry.add_instructions() =",
          "new_line_content": "      *const_instr.mutable_name() =",
          "content_same": false
        },
        {
          "line": 4371,
          "old_api": "insert",
          "new_api": "tuple_index",
          "old_text": "related_ops.insert(id)",
          "new_text": "instr_proto->tuple_index()",
          "old_line_content": "        if (related_ops.insert(id).second) {",
          "new_line_content": "        int64_t id = maybe_tuple_instr->operand_ids(instr_proto->tuple_index());",
          "content_same": false
        },
        {
          "line": 4388,
          "old_api": "end",
          "new_api": "id",
          "old_text": "substitutions.end()",
          "new_text": "root->id()",
          "old_line_content": "  while (it != substitutions.end()) {",
          "new_line_content": "  int64_t root_id = root->id();",
          "content_same": false
        },
        {
          "line": 4390,
          "old_api": "find",
          "new_api": "end",
          "old_text": "substitutions.find(root_id)",
          "new_text": "substitutions.end()",
          "old_line_content": "    it = substitutions.find(root_id);",
          "new_line_content": "  while (it != substitutions.end()) {",
          "content_same": false
        },
        {
          "line": 4392,
          "old_api": "set_root_id",
          "new_api": "find",
          "old_text": "entry.set_root_id(root_id)",
          "new_text": "substitutions.find(root_id)",
          "old_line_content": "  entry.set_root_id(root_id);",
          "new_line_content": "    it = substitutions.find(root_id);",
          "content_same": false
        },
        {
          "line": 4413,
          "old_api": "find",
          "new_api": "clear_operand_ids",
          "old_text": "substitutions.find(operand_id)",
          "new_text": "instr->clear_operand_ids()",
          "old_line_content": "      auto it = substitutions.find(operand_id);",
          "new_line_content": "    instr->clear_operand_ids();",
          "content_same": false
        },
        {
          "line": 4414,
          "old_api": "end",
          "new_api": "operand_ids",
          "old_text": "substitutions.end()",
          "new_text": "instr_src->operand_ids()",
          "old_line_content": "      while (it != substitutions.end()) {",
          "new_line_content": "    for (int64_t operand_id : instr_src->operand_ids()) {",
          "content_same": false
        },
        {
          "line": 4416,
          "old_api": "find",
          "new_api": "end",
          "old_text": "substitutions.find(operand_id)",
          "new_text": "substitutions.end()",
          "old_line_content": "        it = substitutions.find(operand_id);",
          "new_line_content": "      while (it != substitutions.end()) {",
          "content_same": false
        },
        {
          "line": 4418,
          "old_api": "add_operand_ids",
          "new_api": "find",
          "old_text": "instr->add_operand_ids(operand_id)",
          "new_text": "substitutions.find(operand_id)",
          "old_line_content": "      instr->add_operand_ids(operand_id);",
          "new_line_content": "        it = substitutions.find(operand_id);",
          "content_same": false
        },
        {
          "line": 4428,
          "old_api": "name",
          "new_api": "id",
          "old_text": "entry.name()",
          "new_text": "entry.id()",
          "old_line_content": "  module->set_name(entry.name());",
          "new_line_content": "  XlaComputation computation(entry.id());",
          "content_same": false
        },
        {
          "line": 4429,
          "old_api": "id",
          "new_api": "mutable_proto",
          "old_text": "entry.id()",
          "new_text": "computation.mutable_proto()",
          "old_line_content": "  module->set_id(entry.id());",
          "new_line_content": "  HloModuleProto* module = computation.mutable_proto();",
          "content_same": false
        },
        {
          "line": 4432,
          "old_api": "mutable_host_program_shape",
          "new_api": "name",
          "old_text": "module->mutable_host_program_shape()",
          "new_text": "entry.name()",
          "old_line_content": "  *module->mutable_host_program_shape() = *program_shape;",
          "new_line_content": "  module->set_entry_computation_name(entry.name());",
          "content_same": false
        },
        {
          "line": 4434,
          "old_api": "end",
          "new_api": "mutable_host_program_shape",
          "old_text": "related_calls.end()",
          "new_text": "module->mutable_host_program_shape()",
          "old_line_content": "    if (related_calls.find(e.second.id()) != related_calls.end()) {",
          "new_line_content": "  *module->mutable_host_program_shape() = *program_shape;",
          "content_same": false
        },
        {
          "line": 4440,
          "old_api": "DebugString",
          "new_api": "std::move(entry)",
          "old_text": "module->DebugString()",
          "new_text": "std::move(entry)",
          "old_line_content": "    VLOG(4) << \"Constant computation:\\n\" << module->DebugString();",
          "new_line_content": "  *module->add_computations() = std::move(entry);",
          "content_same": false
        },
        {
          "line": 4442,
          "old_api": "std::move(computation)",
          "new_api": "DebugString",
          "old_text": "std::move(computation)",
          "new_text": "module->DebugString()",
          "old_line_content": "  return std::move(computation);",
          "new_line_content": "    VLOG(4) << \"Constant computation:\\n\" << module->DebugString();",
          "content_same": false
        },
        {
          "line": 4458,
          "old_api": "set_output_batch_dimension",
          "new_api": "set_input_batch_dimension",
          "old_text": "dimension_numbers.set_output_batch_dimension(kConvBatchDimension)",
          "new_text": "dimension_numbers.set_input_batch_dimension(kConvBatchDimension)",
          "old_line_content": "  dimension_numbers.set_output_batch_dimension(kConvBatchDimension);",
          "new_line_content": "  dimension_numbers.set_input_batch_dimension(kConvBatchDimension);",
          "content_same": false
        },
        {
          "line": 4459,
          "old_api": "set_output_feature_dimension",
          "new_api": "set_input_feature_dimension",
          "old_text": "dimension_numbers.set_output_feature_dimension(kConvFeatureDimension)",
          "new_text": "dimension_numbers.set_input_feature_dimension(kConvFeatureDimension)",
          "old_line_content": "  dimension_numbers.set_output_feature_dimension(kConvFeatureDimension);",
          "new_line_content": "  dimension_numbers.set_input_feature_dimension(kConvFeatureDimension);",
          "content_same": false
        },
        {
          "line": 4460,
          "old_api": "set_kernel_output_feature_dimension",
          "new_api": "set_output_batch_dimension",
          "old_text": "dimension_numbers.set_kernel_output_feature_dimension(\n      kConvKernelOutputDimension)",
          "new_text": "dimension_numbers.set_output_batch_dimension(kConvBatchDimension)",
          "old_line_content": "  dimension_numbers.set_kernel_output_feature_dimension(",
          "new_line_content": "  dimension_numbers.set_output_batch_dimension(kConvBatchDimension);",
          "content_same": false
        },
        {
          "line": 4462,
          "old_api": "set_kernel_input_feature_dimension",
          "new_api": "set_kernel_output_feature_dimension",
          "old_text": "dimension_numbers.set_kernel_input_feature_dimension(\n      kConvKernelInputDimension)",
          "new_text": "dimension_numbers.set_kernel_output_feature_dimension(\n      kConvKernelOutputDimension)",
          "old_line_content": "  dimension_numbers.set_kernel_input_feature_dimension(",
          "new_line_content": "  dimension_numbers.set_kernel_output_feature_dimension(",
          "content_same": false
        },
        {
          "line": 4467,
          "old_api": "add_output_spatial_dimensions",
          "new_api": "add_input_spatial_dimensions",
          "old_text": "dimension_numbers.add_output_spatial_dimensions(i + 2)",
          "new_text": "dimension_numbers.add_input_spatial_dimensions(i + 2)",
          "old_line_content": "    dimension_numbers.add_output_spatial_dimensions(i + 2);",
          "new_line_content": "    dimension_numbers.add_input_spatial_dimensions(i + 2);",
          "content_same": false
        },
        {
          "line": 4478,
          "old_api": "kernel_spatial_dimensions_size",
          "new_api": "input_spatial_dimensions_size",
          "old_text": "dnum.kernel_spatial_dimensions_size()",
          "new_text": "dnum.input_spatial_dimensions_size()",
          "old_line_content": "  if (dnum.kernel_spatial_dimensions_size() < 2) {",
          "new_line_content": "                              dnum.input_spatial_dimensions_size());",
          "content_same": false
        },
        {
          "line": 4482,
          "old_api": "output_spatial_dimensions_size",
          "new_api": "kernel_spatial_dimensions_size",
          "old_text": "dnum.output_spatial_dimensions_size()",
          "new_text": "dnum.kernel_spatial_dimensions_size()",
          "old_line_content": "  if (dnum.output_spatial_dimensions_size() < 2) {",
          "new_line_content": "                              dnum.kernel_spatial_dimensions_size());",
          "content_same": false
        },
        {
          "line": 4489,
          "old_api": "input_spatial_dimensions",
          "new_api": "input_batch_dimension",
          "old_text": "dnum.input_spatial_dimensions(1)",
          "new_text": "std::set<int64_t>(\n          {dnum.input_batch_dimension(), dnum.input_feature_dimension(),\n           dnum.input_spatial_dimensions(0), dnum.input_spatial_dimensions(1)})\n          .size()",
          "old_line_content": "           dnum.input_spatial_dimensions(0), dnum.input_spatial_dimensions(1)})",
          "new_line_content": "  if (std::set<int64_t>(",
          "content_same": false
        },
        {
          "line": 4491,
          "old_api": "input_batch_dimension",
          "new_api": "input_spatial_dimensions",
          "old_text": "FailedPrecondition(\n        \"dimension numbers for the input are not unique: (%d, %d, %d, \"\n        \"%d)\",\n        dnum.input_batch_dimension(), dnum.input_feature_dimension(),\n        dnum.input_spatial_dimensions(0), dnum.input_spatial_dimensions(1))",
          "new_text": "dnum.input_spatial_dimensions(1)",
          "old_line_content": "    return FailedPrecondition(",
          "new_line_content": "           dnum.input_spatial_dimensions(0), dnum.input_spatial_dimensions(1)})",
          "content_same": false
        },
        {
          "line": 4497,
          "old_api": "kernel_output_feature_dimension",
          "new_api": "input_spatial_dimensions",
          "old_text": "dnum.kernel_output_feature_dimension()",
          "new_text": "dnum.input_spatial_dimensions(1)",
          "old_line_content": "  if (std::set<int64_t>({dnum.kernel_output_feature_dimension(),",
          "new_line_content": "        dnum.input_spatial_dimensions(0), dnum.input_spatial_dimensions(1));",
          "content_same": false
        },
        {
          "line": 4499,
          "old_api": "kernel_spatial_dimensions",
          "new_api": "kernel_output_feature_dimension",
          "old_text": "dnum.kernel_spatial_dimensions(0)",
          "new_text": "dnum.kernel_output_feature_dimension()",
          "old_line_content": "                         dnum.kernel_spatial_dimensions(0),",
          "new_line_content": "  if (std::set<int64_t>({dnum.kernel_output_feature_dimension(),",
          "content_same": false
        },
        {
          "line": 4500,
          "old_api": "kernel_spatial_dimensions",
          "new_api": "kernel_input_feature_dimension",
          "old_text": "dnum.kernel_spatial_dimensions(1)",
          "new_text": "dnum.kernel_input_feature_dimension()",
          "old_line_content": "                         dnum.kernel_spatial_dimensions(1)})",
          "new_line_content": "                         dnum.kernel_input_feature_dimension(),",
          "content_same": false
        },
        {
          "line": 4502,
          "old_api": "kernel_output_feature_dimension",
          "new_api": "kernel_spatial_dimensions",
          "old_text": "FailedPrecondition(\n        \"dimension numbers for the weight are not unique: (%d, %d, %d, \"\n        \"%d)\",\n        dnum.kernel_output_feature_dimension(),\n        dnum.kernel_input_feature_dimension(),\n        dnum.kernel_spatial_dimensions(0), dnum.kernel_spatial_dimensions(1))",
          "new_text": "dnum.kernel_spatial_dimensions(1)",
          "old_line_content": "    return FailedPrecondition(",
          "new_line_content": "                         dnum.kernel_spatial_dimensions(1)})",
          "content_same": false
        },
        {
          "line": 4507,
          "old_api": "kernel_spatial_dimensions",
          "new_api": "kernel_output_feature_dimension",
          "old_text": "dnum.kernel_spatial_dimensions(1)",
          "new_text": "dnum.kernel_output_feature_dimension()",
          "old_line_content": "        dnum.kernel_spatial_dimensions(0), dnum.kernel_spatial_dimensions(1));",
          "new_line_content": "        dnum.kernel_output_feature_dimension(),",
          "content_same": false
        },
        {
          "line": 4509,
          "old_api": "output_batch_dimension",
          "new_api": "kernel_spatial_dimensions",
          "old_text": "dnum.output_batch_dimension()",
          "new_text": "dnum.kernel_spatial_dimensions(1)",
          "old_line_content": "  if (std::set<int64_t>({dnum.output_batch_dimension(),",
          "new_line_content": "        dnum.kernel_spatial_dimensions(0), dnum.kernel_spatial_dimensions(1));",
          "content_same": false
        },
        {
          "line": 4511,
          "old_api": "output_spatial_dimensions",
          "new_api": "output_batch_dimension",
          "old_text": "dnum.output_spatial_dimensions(0)",
          "new_text": "dnum.output_batch_dimension()",
          "old_line_content": "                         dnum.output_spatial_dimensions(0),",
          "new_line_content": "  if (std::set<int64_t>({dnum.output_batch_dimension(),",
          "content_same": false
        },
        {
          "line": 4512,
          "old_api": "output_spatial_dimensions",
          "new_api": "output_feature_dimension",
          "old_text": "dnum.output_spatial_dimensions(1)",
          "new_text": "dnum.output_feature_dimension()",
          "old_line_content": "                         dnum.output_spatial_dimensions(1)})",
          "new_line_content": "                         dnum.output_feature_dimension(),",
          "content_same": false
        },
        {
          "line": 4514,
          "old_api": "output_batch_dimension",
          "new_api": "output_spatial_dimensions",
          "old_text": "FailedPrecondition(\n        \"dimension numbers for the output are not unique: (%d, %d, %d, \"\n        \"%d)\",\n        dnum.output_batch_dimension(), dnum.output_feature_dimension(),\n        dnum.output_spatial_dimensions(0), dnum.output_spatial_dimensions(1))",
          "new_text": "dnum.output_spatial_dimensions(1)",
          "old_line_content": "    return FailedPrecondition(",
          "new_line_content": "                         dnum.output_spatial_dimensions(1)})",
          "content_same": false
        },
        {
          "line": 4520,
          "old_api": "OkStatus",
          "new_api": "output_spatial_dimensions",
          "old_text": "OkStatus()",
          "new_text": "dnum.output_spatial_dimensions(1)",
          "old_line_content": "  return OkStatus();",
          "new_line_content": "        dnum.output_spatial_dimensions(0), dnum.output_spatial_dimensions(1));",
          "content_same": false
        },
        {
          "line": 4528,
          "old_api": "GetNextId",
          "new_api": "TF_RETURN_IF_ERROR",
          "old_text": "GetNextId()",
          "new_text": "TF_RETURN_IF_ERROR(first_error_)",
          "old_line_content": "  const int64_t handle = GetNextId();",
          "new_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "content_same": false
        },
        {
          "line": 4530,
          "old_api": "HloOpcodeString",
          "new_api": "GetNextId",
          "old_text": "HloOpcodeString(opcode)",
          "new_text": "GetNextId()",
          "old_line_content": "  *instr.mutable_opcode() = std::string(HloOpcodeString(opcode));",
          "new_line_content": "  const int64_t handle = GetNextId();",
          "content_same": false
        },
        {
          "line": 4531,
          "old_api": "name",
          "new_api": "set_id",
          "old_text": "instr.name().empty()",
          "new_text": "instr.set_id(handle)",
          "old_line_content": "  if (instr.name().empty()) {",
          "new_line_content": "  instr.set_id(handle);",
          "content_same": false
        },
        {
          "line": 4532,
          "old_api": "opcode",
          "new_api": "HloOpcodeString",
          "old_text": "instr.opcode()",
          "new_text": "HloOpcodeString(opcode)",
          "old_line_content": "    instr.set_name(instr.opcode());",
          "new_line_content": "  *instr.mutable_opcode() = std::string(HloOpcodeString(opcode));",
          "content_same": false
        },
        {
          "line": 4542,
          "old_api": "handle",
          "new_api": "name",
          "old_text": "operand.handle()",
          "new_text": "this->name()",
          "old_line_content": "    instr.add_operand_ids(operand.handle());",
          "new_line_content": "                             operand.builder_->name(), this->name());",
          "content_same": false
        },
        {
          "line": 4547,
          "old_api": "reset",
          "new_api": "has_value",
          "old_text": "one_shot_metadata_.reset()",
          "new_text": "one_shot_metadata_.has_value()",
          "old_line_content": "    one_shot_metadata_.reset();",
          "new_line_content": "  if (one_shot_metadata_.has_value()) {",
          "content_same": false
        },
        {
          "line": 4549,
          "old_api": "mutable_metadata",
          "new_api": "reset",
          "old_text": "instr.mutable_metadata()",
          "new_text": "one_shot_metadata_.reset()",
          "old_line_content": "    *instr.mutable_metadata() = metadata_;",
          "new_line_content": "    one_shot_metadata_.reset();",
          "content_same": false
        },
        {
          "line": 4558,
          "old_api": "ToProto",
          "new_api": "NormalizeTupleSharding",
          "old_text": "sharding.ToProto()",
          "new_text": "sharding.NormalizeTupleSharding(shape)",
          "old_line_content": "    *instr.mutable_sharding() = sharding.ToProto();",
          "new_line_content": "    sharding = sharding.NormalizeTupleSharding(shape);",
          "content_same": false
        },
        {
          "line": 4560,
          "old_api": "mutable_frontend_attributes",
          "new_api": "ToProto",
          "old_text": "instr.mutable_frontend_attributes()",
          "new_text": "sharding.ToProto()",
          "old_line_content": "  *instr.mutable_frontend_attributes() = frontend_attributes_;",
          "new_line_content": "    *instr.mutable_sharding() = sharding.ToProto();",
          "content_same": false
        },
        {
          "line": 4562,
          "old_api": "size",
          "new_api": "mutable_frontend_attributes",
          "old_text": "instructions_.size()",
          "new_text": "instr.mutable_frontend_attributes()",
          "old_line_content": "  handle_to_index_[handle] = instructions_.size();",
          "new_line_content": "  *instr.mutable_frontend_attributes() = frontend_attributes_;",
          "content_same": false
        },
        {
          "line": 4564,
          "old_api": "push_back",
          "new_api": "size",
          "old_text": "instruction_shapes_.push_back(\n      std::make_unique<Shape>(instructions_.back().shape()))",
          "new_text": "instructions_.size()",
          "old_line_content": "  instruction_shapes_.push_back(",
          "new_line_content": "  handle_to_index_[handle] = instructions_.size();",
          "content_same": false
        },
        {
          "line": 4565,
          "old_api": "back",
          "new_api": "std::move(instr)",
          "old_text": "instructions_.back().shape()",
          "new_text": "std::move(instr)",
          "old_line_content": "      std::make_unique<Shape>(instructions_.back().shape()));",
          "new_line_content": "  instructions_.push_back(std::move(instr));",
          "content_same": false
        },
        {
          "line": 4587,
          "old_api": "GetNextId",
          "new_api": "proto",
          "old_text": "GetNextId()",
          "new_text": "computation.proto().computations()",
          "old_line_content": "    int64_t computation_id = GetNextId();",
          "new_line_content": "  for (const HloComputationProto& e : computation.proto().computations()) {",
          "content_same": false
        },
        {
          "line": 4589,
          "old_api": "name",
          "new_api": "GetNextId",
          "old_text": "SetProtoIdAndName(&new_computation,\n                      GetBaseName(new_computation.name(), kNameSeparator),\n                      kNameSeparator, computation_id)",
          "new_text": "GetNextId()",
          "old_line_content": "    SetProtoIdAndName(&new_computation,",
          "new_line_content": "    int64_t computation_id = GetNextId();",
          "content_same": false
        },
        {
          "line": 4590,
          "old_api": "name",
          "new_api": "id",
          "old_text": "new_computation.name()",
          "new_text": "new_computation.id()",
          "old_line_content": "                      GetBaseName(new_computation.name(), kNameSeparator),",
          "new_line_content": "    remapped_ids[new_computation.id()] = computation_id;",
          "content_same": false
        },
        {
          "line": 4592,
          "old_api": "mutable_instructions",
          "new_api": "name",
          "old_text": "new_computation.mutable_instructions()",
          "new_text": "new_computation.name()",
          "old_line_content": "    for (auto& instruction : *new_computation.mutable_instructions()) {",
          "new_line_content": "                      GetBaseName(new_computation.name(), kNameSeparator),",
          "content_same": false
        },
        {
          "line": 4594,
          "old_api": "id",
          "new_api": "mutable_instructions",
          "old_text": "instruction.id()",
          "new_text": "new_computation.mutable_instructions()",
          "old_line_content": "      remapped_ids[instruction.id()] = instruction_id;",
          "new_line_content": "    for (auto& instruction : *new_computation.mutable_instructions()) {",
          "content_same": false
        },
        {
          "line": 4595,
          "old_api": "name",
          "new_api": "GetNextId",
          "old_text": "SetProtoIdAndName(&instruction,\n                        GetBaseName(instruction.name(), kNameSeparator),\n                        kNameSeparator, instruction_id)",
          "new_text": "GetNextId()",
          "old_line_content": "      SetProtoIdAndName(&instruction,",
          "new_line_content": "      int64_t instruction_id = GetNextId();",
          "content_same": false
        },
        {
          "line": 4596,
          "old_api": "name",
          "new_api": "id",
          "old_text": "instruction.name()",
          "new_text": "instruction.id()",
          "old_line_content": "                        GetBaseName(instruction.name(), kNameSeparator),",
          "new_line_content": "      remapped_ids[instruction.id()] = instruction_id;",
          "content_same": false
        },
        {
          "line": 4601,
          "old_api": "std::move(new_computation)",
          "new_api": "root_id",
          "old_text": "std::move(new_computation)",
          "new_text": "new_computation.root_id()",
          "old_line_content": "    imported_computations.push_back(std::move(new_computation));",
          "new_line_content": "    new_computation.set_root_id(remapped_ids.at(new_computation.root_id()));",
          "content_same": false
        },
        {
          "line": 4608,
          "old_api": "mutable_instructions",
          "new_api": "proto",
          "old_text": "imported_computation.mutable_instructions()",
          "new_text": "computation.proto().entry_computation_id()",
          "old_line_content": "    for (auto& instruction : *imported_computation.mutable_instructions()) {",
          "new_line_content": "      remapped_ids.at(computation.proto().entry_computation_id()));",
          "content_same": false
        },
        {
          "line": 4610,
          "old_api": "at",
          "new_api": "mutable_instructions",
          "old_text": "remapped_ids.at(operand_id)",
          "new_text": "imported_computation.mutable_instructions()",
          "old_line_content": "        operand_id = remapped_ids.at(operand_id);",
          "new_line_content": "    for (auto& instruction : *imported_computation.mutable_instructions()) {",
          "content_same": false
        },
        {
          "line": 4630,
          "old_api": "std::move(imported_computation)",
          "new_api": "instructions",
          "old_text": "std::move(imported_computation)",
          "new_text": "imported_computation.instructions(i).id()",
          "old_line_content": "    embedded_.insert({computation_id, std::move(imported_computation)});",
          "new_line_content": "          {imported_computation.instructions(i).id(), imported_instruction});",
          "content_same": false
        },
        {
          "line": 4794,
          "old_api": "primitive_util::IsFloatingPointType(operand_element_type)",
          "new_api": "element_type",
          "old_text": "primitive_util::IsFloatingPointType(operand_element_type)",
          "new_text": "operand_shape.element_type()",
          "old_line_content": "        primitive_util::IsFloatingPointType(operand_element_type)",
          "new_line_content": "    auto operand_element_type = operand_shape.element_type();",
          "content_same": false
        },
        {
          "line": 4796,
          "old_api": "Comparison::DefaultComparisonType(operand_element_type)",
          "new_api": "primitive_util::IsFloatingPointType(operand_element_type)",
          "old_text": "Comparison::DefaultComparisonType(operand_element_type)",
          "new_text": "primitive_util::IsFloatingPointType(operand_element_type)",
          "old_line_content": "            : Comparison::DefaultComparisonType(operand_element_type);",
          "new_line_content": "        primitive_util::IsFloatingPointType(operand_element_type)",
          "content_same": false
        },
        {
          "line": 5029,
          "old_api": "set_unit_diagonal",
          "new_api": "set_left_side",
          "old_text": "options.set_unit_diagonal(unit_diagonal)",
          "new_text": "options.set_left_side(left_side)",
          "old_line_content": "    options.set_unit_diagonal(unit_diagonal);",
          "new_line_content": "    options.set_left_side(left_side);",
          "content_same": false
        },
        {
          "line": 5030,
          "old_api": "set_transpose_a",
          "new_api": "set_lower",
          "old_text": "options.set_transpose_a(transpose_a)",
          "new_text": "options.set_lower(lower)",
          "old_line_content": "    options.set_transpose_a(transpose_a);",
          "new_line_content": "    options.set_lower(lower);",
          "content_same": false
        },
        {
          "line": 5314,
          "old_api": "builder",
          "new_api": "empty",
          "old_text": "operands[0].builder()->Tuple(operands)",
          "new_text": "operands.empty()",
          "old_line_content": "      operands[0].builder()->Tuple(operands), all_gather_dimension, shard_count,",
          "new_line_content": "  CHECK(!operands.empty());",
          "content_same": false
        },
        {
          "line": 5341,
          "old_api": "builder",
          "new_api": "empty",
          "old_text": "operands[0].builder()->Tuple(operands)",
          "new_text": "operands.empty()",
          "old_line_content": "      operands[0].builder()->Tuple(operands), computation, replica_groups,",
          "new_line_content": "  CHECK(!operands.empty());",
          "content_same": false
        },
        {
          "line": 5720,
          "old_api": "tile_assignment_dimensions",
          "new_api": "set_type",
          "old_text": "original.tile_assignment_dimensions().begin()",
          "new_text": "manual.set_type(OpSharding::OTHER)",
          "old_line_content": "      original.tile_assignment_dimensions().begin(),",
          "new_line_content": "  manual.set_type(OpSharding::OTHER);",
          "content_same": false
        },
        {
          "line": 5722,
          "old_api": "push_back",
          "new_api": "tile_assignment_dimensions",
          "old_text": "new_tile_shape.push_back(new_tile_shape[single_dim])",
          "new_text": "original.tile_assignment_dimensions().begin()",
          "old_line_content": "  new_tile_shape.push_back(new_tile_shape[single_dim]);",
          "new_line_content": "      original.tile_assignment_dimensions().begin(),",
          "content_same": false
        },
        {
          "line": 5727,
          "old_api": "size",
          "new_api": "Each",
          "old_text": "indices.size()",
          "new_text": "new_tile.Each([&](absl::Span<const int64_t> indices, int64_t* v) {\n    int64_t src_index = 0;\n    for (int64_t i = 0; i < indices.size() - 1; ++i) {\n      if (i > 0) {\n        src_index *= new_tile_shape[i];\n      }\n      int64_t index = indices[i];\n      if (i == single_dim) {\n        index = indices.back();\n      }\n      src_index += index;\n    }\n    *v = original.tile_assignment_devices(src_index);\n  })",
          "old_line_content": "    for (int64_t i = 0; i < indices.size() - 1; ++i) {",
          "new_line_content": "  new_tile.Each([&](absl::Span<const int64_t> indices, int64_t* v) {",
          "content_same": false
        },
        {
          "line": 5745,
          "old_api": "replicate_on_last_tile_dim",
          "new_api": "add_tile_assignment_devices",
          "old_text": "original.replicate_on_last_tile_dim()",
          "new_text": "manual.add_tile_assignment_devices(device)",
          "old_line_content": "  if (original.replicate_on_last_tile_dim()) {",
          "new_line_content": "    manual.add_tile_assignment_devices(device);",
          "content_same": false
        },
        {
          "line": 5748,
          "old_api": "last_tile_dims",
          "new_api": "add_last_tile_dims",
          "old_text": "original.last_tile_dims()",
          "new_text": "manual.add_last_tile_dims(OpSharding::REPLICATED)",
          "old_line_content": "  for (int64_t type : original.last_tile_dims()) {",
          "new_line_content": "    manual.add_last_tile_dims(OpSharding::REPLICATED);",
          "content_same": false
        },
        {
          "line": 5751,
          "old_api": "add_last_tile_dims",
          "new_api": "static_cast<OpSharding::Type>(type)",
          "old_text": "manual.add_last_tile_dims(OpSharding::MANUAL)",
          "new_text": "static_cast<OpSharding::Type>(type)",
          "old_line_content": "  manual.add_last_tile_dims(OpSharding::MANUAL);",
          "new_line_content": "    manual.add_last_tile_dims(static_cast<OpSharding::Type>(type));",
          "content_same": false
        },
        {
          "line": 5792,
          "old_api": "CustomCall",
          "new_api": "GetManualSharding",
          "old_text": "CustomCall(builder,\n                      /*call_target_name=*/\"SPMDFullToShardShape\",\n                      {input_annotation}, output_shape,\n                      /*opaque=*/\n                      sharding_op_util::EncodeAttributes(unspecified_dims))",
          "new_text": "GetManualSharding(manual_sharding, single_dim)",
          "old_line_content": "    return CustomCall(builder,",
          "new_line_content": "    OpSharding manual = GetManualSharding(manual_sharding, single_dim);",
          "content_same": false
        },
        {
          "line": 5812,
          "old_api": "CustomCall",
          "new_api": "GetManualSharding",
          "old_text": "CustomCall(\n        builder, /*call_target_name=*/\"Sharding\", {input}, input_shape,\n        sharding_op_util::EncodeAttributes(unspecified_dims))",
          "new_text": "GetManualSharding(manual_sharding, single_dim)",
          "old_line_content": "    input_annotation = CustomCall(",
          "new_line_content": "    OpSharding manual = GetManualSharding(manual_sharding, single_dim);",
          "content_same": false
        },
        {
          "line": 5814,
          "old_api": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "new_api": "CustomCall",
          "old_text": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "new_text": "CustomCall(\n        builder, /*call_target_name=*/\"Sharding\", {input}, input_shape,\n        sharding_op_util::EncodeAttributes(unspecified_dims))",
          "old_line_content": "        sharding_op_util::EncodeAttributes(unspecified_dims));",
          "new_line_content": "    input_annotation = CustomCall(",
          "content_same": false
        }
      ],
      "additions": [
        {
          "line": 4096,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Shape passed to SendToHost must have a layout\")",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "      return InvalidArgument(\"Shape passed to SendToHost must have a layout\");",
          "content_same": false
        },
        {
          "line": 4099,
          "old_api": null,
          "new_api": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "old_text": null,
          "new_text": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "old_line_content": "          \"SendToHost shape %s must be compatible with operand shape %s\",",
          "new_line_content": "    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {",
          "content_same": false
        },
        {
          "line": 2052,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions_size",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n    TF_RETURN_IF_ERROR(\n        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers));\n\n    std::vector<int64_t> window_dimensions(\n        dimension_numbers.kernel_spatial_dimensions_size());\n    for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();\n         ++i) {\n      window_dimensions[i] =\n          rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));\n    }\n\n    TF_ASSIGN_OR_RETURN(Window window,\n                        ShapeInference::InferWindowFromDimensions(\n                            window_dimensions, window_strides, padding,\n                            lhs_dilation, rhs_dilation, window_reversal));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferConvolveShape(\n            *lhs_shape, *rhs_shape, feature_group_count, batch_group_count,\n            window, dimension_numbers, preferred_element_type));\n    return ConvGeneralDilatedInternal(shape, lhs, rhs, window, window_strides,\n                                      padding, lhs_dilation, rhs_dilation,\n                                      dimension_numbers, feature_group_count,\n                                      batch_group_count, precision_config);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4102,
          "old_api": null,
          "new_api": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "old_line_content": "    }",
          "new_line_content": "          ShapeUtil::HumanStringWithLayout(shape_with_layout),",
          "content_same": false
        },
        {
          "line": 2055,
          "old_api": null,
          "new_api": "TF_RETURN_IF_ERROR",
          "old_text": null,
          "new_text": "TF_RETURN_IF_ERROR(\n        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers))",
          "old_line_content": "",
          "new_line_content": "    TF_RETURN_IF_ERROR(",
          "content_same": false
        },
        {
          "line": 2056,
          "old_api": null,
          "new_api": "VerifyConvolution",
          "old_text": null,
          "new_text": "VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers)",
          "old_line_content": "    std::vector<int64_t> window_dimensions(",
          "new_line_content": "        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers));",
          "content_same": false
        },
        {
          "line": 4103,
          "old_api": null,
          "new_api": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "old_line_content": "    // TODO(b/111544877): Support tuple shapes.",
          "new_line_content": "          ShapeUtil::HumanStringWithLayout(*operand_shape));",
          "content_same": false
        },
        {
          "line": 2059,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions_size",
          "old_text": null,
          "new_text": "dimension_numbers.kernel_spatial_dimensions_size()",
          "old_line_content": "         ++i) {",
          "new_line_content": "        dimension_numbers.kernel_spatial_dimensions_size());",
          "content_same": false
        },
        {
          "line": 2060,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "window_dimensions.size()",
          "old_line_content": "      window_dimensions[i] =",
          "new_line_content": "    for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();",
          "content_same": false
        },
        {
          "line": 4107,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"SendToHost only supports array shapes, shape: %s\",\n                             ShapeUtil::HumanString(*operand_shape))",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(\"SendToHost only supports array shapes, shape: %s\",",
          "content_same": false
        },
        {
          "line": 4108,
          "old_api": null,
          "new_api": "ShapeUtil::HumanString(*operand_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanString(*operand_shape)",
          "old_line_content": "",
          "new_line_content": "                             ShapeUtil::HumanString(*operand_shape));",
          "content_same": false
        },
        {
          "line": 2063,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.kernel_spatial_dimensions(i)",
          "old_line_content": "",
          "new_line_content": "          rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));",
          "content_same": false
        },
        {
          "line": 4111,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "handle.type()",
          "old_line_content": "    }",
          "new_line_content": "    if (handle.type() != ChannelHandle::DEVICE_TO_HOST) {",
          "content_same": false
        },
        {
          "line": 4112,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"SendToHost must use a device-to-host channel\")",
          "old_line_content": "",
          "new_line_content": "      return InvalidArgument(\"SendToHost must use a device-to-host channel\");",
          "content_same": false
        },
        {
          "line": 4120,
          "old_api": null,
          "new_api": "ShapeUtil::MakeShape(U32, {})",
          "old_text": null,
          "new_text": "ShapeUtil::MakeShape(U32, {})",
          "old_line_content": "            .ToProto();",
          "new_line_content": "                                   ShapeUtil::MakeShape(U32, {}),",
          "content_same": false
        },
        {
          "line": 2075,
          "old_api": null,
          "new_api": "ConvGeneralDilatedInternal",
          "old_text": null,
          "new_text": "ConvGeneralDilatedInternal(shape, lhs, rhs, window, window_strides,\n                                      padding, lhs_dilation, rhs_dilation,\n                                      dimension_numbers, feature_group_count,\n                                      batch_group_count, precision_config)",
          "old_line_content": "                                      dimension_numbers, feature_group_count,",
          "new_line_content": "    return ConvGeneralDilatedInternal(shape, lhs, rhs, window, window_strides,",
          "content_same": false
        },
        {
          "line": 4123,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "handle.handle()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(XlaOp send,",
          "new_line_content": "    send_instr.set_channel_id(handle.handle());",
          "content_same": false
        },
        {
          "line": 4124,
          "old_api": null,
          "new_api": "set_is_host_transfer",
          "old_text": null,
          "new_text": "send_instr.set_is_host_transfer(true)",
          "old_line_content": "                        AddInstruction(std::move(send_instr), HloOpcode::kSend,",
          "new_line_content": "    send_instr.set_is_host_transfer(true);",
          "content_same": false
        },
        {
          "line": 4131,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "handle.handle()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(XlaOp send_done,",
          "new_line_content": "    send_done_instr.set_channel_id(handle.handle());",
          "content_same": false
        },
        {
          "line": 4132,
          "old_api": null,
          "new_api": "set_is_host_transfer",
          "old_text": null,
          "new_text": "send_done_instr.set_is_host_transfer(true)",
          "old_line_content": "                        AddInstruction(std::move(send_done_instr),",
          "new_line_content": "    send_done_instr.set_is_host_transfer(true);",
          "content_same": false
        },
        {
          "line": 2094,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions_size",
          "old_text": null,
          "new_text": "dimension_numbers.kernel_spatial_dimensions_size()",
          "old_line_content": "       ++i) {",
          "new_line_content": "      dimension_numbers.kernel_spatial_dimensions_size());",
          "content_same": false
        },
        {
          "line": 2095,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "window_dimensions.size()",
          "old_line_content": "    window_dimensions[i] =",
          "new_line_content": "  for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();",
          "content_same": false
        },
        {
          "line": 4143,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(shape)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(shape)",
          "old_line_content": "    }",
          "new_line_content": "    if (!LayoutUtil::HasLayout(shape)) {",
          "content_same": false
        },
        {
          "line": 4144,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Shape passed to RecvFromHost must have a layout\")",
          "old_line_content": "",
          "new_line_content": "      return InvalidArgument(\"Shape passed to RecvFromHost must have a layout\");",
          "content_same": false
        },
        {
          "line": 2098,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.kernel_spatial_dimensions(i)",
          "old_line_content": "",
          "new_line_content": "        rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));",
          "content_same": false
        },
        {
          "line": 4148,
          "old_api": null,
          "new_api": "IsArray",
          "old_text": null,
          "new_text": "shape.IsArray()",
          "old_line_content": "          \"RecvFromHost only supports array shapes, shape: %s\",",
          "new_line_content": "    if (!shape.IsArray()) {",
          "content_same": false
        },
        {
          "line": 4151,
          "old_api": null,
          "new_api": "ShapeUtil::HumanString(shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanString(shape)",
          "old_line_content": "",
          "new_line_content": "          ShapeUtil::HumanString(shape));",
          "content_same": false
        },
        {
          "line": 4154,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "handle.type()",
          "old_line_content": "    }",
          "new_line_content": "    if (handle.type() != ChannelHandle::HOST_TO_DEVICE) {",
          "content_same": false
        },
        {
          "line": 4155,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"RecvFromHost must use a host-to-device channel\")",
          "old_line_content": "",
          "new_line_content": "      return InvalidArgument(\"RecvFromHost must use a host-to-device channel\");",
          "content_same": false
        },
        {
          "line": 4162,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTupleShape(\n            {shape, ShapeUtil::MakeShape(U32, {}), ShapeUtil::MakeTokenShape()})\n            .ToProto()",
          "old_line_content": "            .ToProto();",
          "new_line_content": "        ShapeUtil::MakeTupleShape(",
          "content_same": false
        },
        {
          "line": 2116,
          "old_api": null,
          "new_api": "set_batch_group_count",
          "old_text": null,
          "new_text": "instr.set_batch_group_count(batch_group_count)",
          "old_line_content": "",
          "new_line_content": "  instr.set_batch_group_count(batch_group_count);",
          "content_same": false
        },
        {
          "line": 2117,
          "old_api": null,
          "new_api": "set_padding_type",
          "old_text": null,
          "new_text": "instr.set_padding_type(padding_type)",
          "old_line_content": "  if (precision_config != nullptr) {",
          "new_line_content": "  instr.set_padding_type(padding_type);",
          "content_same": false
        },
        {
          "line": 4165,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "handle.handle()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(XlaOp recv, AddInstruction(std::move(recv_instr),",
          "new_line_content": "    recv_instr.set_channel_id(handle.handle());",
          "content_same": false
        },
        {
          "line": 4166,
          "old_api": null,
          "new_api": "set_is_host_transfer",
          "old_text": null,
          "new_text": "recv_instr.set_is_host_transfer(true)",
          "old_line_content": "                                                   HloOpcode::kRecv, {token}));",
          "new_line_content": "    recv_instr.set_is_host_transfer(true);",
          "content_same": false
        },
        {
          "line": 2122,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return std::move(instr);",
          "content_same": false
        },
        {
          "line": 4171,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "recv_done_instr.mutable_shape()",
          "old_line_content": "            .ToProto();",
          "new_line_content": "    *recv_done_instr.mutable_shape() =",
          "content_same": false
        },
        {
          "line": 4175,
          "old_api": null,
          "new_api": "set_is_host_transfer",
          "old_text": null,
          "new_text": "recv_done_instr.set_is_host_transfer(true)",
          "old_line_content": "                          {recv});",
          "new_line_content": "    recv_done_instr.set_is_host_transfer(true);",
          "content_same": false
        },
        {
          "line": 4176,
          "old_api": null,
          "new_api": "std::move(recv_done_instr)",
          "old_text": null,
          "new_text": "std::move(recv_done_instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(recv_done_instr), HloOpcode::kRecvDone,",
          "content_same": false
        },
        {
          "line": 4182,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferGetDimensionSizeShape(\n                                         *operand_shape, dimension));\n    // Calling GetDimensionSize on a static dimension returns a constant\n    // instruction.\n    if (operand_shape->is_static_dimension(dimension)) {\n      return ConstantR0<int32_t>(this, operand_shape->dimensions(dimension));\n    }\n    *instr.mutable_shape() = shape.ToProto();\n    instr.add_dimensions(dimension);\n    return AddInstruction(std::move(instr), HloOpcode::kGetDimensionSize,\n                          {operand});\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2135,
          "old_api": null,
          "new_api": "set_custom_call_target",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(\n        HloInstructionProto instr,\n        DynamicConvInstruction(\n            lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\n            dimension_numbers, feature_group_count, batch_group_count,\n            precision_config, padding_type, preferred_element_type));\n\n    instr.set_custom_call_target(\"DynamicConvolutionInputGrad\");\n\n    return AddInstruction(std::move(instr), HloOpcode::kCustomCall,\n                          {input_sizes, lhs, rhs});\n  })",
          "old_line_content": "        HloInstructionProto instr,",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4189,
          "old_api": null,
          "new_api": "is_static_dimension",
          "old_text": null,
          "new_text": "operand_shape->is_static_dimension(dimension)",
          "old_line_content": "    }",
          "new_line_content": "    if (operand_shape->is_static_dimension(dimension)) {",
          "content_same": false
        },
        {
          "line": 2145,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCustomCall,",
          "content_same": false
        },
        {
          "line": 4193,
          "old_api": null,
          "new_api": "add_dimensions",
          "old_text": null,
          "new_text": "instr.add_dimensions(dimension)",
          "old_line_content": "                          {operand});",
          "new_line_content": "    instr.add_dimensions(dimension);",
          "content_same": false
        },
        {
          "line": 4194,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kGetDimensionSize,",
          "content_same": false
        },
        {
          "line": 4200,
          "old_api": null,
          "new_api": "set_dynamic_dimension",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n\n    Shape shape = *operand_shape;\n    shape.set_dynamic_dimension(dimension, false);\n    // Setting an op's dynamic dimension to its static size removes the dynamic\n    // dimension.\n    XlaOp static_size =\n        ConstantR0<int32_t>(this, operand_shape->dimensions(dimension));\n    return SetDimensionSizeInternal(shape, operand, static_size, dimension);\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4204,
          "old_api": null,
          "new_api": "set_dynamic_dimension",
          "old_text": null,
          "new_text": "shape.set_dynamic_dimension(dimension, false)",
          "old_line_content": "    // dimension.",
          "new_line_content": "    shape.set_dynamic_dimension(dimension, false);",
          "content_same": false
        },
        {
          "line": 2160,
          "old_api": null,
          "new_api": "set_custom_call_target",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(\n        HloInstructionProto instr,\n        DynamicConvInstruction(activations, gradients, window_strides, padding,\n                               lhs_dilation, rhs_dilation, dimension_numbers,\n                               feature_group_count, batch_group_count,\n                               precision_config, padding_type,\n                               preferred_element_type));\n\n    instr.set_custom_call_target(\"DynamicConvolutionKernelGrad\");\n    // The gradient of kernel has kernel shape and shouldn't have any dynamic\n    // sizes.\n    instr.mutable_shape()->clear_is_dynamic_dimension();\n    return AddInstruction(std::move(instr), HloOpcode::kCustomCall,\n                          {activations, gradients});\n  })",
          "old_line_content": "        HloInstructionProto instr,",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4208,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "operand_shape->dimensions(dimension)",
          "old_line_content": "  });",
          "new_line_content": "        ConstantR0<int32_t>(this, operand_shape->dimensions(dimension));",
          "content_same": false
        },
        {
          "line": 4209,
          "old_api": null,
          "new_api": "SetDimensionSizeInternal",
          "old_text": null,
          "new_text": "SetDimensionSizeInternal(shape, operand, static_size, dimension)",
          "old_line_content": "}",
          "new_line_content": "    return SetDimensionSizeInternal(shape, operand, static_size, dimension);",
          "content_same": false
        },
        {
          "line": 4215,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* val_shape, GetShapePtr(val));\n\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferSetDimensionSizeShape(\n                            *operand_shape, *val_shape, dimension));\n    return SetDimensionSizeInternal(shape, operand, val, dimension);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* val_shape, GetShapePtr(val));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2169,
          "old_api": null,
          "new_api": "set_custom_call_target",
          "old_text": null,
          "new_text": "instr.set_custom_call_target(\"DynamicConvolutionKernelGrad\")",
          "old_line_content": "    // sizes.",
          "new_line_content": "    instr.set_custom_call_target(\"DynamicConvolutionKernelGrad\");",
          "content_same": false
        },
        {
          "line": 2172,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "instr.mutable_shape()->clear_is_dynamic_dimension()",
          "old_line_content": "                          {activations, gradients});",
          "new_line_content": "    instr.mutable_shape()->clear_is_dynamic_dimension();",
          "content_same": false
        },
        {
          "line": 2173,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCustomCall,",
          "content_same": false
        },
        {
          "line": 4222,
          "old_api": null,
          "new_api": "SetDimensionSizeInternal",
          "old_text": null,
          "new_text": "SetDimensionSizeInternal(shape, operand, val, dimension)",
          "old_line_content": "}",
          "new_line_content": "    return SetDimensionSizeInternal(shape, operand, val, dimension);",
          "content_same": false
        },
        {
          "line": 2187,
          "old_api": null,
          "new_api": "set_custom_call_target",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(\n        HloInstructionProto instr,\n        DynamicConvInstruction(\n            lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\n            dimension_numbers, feature_group_count, batch_group_count,\n            precision_config, padding_type, preferred_element_type));\n    instr.set_custom_call_target(\"DynamicConvolutionForward\");\n\n    return AddInstruction(std::move(instr), HloOpcode::kCustomCall, {lhs, rhs});\n  })",
          "old_line_content": "        HloInstructionProto instr,",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4240,
          "old_api": null,
          "new_api": "add_dimensions",
          "old_text": null,
          "new_text": "instr.add_dimensions(dimension)",
          "old_line_content": "                        {operand, val});",
          "new_line_content": "  instr.add_dimensions(dimension);",
          "content_same": false
        },
        {
          "line": 4241,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kSetDimensionSize,",
          "content_same": false
        },
        {
          "line": 2196,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCustomCall, {lhs, rhs});",
          "content_same": false
        },
        {
          "line": 4246,
          "old_api": null,
          "new_api": "TF_RETURN_IF_ERROR",
          "old_text": null,
          "new_text": "TF_RETURN_IF_ERROR(first_error_)",
          "old_line_content": "  // Verify that the handle is valid.",
          "new_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "content_same": false
        },
        {
          "line": 4249,
          "old_api": null,
          "new_api": "status",
          "old_text": null,
          "new_text": "LookUpInstruction(operand).status()",
          "old_line_content": "  bool is_constant = true;",
          "new_line_content": "  TF_RETURN_IF_ERROR(LookUpInstruction(operand).status());",
          "content_same": false
        },
        {
          "line": 4253,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "operand.handle()",
          "old_line_content": "}",
          "new_line_content": "  IsConstantVisitor(operand.handle(), /*depth=*/0, &visited, &is_constant);",
          "content_same": false
        },
        {
          "line": 2214,
          "old_api": null,
          "new_api": "set_feature_group_count",
          "old_text": null,
          "new_text": "instr.set_feature_group_count(feature_group_count)",
          "old_line_content": "",
          "new_line_content": "  instr.set_feature_group_count(feature_group_count);",
          "content_same": false
        },
        {
          "line": 2215,
          "old_api": null,
          "new_api": "set_batch_group_count",
          "old_text": null,
          "new_text": "instr.set_batch_group_count(batch_group_count)",
          "old_line_content": "  if (precision_config != nullptr) {",
          "new_line_content": "  instr.set_batch_group_count(batch_group_count);",
          "content_same": false
        },
        {
          "line": 4263,
          "old_api": null,
          "new_api": "value",
          "old_text": null,
          "new_text": "op_status.value()->name()",
          "old_line_content": "        \"Operand to BuildConstantSubGraph depends on a parameter.\\n\\n\"",
          "new_line_content": "        op_status.ok() ? op_status.value()->name() : \"<unknown operation>\";",
          "content_same": false
        },
        {
          "line": 4264,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n        \"Operand to BuildConstantSubGraph depends on a parameter.\\n\\n\"\n        \"  op requested for constant subgraph: %s\\n\\n\"\n        \"This is an internal error that typically happens when the XLA user \"\n        \"(e.g. TensorFlow) is attempting to determine a value that must be a \"\n        \"compile-time constant (e.g. an array dimension) but it is not capable \"\n        \"of being evaluated at XLA compile time.\\n\\n\"\n        \"Please file a usability bug with the framework being used (e.g. \"\n        \"TensorFlow).\",\n        op_string)",
          "old_line_content": "        \"  op requested for constant subgraph: %s\\n\\n\"",
          "new_line_content": "    return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 2218,
          "old_api": null,
          "new_api": "mutable_precision_config",
          "old_text": null,
          "new_text": "instr.mutable_precision_config()",
          "old_line_content": "",
          "new_line_content": "    *instr.mutable_precision_config() = *precision_config;",
          "content_same": false
        },
        {
          "line": 2221,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kConvolution, {lhs, rhs});",
          "content_same": false
        },
        {
          "line": 2226,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferFftShape(\n                                         *operand_shape, fft_type, fft_length));\n    return FftInternal(shape, operand, fft_type, fft_length);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferFftShape(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2230,
          "old_api": null,
          "new_api": "FftInternal",
          "old_text": null,
          "new_text": "FftInternal(shape, operand, fft_type, fft_length)",
          "old_line_content": "}",
          "new_line_content": "    return FftInternal(shape, operand, fft_type, fft_length);",
          "content_same": false
        },
        {
          "line": 4278,
          "old_api": null,
          "new_api": "VLOG_IS_ON",
          "old_text": null,
          "new_text": "VLOG_IS_ON(4)",
          "old_line_content": "  }",
          "new_line_content": "  if (VLOG_IS_ON(4)) {",
          "content_same": false
        },
        {
          "line": 4279,
          "old_api": null,
          "new_api": "OpToString",
          "old_text": null,
          "new_text": "OpToString(root_op)",
          "old_line_content": "",
          "new_line_content": "    VLOG(4) << \"Build constant subgraph for:\\n\" << OpToString(root_op);",
          "content_same": false
        },
        {
          "line": 4285,
          "old_api": null,
          "new_api": "mutable_program_shape",
          "old_text": null,
          "new_text": "entry.mutable_program_shape()",
          "old_line_content": "",
          "new_line_content": "  ProgramShapeProto* program_shape = entry.mutable_program_shape();",
          "content_same": false
        },
        {
          "line": 2238,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "  for (int64_t i : fft_length) {",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4286,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "root->shape()",
          "old_line_content": "  // We use std::set to keep the instruction ids in ascending order (which is",
          "new_line_content": "  *program_shape->mutable_result() = root->shape();",
          "content_same": false
        },
        {
          "line": 2241,
          "old_api": null,
          "new_api": "add_fft_length",
          "old_text": null,
          "new_text": "instr.add_fft_length(i)",
          "old_line_content": "",
          "new_line_content": "    instr.add_fft_length(i);",
          "content_same": false
        },
        {
          "line": 2244,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kFft, {operand});",
          "content_same": false
        },
        {
          "line": 4295,
          "old_api": null,
          "new_api": "id",
          "old_text": null,
          "new_text": "root->id()",
          "old_line_content": "",
          "new_line_content": "  worklist.push(root->id());",
          "content_same": false
        },
        {
          "line": 2250,
          "old_api": null,
          "new_api": "std::move(options)",
          "old_text": null,
          "new_text": "std::move(options)",
          "old_line_content": "",
          "new_line_content": "  *instr.mutable_triangular_solve_options() = std::move(options);",
          "content_same": false
        },
        {
          "line": 4299,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "worklist.front()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const HloInstructionProto* instr_proto,",
          "new_line_content": "    int64_t handle = worklist.front();",
          "content_same": false
        },
        {
          "line": 4300,
          "old_api": null,
          "new_api": "pop",
          "old_text": null,
          "new_text": "worklist.pop()",
          "old_line_content": "                        LookUpInstructionByHandle(handle));",
          "new_line_content": "    worklist.pop();",
          "content_same": false
        },
        {
          "line": 2253,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kTriangularSolve, {a, b});",
          "content_same": false
        },
        {
          "line": 4307,
          "old_api": null,
          "new_api": "insert",
          "old_text": null,
          "new_text": "related_ops.insert(id)",
          "old_line_content": "        }",
          "new_line_content": "        if (related_ops.insert(id).second) {",
          "content_same": false
        },
        {
          "line": 2260,
          "old_api": null,
          "new_api": "set_lower",
          "old_text": null,
          "new_text": "options.set_lower(lower)",
          "old_line_content": "",
          "new_line_content": "  options.set_lower(lower);",
          "content_same": false
        },
        {
          "line": 4308,
          "old_api": null,
          "new_api": "push",
          "old_text": null,
          "new_text": "worklist.push(id)",
          "old_line_content": "      }",
          "new_line_content": "          worklist.push(id);",
          "content_same": false
        },
        {
          "line": 2263,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kCholesky, {a});",
          "content_same": false
        },
        {
          "line": 4311,
          "old_api": null,
          "new_api": "called_computation_ids",
          "old_text": null,
          "new_text": "instr_proto->called_computation_ids()",
          "old_line_content": "      }",
          "new_line_content": "      for (int64_t called_id : instr_proto->called_computation_ids()) {",
          "content_same": false
        },
        {
          "line": 4312,
          "old_api": null,
          "new_api": "insert",
          "old_text": null,
          "new_text": "related_calls.insert(called_id)",
          "old_line_content": "    };",
          "new_line_content": "        related_calls.insert(called_id);",
          "content_same": false
        },
        {
          "line": 2269,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(shape)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(shape)",
          "old_line_content": "    }",
          "new_line_content": "    if (!LayoutUtil::HasLayout(shape)) {",
          "content_same": false
        },
        {
          "line": 2270,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Given shape to Infeed must have a layout\")",
          "old_line_content": "    const Shape infeed_instruction_shape =",
          "new_line_content": "      return InvalidArgument(\"Given shape to Infeed must have a layout\");",
          "content_same": false
        },
        {
          "line": 4317,
          "old_api": null,
          "new_api": "HloOpcodeString",
          "old_text": null,
          "new_text": "HloOpcodeString(HloOpcode::kGetDimensionSize)",
          "old_line_content": "      int32_t constant_value = -1;",
          "new_line_content": "            HloOpcodeString(HloOpcode::kGetDimensionSize) ||",
          "content_same": false
        },
        {
          "line": 4318,
          "old_api": null,
          "new_api": "InstrIsSetBound",
          "old_text": null,
          "new_text": "InstrIsSetBound(instr_proto)",
          "old_line_content": "      HloInstructionProto const_instr;",
          "new_line_content": "        InstrIsSetBound(instr_proto)) {",
          "content_same": false
        },
        {
          "line": 2274,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "infeed_instruction_shape.ToProto()",
          "old_line_content": "",
          "new_line_content": "    *instr.mutable_shape() = infeed_instruction_shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4322,
          "old_api": null,
          "new_api": "opcode",
          "old_text": null,
          "new_text": "instr_proto->opcode()",
          "old_line_content": "        // At this point, BuildConstantSubGraph should never encounter a",
          "new_line_content": "      if (instr_proto->opcode() ==",
          "content_same": false
        },
        {
          "line": 4323,
          "old_api": null,
          "new_api": "HloOpcodeString",
          "old_text": null,
          "new_text": "HloOpcodeString(HloOpcode::kGetDimensionSize)",
          "old_line_content": "        // GetDimensionSize with a dynamic dimension. IsConstant check would",
          "new_line_content": "          HloOpcodeString(HloOpcode::kGetDimensionSize)) {",
          "content_same": false
        },
        {
          "line": 2277,
          "old_api": null,
          "new_api": "sharding",
          "old_text": null,
          "new_text": "sharding()",
          "old_line_content": "      // TODO(b/110793772): Support tiled array-shaped infeeds.",
          "new_line_content": "    if (shape.IsArray() && sharding() &&",
          "content_same": false
        },
        {
          "line": 2280,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Tiled sharding is not yet supported for array-shaped infeeds\")",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4330,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "instr_proto->dimensions(0)",
          "old_line_content": "        TF_ASSIGN_OR_RETURN(const HloInstructionProto* operand_proto,",
          "new_line_content": "        int64_t dimension = instr_proto->dimensions(0);",
          "content_same": false
        },
        {
          "line": 4331,
          "old_api": null,
          "new_api": "operand_ids",
          "old_text": null,
          "new_text": "instr_proto->operand_ids(0)",
          "old_line_content": "                            LookUpInstructionByHandle(operand_handle));",
          "new_line_content": "        int64_t operand_handle = instr_proto->operand_ids(0);",
          "content_same": false
        },
        {
          "line": 2284,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "sharding()->type()",
          "old_line_content": "          \"Replicated sharding is not yet supported for infeeds\");",
          "new_line_content": "    if (sharding() && sharding()->type() == OpSharding::REPLICATED) {",
          "content_same": false
        },
        {
          "line": 2285,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Replicated sharding is not yet supported for infeeds\")",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4337,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "static_cast<int32_t>(\n              operand_proto->shape().dimensions(dimension))",
          "old_line_content": "        }",
          "new_line_content": "          constant_value = static_cast<int32_t>(",
          "content_same": false
        },
        {
          "line": 4341,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "literal.ToProto()",
          "old_line_content": "      } else {",
          "new_line_content": "        *const_instr.mutable_literal() = literal.ToProto();",
          "content_same": false
        },
        {
          "line": 2294,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "    };",
          "new_line_content": "      *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 4344,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "instr_proto->literal().shape().element_type()",
          "old_line_content": "              // First literal of SetBound contains bounds, second literal",
          "new_line_content": "        if (instr_proto->literal().shape().element_type() == TUPLE) {",
          "content_same": false
        },
        {
          "line": 4345,
          "old_api": null,
          "new_api": "mutable_literal",
          "old_text": null,
          "new_text": "const_instr.mutable_literal()",
          "old_line_content": "              // contains dynamism indicators.",
          "new_line_content": "          *const_instr.mutable_literal() =",
          "content_same": false
        },
        {
          "line": 4350,
          "old_api": null,
          "new_api": "literal",
          "old_text": null,
          "new_text": "instr_proto->literal()",
          "old_line_content": "",
          "new_line_content": "          *const_instr.mutable_literal() = instr_proto->literal();",
          "content_same": false
        },
        {
          "line": 2303,
          "old_api": null,
          "new_api": "make_token",
          "old_text": null,
          "new_text": "make_token()",
          "old_line_content": "",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(token, make_token());",
          "content_same": false
        },
        {
          "line": 2311,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "sharding()->type()",
          "old_line_content": "      // tokens.",
          "new_line_content": "    if (sharding() && sharding()->type() == OpSharding::TUPLE) {",
          "content_same": false
        },
        {
          "line": 4359,
          "old_api": null,
          "new_api": "id",
          "old_text": null,
          "new_text": "const_instr.id()",
          "old_line_content": "          const_instr;  // Add to the result constant graph.",
          "new_line_content": "          GetFullName(const_instr.opcode(), kNameSeparator, const_instr.id());",
          "content_same": false
        },
        {
          "line": 4360,
          "old_api": null,
          "new_api": "add_instructions",
          "old_text": null,
          "new_text": "entry.add_instructions()",
          "old_line_content": "",
          "new_line_content": "      *entry.add_instructions() =",
          "content_same": false
        },
        {
          "line": 4363,
          "old_api": null,
          "new_api": "opcode",
          "old_text": null,
          "new_text": "instr_proto->opcode()",
          "old_line_content": "      // Look through GTE(Tuple(..), i).",
          "new_line_content": "    } else if (instr_proto->opcode() ==",
          "content_same": false
        },
        {
          "line": 2316,
          "old_api": null,
          "new_api": "add_tuple_shardings",
          "old_text": null,
          "new_text": "infeed_instruction_sharding.add_tuple_shardings()",
          "old_line_content": "      XlaScopedShardingAssignment scoped_sharding(this,",
          "new_line_content": "      *infeed_instruction_sharding.add_tuple_shardings() =",
          "content_same": false
        },
        {
          "line": 2317,
          "old_api": null,
          "new_api": "sharding_builder::AssignDevice(0)",
          "old_text": null,
          "new_text": "sharding_builder::AssignDevice(0)",
          "old_line_content": "                                                  infeed_instruction_sharding);",
          "new_line_content": "          sharding_builder::AssignDevice(0);",
          "content_same": false
        },
        {
          "line": 4364,
          "old_api": null,
          "new_api": "HloOpcodeString",
          "old_text": null,
          "new_text": "HloOpcodeString(HloOpcode::kGetTupleElement)",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(",
          "new_line_content": "               HloOpcodeString(HloOpcode::kGetTupleElement)) {",
          "content_same": false
        },
        {
          "line": 2320,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "    } else {",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(infeed, AddInstruction(std::move(instr),",
          "content_same": false
        },
        {
          "line": 4370,
          "old_api": null,
          "new_api": "HloOpcodeString",
          "old_text": null,
          "new_text": "HloOpcodeString(HloOpcode::kTuple)",
          "old_line_content": "        // Enqueue any dependencies of `id`.",
          "new_line_content": "      if (maybe_tuple_instr->opcode() == HloOpcodeString(HloOpcode::kTuple)) {",
          "content_same": false
        },
        {
          "line": 2323,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "    }",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(infeed, AddInstruction(std::move(instr),",
          "content_same": false
        },
        {
          "line": 4373,
          "old_api": null,
          "new_api": "insert",
          "old_text": null,
          "new_text": "related_ops.insert(id)",
          "old_line_content": "        }",
          "new_line_content": "        if (related_ops.insert(id).second) {",
          "content_same": false
        },
        {
          "line": 4374,
          "old_api": null,
          "new_api": "push",
          "old_text": null,
          "new_text": "worklist.push(id)",
          "old_line_content": "        substitutions[handle] = id;",
          "new_line_content": "          worklist.push(id);",
          "content_same": false
        },
        {
          "line": 4379,
          "old_api": null,
          "new_api": "default_behavior",
          "old_text": null,
          "new_text": "default_behavior()",
          "old_line_content": "",
          "new_line_content": "        default_behavior();",
          "content_same": false
        },
        {
          "line": 2333,
          "old_api": null,
          "new_api": "set_tuple_index",
          "old_text": null,
          "new_text": "infeed_data.set_tuple_index(0)",
          "old_line_content": "                          {infeed});",
          "new_line_content": "    infeed_data.set_tuple_index(0);",
          "content_same": false
        },
        {
          "line": 2334,
          "old_api": null,
          "new_api": "std::move(infeed_data)",
          "old_text": null,
          "new_text": "std::move(infeed_data)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(infeed_data), HloOpcode::kGetTupleElement,",
          "content_same": false
        },
        {
          "line": 4383,
          "old_api": null,
          "new_api": "default_behavior",
          "old_text": null,
          "new_text": "default_behavior()",
          "old_line_content": "  }",
          "new_line_content": "      default_behavior();",
          "content_same": false
        },
        {
          "line": 4389,
          "old_api": null,
          "new_api": "find",
          "old_text": null,
          "new_text": "substitutions.find(root_id)",
          "old_line_content": "    root_id = it->second;",
          "new_line_content": "  auto it = substitutions.find(root_id);",
          "content_same": false
        },
        {
          "line": 2342,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(shape)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(shape)",
          "old_line_content": "    }",
          "new_line_content": "    if (!LayoutUtil::HasLayout(shape)) {",
          "content_same": false
        },
        {
          "line": 2343,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Given shape to Infeed must have a layout\")",
          "old_line_content": "    const Shape infeed_instruction_shape =",
          "new_line_content": "      return InvalidArgument(\"Given shape to Infeed must have a layout\");",
          "content_same": false
        },
        {
          "line": 4394,
          "old_api": null,
          "new_api": "set_root_id",
          "old_text": null,
          "new_text": "entry.set_root_id(root_id)",
          "old_line_content": "  // Add related ops to the computation.",
          "new_line_content": "  entry.set_root_id(root_id);",
          "content_same": false
        },
        {
          "line": 2348,
          "old_api": null,
          "new_api": "sharding",
          "old_text": null,
          "new_text": "sharding()",
          "old_line_content": "      // TODO(b/110793772): Support tiled array-shaped infeeds.",
          "new_line_content": "    if (shape.IsArray() && sharding() &&",
          "content_same": false
        },
        {
          "line": 4398,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "substitutions.end()",
          "old_line_content": "      // substitution instruction's id.",
          "new_line_content": "    if (substitutions.find(id) != substitutions.end()) {",
          "content_same": false
        },
        {
          "line": 2351,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Tiled sharding is not yet supported for array-shaped infeeds\")",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 2355,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "sharding()->type()",
          "old_line_content": "          \"Replicated sharding is not yet supported for infeeds\");",
          "new_line_content": "    if (sharding() && sharding()->type() == OpSharding::REPLICATED) {",
          "content_same": false
        },
        {
          "line": 2356,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Replicated sharding is not yet supported for infeeds\")",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4406,
          "old_api": null,
          "new_api": "HloOpcodeString",
          "old_text": null,
          "new_text": "HloOpcodeString(HloOpcode::kGetDimensionSize)",
          "old_line_content": "      continue;",
          "new_line_content": "    if (instr_src->opcode() == HloOpcodeString(HloOpcode::kGetDimensionSize) ||",
          "content_same": false
        },
        {
          "line": 2359,
          "old_api": null,
          "new_api": "InfeedWithTokenInternal",
          "old_text": null,
          "new_text": "InfeedWithTokenInternal(infeed_instruction_shape, token, config)",
          "old_line_content": "}",
          "new_line_content": "    return InfeedWithTokenInternal(infeed_instruction_shape, token, config);",
          "content_same": false
        },
        {
          "line": 4407,
          "old_api": null,
          "new_api": "InstrIsSetBound",
          "old_text": null,
          "new_text": "InstrIsSetBound(instr_src)",
          "old_line_content": "    }",
          "new_line_content": "        InstrIsSetBound(instr_src)) {",
          "content_same": false
        },
        {
          "line": 4410,
          "old_api": null,
          "new_api": "add_instructions",
          "old_text": null,
          "new_text": "entry.add_instructions()",
          "old_line_content": "    // Replace operands in case we have substitutions mapped.",
          "new_line_content": "    HloInstructionProto* instr = entry.add_instructions();",
          "content_same": false
        },
        {
          "line": 4415,
          "old_api": null,
          "new_api": "find",
          "old_text": null,
          "new_text": "substitutions.find(operand_id)",
          "old_line_content": "        operand_id = it->second;",
          "new_line_content": "      auto it = substitutions.find(operand_id);",
          "content_same": false
        },
        {
          "line": 2368,
          "old_api": null,
          "new_api": "set_infeed_config",
          "old_text": null,
          "new_text": "instr.set_infeed_config(config)",
          "old_line_content": "}",
          "new_line_content": "  instr.set_infeed_config(config);",
          "content_same": false
        },
        {
          "line": 2369,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kInfeed, {token});",
          "content_same": false
        },
        {
          "line": 4420,
          "old_api": null,
          "new_api": "add_operand_ids",
          "old_text": null,
          "new_text": "instr->add_operand_ids(operand_id)",
          "old_line_content": "    // Ensures that the instruction names are unique among the graph.",
          "new_line_content": "      instr->add_operand_ids(operand_id);",
          "content_same": false
        },
        {
          "line": 2374,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n\n    // Check and set outfeed shape.\n    if (!LayoutUtil::HasLayout(shape_with_layout)) {\n      return InvalidArgument(\"Given shape to Outfeed must have a layout\");\n    }\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {\n      return InvalidArgument(\n          \"Outfeed shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape));\n    }\n    *instr.mutable_outfeed_shape() = shape_with_layout.ToProto();\n\n    instr.set_outfeed_config(outfeed_config);\n\n    // Outfeed takes a token as its second operand. Generate the token to pass\n    // to the outfeed.\n    XlaOp token;\n    auto make_token = [&]() {\n      HloInstructionProto token_instr;\n      *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n      return AddInstruction(std::move(token_instr), HloOpcode::kAfterAll, {});\n    };\n    auto make_outfeed = [&](XlaOp token) {\n      return AddInstruction(std::move(instr), HloOpcode::kOutfeed,\n                            {operand, token});\n    };\n    if (sharding()) {\n      XlaScopedShardingAssignment scoped_sharding(\n          this, sharding_builder::AssignDevice(0));\n      TF_ASSIGN_OR_RETURN(token, make_token());\n    } else {\n      TF_ASSIGN_OR_RETURN(token, make_token());\n    }\n    if (sharding()) {\n      OpSharding tuple_sharding = *sharding();\n      if (tuple_sharding.type() != OpSharding::TUPLE) {\n        tuple_sharding = sharding_builder::Tuple({});\n        *tuple_sharding.add_tuple_shardings() = *sharding();\n      }\n      *tuple_sharding.add_tuple_shardings() = sharding_builder::AssignDevice(0);\n      XlaScopedShardingAssignment scoped_sharding(this, tuple_sharding);\n      TF_RETURN_IF_ERROR(make_outfeed(token).status());\n    } else {\n      TF_RETURN_IF_ERROR(make_outfeed(token).status());\n    }\n    // The outfeed instruction produces a token. However, existing users expect\n    // a nil shape (empty tuple). This should only be relevant if the outfeed is\n    // the root of a computation.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto tuple_instr;\n    *tuple_instr.mutable_shape() = ShapeUtil::MakeNil().ToProto();\n\n    // The dummy tuple should have no sharding.\n    {\n      XlaScopedShardingAssignment scoped_sharding(this, std::nullopt);\n      TF_ASSIGN_OR_RETURN(\n          XlaOp empty_tuple,\n          AddInstruction(std::move(tuple_instr), HloOpcode::kTuple, {}));\n      return empty_tuple;\n    }\n  })",
          "old_line_content": "",
          "new_line_content": "  ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4424,
          "old_api": null,
          "new_api": "id",
          "old_text": null,
          "new_text": "instr->id()",
          "old_line_content": "  }",
          "new_line_content": "        StrCat(instr->name(), \".\", entry.id(), \".\", instr->id());",
          "content_same": false
        },
        {
          "line": 2377,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "    // Check and set outfeed shape.",
          "new_line_content": "    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 4425,
          "old_api": null,
          "new_api": "set_name",
          "old_text": null,
          "new_text": "instr->set_name(new_name)",
          "old_line_content": "",
          "new_line_content": "    instr->set_name(new_name);",
          "content_same": false
        },
        {
          "line": 2380,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(shape_with_layout)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(shape_with_layout)",
          "old_line_content": "    }",
          "new_line_content": "    if (!LayoutUtil::HasLayout(shape_with_layout)) {",
          "content_same": false
        },
        {
          "line": 2381,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Given shape to Outfeed must have a layout\")",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "      return InvalidArgument(\"Given shape to Outfeed must have a layout\");",
          "content_same": false
        },
        {
          "line": 2384,
          "old_api": null,
          "new_api": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "old_text": null,
          "new_text": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "old_line_content": "          \"Outfeed shape %s must be compatible with operand shape %s\",",
          "new_line_content": "    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {",
          "content_same": false
        },
        {
          "line": 4433,
          "old_api": null,
          "new_api": "id",
          "old_text": null,
          "new_text": "entry.id()",
          "old_line_content": "  for (auto& e : embedded_) {",
          "new_line_content": "  module->set_entry_computation_id(entry.id());",
          "content_same": false
        },
        {
          "line": 2387,
          "old_api": null,
          "new_api": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "old_line_content": "    }",
          "new_line_content": "          ShapeUtil::HumanStringWithLayout(shape_with_layout),",
          "content_same": false
        },
        {
          "line": 4436,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "related_calls.end()",
          "old_line_content": "    }",
          "new_line_content": "    if (related_calls.find(e.second.id()) != related_calls.end()) {",
          "content_same": false
        },
        {
          "line": 4437,
          "old_api": null,
          "new_api": "add_computations",
          "old_text": null,
          "new_text": "module->add_computations()",
          "old_line_content": "  }",
          "new_line_content": "      *module->add_computations() = e.second;",
          "content_same": false
        },
        {
          "line": 2392,
          "old_api": null,
          "new_api": "set_outfeed_config",
          "old_text": null,
          "new_text": "instr.set_outfeed_config(outfeed_config)",
          "old_line_content": "    // Outfeed takes a token as its second operand. Generate the token to pass",
          "new_line_content": "    instr.set_outfeed_config(outfeed_config);",
          "content_same": false
        },
        {
          "line": 4441,
          "old_api": null,
          "new_api": "VLOG_IS_ON",
          "old_text": null,
          "new_text": "VLOG_IS_ON(4)",
          "old_line_content": "  }",
          "new_line_content": "  if (VLOG_IS_ON(4)) {",
          "content_same": false
        },
        {
          "line": 4444,
          "old_api": null,
          "new_api": "std::move(computation)",
          "old_text": null,
          "new_text": "std::move(computation)",
          "old_line_content": "",
          "new_line_content": "  return std::move(computation);",
          "content_same": false
        },
        {
          "line": 2399,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "    };",
          "new_line_content": "      *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 2400,
          "old_api": null,
          "new_api": "std::move(token_instr)",
          "old_text": null,
          "new_text": "std::move(token_instr)",
          "old_line_content": "    auto make_outfeed = [&](XlaOp token) {",
          "new_line_content": "      return AddInstruction(std::move(token_instr), HloOpcode::kAfterAll, {});",
          "content_same": false
        },
        {
          "line": 4449,
          "old_api": null,
          "new_api": "std::make_unique<XlaBuilder>(computation_name)",
          "old_text": null,
          "new_text": "std::make_unique<XlaBuilder>(computation_name)",
          "old_line_content": "  sub_builder->die_immediately_on_error_ = this->die_immediately_on_error_;",
          "new_line_content": "  auto sub_builder = std::make_unique<XlaBuilder>(computation_name);",
          "content_same": false
        },
        {
          "line": 2403,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "    };",
          "new_line_content": "      return AddInstruction(std::move(instr), HloOpcode::kOutfeed,",
          "content_same": false
        },
        {
          "line": 2408,
          "old_api": null,
          "new_api": "sharding_builder::AssignDevice(0)",
          "old_text": null,
          "new_text": "sharding_builder::AssignDevice(0)",
          "old_line_content": "    } else {",
          "new_line_content": "          this, sharding_builder::AssignDevice(0));",
          "content_same": false
        },
        {
          "line": 4461,
          "old_api": null,
          "new_api": "set_output_feature_dimension",
          "old_text": null,
          "new_text": "dimension_numbers.set_output_feature_dimension(kConvFeatureDimension)",
          "old_line_content": "      kConvKernelOutputDimension);",
          "new_line_content": "  dimension_numbers.set_output_feature_dimension(kConvFeatureDimension);",
          "content_same": false
        },
        {
          "line": 2416,
          "old_api": null,
          "new_api": "sharding_builder::Tuple({})",
          "old_text": null,
          "new_text": "sharding_builder::Tuple({})",
          "old_line_content": "      }",
          "new_line_content": "        tuple_sharding = sharding_builder::Tuple({});",
          "content_same": false
        },
        {
          "line": 4464,
          "old_api": null,
          "new_api": "set_kernel_input_feature_dimension",
          "old_text": null,
          "new_text": "dimension_numbers.set_kernel_input_feature_dimension(\n      kConvKernelInputDimension)",
          "old_line_content": "  for (int i = 0; i < num_spatial_dims; ++i) {",
          "new_line_content": "  dimension_numbers.set_kernel_input_feature_dimension(",
          "content_same": false
        },
        {
          "line": 4468,
          "old_api": null,
          "new_api": "add_kernel_spatial_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.add_kernel_spatial_dimensions(i + 2)",
          "old_line_content": "  }",
          "new_line_content": "    dimension_numbers.add_kernel_spatial_dimensions(i + 2);",
          "content_same": false
        },
        {
          "line": 4469,
          "old_api": null,
          "new_api": "add_output_spatial_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.add_output_spatial_dimensions(i + 2)",
          "old_line_content": "  return dimension_numbers;",
          "new_line_content": "    dimension_numbers.add_output_spatial_dimensions(i + 2);",
          "content_same": false
        },
        {
          "line": 2423,
          "old_api": null,
          "new_api": "status",
          "old_text": null,
          "new_text": "make_outfeed(token).status()",
          "old_line_content": "    // The outfeed instruction produces a token. However, existing users expect",
          "new_line_content": "      TF_RETURN_IF_ERROR(make_outfeed(token).status());",
          "content_same": false
        },
        {
          "line": 4477,
          "old_api": null,
          "new_api": "input_spatial_dimensions_size",
          "old_text": null,
          "new_text": "FailedPrecondition(\"input spacial dimension < 2: %d\",\n                              dnum.input_spatial_dimensions_size())",
          "old_line_content": "  }",
          "new_line_content": "    return FailedPrecondition(\"input spacial dimension < 2: %d\",",
          "content_same": false
        },
        {
          "line": 2431,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeNil().ToProto()",
          "old_line_content": "    // The dummy tuple should have no sharding.",
          "new_line_content": "    *tuple_instr.mutable_shape() = ShapeUtil::MakeNil().ToProto();",
          "content_same": false
        },
        {
          "line": 4481,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions_size",
          "old_text": null,
          "new_text": "FailedPrecondition(\"kernel spacial dimension < 2: %d\",\n                              dnum.kernel_spatial_dimensions_size())",
          "old_line_content": "  }",
          "new_line_content": "    return FailedPrecondition(\"kernel spacial dimension < 2: %d\",",
          "content_same": false
        },
        {
          "line": 4485,
          "old_api": null,
          "new_api": "output_spatial_dimensions_size",
          "old_text": null,
          "new_text": "FailedPrecondition(\"output spacial dimension < 2: %d\",\n                              dnum.output_spatial_dimensions_size())",
          "old_line_content": "  }",
          "new_line_content": "    return FailedPrecondition(\"output spacial dimension < 2: %d\",",
          "content_same": false
        },
        {
          "line": 4486,
          "old_api": null,
          "new_api": "output_spatial_dimensions_size",
          "old_text": null,
          "new_text": "dnum.output_spatial_dimensions_size()",
          "old_line_content": "",
          "new_line_content": "                              dnum.output_spatial_dimensions_size());",
          "content_same": false
        },
        {
          "line": 4490,
          "old_api": null,
          "new_api": "input_feature_dimension",
          "old_text": null,
          "new_text": "dnum.input_feature_dimension()",
          "old_line_content": "          .size() != 4) {",
          "new_line_content": "          {dnum.input_batch_dimension(), dnum.input_feature_dimension(),",
          "content_same": false
        },
        {
          "line": 4493,
          "old_api": null,
          "new_api": "input_batch_dimension",
          "old_text": null,
          "new_text": "FailedPrecondition(\n        \"dimension numbers for the input are not unique: (%d, %d, %d, \"\n        \"%d)\",\n        dnum.input_batch_dimension(), dnum.input_feature_dimension(),\n        dnum.input_spatial_dimensions(0), dnum.input_spatial_dimensions(1))",
          "old_line_content": "        \"%d)\",",
          "new_line_content": "    return FailedPrecondition(",
          "content_same": false
        },
        {
          "line": 4496,
          "old_api": null,
          "new_api": "input_feature_dimension",
          "old_text": null,
          "new_text": "dnum.input_feature_dimension()",
          "old_line_content": "  }",
          "new_line_content": "        dnum.input_batch_dimension(), dnum.input_feature_dimension(),",
          "content_same": false
        },
        {
          "line": 2449,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(shape_with_layout)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(shape_with_layout)",
          "old_line_content": "    }",
          "new_line_content": "    if (!LayoutUtil::HasLayout(shape_with_layout)) {",
          "content_same": false
        },
        {
          "line": 2450,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Given shape to Outfeed must have a layout\")",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "      return InvalidArgument(\"Given shape to Outfeed must have a layout\");",
          "content_same": false
        },
        {
          "line": 2453,
          "old_api": null,
          "new_api": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "old_text": null,
          "new_text": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "old_line_content": "          \"Outfeed shape %s must be compatible with operand shape %s\",",
          "new_line_content": "    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {",
          "content_same": false
        },
        {
          "line": 4501,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions",
          "old_text": null,
          "new_text": "dnum.kernel_spatial_dimensions(0)",
          "old_line_content": "          .size() != 4) {",
          "new_line_content": "                         dnum.kernel_spatial_dimensions(0),",
          "content_same": false
        },
        {
          "line": 2456,
          "old_api": null,
          "new_api": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanStringWithLayout(shape_with_layout)",
          "old_line_content": "    }",
          "new_line_content": "          ShapeUtil::HumanStringWithLayout(shape_with_layout),",
          "content_same": false
        },
        {
          "line": 4504,
          "old_api": null,
          "new_api": "kernel_output_feature_dimension",
          "old_text": null,
          "new_text": "FailedPrecondition(\n        \"dimension numbers for the weight are not unique: (%d, %d, %d, \"\n        \"%d)\",\n        dnum.kernel_output_feature_dimension(),\n        dnum.kernel_input_feature_dimension(),\n        dnum.kernel_spatial_dimensions(0), dnum.kernel_spatial_dimensions(1))",
          "old_line_content": "        \"%d)\",",
          "new_line_content": "    return FailedPrecondition(",
          "content_same": false
        },
        {
          "line": 2459,
          "old_api": null,
          "new_api": "OutfeedWithTokenInternal",
          "old_text": null,
          "new_text": "OutfeedWithTokenInternal(operand, token, shape_with_layout,\n                                    outfeed_config)",
          "old_line_content": "  });",
          "new_line_content": "    return OutfeedWithTokenInternal(operand, token, shape_with_layout,",
          "content_same": false
        },
        {
          "line": 4508,
          "old_api": null,
          "new_api": "kernel_input_feature_dimension",
          "old_text": null,
          "new_text": "dnum.kernel_input_feature_dimension()",
          "old_line_content": "  }",
          "new_line_content": "        dnum.kernel_input_feature_dimension(),",
          "content_same": false
        },
        {
          "line": 4513,
          "old_api": null,
          "new_api": "output_spatial_dimensions",
          "old_text": null,
          "new_text": "dnum.output_spatial_dimensions(0)",
          "old_line_content": "          .size() != 4) {",
          "new_line_content": "                         dnum.output_spatial_dimensions(0),",
          "content_same": false
        },
        {
          "line": 4516,
          "old_api": null,
          "new_api": "output_batch_dimension",
          "old_text": null,
          "new_text": "FailedPrecondition(\n        \"dimension numbers for the output are not unique: (%d, %d, %d, \"\n        \"%d)\",\n        dnum.output_batch_dimension(), dnum.output_feature_dimension(),\n        dnum.output_spatial_dimensions(0), dnum.output_spatial_dimensions(1))",
          "old_line_content": "        \"%d)\",",
          "new_line_content": "    return FailedPrecondition(",
          "content_same": false
        },
        {
          "line": 2470,
          "old_api": null,
          "new_api": "set_outfeed_config",
          "old_text": null,
          "new_text": "instr.set_outfeed_config(outfeed_config)",
          "old_line_content": "                        {operand, token});",
          "new_line_content": "  instr.set_outfeed_config(outfeed_config);",
          "content_same": false
        },
        {
          "line": 2471,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kOutfeed,",
          "content_same": false
        },
        {
          "line": 4519,
          "old_api": null,
          "new_api": "output_feature_dimension",
          "old_text": null,
          "new_text": "dnum.output_feature_dimension()",
          "old_line_content": "  }",
          "new_line_content": "        dnum.output_batch_dimension(), dnum.output_feature_dimension(),",
          "content_same": false
        },
        {
          "line": 4522,
          "old_api": null,
          "new_api": "OkStatus",
          "old_text": null,
          "new_text": "OkStatus()",
          "old_line_content": "",
          "new_line_content": "  return OkStatus();",
          "content_same": false
        },
        {
          "line": 2478,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "  });",
          "new_line_content": "    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 2479,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kAfterAll);",
          "content_same": false
        },
        {
          "line": 2485,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "tokens.empty()",
          "old_line_content": "    }",
          "new_line_content": "    if (tokens.empty()) {",
          "content_same": false
        },
        {
          "line": 4533,
          "old_api": null,
          "new_api": "name",
          "old_text": null,
          "new_text": "instr.name().empty()",
          "old_line_content": "  }",
          "new_line_content": "  if (instr.name().empty()) {",
          "content_same": false
        },
        {
          "line": 4534,
          "old_api": null,
          "new_api": "opcode",
          "old_text": null,
          "new_text": "instr.opcode()",
          "old_line_content": "  for (const auto& operand : operands) {",
          "new_line_content": "    instr.set_name(instr.opcode());",
          "content_same": false
        },
        {
          "line": 2488,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "tokens.size()",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "    for (int i = 0, end = tokens.size(); i < end; ++i) {",
          "content_same": false
        },
        {
          "line": 4538,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "operand.handle()",
          "old_line_content": "    if (operand.builder_ != this) {",
          "new_line_content": "      return InvalidArgument(\"invalid XlaOp with handle %d\", operand.handle());",
          "content_same": false
        },
        {
          "line": 2491,
          "old_api": null,
          "new_api": "IsToken",
          "old_text": null,
          "new_text": "operand_shape->IsToken()",
          "old_line_content": "            \"All operands to AfterAll must be tokens; operand %d has shape %s\",",
          "new_line_content": "      if (!operand_shape->IsToken()) {",
          "content_same": false
        },
        {
          "line": 4541,
          "old_api": null,
          "new_api": "name",
          "old_text": null,
          "new_text": "InvalidArgument(\"Do not add XlaOp from builder %s to builder %s\",\n                             operand.builder_->name(), this->name())",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(\"Do not add XlaOp from builder %s to builder %s\",",
          "content_same": false
        },
        {
          "line": 2494,
          "old_api": null,
          "new_api": "ShapeUtil::HumanString(*operand_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanString(*operand_shape)",
          "old_line_content": "    }",
          "new_line_content": "            i, ShapeUtil::HumanString(*operand_shape));",
          "content_same": false
        },
        {
          "line": 4544,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "operand.handle()",
          "old_line_content": "",
          "new_line_content": "    instr.add_operand_ids(operand.handle());",
          "content_same": false
        },
        {
          "line": 2498,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "  });",
          "new_line_content": "    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 2499,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kAfterAll, tokens);",
          "content_same": false
        },
        {
          "line": 4548,
          "old_api": null,
          "new_api": "value",
          "old_text": null,
          "new_text": "one_shot_metadata_.value()",
          "old_line_content": "  } else {",
          "new_line_content": "    *instr.mutable_metadata() = one_shot_metadata_.value();",
          "content_same": false
        },
        {
          "line": 4551,
          "old_api": null,
          "new_api": "mutable_metadata",
          "old_text": null,
          "new_text": "instr.mutable_metadata()",
          "old_line_content": "  if (sharding_) {",
          "new_line_content": "    *instr.mutable_metadata() = metadata_;",
          "content_same": false
        },
        {
          "line": 4555,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "instr.shape()",
          "old_line_content": "                        HloSharding::FromProto(*sharding_));",
          "new_line_content": "    Shape shape(instr.shape());",
          "content_same": false
        },
        {
          "line": 4559,
          "old_api": null,
          "new_api": "Validate",
          "old_text": null,
          "new_text": "sharding.Validate(shape)",
          "old_line_content": "  }",
          "new_line_content": "    TF_RETURN_IF_ERROR(sharding.Validate(shape));",
          "content_same": false
        },
        {
          "line": 2514,
          "old_api": null,
          "new_api": "absl::StartsWith(call_target_name, \"$\")",
          "old_text": null,
          "new_text": "absl::StartsWith(call_target_name, \"$\")",
          "old_line_content": "          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"",
          "new_line_content": "    if (absl::StartsWith(call_target_name, \"$\")) {",
          "content_same": false
        },
        {
          "line": 2515,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"\n          \"are reserved for internal use.\",\n          call_target_name)",
          "old_line_content": "          \"are reserved for internal use.\",",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4566,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "instruction_shapes_.push_back(\n      std::make_unique<Shape>(instructions_.back().shape()))",
          "old_line_content": "",
          "new_line_content": "  instruction_shapes_.push_back(",
          "content_same": false
        },
        {
          "line": 4567,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "instructions_.back().shape()",
          "old_line_content": "  XlaOp op(handle, this);",
          "new_line_content": "      std::make_unique<Shape>(instructions_.back().shape()));",
          "content_same": false
        },
        {
          "line": 2521,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(shape)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(shape)",
          "old_line_content": "            \"Result shape must have layout for custom call with constrained \"",
          "new_line_content": "      if (!LayoutUtil::HasLayout(shape)) {",
          "content_same": false
        },
        {
          "line": 2522,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n            \"Result shape must have layout for custom call with constrained \"\n            \"layout.\")",
          "old_line_content": "            \"layout.\");",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 2526,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "operand_shapes_with_layout->size()",
          "old_line_content": "            \"Must specify a shape with layout for each operand for custom call \"",
          "new_line_content": "      if (operands.size() != operand_shapes_with_layout->size()) {",
          "content_same": false
        },
        {
          "line": 2527,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "InvalidArgument(\n            \"Must specify a shape with layout for each operand for custom call \"\n            \"with constrained layout; given %d shapes, expected %d\",\n            operand_shapes_with_layout->size(), operands.size())",
          "old_line_content": "            \"with constrained layout; given %d shapes, expected %d\",",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4576,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "}",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4577,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), opcode, operands);",
          "content_same": false
        },
        {
          "line": 2530,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "operands.size()",
          "old_line_content": "      int64_t operand_num = 0;",
          "new_line_content": "            operand_shapes_with_layout->size(), operands.size());",
          "content_same": false
        },
        {
          "line": 2534,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(operand_shape)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(operand_shape)",
          "old_line_content": "              \"No layout specified for operand %d for custom call with \"",
          "new_line_content": "        if (!LayoutUtil::HasLayout(operand_shape)) {",
          "content_same": false
        },
        {
          "line": 2535,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n              \"No layout specified for operand %d for custom call with \"\n              \"constrained layout.\",\n              operand_num)",
          "old_line_content": "              \"constrained layout.\",",
          "new_line_content": "          return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4584,
          "old_api": null,
          "new_api": "proto",
          "old_text": null,
          "new_text": "computation.proto().computations_size()",
          "old_line_content": "  // old->new mappings in remapped_ids.",
          "new_line_content": "  imported_computations.reserve(computation.proto().computations_size());",
          "content_same": false
        },
        {
          "line": 2543,
          "old_api": null,
          "new_api": "CustomCallInternal",
          "old_text": null,
          "new_text": "CustomCallInternal(\n        call_target_name, operands, /*computation=*/nullptr, shape, opaque,\n        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,\n        literal, window, dnums, schedule, api_version)",
          "old_line_content": "        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,",
          "new_line_content": "    return CustomCallInternal(",
          "content_same": false
        },
        {
          "line": 4591,
          "old_api": null,
          "new_api": "name",
          "old_text": null,
          "new_text": "SetProtoIdAndName(&new_computation,\n                      GetBaseName(new_computation.name(), kNameSeparator),\n                      kNameSeparator, computation_id)",
          "old_line_content": "                      kNameSeparator, computation_id);",
          "new_line_content": "    SetProtoIdAndName(&new_computation,",
          "content_same": false
        },
        {
          "line": 4597,
          "old_api": null,
          "new_api": "name",
          "old_text": null,
          "new_text": "SetProtoIdAndName(&instruction,\n                        GetBaseName(instruction.name(), kNameSeparator),\n                        kNameSeparator, instruction_id)",
          "old_line_content": "                        kNameSeparator, instruction_id);",
          "new_line_content": "      SetProtoIdAndName(&instruction,",
          "content_same": false
        },
        {
          "line": 4598,
          "old_api": null,
          "new_api": "name",
          "old_text": null,
          "new_text": "instruction.name()",
          "old_line_content": "    }",
          "new_line_content": "                        GetBaseName(instruction.name(), kNameSeparator),",
          "content_same": false
        },
        {
          "line": 4603,
          "old_api": null,
          "new_api": "std::move(new_computation)",
          "old_text": null,
          "new_text": "std::move(new_computation)",
          "old_line_content": "  // Once we have imported all the computations, and captured all the ID",
          "new_line_content": "    imported_computations.push_back(std::move(new_computation));",
          "content_same": false
        },
        {
          "line": 4607,
          "old_api": null,
          "new_api": "at",
          "old_text": null,
          "new_text": "instr->add_called_computation_ids(\n      remapped_ids.at(computation.proto().entry_computation_id()))",
          "old_line_content": "  for (auto& imported_computation : imported_computations) {",
          "new_line_content": "  instr->add_called_computation_ids(",
          "content_same": false
        },
        {
          "line": 4611,
          "old_api": null,
          "new_api": "mutable_operand_ids",
          "old_text": null,
          "new_text": "instruction.mutable_operand_ids()",
          "old_line_content": "      }",
          "new_line_content": "      for (auto& operand_id : *instruction.mutable_operand_ids()) {",
          "content_same": false
        },
        {
          "line": 4612,
          "old_api": null,
          "new_api": "at",
          "old_text": null,
          "new_text": "remapped_ids.at(operand_id)",
          "old_line_content": "      for (auto& control_predecessor_id :",
          "new_line_content": "        operand_id = remapped_ids.at(operand_id);",
          "content_same": false
        },
        {
          "line": 4615,
          "old_api": null,
          "new_api": "mutable_control_predecessor_ids",
          "old_text": null,
          "new_text": "instruction.mutable_control_predecessor_ids()",
          "old_line_content": "      }",
          "new_line_content": "           *instruction.mutable_control_predecessor_ids()) {",
          "content_same": false
        },
        {
          "line": 4616,
          "old_api": null,
          "new_api": "at",
          "old_text": null,
          "new_text": "remapped_ids.at(control_predecessor_id)",
          "old_line_content": "      for (auto& called_computation_id :",
          "new_line_content": "        control_predecessor_id = remapped_ids.at(control_predecessor_id);",
          "content_same": false
        },
        {
          "line": 4619,
          "old_api": null,
          "new_api": "mutable_called_computation_ids",
          "old_text": null,
          "new_text": "instruction.mutable_called_computation_ids()",
          "old_line_content": "      }",
          "new_line_content": "           *instruction.mutable_called_computation_ids()) {",
          "content_same": false
        },
        {
          "line": 4620,
          "old_api": null,
          "new_api": "at",
          "old_text": null,
          "new_text": "remapped_ids.at(called_computation_id)",
          "old_line_content": "    }",
          "new_line_content": "        called_computation_id = remapped_ids.at(called_computation_id);",
          "content_same": false
        },
        {
          "line": 4624,
          "old_api": null,
          "new_api": "id",
          "old_text": null,
          "new_text": "imported_computation.id()",
          "old_line_content": "      ImportedInstruction imported_instruction;",
          "new_line_content": "    int64_t computation_id = imported_computation.id();",
          "content_same": false
        },
        {
          "line": 2577,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "operand_shapes_with_layout.has_value()",
          "old_line_content": "    for (const Shape& operand_shape : *operand_shapes_with_layout) {",
          "new_line_content": "  if (operand_shapes_with_layout.has_value()) {",
          "content_same": false
        },
        {
          "line": 4625,
          "old_api": null,
          "new_api": "instructions_size",
          "old_text": null,
          "new_text": "imported_computation.instructions_size()",
          "old_line_content": "      imported_instruction.computation_id = computation_id;",
          "new_line_content": "    for (int64_t i = 0; i < imported_computation.instructions_size(); ++i) {",
          "content_same": false
        },
        {
          "line": 2580,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "operand_shape.ToProto()",
          "old_line_content": "  }",
          "new_line_content": "      *instr.add_operand_shapes_with_layout() = operand_shape.ToProto();",
          "content_same": false
        },
        {
          "line": 4629,
          "old_api": null,
          "new_api": "insert",
          "old_text": null,
          "new_text": "handle_to_imported_index_.insert(\n          {imported_computation.instructions(i).id(), imported_instruction})",
          "old_line_content": "    }",
          "new_line_content": "      handle_to_imported_index_.insert(",
          "content_same": false
        },
        {
          "line": 4632,
          "old_api": null,
          "new_api": "std::move(imported_computation)",
          "old_text": null,
          "new_text": "std::move(imported_computation)",
          "old_line_content": "}",
          "new_line_content": "    embedded_.insert({computation_id, std::move(imported_computation)});",
          "content_same": false
        },
        {
          "line": 2587,
          "old_api": null,
          "new_api": "IsNull",
          "old_text": null,
          "new_text": "computation->IsNull()",
          "old_line_content": "  }",
          "new_line_content": "  if (computation != nullptr && !computation->IsNull()) {",
          "content_same": false
        },
        {
          "line": 2588,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(*computation, &instr)",
          "old_line_content": "  for (const auto& pair : output_operand_aliasing) {",
          "new_line_content": "    AddCalledComputation(*computation, &instr);",
          "content_same": false
        },
        {
          "line": 4638,
          "old_api": null,
          "new_api": "TF_RETURN_IF_ERROR",
          "old_text": null,
          "new_text": "TF_RETURN_IF_ERROR(first_error_)",
          "old_line_content": "}",
          "new_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "content_same": false
        },
        {
          "line": 2591,
          "old_api": null,
          "new_api": "add_output_operand_aliasing",
          "old_text": null,
          "new_text": "instr.add_output_operand_aliasing()",
          "old_line_content": "    for (int64_t index : pair.second.second) {",
          "new_line_content": "    auto aliasing = instr.add_output_operand_aliasing();",
          "content_same": false
        },
        {
          "line": 4639,
          "old_api": null,
          "new_api": "LookUpInstructionInternal<const HloInstructionProto*>(op)",
          "old_text": null,
          "new_text": "LookUpInstructionInternal<const HloInstructionProto*>(op)",
          "old_line_content": "",
          "new_line_content": "  return LookUpInstructionInternal<const HloInstructionProto*>(op);",
          "content_same": false
        },
        {
          "line": 2594,
          "old_api": null,
          "new_api": "add_operand_shape_index",
          "old_text": null,
          "new_text": "aliasing->add_operand_shape_index(index)",
          "old_line_content": "    for (int64_t index : pair.first) {",
          "new_line_content": "      aliasing->add_operand_shape_index(index);",
          "content_same": false
        },
        {
          "line": 4644,
          "old_api": null,
          "new_api": "LookUpInstructionByHandleInternal<const HloInstructionProto*>(handle)",
          "old_text": null,
          "new_text": "LookUpInstructionByHandleInternal<const HloInstructionProto*>(handle)",
          "old_line_content": "",
          "new_line_content": "  return LookUpInstructionByHandleInternal<const HloInstructionProto*>(handle);",
          "content_same": false
        },
        {
          "line": 2597,
          "old_api": null,
          "new_api": "add_output_shape_index",
          "old_text": null,
          "new_text": "aliasing->add_output_shape_index(index)",
          "old_line_content": "  }",
          "new_line_content": "      aliasing->add_output_shape_index(index);",
          "content_same": false
        },
        {
          "line": 2600,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "window.has_value()",
          "old_line_content": "  }",
          "new_line_content": "  if (window.has_value()) {",
          "content_same": false
        },
        {
          "line": 4649,
          "old_api": null,
          "new_api": "TF_RETURN_IF_ERROR",
          "old_text": null,
          "new_text": "TF_RETURN_IF_ERROR(first_error_)",
          "old_line_content": "}",
          "new_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "content_same": false
        },
        {
          "line": 4650,
          "old_api": null,
          "new_api": "LookUpInstructionInternal<HloInstructionProto*>(op)",
          "old_text": null,
          "new_text": "LookUpInstructionInternal<HloInstructionProto*>(op)",
          "old_line_content": "",
          "new_line_content": "  return LookUpInstructionInternal<HloInstructionProto*>(op);",
          "content_same": false
        },
        {
          "line": 2603,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "dnums.has_value()",
          "old_line_content": "  }",
          "new_line_content": "  if (dnums.has_value()) {",
          "content_same": false
        },
        {
          "line": 2607,
          "old_api": null,
          "new_api": "set_custom_call_api_version",
          "old_text": null,
          "new_text": "instr.set_custom_call_api_version(api_version)",
          "old_line_content": "}",
          "new_line_content": "  instr.set_custom_call_api_version(api_version);",
          "content_same": false
        },
        {
          "line": 2608,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kCustomCall, operands);",
          "content_same": false
        },
        {
          "line": 4655,
          "old_api": null,
          "new_api": "LookUpInstructionByHandleInternal<HloInstructionProto*>(handle)",
          "old_text": null,
          "new_text": "LookUpInstructionByHandleInternal<HloInstructionProto*>(handle)",
          "old_line_content": "",
          "new_line_content": "  return LookUpInstructionByHandleInternal<HloInstructionProto*>(handle);",
          "content_same": false
        },
        {
          "line": 4663,
          "old_api": null,
          "new_api": "Parameter",
          "old_text": null,
          "new_text": "Parameter(builder, parameter_number, shape, name, empty_bools)",
          "old_line_content": "",
          "new_line_content": "  return Parameter(builder, parameter_number, shape, name, empty_bools);",
          "content_same": false
        },
        {
          "line": 4669,
          "old_api": null,
          "new_api": "Parameter",
          "old_text": null,
          "new_text": "builder->Parameter(parameter_number, shape, name,\n                            replicated_at_leaf_buffers)",
          "old_line_content": "}",
          "new_line_content": "  return builder->Parameter(parameter_number, shape, name,",
          "content_same": false
        },
        {
          "line": 2622,
          "old_api": null,
          "new_api": "absl::StartsWith(call_target_name, \"$\")",
          "old_text": null,
          "new_text": "absl::StartsWith(call_target_name, \"$\")",
          "old_line_content": "          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"",
          "new_line_content": "    if (absl::StartsWith(call_target_name, \"$\")) {",
          "content_same": false
        },
        {
          "line": 2623,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"\n          \"are reserved for internal use.\",\n          call_target_name)",
          "old_line_content": "          \"are reserved for internal use.\",",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4676,
          "old_api": null,
          "new_api": "ConstantLiteral",
          "old_text": null,
          "new_text": "builder->ConstantLiteral(literal)",
          "old_line_content": "",
          "new_line_content": "  return builder->ConstantLiteral(literal);",
          "content_same": false
        },
        {
          "line": 2629,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(shape)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(shape)",
          "old_line_content": "            \"Result shape must have layout for custom call with constrained \"",
          "new_line_content": "      if (!LayoutUtil::HasLayout(shape)) {",
          "content_same": false
        },
        {
          "line": 2630,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n            \"Result shape must have layout for custom call with constrained \"\n            \"layout.\")",
          "old_line_content": "            \"layout.\");",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4681,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Broadcast(operand, broadcast_sizes)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Broadcast(operand, broadcast_sizes);",
          "content_same": false
        },
        {
          "line": 2634,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "operand_shapes_with_layout->size()",
          "old_line_content": "            \"Must specify a shape with layout for each operand for custom call \"",
          "new_line_content": "      if (operands.size() != operand_shapes_with_layout->size()) {",
          "content_same": false
        },
        {
          "line": 2635,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "InvalidArgument(\n            \"Must specify a shape with layout for each operand for custom call \"\n            \"with constrained layout; given %d shapes, expected %d\",\n            operand_shapes_with_layout->size(), operands.size())",
          "old_line_content": "            \"with constrained layout; given %d shapes, expected %d\",",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 2638,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "operands.size()",
          "old_line_content": "      int64_t operand_num = 0;",
          "new_line_content": "            operand_shapes_with_layout->size(), operands.size());",
          "content_same": false
        },
        {
          "line": 4687,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->BroadcastInDim(operand, out_dim_size,\n                                           broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->BroadcastInDim(operand, out_dim_size,",
          "content_same": false
        },
        {
          "line": 2642,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(operand_shape)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(operand_shape)",
          "old_line_content": "              \"No layout specified for operand %d for custom call with \"",
          "new_line_content": "        if (!LayoutUtil::HasLayout(operand_shape)) {",
          "content_same": false
        },
        {
          "line": 2643,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n              \"No layout specified for operand %d for custom call with \"\n              \"constrained layout.\",\n              operand_num)",
          "old_line_content": "              \"constrained layout.\",",
          "new_line_content": "          return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 4694,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->DynamicBroadcastInDim(\n      operand, output_dimensions, broadcast_dimensions, output_shape)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->DynamicBroadcastInDim(",
          "content_same": false
        },
        {
          "line": 2651,
          "old_api": null,
          "new_api": "CustomCallInternal",
          "old_text": null,
          "new_text": "CustomCallInternal(\n        call_target_name, operands, &computation, shape, opaque,\n        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,\n        literal, /*window=*/{}, /*dnums=*/{}, schedule, api_version)",
          "old_line_content": "        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,",
          "new_line_content": "    return CustomCallInternal(",
          "content_same": false
        },
        {
          "line": 4699,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kCopy, operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kCopy, operand);",
          "content_same": false
        },
        {
          "line": 4704,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Pad(operand, padding_value, padding_config)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Pad(operand, padding_value, padding_config);",
          "content_same": false
        },
        {
          "line": 2659,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    Shape shape = *operand_shape;\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kOptimizationBarrier,\n                          {operand});\n  })",
          "old_line_content": "    Shape shape = *operand_shape;",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4709,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->PadInDim(operand, padding_value, dimno, pad_lo,\n                                     pad_hi)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->PadInDim(operand, padding_value, dimno, pad_lo,",
          "content_same": false
        },
        {
          "line": 2663,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "                          {operand});",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2664,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kOptimizationBarrier,",
          "content_same": false
        },
        {
          "line": 4715,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Reshape(operand, dimensions, new_sizes)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Reshape(operand, dimensions, new_sizes);",
          "content_same": false
        },
        {
          "line": 2671,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferTransposeShape(\n                                         *operand_shape, permutation));\n    return TransposeInternal(shape, operand, permutation);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferTransposeShape(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4719,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Reshape(operand, new_sizes)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Reshape(operand, new_sizes);",
          "content_same": false
        },
        {
          "line": 2675,
          "old_api": null,
          "new_api": "TransposeInternal",
          "old_text": null,
          "new_text": "TransposeInternal(shape, operand, permutation)",
          "old_line_content": "}",
          "new_line_content": "    return TransposeInternal(shape, operand, permutation);",
          "content_same": false
        },
        {
          "line": 4723,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Reshape(shape, operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Reshape(shape, operand);",
          "content_same": false
        },
        {
          "line": 4729,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->DynamicReshape(operand, dim_sizes, new_size_bounds,\n                                           dims_are_dynamic)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->DynamicReshape(operand, dim_sizes, new_size_bounds,",
          "content_same": false
        },
        {
          "line": 2686,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kTranspose, {operand});",
          "content_same": false
        },
        {
          "line": 4736,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Reshape(operand, new_sizes, inferred_dimension)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Reshape(operand, new_sizes, inferred_dimension);",
          "content_same": false
        },
        {
          "line": 2690,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferReverseShape(\n                                         *operand_shape, dimensions));\n    return RevInternal(shape, operand, dimensions);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferReverseShape(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4740,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Collapse(operand, dimensions)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Collapse(operand, dimensions);",
          "content_same": false
        },
        {
          "line": 2694,
          "old_api": null,
          "new_api": "RevInternal",
          "old_text": null,
          "new_text": "RevInternal(shape, operand, dimensions)",
          "old_line_content": "}",
          "new_line_content": "    return RevInternal(shape, operand, dimensions);",
          "content_same": false
        },
        {
          "line": 4746,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Slice(operand, start_indices, limit_indices,\n                                  strides)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->Slice(operand, start_indices, limit_indices,",
          "content_same": false
        },
        {
          "line": 4752,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->SliceInDim(operand, start_index, limit_index,\n                                       stride, dimno)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->SliceInDim(operand, start_index, limit_index,",
          "content_same": false
        },
        {
          "line": 2705,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kReverse, {operand});",
          "content_same": false
        },
        {
          "line": 4758,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->DynamicSlice(operand, start_indices, slice_sizes)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->DynamicSlice(operand, start_indices, slice_sizes);",
          "content_same": false
        },
        {
          "line": 2714,
          "old_api": null,
          "new_api": "GetOperandShapes",
          "old_text": null,
          "new_text": "GetOperandShapes(operands)",
          "old_line_content": "                      [](const Shape& shape) { return &shape; });",
          "new_line_content": "                        GetOperandShapes(operands));",
          "content_same": false
        },
        {
          "line": 2715,
          "old_api": null,
          "new_api": "std::back_inserter(operand_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(operand_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferVariadicOpShape(",
          "new_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 4763,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->DynamicUpdateSlice(operand, update, start_indices)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->DynamicUpdateSlice(operand, update, start_indices);",
          "content_same": false
        },
        {
          "line": 2719,
          "old_api": null,
          "new_api": "SortInternal",
          "old_text": null,
          "new_text": "SortInternal(shape, operands, comparator, dimension, is_stable)",
          "old_line_content": "}",
          "new_line_content": "    return SortInternal(shape, operands, comparator, dimension, is_stable);",
          "content_same": false
        },
        {
          "line": 4768,
          "old_api": null,
          "new_api": "ConcatInDim",
          "old_text": null,
          "new_text": "builder->ConcatInDim(operands, dimension)",
          "old_line_content": "",
          "new_line_content": "  return builder->ConcatInDim(operands, dimension);",
          "content_same": false
        },
        {
          "line": 4772,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "pred.builder()->Select(pred, on_true, on_false)",
          "old_line_content": "",
          "new_line_content": "  return pred.builder()->Select(pred, on_true, on_false);",
          "content_same": false
        },
        {
          "line": 2728,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "  if (dimension == -1) {",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2729,
          "old_api": null,
          "new_api": "set_is_stable",
          "old_text": null,
          "new_text": "instr.set_is_stable(is_stable)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* keys_shape, GetShapePtr(operands[0]));",
          "new_line_content": "  instr.set_is_stable(is_stable);",
          "content_same": false
        },
        {
          "line": 4776,
          "old_api": null,
          "new_api": "Tuple",
          "old_text": null,
          "new_text": "builder->Tuple(elements)",
          "old_line_content": "",
          "new_line_content": "  return builder->Tuple(elements);",
          "content_same": false
        },
        {
          "line": 4780,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "tuple_data.builder()->GetTupleElement(tuple_data, index)",
          "old_line_content": "",
          "new_line_content": "  return tuple_data.builder()->GetTupleElement(tuple_data, index);",
          "content_same": false
        },
        {
          "line": 2735,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(comparator, &instr)",
          "old_line_content": "}",
          "new_line_content": "  AddCalledComputation(comparator, &instr);",
          "content_same": false
        },
        {
          "line": 2736,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kSort, operands);",
          "content_same": false
        },
        {
          "line": 4785,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kEq)",
          "old_line_content": "",
          "new_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kEq);",
          "content_same": false
        },
        {
          "line": 2740,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferTopKShape(*operand_shape, k));\n    return TopKInternal(shape, operand, k, largest);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4791,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(auto operand_shape, b->GetShape(lhs));",
          "new_line_content": "  auto b = lhs.builder();",
          "content_same": false
        },
        {
          "line": 2745,
          "old_api": null,
          "new_api": "TopKInternal",
          "old_text": null,
          "new_text": "TopKInternal(shape, operand, k, largest)",
          "old_line_content": "}",
          "new_line_content": "    return TopKInternal(shape, operand, k, largest);",
          "content_same": false
        },
        {
          "line": 4798,
          "old_api": null,
          "new_api": "Comparison::DefaultComparisonType(operand_element_type)",
          "old_text": null,
          "new_text": "Comparison::DefaultComparisonType(operand_element_type)",
          "old_line_content": "                   compare_type);",
          "new_line_content": "            : Comparison::DefaultComparisonType(operand_element_type);",
          "content_same": false
        },
        {
          "line": 4799,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(lhs, rhs, broadcast_dimensions, comparison_direction,\n                   compare_type)",
          "old_line_content": "  });",
          "new_line_content": "    return Compare(lhs, rhs, broadcast_dimensions, comparison_direction,",
          "content_same": false
        },
        {
          "line": 2754,
          "old_api": null,
          "new_api": "set_largest",
          "old_text": null,
          "new_text": "instr.set_largest(largest)",
          "old_line_content": "}",
          "new_line_content": "  instr.set_largest(largest);",
          "content_same": false
        },
        {
          "line": 2755,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kTopK, {operand});",
          "content_same": false
        },
        {
          "line": 4806,
          "old_api": null,
          "new_api": "CompareTotalOrder",
          "old_text": null,
          "new_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kEq)",
          "old_line_content": "}",
          "new_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2760,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferConvertShape(\n                                         *operand_shape, new_element_type));\n    if (primitive_util::IsComplexType(operand_shape->element_type()) &&\n        !primitive_util::IsComplexType(new_element_type)) {\n      operand = Real(operand);\n    }\n    return AddOpWithShape(HloOpcode::kConvert, shape, {operand});\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferConvertShape(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4812,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kNe)",
          "old_line_content": "",
          "new_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kNe);",
          "content_same": false
        },
        {
          "line": 2765,
          "old_api": null,
          "new_api": "primitive_util::IsComplexType(new_element_type)",
          "old_text": null,
          "new_text": "primitive_util::IsComplexType(new_element_type)",
          "old_line_content": "    }",
          "new_line_content": "        !primitive_util::IsComplexType(new_element_type)) {",
          "content_same": false
        },
        {
          "line": 2768,
          "old_api": null,
          "new_api": "AddOpWithShape",
          "old_text": null,
          "new_text": "AddOpWithShape(HloOpcode::kConvert, shape, {operand})",
          "old_line_content": "}",
          "new_line_content": "    return AddOpWithShape(HloOpcode::kConvert, shape, {operand});",
          "content_same": false
        },
        {
          "line": 4817,
          "old_api": null,
          "new_api": "CompareTotalOrder",
          "old_text": null,
          "new_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kNe)",
          "old_line_content": "}",
          "new_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2774,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferBitcastConvertShape(\n                                         *operand_shape, new_element_type));\n    return BitcastConvertTypeInternal(shape, operand);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferBitcastConvertShape(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4823,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kGe)",
          "old_line_content": "",
          "new_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kGe);",
          "content_same": false
        },
        {
          "line": 2778,
          "old_api": null,
          "new_api": "BitcastConvertTypeInternal",
          "old_text": null,
          "new_text": "BitcastConvertTypeInternal(shape, operand)",
          "old_line_content": "}",
          "new_line_content": "    return BitcastConvertTypeInternal(shape, operand);",
          "content_same": false
        },
        {
          "line": 4828,
          "old_api": null,
          "new_api": "CompareTotalOrder",
          "old_text": null,
          "new_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kGe)",
          "old_line_content": "}",
          "new_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2785,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "                        {operand});",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2786,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kBitcastConvert,",
          "content_same": false
        },
        {
          "line": 4834,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kGt)",
          "old_line_content": "",
          "new_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kGt);",
          "content_same": false
        },
        {
          "line": 4839,
          "old_api": null,
          "new_api": "CompareTotalOrder",
          "old_text": null,
          "new_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kGt)",
          "old_line_content": "}",
          "new_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2792,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* random_shape, GetShapePtr(random));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferStochasticConvertShape(\n                            *operand_shape, *random_shape, new_element_type));\n    return AddOpWithShape(HloOpcode::kStochasticConvert, shape,\n                          {operand, random});\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* random_shape, GetShapePtr(random));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4845,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kLe)",
          "old_line_content": "",
          "new_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kLe);",
          "content_same": false
        },
        {
          "line": 2798,
          "old_api": null,
          "new_api": "AddOpWithShape",
          "old_text": null,
          "new_text": "AddOpWithShape(HloOpcode::kStochasticConvert, shape,\n                          {operand, random})",
          "old_line_content": "  });",
          "new_line_content": "    return AddOpWithShape(HloOpcode::kStochasticConvert, shape,",
          "content_same": false
        },
        {
          "line": 4850,
          "old_api": null,
          "new_api": "CompareTotalOrder",
          "old_text": null,
          "new_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kLe)",
          "old_line_content": "}",
          "new_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2804,
          "old_api": null,
          "new_api": "TernaryOp",
          "old_text": null,
          "new_text": "TernaryOp(HloOpcode::kClamp, min, operand, max)",
          "old_line_content": "",
          "new_line_content": "  return TernaryOp(HloOpcode::kClamp, min, operand, max);",
          "content_same": false
        },
        {
          "line": 4856,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kLt)",
          "old_line_content": "",
          "new_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kLt);",
          "content_same": false
        },
        {
          "line": 2812,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "static_operands.empty()",
          "old_line_content": "    }",
          "new_line_content": "    if (!static_operands.empty()) {",
          "content_same": false
        },
        {
          "line": 2813,
          "old_api": null,
          "new_api": "Unimplemented",
          "old_text": null,
          "new_text": "Unimplemented(\"static_operands is not supported in Map\")",
          "old_line_content": "",
          "new_line_content": "      return Unimplemented(\"static_operands is not supported in Map\");",
          "content_same": false
        },
        {
          "line": 4861,
          "old_api": null,
          "new_api": "CompareTotalOrder",
          "old_text": null,
          "new_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kLt)",
          "old_line_content": "}",
          "new_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2819,
          "old_api": null,
          "new_api": "std::back_inserter(operand_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(operand_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const ProgramShape& called_program_shape,",
          "new_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 4868,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kCompare, lhs, rhs,\n                                 broadcast_dimensions, direction)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kCompare, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 4875,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kCompare, lhs, rhs,\n                                 broadcast_dimensions, direction, compare_type)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kCompare, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 2830,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(computation, &instr)",
          "old_line_content": "    for (XlaOp& new_operand : new_operands) {",
          "new_line_content": "    AddCalledComputation(computation, &instr);",
          "content_same": false
        },
        {
          "line": 2831,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "operands.end()",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(new_operand));",
          "new_line_content": "    std::vector<XlaOp> new_operands(operands.begin(), operands.end());",
          "content_same": false
        },
        {
          "line": 4880,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(lhs, rhs, {}, direction)",
          "old_line_content": "",
          "new_line_content": "  return Compare(lhs, rhs, {}, direction);",
          "content_same": false
        },
        {
          "line": 2837,
          "old_api": null,
          "new_api": "InDimBroadcast",
          "old_text": null,
          "new_text": "InDimBroadcast(output_shape, new_operand, {})",
          "old_line_content": "      }",
          "new_line_content": "                            InDimBroadcast(output_shape, new_operand, {}));",
          "content_same": false
        },
        {
          "line": 4886,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->Dot(lhs, rhs, precision_config, preferred_element_type)",
          "old_line_content": "",
          "new_line_content": "  return lhs.builder()->Dot(lhs, rhs, precision_config, preferred_element_type);",
          "content_same": false
        },
        {
          "line": 2841,
          "old_api": null,
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": null,
          "new_text": "TF_ASSIGN_OR_RETURN(new_operand,\n                            AddBroadcastSequence(output_shape, new_operand))",
          "old_line_content": "      }",
          "new_line_content": "        TF_ASSIGN_OR_RETURN(new_operand,",
          "content_same": false
        },
        {
          "line": 2842,
          "old_api": null,
          "new_api": "AddBroadcastSequence",
          "old_text": null,
          "new_text": "AddBroadcastSequence(output_shape, new_operand)",
          "old_line_content": "    }",
          "new_line_content": "                            AddBroadcastSequence(output_shape, new_operand));",
          "content_same": false
        },
        {
          "line": 4893,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->DotGeneral(lhs, rhs, dimension_numbers,\n                                   precision_config, preferred_element_type)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->DotGeneral(lhs, rhs, dimension_numbers,",
          "content_same": false
        },
        {
          "line": 2846,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kMap, new_operands);",
          "content_same": false
        },
        {
          "line": 2853,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Check the number of parameters per RNG distribution.\n    switch (distribution) {\n      case RandomDistribution::RNG_NORMAL:\n      case RandomDistribution::RNG_UNIFORM:\n        if (parameters.size() != 2) {\n          return InvalidArgument(\n              \"RNG distribution (%s) expects 2 parameters, but got %ld\",\n              RandomDistribution_Name(distribution), parameters.size());\n        }\n        break;\n      default:\n        LOG(FATAL) << \"unhandled distribution \" << distribution;\n    }\n\n    TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));\n    return RngOpInternal(distribution, parameters, shape);\n  })",
          "old_line_content": "    switch (distribution) {",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4903,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->SparseDot(lhs, rhs, sparse_meta, sparsity,\n                                  dimension_numbers, precision_config,\n                                  preferred_element_type)",
          "old_line_content": "                                  preferred_element_type);",
          "new_line_content": "  return lhs.builder()->SparseDot(lhs, rhs, sparse_meta, sparsity,",
          "content_same": false
        },
        {
          "line": 2858,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "parameters.size()",
          "old_line_content": "              \"RNG distribution (%s) expects 2 parameters, but got %ld\",",
          "new_line_content": "        if (parameters.size() != 2) {",
          "content_same": false
        },
        {
          "line": 2861,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "parameters.size()",
          "old_line_content": "        break;",
          "new_line_content": "              RandomDistribution_Name(distribution), parameters.size());",
          "content_same": false
        },
        {
          "line": 2865,
          "old_api": null,
          "new_api": "LOG",
          "old_text": null,
          "new_text": "LOG(FATAL)",
          "old_line_content": "",
          "new_line_content": "        LOG(FATAL) << \"unhandled distribution \" << distribution;",
          "content_same": false
        },
        {
          "line": 4913,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->Conv(lhs, rhs, window_strides, padding,\n                             feature_group_count, batch_group_count,\n                             precision_config, preferred_element_type)",
          "old_line_content": "                             precision_config, preferred_element_type);",
          "new_line_content": "  return lhs.builder()->Conv(lhs, rhs, window_strides, padding,",
          "content_same": false
        },
        {
          "line": 2868,
          "old_api": null,
          "new_api": "ShapeUtil::ValidateShapeWithOptionalLayout(shape)",
          "old_text": null,
          "new_text": "ShapeUtil::ValidateShapeWithOptionalLayout(shape)",
          "old_line_content": "  });",
          "new_line_content": "    TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));",
          "content_same": false
        },
        {
          "line": 2869,
          "old_api": null,
          "new_api": "RngOpInternal",
          "old_text": null,
          "new_text": "RngOpInternal(distribution, parameters, shape)",
          "old_line_content": "}",
          "new_line_content": "    return RngOpInternal(distribution, parameters, shape);",
          "content_same": false
        },
        {
          "line": 4924,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->ConvWithGeneralPadding(\n      lhs, rhs, window_strides, padding, feature_group_count, batch_group_count,\n      precision_config, preferred_element_type)",
          "old_line_content": "      precision_config, preferred_element_type);",
          "new_line_content": "  return lhs.builder()->ConvWithGeneralPadding(",
          "content_same": false
        },
        {
          "line": 2877,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2880,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kRng, parameters);",
          "content_same": false
        },
        {
          "line": 2884,
          "old_api": null,
          "new_api": "RngOp",
          "old_text": null,
          "new_text": "RngOp(RandomDistribution::RNG_NORMAL, {mu, sigma}, shape)",
          "old_line_content": "",
          "new_line_content": "  return RngOp(RandomDistribution::RNG_NORMAL, {mu, sigma}, shape);",
          "content_same": false
        },
        {
          "line": 4935,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->ConvWithGeneralDimensions(\n      lhs, rhs, window_strides, padding, dimension_numbers, feature_group_count,\n      batch_group_count, precision_config, preferred_element_type)",
          "old_line_content": "      batch_group_count, precision_config, preferred_element_type);",
          "new_line_content": "  return lhs.builder()->ConvWithGeneralDimensions(",
          "content_same": false
        },
        {
          "line": 2888,
          "old_api": null,
          "new_api": "RngOp",
          "old_text": null,
          "new_text": "RngOp(RandomDistribution::RNG_UNIFORM, {a, b}, shape)",
          "old_line_content": "",
          "new_line_content": "  return RngOp(RandomDistribution::RNG_UNIFORM, {a, b}, shape);",
          "content_same": false
        },
        {
          "line": 2893,
          "old_api": null,
          "new_api": "set_element_type",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));\n    TF_ASSIGN_OR_RETURN(Shape state_shape, GetShape(initial_state));\n    Shape output_shape = shape;\n    output_shape.set_element_type(PRIMITIVE_TYPE_INVALID);\n    if (primitive_util::IsArrayType(shape.element_type())) {\n      output_shape.set_element_type(\n          primitive_util::UnsignedIntegralTypeForBitWidth(\n              primitive_util::BitWidth(shape.element_type())));\n    }\n    if (!primitive_util::IsUnsignedIntegralType(output_shape.element_type())) {\n      return InvalidArgument(\"Unsupported shape for RngBitGenerator: %s\",\n                             PrimitiveType_Name(shape.element_type()));\n    }\n    return RngBitGeneratorInternal(\n        ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape}),\n        algorithm, initial_state);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape state_shape, GetShape(initial_state));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2894,
          "old_api": null,
          "new_api": "ShapeUtil::ValidateShapeWithOptionalLayout(shape)",
          "old_text": null,
          "new_text": "ShapeUtil::ValidateShapeWithOptionalLayout(shape)",
          "old_line_content": "    Shape output_shape = shape;",
          "new_line_content": "    TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));",
          "content_same": false
        },
        {
          "line": 4947,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->ConvGeneral(\n      lhs, rhs, window_strides, padding, dimension_numbers, feature_group_count,\n      batch_group_count, precision_config, preferred_element_type)",
          "old_line_content": "      batch_group_count, precision_config, preferred_element_type);",
          "new_line_content": "  return lhs.builder()->ConvGeneral(",
          "content_same": false
        },
        {
          "line": 2900,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "primitive_util::UnsignedIntegralTypeForBitWidth(\n              primitive_util::BitWidth(shape.element_type()))",
          "old_line_content": "    }",
          "new_line_content": "          primitive_util::UnsignedIntegralTypeForBitWidth(",
          "content_same": false
        },
        {
          "line": 2904,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "InvalidArgument(\"Unsupported shape for RngBitGenerator: %s\",\n                             PrimitiveType_Name(shape.element_type()))",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(\"Unsupported shape for RngBitGenerator: %s\",",
          "content_same": false
        },
        {
          "line": 2907,
          "old_api": null,
          "new_api": "RngBitGeneratorInternal",
          "old_text": null,
          "new_text": "RngBitGeneratorInternal(\n        ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape}),\n        algorithm, initial_state)",
          "old_line_content": "        algorithm, initial_state);",
          "new_line_content": "    return RngBitGeneratorInternal(",
          "content_same": false
        },
        {
          "line": 2908,
          "old_api": null,
          "new_api": "ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape})",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape})",
          "old_line_content": "  });",
          "new_line_content": "        ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape}),",
          "content_same": false
        },
        {
          "line": 4962,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->ConvGeneralDilated(\n      lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\n      dimension_numbers, feature_group_count, batch_group_count,\n      precision_config, preferred_element_type, window_reversal)",
          "old_line_content": "      dimension_numbers, feature_group_count, batch_group_count,",
          "new_line_content": "  return lhs.builder()->ConvGeneralDilated(",
          "content_same": false
        },
        {
          "line": 2918,
          "old_api": null,
          "new_api": "set_rng_algorithm",
          "old_text": null,
          "new_text": "instr.set_rng_algorithm(algorithm)",
          "old_line_content": "                        {initial_state});",
          "new_line_content": "  instr.set_rng_algorithm(algorithm);",
          "content_same": false
        },
        {
          "line": 2919,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kRngBitGenerator,",
          "content_same": false
        },
        {
          "line": 2925,
          "old_api": null,
          "new_api": "GetProgramShape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Infer shape.\n    TF_ASSIGN_OR_RETURN(const auto& body_program_shape, body.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(const auto& condition_program_shape,\n                        condition.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(const Shape* init_shape, GetShapePtr(init));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferWhileShape(\n                                         condition_program_shape,\n                                         body_program_shape, *init_shape));\n    return WhileInternal(shape, condition, body, init);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const auto& body_program_shape, body.GetProgramShape());",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4978,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->DynamicConvInputGrad(\n      input_sizes, lhs, rhs, window_strides, padding, lhs_dilation,\n      rhs_dilation, dimension_numbers, feature_group_count, batch_group_count,\n      precision_config, padding_type, preferred_element_type)",
          "old_line_content": "      rhs_dilation, dimension_numbers, feature_group_count, batch_group_count,",
          "new_line_content": "  return lhs.builder()->DynamicConvInputGrad(",
          "content_same": false
        },
        {
          "line": 2934,
          "old_api": null,
          "new_api": "WhileInternal",
          "old_text": null,
          "new_text": "WhileInternal(shape, condition, body, init)",
          "old_line_content": "}",
          "new_line_content": "    return WhileInternal(shape, condition, body, init);",
          "content_same": false
        },
        {
          "line": 2946,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(condition, &instr)",
          "old_line_content": "}",
          "new_line_content": "  AddCalledComputation(condition, &instr);",
          "content_same": false
        },
        {
          "line": 2947,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kWhile, {init});",
          "content_same": false
        },
        {
          "line": 4994,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "activations.builder()->DynamicConvKernelGrad(\n      activations, gradients, window_strides, padding, lhs_dilation,\n      rhs_dilation, dimension_numbers, feature_group_count, batch_group_count,\n      precision_config, padding_type, preferred_element_type)",
          "old_line_content": "      rhs_dilation, dimension_numbers, feature_group_count, batch_group_count,",
          "new_line_content": "  return activations.builder()->DynamicConvKernelGrad(",
          "content_same": false
        },
        {
          "line": 2954,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* input_shape, GetShapePtr(input));\n    TF_ASSIGN_OR_RETURN(const Shape* start_indices_shape,\n                        GetShapePtr(start_indices));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferGatherShape(\n                                         *input_shape, *start_indices_shape,\n                                         dimension_numbers, slice_sizes));\n    return GatherInternal(shape, input, start_indices, dimension_numbers,\n                          slice_sizes, indices_are_sorted);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* start_indices_shape,",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2961,
          "old_api": null,
          "new_api": "GatherInternal",
          "old_text": null,
          "new_text": "GatherInternal(shape, input, start_indices, dimension_numbers,\n                          slice_sizes, indices_are_sorted)",
          "old_line_content": "  });",
          "new_line_content": "    return GatherInternal(shape, input, start_indices, dimension_numbers,",
          "content_same": false
        },
        {
          "line": 5010,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->DynamicConvForward(\n      lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\n      dimension_numbers, feature_group_count, batch_group_count,\n      precision_config, padding_type, preferred_element_type)",
          "old_line_content": "      dimension_numbers, feature_group_count, batch_group_count,",
          "new_line_content": "  return lhs.builder()->DynamicConvForward(",
          "content_same": false
        },
        {
          "line": 5018,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Fft(operand, fft_type, fft_length)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Fft(operand, fft_type, fft_length);",
          "content_same": false
        },
        {
          "line": 2972,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "  for (int64_t bound : slice_sizes) {",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 2975,
          "old_api": null,
          "new_api": "add_gather_slice_sizes",
          "old_text": null,
          "new_text": "instr.add_gather_slice_sizes(bound)",
          "old_line_content": "",
          "new_line_content": "    instr.add_gather_slice_sizes(bound);",
          "content_same": false
        },
        {
          "line": 5024,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "a.builder()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* a_shape, builder->GetShapePtr(a));",
          "new_line_content": "  XlaBuilder* builder = a.builder();",
          "content_same": false
        },
        {
          "line": 5025,
          "old_api": null,
          "new_api": "set_left_side",
          "old_text": null,
          "new_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* a_shape, builder->GetShapePtr(a));\n    TF_ASSIGN_OR_RETURN(const Shape* b_shape, builder->GetShapePtr(b));\n    TriangularSolveOptions options;\n    options.set_left_side(left_side);\n    options.set_lower(lower);\n    options.set_unit_diagonal(unit_diagonal);\n    options.set_transpose_a(transpose_a);\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferTriangularSolveShape(\n                                         *a_shape, *b_shape, options));\n    return builder->TriangularSolveInternal(shape, a, b, std::move(options));\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* b_shape, builder->GetShapePtr(b));",
          "new_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2978,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kGather,",
          "content_same": false
        },
        {
          "line": 5031,
          "old_api": null,
          "new_api": "set_unit_diagonal",
          "old_text": null,
          "new_text": "options.set_unit_diagonal(unit_diagonal)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferTriangularSolveShape(",
          "new_line_content": "    options.set_unit_diagonal(unit_diagonal);",
          "content_same": false
        },
        {
          "line": 5032,
          "old_api": null,
          "new_api": "set_transpose_a",
          "old_text": null,
          "new_text": "options.set_transpose_a(transpose_a)",
          "old_line_content": "                                         *a_shape, *b_shape, options));",
          "new_line_content": "    options.set_transpose_a(transpose_a);",
          "content_same": false
        },
        {
          "line": 2986,
          "old_api": null,
          "new_api": "absl::MakeConstSpan(&input, 1)",
          "old_text": null,
          "new_text": "absl::MakeConstSpan(&input, 1)",
          "old_line_content": "                 dimension_numbers, indices_are_sorted, unique_indices);",
          "new_line_content": "  return Scatter(absl::MakeConstSpan(&input, 1), scatter_indices,",
          "content_same": false
        },
        {
          "line": 2987,
          "old_api": null,
          "new_api": "absl::MakeConstSpan(&updates, 1)",
          "old_text": null,
          "new_text": "absl::MakeConstSpan(&updates, 1)",
          "old_line_content": "}",
          "new_line_content": "                 absl::MakeConstSpan(&updates, 1), update_computation,",
          "content_same": false
        },
        {
          "line": 5035,
          "old_api": null,
          "new_api": "std::move(options)",
          "old_text": null,
          "new_text": "std::move(options)",
          "old_line_content": "}",
          "new_line_content": "    return builder->TriangularSolveInternal(shape, a, b, std::move(options));",
          "content_same": false
        },
        {
          "line": 5040,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "a.builder()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* a_shape, builder->GetShapePtr(a));",
          "new_line_content": "  XlaBuilder* builder = a.builder();",
          "content_same": false
        },
        {
          "line": 5041,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* a_shape, builder->GetShapePtr(a));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferCholeskyShape(*a_shape));\n    return builder->CholeskyInternal(shape, a, lower);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape,",
          "new_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2997,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "inputs.empty()",
          "old_line_content": "    }",
          "new_line_content": "    if (inputs.empty()) {",
          "content_same": false
        },
        {
          "line": 5045,
          "old_api": null,
          "new_api": "CholeskyInternal",
          "old_text": null,
          "new_text": "builder->CholeskyInternal(shape, a, lower)",
          "old_line_content": "}",
          "new_line_content": "    return builder->CholeskyInternal(shape, a, lower);",
          "content_same": false
        },
        {
          "line": 3000,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "updates.size()",
          "old_line_content": "          \"Scatter should have same number of inputs and updates: %d vs %d.\",",
          "new_line_content": "    if (inputs.size() != updates.size()) {",
          "content_same": false
        },
        {
          "line": 3003,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "updates.size()",
          "old_line_content": "    absl::InlinedVector<const Shape*, 3> operand_shapes;",
          "new_line_content": "          inputs.size(), updates.size());",
          "content_same": false
        },
        {
          "line": 5051,
          "old_api": null,
          "new_api": "Infeed",
          "old_text": null,
          "new_text": "builder->Infeed(shape, config)",
          "old_line_content": "",
          "new_line_content": "  return builder->Infeed(shape, config);",
          "content_same": false
        },
        {
          "line": 3006,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "updates.size()",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(const Shape* input_shape, GetShapePtr(input));",
          "new_line_content": "    operand_shapes.reserve(inputs.size() + 1 + updates.size());",
          "content_same": false
        },
        {
          "line": 5056,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Outfeed(operand, shape_with_layout, outfeed_config)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Outfeed(operand, shape_with_layout, outfeed_config);",
          "content_same": false
        },
        {
          "line": 3009,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operand_shapes.push_back(input_shape)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* scatter_indices_shape,",
          "new_line_content": "      operand_shapes.push_back(input_shape);",
          "content_same": false
        },
        {
          "line": 3013,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operand_shapes.push_back(scatter_indices_shape)",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(const Shape* update_shape, GetShapePtr(update));",
          "new_line_content": "    operand_shapes.push_back(scatter_indices_shape);",
          "content_same": false
        },
        {
          "line": 5061,
          "old_api": null,
          "new_api": "Call",
          "old_text": null,
          "new_text": "builder->Call(computation, operands)",
          "old_line_content": "",
          "new_line_content": "  return builder->Call(computation, operands);",
          "content_same": false
        },
        {
          "line": 3016,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operand_shapes.push_back(update_shape)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const ProgramShape& to_apply_shape,",
          "new_line_content": "      operand_shapes.push_back(update_shape);",
          "content_same": false
        },
        {
          "line": 3023,
          "old_api": null,
          "new_api": "ScatterInternal",
          "old_text": null,
          "new_text": "ScatterInternal(shape, inputs, scatter_indices, updates,\n                           update_computation, dimension_numbers,\n                           indices_are_sorted, unique_indices)",
          "old_line_content": "                           indices_are_sorted, unique_indices);",
          "new_line_content": "    return ScatterInternal(shape, inputs, scatter_indices, updates,",
          "content_same": false
        },
        {
          "line": 5072,
          "old_api": null,
          "new_api": "CustomCall",
          "old_text": null,
          "new_text": "builder->CustomCall(call_target_name, operands, shape, opaque,\n                             /*operand_shapes_with_layout=*/std::nullopt,\n                             has_side_effect, output_operand_aliasing, literal,\n                             /*window=*/std::nullopt, /*dnums=*/std::nullopt,\n                             schedule, api_version)",
          "old_line_content": "                             has_side_effect, output_operand_aliasing, literal,",
          "new_line_content": "  return builder->CustomCall(call_target_name, operands, shape, opaque,",
          "content_same": false
        },
        {
          "line": 3038,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 5087,
          "old_api": null,
          "new_api": "CustomCall",
          "old_text": null,
          "new_text": "builder->CustomCall(\n      call_target_name, operands, computation, shape, opaque,\n      /*operand_shapes_with_layout=*/std::nullopt, has_side_effect,\n      output_operand_aliasing, literal, schedule, api_version)",
          "old_line_content": "      /*operand_shapes_with_layout=*/std::nullopt, has_side_effect,",
          "new_line_content": "  return builder->CustomCall(",
          "content_same": false
        },
        {
          "line": 3046,
          "old_api": null,
          "new_api": "std::back_inserter(operands)",
          "old_text": null,
          "new_text": "std::back_inserter(operands)",
          "old_line_content": "  });",
          "new_line_content": "    absl::c_copy(updates, std::back_inserter(operands));",
          "content_same": false
        },
        {
          "line": 3047,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kScatter, operands);",
          "content_same": false
        },
        {
          "line": 5102,
          "old_api": null,
          "new_api": "CustomCall",
          "old_text": null,
          "new_text": "builder->CustomCall(\n      call_target_name, operands, shape, opaque, operand_shapes_with_layout,\n      has_side_effect, output_operand_aliasing, literal,\n      /*window=*/std::nullopt, /*dnums=*/std::nullopt, schedule, api_version)",
          "old_line_content": "      has_side_effect, output_operand_aliasing, literal,",
          "new_line_content": "  return builder->CustomCall(",
          "content_same": false
        },
        {
          "line": 3055,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(predicate));\n\n    if (!ShapeUtil::IsScalar(*shape) || shape->element_type() != PRED) {\n      return InvalidArgument(\n          \"Argument to predicated-Conditional is not a scalar of PRED type \"\n          \"(%s).\",\n          ShapeUtil::HumanString(*shape));\n    }\n    // The index of true_computation must be 0 and that of false computation\n    // must be 1.\n    return ConditionalImpl(predicate, {&true_computation, &false_computation},\n                           {true_operand, false_operand});\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3058,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape->element_type()",
          "old_line_content": "          \"Argument to predicated-Conditional is not a scalar of PRED type \"",
          "new_line_content": "    if (!ShapeUtil::IsScalar(*shape) || shape->element_type() != PRED) {",
          "content_same": false
        },
        {
          "line": 3059,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Argument to predicated-Conditional is not a scalar of PRED type \"\n          \"(%s).\",\n          ShapeUtil::HumanString(*shape))",
          "old_line_content": "          \"(%s).\",",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 3062,
          "old_api": null,
          "new_api": "ShapeUtil::HumanString(*shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanString(*shape)",
          "old_line_content": "    // The index of true_computation must be 0 and that of false computation",
          "new_line_content": "          ShapeUtil::HumanString(*shape));",
          "content_same": false
        },
        {
          "line": 3066,
          "old_api": null,
          "new_api": "ConditionalImpl",
          "old_text": null,
          "new_text": "ConditionalImpl(predicate, {&true_computation, &false_computation},\n                           {true_operand, false_operand})",
          "old_line_content": "  });",
          "new_line_content": "    return ConditionalImpl(predicate, {&true_computation, &false_computation},",
          "content_same": false
        },
        {
          "line": 5118,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "operand_shapes_with_layout.empty()",
          "old_line_content": "  }",
          "new_line_content": "  if (!operand_shapes_with_layout.empty()) {",
          "content_same": false
        },
        {
          "line": 5121,
          "old_api": null,
          "new_api": "CustomCall",
          "old_text": null,
          "new_text": "builder->CustomCall(call_target_name, operands, shape, opaque,\n                             maybe_operand_shapes, has_side_effect,\n                             output_operand_aliasing, literal, window, dnums,\n                             schedule, api_version)",
          "old_line_content": "                             output_operand_aliasing, literal, window, dnums,",
          "new_line_content": "  return builder->CustomCall(call_target_name, operands, shape, opaque,",
          "content_same": false
        },
        {
          "line": 3075,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(branch_index));\n\n    if (!ShapeUtil::IsScalar(*shape) || shape->element_type() != S32) {\n      return InvalidArgument(\n          \"Argument to indexed-Conditional is not a scalar of S32 type (%s).\",\n          ShapeUtil::HumanString(*shape));\n    }\n    return ConditionalImpl(branch_index, branch_computations, branch_operands);\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3078,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape->element_type()",
          "old_line_content": "          \"Argument to indexed-Conditional is not a scalar of S32 type (%s).\",",
          "new_line_content": "    if (!ShapeUtil::IsScalar(*shape) || shape->element_type() != S32) {",
          "content_same": false
        },
        {
          "line": 5128,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->OptimizationBarrier(operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->OptimizationBarrier(operand);",
          "content_same": false
        },
        {
          "line": 3083,
          "old_api": null,
          "new_api": "ConditionalImpl",
          "old_text": null,
          "new_text": "ConditionalImpl(branch_index, branch_computations, branch_operands)",
          "old_line_content": "}",
          "new_line_content": "    return ConditionalImpl(branch_index, branch_computations, branch_operands);",
          "content_same": false
        },
        {
          "line": 5133,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "real.builder()->BinaryOp(HloOpcode::kComplex, real, imag,\n                                  broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return real.builder()->BinaryOp(HloOpcode::kComplex, real, imag,",
          "content_same": false
        },
        {
          "line": 5138,
          "old_api": null,
          "new_api": "Imag",
          "old_text": null,
          "new_text": "Imag(operand)",
          "old_line_content": "",
          "new_line_content": "  return Complex(Real(operand), Neg(Imag(operand)));",
          "content_same": false
        },
        {
          "line": 3094,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<const Shape*> operand_shapes;\n    std::vector<XlaOp> operands;\n    if (operand_shape->IsTuple()) {\n      if (operand_shape->tuple_shapes_size() == 0) {\n        return Unimplemented(\"0 element tuple AllReduce is not supported\");\n      }\n      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {\n        if (operand_shape->tuple_shapes(i).element_type() !=\n            operand_shape->tuple_shapes(0).element_type()) {\n          return Unimplemented(\n              \"All the shapes of a tuple input of AllReduce must have the same \"\n              \"element type\");\n        }\n        operand_shapes.push_back(&operand_shape->tuple_shapes(i));\n        operands.push_back(GetTupleElement(operand, i));\n      }\n    } else {\n      operand_shapes.push_back(operand_shape);\n      operands.push_back(operand);\n    }\n\n    TF_ASSIGN_OR_RETURN(Shape inferred_shape,\n                        ShapeInference::InferAllReduceShape(operand_shapes));\n    if (layout) {\n      if (!LayoutUtil::HasLayout(*layout)) {\n        return InvalidArgument(\"shape_with_layout must have the layout set: %s\",\n                               layout->ToString());\n      }\n      if (!ShapeUtil::Compatible(*layout, *operand_shape)) {\n        return InvalidArgument(\n            \"Provided shape_with_layout must be compatible with the \"\n            \"operand shape: %s vs %s\",\n            layout->ToString(), operand_shape->ToString());\n      }\n      instr.set_constrain_layout(true);\n      if (operand_shape->IsTuple() && !inferred_shape.IsTuple()) {\n        // For a single-element tuple, take the tuple element shape.\n        TF_RET_CHECK(layout->tuple_shapes_size() == 1);\n        *instr.mutable_shape() = layout->tuple_shapes(0).ToProto();\n      } else {\n        *instr.mutable_shape() = layout->ToProto();\n      }\n    } else {\n      *instr.mutable_shape() = inferred_shape.ToProto();\n    }\n\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n\n    if (use_global_device_ids.has_value()) {\n      instr.set_use_global_device_ids(*use_global_device_ids);\n    }\n\n    AddCalledComputation(computation, &instr);\n\n    TF_ASSIGN_OR_RETURN(auto all_reduce,\n                        AddInstruction(std::move(instr),\n                                       async ? HloOpcode::kAllReduceStart\n                                             : HloOpcode::kAllReduce,\n                                       operands));\n    if (operand_shape->IsTuple() && !inferred_shape.IsTuple()) {\n      // For a single-element tuple, wrap the result into a tuple.\n      TF_RET_CHECK(operand_shapes.size() == 1);\n      TF_RET_CHECK(ShapeUtil::Compatible(*operand_shapes[0], inferred_shape));\n      return Tuple({all_reduce});\n    }\n    return all_reduce;\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5143,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kAdd, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kAdd, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3100,
          "old_api": null,
          "new_api": "tuple_shapes_size",
          "old_text": null,
          "new_text": "operand_shape->tuple_shapes_size()",
          "old_line_content": "      }",
          "new_line_content": "      if (operand_shape->tuple_shapes_size() == 0) {",
          "content_same": false
        },
        {
          "line": 5149,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kSubtract, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kSubtract, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3105,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "operand_shape->tuple_shapes(0).element_type()",
          "old_line_content": "              \"All the shapes of a tuple input of AllReduce must have the same \"",
          "new_line_content": "            operand_shape->tuple_shapes(0).element_type()) {",
          "content_same": false
        },
        {
          "line": 3106,
          "old_api": null,
          "new_api": "Unimplemented",
          "old_text": null,
          "new_text": "Unimplemented(\n              \"All the shapes of a tuple input of AllReduce must have the same \"\n              \"element type\")",
          "old_line_content": "              \"element type\");",
          "new_line_content": "          return Unimplemented(",
          "content_same": false
        },
        {
          "line": 5155,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kMultiply, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kMultiply, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3110,
          "old_api": null,
          "new_api": "tuple_shapes",
          "old_text": null,
          "new_text": "operand_shape->tuple_shapes(i)",
          "old_line_content": "      }",
          "new_line_content": "        operand_shapes.push_back(&operand_shape->tuple_shapes(i));",
          "content_same": false
        },
        {
          "line": 3111,
          "old_api": null,
          "new_api": "GetTupleElement",
          "old_text": null,
          "new_text": "GetTupleElement(operand, i)",
          "old_line_content": "    } else {",
          "new_line_content": "        operands.push_back(GetTupleElement(operand, i));",
          "content_same": false
        },
        {
          "line": 5161,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kDivide, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kDivide, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3114,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operand_shapes.push_back(operand_shape)",
          "old_line_content": "    }",
          "new_line_content": "      operand_shapes.push_back(operand_shape);",
          "content_same": false
        },
        {
          "line": 3115,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operands.push_back(operand)",
          "old_line_content": "",
          "new_line_content": "      operands.push_back(operand);",
          "content_same": false
        },
        {
          "line": 5167,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kRemainder, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kRemainder, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3122,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "InvalidArgument(\"shape_with_layout must have the layout set: %s\",\n                               layout->ToString())",
          "old_line_content": "      }",
          "new_line_content": "        return InvalidArgument(\"shape_with_layout must have the layout set: %s\",",
          "content_same": false
        },
        {
          "line": 3125,
          "old_api": null,
          "new_api": "ShapeUtil::Compatible(*layout, *operand_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::Compatible(*layout, *operand_shape)",
          "old_line_content": "            \"Provided shape_with_layout must be compatible with the \"",
          "new_line_content": "      if (!ShapeUtil::Compatible(*layout, *operand_shape)) {",
          "content_same": false
        },
        {
          "line": 3126,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "InvalidArgument(\n            \"Provided shape_with_layout must be compatible with the \"\n            \"operand shape: %s vs %s\",\n            layout->ToString(), operand_shape->ToString())",
          "old_line_content": "            \"operand shape: %s vs %s\",",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 5173,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kMaximum, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kMaximum, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3131,
          "old_api": null,
          "new_api": "set_constrain_layout",
          "old_text": null,
          "new_text": "instr.set_constrain_layout(true)",
          "old_line_content": "        // For a single-element tuple, take the tuple element shape.",
          "new_line_content": "      instr.set_constrain_layout(true);",
          "content_same": false
        },
        {
          "line": 5179,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kMinimum, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kMinimum, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3134,
          "old_api": null,
          "new_api": "tuple_shapes_size",
          "old_text": null,
          "new_text": "layout->tuple_shapes_size()",
          "old_line_content": "      } else {",
          "new_line_content": "        TF_RET_CHECK(layout->tuple_shapes_size() == 1);",
          "content_same": false
        },
        {
          "line": 3137,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "layout->ToProto()",
          "old_line_content": "    } else {",
          "new_line_content": "        *instr.mutable_shape() = layout->ToProto();",
          "content_same": false
        },
        {
          "line": 5185,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kAnd, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kAnd, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3140,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "inferred_shape.ToProto()",
          "old_line_content": "",
          "new_line_content": "      *instr.mutable_shape() = inferred_shape.ToProto();",
          "content_same": false
        },
        {
          "line": 5191,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kOr, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kOr, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3144,
          "old_api": null,
          "new_api": "add_replica_groups",
          "old_text": null,
          "new_text": "instr.add_replica_groups()",
          "old_line_content": "",
          "new_line_content": "      *instr.add_replica_groups() = group;",
          "content_same": false
        },
        {
          "line": 3147,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "channel_id.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (channel_id.has_value()) {",
          "content_same": false
        },
        {
          "line": 3148,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "channel_id->handle()",
          "old_line_content": "",
          "new_line_content": "      instr.set_channel_id(channel_id->handle());",
          "content_same": false
        },
        {
          "line": 5197,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kXor, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kXor, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3151,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "use_global_device_ids.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (use_global_device_ids.has_value()) {",
          "content_same": false
        },
        {
          "line": 3152,
          "old_api": null,
          "new_api": "set_use_global_device_ids",
          "old_text": null,
          "new_text": "instr.set_use_global_device_ids(*use_global_device_ids)",
          "old_line_content": "",
          "new_line_content": "      instr.set_use_global_device_ids(*use_global_device_ids);",
          "content_same": false
        },
        {
          "line": 5202,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kNot, operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kNot, operand);",
          "content_same": false
        },
        {
          "line": 3155,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(computation, &instr)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(auto all_reduce,",
          "new_line_content": "    AddCalledComputation(computation, &instr);",
          "content_same": false
        },
        {
          "line": 5206,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kPopulationCount, operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kPopulationCount, operand);",
          "content_same": false
        },
        {
          "line": 5211,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kShiftLeft, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kShiftLeft, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3165,
          "old_api": null,
          "new_api": "ShapeUtil::Compatible(*operand_shapes[0], inferred_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::Compatible(*operand_shapes[0], inferred_shape)",
          "old_line_content": "    }",
          "new_line_content": "      TF_RET_CHECK(ShapeUtil::Compatible(*operand_shapes[0], inferred_shape));",
          "content_same": false
        },
        {
          "line": 3166,
          "old_api": null,
          "new_api": "Tuple",
          "old_text": null,
          "new_text": "Tuple({all_reduce})",
          "old_line_content": "    return all_reduce;",
          "new_line_content": "      return Tuple({all_reduce});",
          "content_same": false
        },
        {
          "line": 5217,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kShiftRightArithmetic, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kShiftRightArithmetic, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 5223,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kShiftRightLogical, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kShiftRightLogical, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3180,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n\n    std::vector<const Shape*> operand_shapes;\n    std::vector<XlaOp> operands;\n    if (operand_shape->IsTuple()) {\n      if (operand_shape->tuple_shapes_size() == 0) {\n        return Unimplemented(\"0 element tuple AllGather is not supported\");\n      }\n      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {\n        operand_shapes.push_back(&operand_shape->tuple_shapes(i));\n        operands.push_back(GetTupleElement(operand, i));\n      }\n    } else {\n      operand_shapes.push_back(operand_shape);\n      operands.push_back(operand);\n    }\n\n    TF_ASSIGN_OR_RETURN(Shape inferred_shape,\n                        ShapeInference::InferAllGatherShape(\n                            operand_shapes, all_gather_dimension, shard_count));\n    if (layout) {\n      *inferred_shape.mutable_layout() = *layout;\n      instr.set_constrain_layout(true);\n    }\n    *instr.mutable_shape() = inferred_shape.ToProto();\n\n    instr.add_dimensions(all_gather_dimension);\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n    if (use_global_device_ids.has_value()) {\n      instr.set_use_global_device_ids(use_global_device_ids.value());\n    }\n\n    TF_ASSIGN_OR_RETURN(auto all_gather,\n                        AddInstruction(std::move(instr),\n                                       async ? HloOpcode::kAllGatherStart\n                                             : HloOpcode::kAllGather,\n                                       operands));\n    return all_gather;\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5230,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Reduce(operand, init_value, computation,\n                                   dimensions_to_reduce)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->Reduce(operand, init_value, computation,",
          "content_same": false
        },
        {
          "line": 3187,
          "old_api": null,
          "new_api": "tuple_shapes_size",
          "old_text": null,
          "new_text": "operand_shape->tuple_shapes_size()",
          "old_line_content": "      }",
          "new_line_content": "      if (operand_shape->tuple_shapes_size() == 0) {",
          "content_same": false
        },
        {
          "line": 3191,
          "old_api": null,
          "new_api": "tuple_shapes",
          "old_text": null,
          "new_text": "operand_shape->tuple_shapes(i)",
          "old_line_content": "      }",
          "new_line_content": "        operand_shapes.push_back(&operand_shape->tuple_shapes(i));",
          "content_same": false
        },
        {
          "line": 3192,
          "old_api": null,
          "new_api": "GetTupleElement",
          "old_text": null,
          "new_text": "GetTupleElement(operand, i)",
          "old_line_content": "    } else {",
          "new_line_content": "        operands.push_back(GetTupleElement(operand, i));",
          "content_same": false
        },
        {
          "line": 5240,
          "old_api": null,
          "new_api": "Reduce",
          "old_text": null,
          "new_text": "builder->Reduce(operands, init_values, computation,\n                         dimensions_to_reduce)",
          "old_line_content": "}",
          "new_line_content": "  return builder->Reduce(operands, init_values, computation,",
          "content_same": false
        },
        {
          "line": 3195,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operand_shapes.push_back(operand_shape)",
          "old_line_content": "    }",
          "new_line_content": "      operand_shapes.push_back(operand_shape);",
          "content_same": false
        },
        {
          "line": 3196,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operands.push_back(operand)",
          "old_line_content": "",
          "new_line_content": "      operands.push_back(operand);",
          "content_same": false
        },
        {
          "line": 5246,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->ReduceAll(operand, init_value, computation)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->ReduceAll(operand, init_value, computation);",
          "content_same": false
        },
        {
          "line": 3203,
          "old_api": null,
          "new_api": "mutable_layout",
          "old_text": null,
          "new_text": "inferred_shape.mutable_layout()",
          "old_line_content": "    }",
          "new_line_content": "      *inferred_shape.mutable_layout() = *layout;",
          "content_same": false
        },
        {
          "line": 5253,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->ReduceWindow(operand, init_value, computation,\n                                         window_dimensions, window_strides,\n                                         padding)",
          "old_line_content": "                                         padding);",
          "new_line_content": "  return operand.builder()->ReduceWindow(operand, init_value, computation,",
          "content_same": false
        },
        {
          "line": 3212,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "channel_id.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (channel_id.has_value()) {",
          "content_same": false
        },
        {
          "line": 3215,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "use_global_device_ids.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (use_global_device_ids.has_value()) {",
          "content_same": false
        },
        {
          "line": 3216,
          "old_api": null,
          "new_api": "value",
          "old_text": null,
          "new_text": "use_global_device_ids.value()",
          "old_line_content": "",
          "new_line_content": "      instr.set_use_global_device_ids(use_global_device_ids.value());",
          "content_same": false
        },
        {
          "line": 5263,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "operands.empty()",
          "old_line_content": "                                             window_dimensions, window_strides,",
          "new_line_content": "  CHECK(!operands.empty());",
          "content_same": false
        },
        {
          "line": 5264,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operands[0].builder()->ReduceWindow(operands, init_values, computation,\n                                             window_dimensions, window_strides,\n                                             padding)",
          "old_line_content": "                                             padding);",
          "new_line_content": "  return operands[0].builder()->ReduceWindow(operands, init_values, computation,",
          "content_same": false
        },
        {
          "line": 5277,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->ReduceWindowWithGeneralPadding(\n      absl::MakeSpan(&operand, 1), absl::MakeSpan(&init_value, 1), computation,\n      window_dimensions, window_strides, base_dilations, window_dilations,\n      padding)",
          "old_line_content": "      window_dimensions, window_strides, base_dilations, window_dilations,",
          "new_line_content": "  return operand.builder()->ReduceWindowWithGeneralPadding(",
          "content_same": false
        },
        {
          "line": 5278,
          "old_api": null,
          "new_api": "absl::MakeSpan(&init_value, 1)",
          "old_text": null,
          "new_text": "absl::MakeSpan(&init_value, 1)",
          "old_line_content": "      padding);",
          "new_line_content": "      absl::MakeSpan(&operand, 1), absl::MakeSpan(&init_value, 1), computation,",
          "content_same": false
        },
        {
          "line": 3232,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    TF_ASSIGN_OR_RETURN(const Shape* branch_index_shape,\n                        GetShapePtr(branch_index));\n    std::vector<Shape> branch_operand_shapes(branch_operands.size());\n    std::vector<ProgramShape> branch_computation_shapes(\n        branch_computations.size());\n    for (int j = 0, end = branch_operands.size(); j < end; ++j) {\n      TF_ASSIGN_OR_RETURN(branch_operand_shapes[j],\n                          GetShape(branch_operands[j]));\n      TF_ASSIGN_OR_RETURN(branch_computation_shapes[j],\n                          branch_computations[j]->GetProgramShape());\n    }\n    TF_ASSIGN_OR_RETURN(const Shape shape,\n                        ShapeInference::InferConditionalShape(\n                            *branch_index_shape, branch_computation_shapes,\n                            branch_operand_shapes));\n    *instr.mutable_shape() = shape.ToProto();\n\n    for (const XlaComputation* branch_computation : branch_computations) {\n      AddCalledComputation(*branch_computation, &instr);\n    }\n\n    std::vector<XlaOp> operands(1, branch_index);\n    for (const XlaOp branch_operand : branch_operands) {\n      operands.emplace_back(branch_operand);\n    }\n    return AddInstruction(std::move(instr), HloOpcode::kConditional,\n                          absl::MakeSpan(operands));\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1185,
          "old_api": null,
          "new_api": "ShapeUtil::SameDimensions(*lhs_shape, *rhs_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::SameDimensions(*lhs_shape, *rhs_shape)",
          "old_line_content": "        Shape output_shape = shape;",
          "new_line_content": "        if (!ShapeUtil::SameDimensions(*lhs_shape, *rhs_shape)) {",
          "content_same": false
        },
        {
          "line": 1187,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "lhs_shape->element_type()",
          "old_line_content": "        TF_ASSIGN_OR_RETURN(UnboundedBroadcastResult broadcast_result,",
          "new_line_content": "          output_shape.set_element_type(lhs_shape->element_type());",
          "content_same": false
        },
        {
          "line": 3243,
          "old_api": null,
          "new_api": "GetProgramShape",
          "old_text": null,
          "new_text": "TF_ASSIGN_OR_RETURN(branch_computation_shapes[j],\n                          branch_computations[j]->GetProgramShape())",
          "old_line_content": "    }",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(branch_computation_shapes[j],",
          "content_same": false
        },
        {
          "line": 3244,
          "old_api": null,
          "new_api": "GetProgramShape",
          "old_text": null,
          "new_text": "branch_computations[j]->GetProgramShape()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape shape,",
          "new_line_content": "                          branch_computations[j]->GetProgramShape());",
          "content_same": false
        },
        {
          "line": 5291,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "operands.empty()",
          "old_line_content": "      operands, init_values, computation, window_dimensions, window_strides,",
          "new_line_content": "  CHECK(!operands.empty());",
          "content_same": false
        },
        {
          "line": 5292,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operands[0].builder()->ReduceWindowWithGeneralPadding(\n      operands, init_values, computation, window_dimensions, window_strides,\n      base_dilations, window_dilations, padding)",
          "old_line_content": "      base_dilations, window_dilations, padding);",
          "new_line_content": "  return operands[0].builder()->ReduceWindowWithGeneralPadding(",
          "content_same": false
        },
        {
          "line": 1199,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "direction.has_value()",
          "old_line_content": "            \"kCompare expects a ComparisonDirection, but none provided.\");",
          "new_line_content": "      if (!direction.has_value()) {",
          "content_same": false
        },
        {
          "line": 1200,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n            \"kCompare expects a ComparisonDirection, but none provided.\")",
          "old_line_content": "      }",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 3250,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "    for (const XlaComputation* branch_computation : branch_computations) {",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3253,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(*branch_computation, &instr)",
          "old_line_content": "",
          "new_line_content": "      AddCalledComputation(*branch_computation, &instr);",
          "content_same": false
        },
        {
          "line": 1206,
          "old_api": null,
          "new_api": "Compare",
          "old_text": null,
          "new_text": "Compare(shape, updated_lhs, updated_rhs, *direction, *type)",
          "old_line_content": "    }",
          "new_line_content": "        return Compare(shape, updated_lhs, updated_rhs, *direction, *type);",
          "content_same": false
        },
        {
          "line": 5303,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->AllGather(operand, all_gather_dimension,\n                                      shard_count, replica_groups, channel_id,\n                                      layout, use_global_device_ids)",
          "old_line_content": "                                      layout, use_global_device_ids);",
          "new_line_content": "  return operand.builder()->AllGather(operand, all_gather_dimension,",
          "content_same": false
        },
        {
          "line": 1210,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "direction.has_value()",
          "old_line_content": "          \"A comparison direction is provided for a non-compare opcode: %s.\",",
          "new_line_content": "    if (direction.has_value()) {",
          "content_same": false
        },
        {
          "line": 3260,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kConditional,",
          "content_same": false
        },
        {
          "line": 3261,
          "old_api": null,
          "new_api": "absl::MakeSpan(operands)",
          "old_text": null,
          "new_text": "absl::MakeSpan(operands)",
          "old_line_content": "}",
          "new_line_content": "                          absl::MakeSpan(operands));",
          "content_same": false
        },
        {
          "line": 1215,
          "old_api": null,
          "new_api": "BinaryOpNoBroadcast",
          "old_text": null,
          "new_text": "BinaryOpNoBroadcast(binop, shape, updated_lhs, updated_rhs)",
          "old_line_content": "}",
          "new_line_content": "    return BinaryOpNoBroadcast(binop, shape, updated_lhs, updated_rhs);",
          "content_same": false
        },
        {
          "line": 3266,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "op.builder()",
          "old_line_content": "        \"XlaOp with handle %d is built by builder '%s', but is trying to use \"",
          "new_line_content": "  if (this != op.builder()) {",
          "content_same": false
        },
        {
          "line": 3267,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "InvalidArgument(\n        \"XlaOp with handle %d is built by builder '%s', but is trying to use \"\n        \"it in builder '%s'\",\n        op.handle(), op.builder()->name(), name())",
          "old_line_content": "        \"it in builder '%s'\",",
          "new_line_content": "    return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 5315,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operands[0].builder()->AllGather(\n      operands[0].builder()->Tuple(operands), all_gather_dimension, shard_count,\n      replica_groups, channel_id, layout, use_global_device_ids)",
          "old_line_content": "      replica_groups, channel_id, layout, use_global_device_ids);",
          "new_line_content": "  return operands[0].builder()->AllGather(",
          "content_same": false
        },
        {
          "line": 5316,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operands[0].builder()->Tuple(operands)",
          "old_line_content": "}",
          "new_line_content": "      operands[0].builder()->Tuple(operands), all_gather_dimension, shard_count,",
          "content_same": false
        },
        {
          "line": 1223,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "  });",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1224,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), binop, {lhs, rhs});",
          "content_same": false
        },
        {
          "line": 3272,
          "old_api": null,
          "new_api": "OkStatus",
          "old_text": null,
          "new_text": "OkStatus()",
          "old_line_content": "",
          "new_line_content": "  return OkStatus();",
          "content_same": false
        },
        {
          "line": 5322,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->CrossReplicaSum(operand, replica_groups)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->CrossReplicaSum(operand, replica_groups);",
          "content_same": false
        },
        {
          "line": 3278,
          "old_api": null,
          "new_api": "absl::Span<const XlaOp>({operand})",
          "old_text": null,
          "new_text": "absl::Span<const XlaOp>({operand})",
          "old_line_content": "                dimensions_to_reduce);",
          "new_line_content": "  return Reduce(absl::Span<const XlaOp>({operand}),",
          "content_same": false
        },
        {
          "line": 3279,
          "old_api": null,
          "new_api": "absl::Span<const XlaOp>({init_value})",
          "old_text": null,
          "new_text": "absl::Span<const XlaOp>({init_value})",
          "old_line_content": "}",
          "new_line_content": "                absl::Span<const XlaOp>({init_value}), computation,",
          "content_same": false
        },
        {
          "line": 1233,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "operand_shape.element_type()",
          "old_line_content": "",
          "new_line_content": "      Comparison::DefaultComparisonType(operand_shape.element_type()));",
          "content_same": false
        },
        {
          "line": 5330,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->AllReduce(operand, computation, replica_groups,\n                                      channel_id, shape_with_layout,\n                                      use_global_device_ids)",
          "old_line_content": "                                      use_global_device_ids);",
          "new_line_content": "  return operand.builder()->AllReduce(operand, computation, replica_groups,",
          "content_same": false
        },
        {
          "line": 3287,
          "old_api": null,
          "new_api": "GetProgramShape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const ProgramShape& called_program_shape,\n                        computation.GetProgramShape());\n\n    std::vector<XlaOp> all_operands;\n    all_operands.insert(all_operands.end(), operands.begin(), operands.end());\n    all_operands.insert(all_operands.end(), init_values.begin(),\n                        init_values.end());\n\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes,\n                        GetOperandShapes(all_operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferReduceShape(\n            operand_shape_ptrs, dimensions_to_reduce, called_program_shape));\n    return ReduceInternal(shape, all_operands, computation,\n                          dimensions_to_reduce);\n  })",
          "old_line_content": "                        computation.GetProgramShape());",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1242,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "}",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1243,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kCompare, {lhs, rhs});",
          "content_same": false
        },
        {
          "line": 3293,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "init_values.begin()",
          "old_line_content": "",
          "new_line_content": "    all_operands.insert(all_operands.end(), init_values.begin(),",
          "content_same": false
        },
        {
          "line": 3294,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "init_values.end()",
          "old_line_content": "    std::vector<const Shape*> operand_shape_ptrs;",
          "new_line_content": "                        init_values.end());",
          "content_same": false
        },
        {
          "line": 5342,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operands[0].builder()->AllReduce(\n      operands[0].builder()->Tuple(operands), computation, replica_groups,\n      channel_id, shape_with_layout, use_global_device_ids)",
          "old_line_content": "      channel_id, shape_with_layout, use_global_device_ids);",
          "new_line_content": "  return operands[0].builder()->AllReduce(",
          "content_same": false
        },
        {
          "line": 5343,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operands[0].builder()->Tuple(operands)",
          "old_line_content": "}",
          "new_line_content": "      operands[0].builder()->Tuple(operands), computation, replica_groups,",
          "content_same": false
        },
        {
          "line": 3299,
          "old_api": null,
          "new_api": "std::back_inserter(operand_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(operand_shape_ptrs)",
          "old_line_content": "",
          "new_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 1255,
          "old_api": null,
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": null,
          "new_text": "TF_ASSIGN_OR_RETURN(updated_output,\n                        BroadcastScalarToOutputShapeWithUnbounded(\n                            this, scalar, output, output_shape_copy))",
          "old_line_content": "                            this, scalar, output, output_shape_copy));",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(updated_output,",
          "content_same": false
        },
        {
          "line": 1256,
          "old_api": null,
          "new_api": "BroadcastScalarToOutputShapeWithUnbounded",
          "old_text": null,
          "new_text": "BroadcastScalarToOutputShapeWithUnbounded(\n                            this, scalar, output, output_shape_copy)",
          "old_line_content": "    return updated_output;",
          "new_line_content": "                        BroadcastScalarToOutputShapeWithUnbounded(",
          "content_same": false
        },
        {
          "line": 5353,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->ReduceScatter(\n      operand, computation, scatter_dimension, shard_count, replica_groups,\n      channel_id, layout, use_global_device_ids)",
          "old_line_content": "      channel_id, layout, use_global_device_ids);",
          "new_line_content": "  return operand.builder()->ReduceScatter(",
          "content_same": false
        },
        {
          "line": 3306,
          "old_api": null,
          "new_api": "ReduceInternal",
          "old_text": null,
          "new_text": "ReduceInternal(shape, all_operands, computation,\n                          dimensions_to_reduce)",
          "old_line_content": "  });",
          "new_line_content": "    return ReduceInternal(shape, all_operands, computation,",
          "content_same": false
        },
        {
          "line": 1261,
          "old_api": null,
          "new_api": "TF_ASSIGN_OR_RETURN",
          "old_text": null,
          "new_text": "TF_ASSIGN_OR_RETURN(updated_output,\n                      AddBroadcastSequence(*output_shape, updated_output))",
          "old_line_content": "  return updated_output;",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(updated_output,",
          "content_same": false
        },
        {
          "line": 1262,
          "old_api": null,
          "new_api": "AddBroadcastSequence",
          "old_text": null,
          "new_text": "AddBroadcastSequence(*output_shape, updated_output)",
          "old_line_content": "}",
          "new_line_content": "                      AddBroadcastSequence(*output_shape, updated_output));",
          "content_same": false
        },
        {
          "line": 1267,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    XlaOp updated_lhs = lhs;\n    XlaOp updated_rhs = rhs;\n    XlaOp updated_ehs = ehs;\n\n    // The client API supports implicit broadcast for kSelect and kClamp, but\n    // XLA does not support implicit broadcast. Make implicit broadcast explicit\n    // and update the operands.\n    if (triop == HloOpcode::kSelect || triop == HloOpcode::kClamp) {\n      TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n      TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n      TF_ASSIGN_OR_RETURN(const Shape* ehs_shape, GetShapePtr(ehs));\n      TF_ASSIGN_OR_RETURN(\n          std::optional<Shape> output_shape,\n          ShapeInference::InferScalarBroadcastShape(\n              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})));\n\n      // Scalar broadcast if mix of scalars and non-scalars\n      if (output_shape.has_value()) {\n        if (ShapeUtil::IsScalar(*lhs_shape)) {\n          TF_ASSIGN_OR_RETURN(\n              updated_lhs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/lhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *rhs_shape) ? rhs : ehs));\n        }\n        if (ShapeUtil::IsScalar(*rhs_shape)) {\n          TF_ASSIGN_OR_RETURN(\n              updated_rhs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/rhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : ehs));\n        }\n        if (ShapeUtil::IsScalar(*ehs_shape)) {\n          TF_ASSIGN_OR_RETURN(\n              updated_ehs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/ehs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : rhs));\n        }\n      }\n    }\n\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(updated_lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(updated_rhs));\n    TF_ASSIGN_OR_RETURN(const Shape* ehs_shape, GetShapePtr(updated_ehs));\n    TF_ASSIGN_OR_RETURN(const Shape inferred_shape,\n                        ShapeInference::InferTernaryOpShape(\n                            triop, *lhs_shape, *rhs_shape, *ehs_shape));\n\n    return AddOpWithShape(triop, inferred_shape,\n                          {updated_lhs, updated_rhs, updated_ehs});\n  })",
          "old_line_content": "    XlaOp updated_rhs = rhs;",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5363,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->AllToAll(operand, split_dimension, concat_dimension,\n                                     split_count, replica_groups, layout,\n                                     channel_id)",
          "old_line_content": "                                     channel_id);",
          "new_line_content": "  return operand.builder()->AllToAll(operand, split_dimension, concat_dimension,",
          "content_same": false
        },
        {
          "line": 3317,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "    for (int64_t dim : dimensions_to_reduce) {",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3320,
          "old_api": null,
          "new_api": "add_dimensions",
          "old_text": null,
          "new_text": "instr.add_dimensions(dim)",
          "old_line_content": "",
          "new_line_content": "      instr.add_dimensions(dim);",
          "content_same": false
        },
        {
          "line": 3323,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(computation, &instr)",
          "old_line_content": "  });",
          "new_line_content": "    AddCalledComputation(computation, &instr);",
          "content_same": false
        },
        {
          "line": 3324,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kReduce, all_operands);",
          "content_same": false
        },
        {
          "line": 5372,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "operands.empty()",
          "old_line_content": "                                              channel_id);",
          "new_line_content": "  CHECK(!operands.empty());",
          "content_same": false
        },
        {
          "line": 5373,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operands[0].builder()->AllToAllTuple(operands, replica_groups, layout,\n                                              channel_id)",
          "old_line_content": "}",
          "new_line_content": "  return operands[0].builder()->AllToAllTuple(operands, replica_groups, layout,",
          "content_same": false
        },
        {
          "line": 1281,
          "old_api": null,
          "new_api": "ShapeInference::InferScalarBroadcastShape(\n              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape}))",
          "old_text": null,
          "new_text": "ShapeInference::InferScalarBroadcastShape(\n              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape}))",
          "old_line_content": "",
          "new_line_content": "          ShapeInference::InferScalarBroadcastShape(",
          "content_same": false
        },
        {
          "line": 1282,
          "old_api": null,
          "new_api": "absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})",
          "old_text": null,
          "new_text": "absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})",
          "old_line_content": "      // Scalar broadcast if mix of scalars and non-scalars",
          "new_line_content": "              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})));",
          "content_same": false
        },
        {
          "line": 3333,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "all_dimnos.end()",
          "old_line_content": "  });",
          "new_line_content": "    std::iota(all_dimnos.begin(), all_dimnos.end(), 0);",
          "content_same": false
        },
        {
          "line": 1286,
          "old_api": null,
          "new_api": "ShapeUtil::IsScalar(*lhs_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::IsScalar(*lhs_shape)",
          "old_line_content": "              updated_lhs,",
          "new_line_content": "        if (ShapeUtil::IsScalar(*lhs_shape)) {",
          "content_same": false
        },
        {
          "line": 3334,
          "old_api": null,
          "new_api": "Reduce",
          "old_text": null,
          "new_text": "Reduce(operand, init_value, computation, all_dimnos)",
          "old_line_content": "}",
          "new_line_content": "    return Reduce(operand, init_value, computation, all_dimnos);",
          "content_same": false
        },
        {
          "line": 5382,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->AllToAllTuple(operand, split_dimension,\n                                          concat_dimension, split_count,\n                                          replica_groups, layout, channel_id)",
          "old_line_content": "                                          replica_groups, layout, channel_id);",
          "new_line_content": "  return operand.builder()->AllToAllTuple(operand, split_dimension,",
          "content_same": false
        },
        {
          "line": 1289,
          "old_api": null,
          "new_api": "BroadcastScalarToOutputShape",
          "old_text": null,
          "new_text": "BroadcastScalarToOutputShape(\n                  /*scalar=*/lhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *rhs_shape) ? rhs : ehs)",
          "old_line_content": "                  /*output=*/",
          "new_line_content": "              BroadcastScalarToOutputShape(",
          "content_same": false
        },
        {
          "line": 1294,
          "old_api": null,
          "new_api": "ShapeUtil::IsScalar(*rhs_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::IsScalar(*rhs_shape)",
          "old_line_content": "              updated_rhs,",
          "new_line_content": "        if (ShapeUtil::IsScalar(*rhs_shape)) {",
          "content_same": false
        },
        {
          "line": 3343,
          "old_api": null,
          "new_api": "absl::MakeSpan(&operand, 1)",
          "old_text": null,
          "new_text": "absl::MakeSpan(&operand, 1)",
          "old_line_content": "                      window_dimensions, window_strides, padding);",
          "new_line_content": "  return ReduceWindow(absl::MakeSpan(&operand, 1),",
          "content_same": false
        },
        {
          "line": 3344,
          "old_api": null,
          "new_api": "absl::MakeSpan(&init_value, 1)",
          "old_text": null,
          "new_text": "absl::MakeSpan(&init_value, 1)",
          "old_line_content": "}",
          "new_line_content": "                      absl::MakeSpan(&init_value, 1), computation,",
          "content_same": false
        },
        {
          "line": 1297,
          "old_api": null,
          "new_api": "BroadcastScalarToOutputShape",
          "old_text": null,
          "new_text": "BroadcastScalarToOutputShape(\n                  /*scalar=*/rhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : ehs)",
          "old_line_content": "                  /*output=*/",
          "new_line_content": "              BroadcastScalarToOutputShape(",
          "content_same": false
        },
        {
          "line": 5390,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->CollectiveBroadcast(operand, replica_groups,\n                                                channel_id)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->CollectiveBroadcast(operand, replica_groups,",
          "content_same": false
        },
        {
          "line": 1302,
          "old_api": null,
          "new_api": "ShapeUtil::IsScalar(*ehs_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::IsScalar(*ehs_shape)",
          "old_line_content": "              updated_ehs,",
          "new_line_content": "        if (ShapeUtil::IsScalar(*ehs_shape)) {",
          "content_same": false
        },
        {
          "line": 5398,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->CollectivePermute(operand, source_target_pairs,\n                                              channel_id)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->CollectivePermute(operand, source_target_pairs,",
          "content_same": false
        },
        {
          "line": 1305,
          "old_api": null,
          "new_api": "BroadcastScalarToOutputShape",
          "old_text": null,
          "new_text": "BroadcastScalarToOutputShape(\n                  /*scalar=*/ehs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : rhs)",
          "old_line_content": "                  /*output=*/",
          "new_line_content": "              BroadcastScalarToOutputShape(",
          "content_same": false
        },
        {
          "line": 3354,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    const Shape* operand_shape = nullptr;\n    for (const auto& operand : operands) {\n      TF_ASSIGN_OR_RETURN(operand_shape, GetShapePtr(operand));\n      TF_RETURN_IF_ERROR(ValidatePaddingValues(\n          operand_shape->dimensions(), window_dimensions, window_strides));\n    }\n    CHECK(operand_shape != nullptr);\n    std::vector<std::pair<int64_t, int64_t>> padding_values =\n        MakePadding(operand_shape->dimensions(), window_dimensions,\n                    window_strides, padding);\n    TF_ASSIGN_OR_RETURN(auto window,\n                        ShapeInference::InferWindowFromDimensions(\n                            window_dimensions, window_strides, padding_values,\n                            /*lhs_dilation=*/{},\n                            /*rhs_dilation=*/{}));\n    PaddingType padding_type = PADDING_INVALID;\n    for (int64_t i = 0; i < operand_shape->rank(); ++i) {\n      if (operand_shape->is_dynamic_dimension(i) &&\n          !window_util::IsTrivialWindowDimension(window.dimensions(i)) &&\n          padding == Padding::kSame) {\n        // SAME padding can create dynamic padding sizes. The padding size\n        // need to be rewritten by dynamic padder using HloInstructions. We\n        // create a CustomCall to handle this.\n        padding_type = PADDING_SAME;\n      }\n    }\n    if (padding_type == PADDING_SAME) {\n      TF_ASSIGN_OR_RETURN(\n          HloInstructionProto instr,\n          ReduceWindowInternal(operands, init_values, computation,\n                               window_dimensions, window_strides, {}, {},\n                               padding_values));\n      instr.set_custom_call_target(\"DynamicReduceWindowSamePadding\");\n      std::vector<XlaOp> args;\n      args.insert(args.end(), operands.begin(), operands.end());\n      args.insert(args.end(), init_values.begin(), init_values.end());\n      return AddInstruction(std::move(instr), HloOpcode::kCustomCall, args);\n    }\n    return ReduceWindowWithGeneralPadding(\n        operands, init_values, computation, window_dimensions, window_strides,\n        /*base_dilations=*/{}, /*window_dilations=*/{}, padding_values);\n  })",
          "old_line_content": "    for (const auto& operand : operands) {",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5402,
          "old_api": null,
          "new_api": "ReplicaId",
          "old_text": null,
          "new_text": "builder->ReplicaId()",
          "old_line_content": "XlaOp SelectAndScatter(const XlaOp operand, const XlaComputation& select,",
          "new_line_content": "XlaOp ReplicaId(XlaBuilder* builder) { return builder->ReplicaId(); }",
          "content_same": false
        },
        {
          "line": 1308,
          "old_api": null,
          "new_api": "ShapeUtil::Equal(*output_shape, *lhs_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::Equal(*output_shape, *lhs_shape)",
          "old_line_content": "      }",
          "new_line_content": "                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : rhs));",
          "content_same": false
        },
        {
          "line": 3358,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "ValidatePaddingValues(\n          operand_shape->dimensions(), window_dimensions, window_strides)",
          "old_line_content": "    }",
          "new_line_content": "      TF_RETURN_IF_ERROR(ValidatePaddingValues(",
          "content_same": false
        },
        {
          "line": 5409,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->SelectAndScatter(operand, select, window_dimensions,\n                                             window_strides, padding, source,\n                                             init_value, scatter)",
          "old_line_content": "                                             init_value, scatter);",
          "new_line_content": "  return operand.builder()->SelectAndScatter(operand, select, window_dimensions,",
          "content_same": false
        },
        {
          "line": 3363,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "operand_shape->dimensions()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(auto window,",
          "new_line_content": "        MakePadding(operand_shape->dimensions(), window_dimensions,",
          "content_same": false
        },
        {
          "line": 1320,
          "old_api": null,
          "new_api": "AddOpWithShape",
          "old_text": null,
          "new_text": "AddOpWithShape(triop, inferred_shape,\n                          {updated_lhs, updated_rhs, updated_ehs})",
          "old_line_content": "  });",
          "new_line_content": "    return AddOpWithShape(triop, inferred_shape,",
          "content_same": false
        },
        {
          "line": 3372,
          "old_api": null,
          "new_api": "is_dynamic_dimension",
          "old_text": null,
          "new_text": "operand_shape->is_dynamic_dimension(i)",
          "old_line_content": "          padding == Padding::kSame) {",
          "new_line_content": "      if (operand_shape->is_dynamic_dimension(i) &&",
          "content_same": false
        },
        {
          "line": 3373,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "window.dimensions(i)",
          "old_line_content": "        // SAME padding can create dynamic padding sizes. The padding size",
          "new_line_content": "          !window_util::IsTrivialWindowDimension(window.dimensions(i)) &&",
          "content_same": false
        },
        {
          "line": 5420,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->SelectAndScatterWithGeneralPadding(\n      operand, select, window_dimensions, window_strides, padding, source,\n      init_value, scatter)",
          "old_line_content": "      init_value, scatter);",
          "new_line_content": "  return operand.builder()->SelectAndScatterWithGeneralPadding(",
          "content_same": false
        },
        {
          "line": 1328,
          "old_api": null,
          "new_api": "IsAllFirst",
          "old_text": null,
          "new_text": "literal.IsAllFirst()",
          "old_line_content": "      HloInstructionProto instr;",
          "new_line_content": "        literal.IsAllFirst()) {",
          "content_same": false
        },
        {
          "line": 5426,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kAbs, operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kAbs, operand);",
          "content_same": false
        },
        {
          "line": 1331,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "scalar.shape().ToProto()",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(",
          "new_line_content": "      *instr.mutable_shape() = scalar.shape().ToProto();",
          "content_same": false
        },
        {
          "line": 1332,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "scalar.ToProto()",
          "old_line_content": "          XlaOp scalar_op,",
          "new_line_content": "      *instr.mutable_literal() = scalar.ToProto();",
          "content_same": false
        },
        {
          "line": 5431,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "y.builder()->BinaryOp(HloOpcode::kAtan2, y, x, broadcast_dimensions)",
          "old_line_content": "",
          "new_line_content": "  return y.builder()->BinaryOp(HloOpcode::kAtan2, y, x, broadcast_dimensions);",
          "content_same": false
        },
        {
          "line": 1336,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "literal.shape().dimensions()",
          "old_line_content": "      HloInstructionProto instr;",
          "new_line_content": "      return Broadcast(scalar_op, literal.shape().dimensions());",
          "content_same": false
        },
        {
          "line": 5435,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kExp, operand)",
          "old_line_content": "XlaOp Expm1(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kExp, operand);",
          "content_same": false
        },
        {
          "line": 1340,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "literal.ToProto()",
          "old_line_content": "    }",
          "new_line_content": "      *instr.mutable_literal() = literal.ToProto();",
          "content_same": false
        },
        {
          "line": 1341,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "      return AddInstruction(std::move(instr), HloOpcode::kConstant);",
          "content_same": false
        },
        {
          "line": 3390,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "init_values.end()",
          "old_line_content": "    }",
          "new_line_content": "      args.insert(args.end(), init_values.begin(), init_values.end());",
          "content_same": false
        },
        {
          "line": 5438,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kExpm1, operand)",
          "old_line_content": "XlaOp Floor(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kExpm1, operand);",
          "content_same": false
        },
        {
          "line": 3393,
          "old_api": null,
          "new_api": "ReduceWindowWithGeneralPadding",
          "old_text": null,
          "new_text": "ReduceWindowWithGeneralPadding(\n        operands, init_values, computation, window_dimensions, window_strides,\n        /*base_dilations=*/{}, /*window_dilations=*/{}, padding_values)",
          "old_line_content": "        /*base_dilations=*/{}, /*window_dilations=*/{}, padding_values);",
          "new_line_content": "    return ReduceWindowWithGeneralPadding(",
          "content_same": false
        },
        {
          "line": 5441,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kFloor, operand)",
          "old_line_content": "XlaOp Ceil(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kFloor, operand);",
          "content_same": false
        },
        {
          "line": 1348,
          "old_api": null,
          "new_api": "is_static",
          "old_text": null,
          "new_text": "shape.is_static()",
          "old_line_content": "          \"The output of iota must not have dynamic dimensions: %s\",",
          "new_line_content": "    if (!shape.is_static()) {",
          "content_same": false
        },
        {
          "line": 5444,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kCeil, operand)",
          "old_line_content": "XlaOp Round(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kCeil, operand);",
          "content_same": false
        },
        {
          "line": 1351,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "shape.ToString()",
          "old_line_content": "    HloInstructionProto instr;",
          "new_line_content": "          shape.ToString());",
          "content_same": false
        },
        {
          "line": 5447,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kRoundNearestAfz, operand)",
          "old_line_content": "XlaOp RoundNearestEven(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kRoundNearestAfz, operand);",
          "content_same": false
        },
        {
          "line": 5450,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kRoundNearestEven, operand)",
          "old_line_content": "XlaOp Log(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kRoundNearestEven, operand);",
          "content_same": false
        },
        {
          "line": 1355,
          "old_api": null,
          "new_api": "add_dimensions",
          "old_text": null,
          "new_text": "instr.add_dimensions(iota_dimension)",
          "old_line_content": "  });",
          "new_line_content": "    instr.add_dimensions(iota_dimension);",
          "content_same": false
        },
        {
          "line": 1356,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kIota);",
          "content_same": false
        },
        {
          "line": 5453,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kLog, operand)",
          "old_line_content": "XlaOp Log1p(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kLog, operand);",
          "content_same": false
        },
        {
          "line": 3408,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (operands.size() == 1) {\n      const auto& operand = operands[0];\n      const auto& init_value = init_values[0];\n      TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n      operand_shapes.push_back(operand_shape);\n      TF_ASSIGN_OR_RETURN(const Shape* init_shape, GetShapePtr(init_value));\n      init_shapes.push_back(init_shape);\n\n      TF_ASSIGN_OR_RETURN(const ProgramShape& to_apply_shape,\n                          computation.GetProgramShape());\n      TF_ASSIGN_OR_RETURN(auto window,\n                          ShapeInference::InferWindowFromDimensions(\n                              window_dimensions, window_strides, padding,\n                              /*lhs_dilation=*/base_dilations,\n                              /*rhs_dilation=*/window_dilations));\n      TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferReduceWindowShape(\n                                           absl::MakeSpan(operand_shapes),\n                                           absl::MakeSpan(init_shapes), window,\n                                           to_apply_shape));\n      return ReduceWindowInternal(shape, operands[0], init_values[0],\n                                  computation, window);\n    }\n\n    TF_ASSIGN_OR_RETURN(\n        HloInstructionProto instr,\n        ReduceWindowInternal(operands, init_values, computation,\n                             window_dimensions, window_strides, base_dilations,\n                             window_dilations, padding));\n    std::vector<XlaOp> args;\n    args.insert(args.end(), operands.begin(), operands.end());\n    args.insert(args.end(), init_values.begin(), init_values.end());\n    return AddInstruction(std::move(instr), HloOpcode::kReduceWindow, args);\n  })",
          "old_line_content": "      const auto& operand = operands[0];",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1361,
          "old_api": null,
          "new_api": "ShapeUtil::MakeShape(type, {size})",
          "old_text": null,
          "new_text": "ShapeUtil::MakeShape(type, {size})",
          "old_line_content": "",
          "new_line_content": "  return Iota(ShapeUtil::MakeShape(type, {size}), /*iota_dimension=*/0);",
          "content_same": false
        },
        {
          "line": 3409,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "operands.size()",
          "old_line_content": "      const auto& init_value = init_values[0];",
          "new_line_content": "    if (operands.size() == 1) {",
          "content_same": false
        },
        {
          "line": 5456,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kLog1p, operand)",
          "old_line_content": "XlaOp Erf(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kLog1p, operand);",
          "content_same": false
        },
        {
          "line": 5459,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kErf, operand)",
          "old_line_content": "XlaOp Logistic(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kErf, operand);",
          "content_same": false
        },
        {
          "line": 1366,
          "old_api": null,
          "new_api": "GetProgramShape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(const ProgramShape& called_program_shape,\n                        computation.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferCallShape(\n                                         operand_shape_ptrs,\n                                         /*to_apply=*/called_program_shape));\n    *instr.mutable_shape() = shape.ToProto();\n\n    AddCalledComputation(computation, &instr);\n\n    return AddInstruction(std::move(instr), HloOpcode::kCall, operands);\n  })",
          "old_line_content": "    std::vector<const Shape*> operand_shape_ptrs;",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3415,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "init_shapes.push_back(init_shape)",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(const ProgramShape& to_apply_shape,",
          "new_line_content": "      init_shapes.push_back(init_shape);",
          "content_same": false
        },
        {
          "line": 5462,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kLogistic, operand)",
          "old_line_content": "XlaOp Sign(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kLogistic, operand);",
          "content_same": false
        },
        {
          "line": 5465,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kSign, operand)",
          "old_line_content": "XlaOp Clz(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kSign, operand);",
          "content_same": false
        },
        {
          "line": 1370,
          "old_api": null,
          "new_api": "std::back_inserter(operand_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(operand_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const ProgramShape& called_program_shape,",
          "new_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 5468,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kClz, operand)",
          "old_line_content": "XlaOp Cos(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kClz, operand);",
          "content_same": false
        },
        {
          "line": 5471,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kCos, operand)",
          "old_line_content": "XlaOp Sin(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kCos, operand);",
          "content_same": false
        },
        {
          "line": 5474,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kSin, operand)",
          "old_line_content": "XlaOp Tan(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kSin, operand);",
          "content_same": false
        },
        {
          "line": 3428,
          "old_api": null,
          "new_api": "ReduceWindowInternal",
          "old_text": null,
          "new_text": "ReduceWindowInternal(shape, operands[0], init_values[0],\n                                  computation, window)",
          "old_line_content": "    }",
          "new_line_content": "      return ReduceWindowInternal(shape, operands[0], init_values[0],",
          "content_same": false
        },
        {
          "line": 1381,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCall, operands);",
          "content_same": false
        },
        {
          "line": 5477,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kTan, operand)",
          "old_line_content": "XlaOp Tanh(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kTan, operand);",
          "content_same": false
        },
        {
          "line": 5480,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kTanh, operand)",
          "old_line_content": "XlaOp Real(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kTanh, operand);",
          "content_same": false
        },
        {
          "line": 5483,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kReal, operand)",
          "old_line_content": "XlaOp Imag(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kReal, operand);",
          "content_same": false
        },
        {
          "line": 1390,
          "old_api": null,
          "new_api": "insert",
          "old_text": null,
          "new_text": "parameter_numbers_.insert(parameter_number)",
          "old_line_content": "                             parameter_number);",
          "new_line_content": "    if (!parameter_numbers_.insert(parameter_number).second) {",
          "content_same": false
        },
        {
          "line": 1391,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"parameter %d already registered\",\n                             parameter_number)",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(\"parameter %d already registered\",",
          "content_same": false
        },
        {
          "line": 3439,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "init_values.end()",
          "old_line_content": "  });",
          "new_line_content": "    args.insert(args.end(), init_values.begin(), init_values.end());",
          "content_same": false
        },
        {
          "line": 3440,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kReduceWindow, args);",
          "content_same": false
        },
        {
          "line": 5486,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kImag, operand)",
          "old_line_content": "XlaOp Sqrt(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kImag, operand);",
          "content_same": false
        },
        {
          "line": 5489,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kSqrt, operand)",
          "old_line_content": "XlaOp Cbrt(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kSqrt, operand);",
          "content_same": false
        },
        {
          "line": 5492,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kCbrt, operand)",
          "old_line_content": "XlaOp Rsqrt(const XlaOp operand) {",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kCbrt, operand);",
          "content_same": false
        },
        {
          "line": 1397,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "replicated_at_leaf_buffers.empty()",
          "old_line_content": "      for (bool replicated : replicated_at_leaf_buffers) {",
          "new_line_content": "    if (!replicated_at_leaf_buffers.empty()) {",
          "content_same": false
        },
        {
          "line": 5495,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kRsqrt, operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kRsqrt, operand);",
          "content_same": false
        },
        {
          "line": 1400,
          "old_api": null,
          "new_api": "add_replicated_at_leaf_buffers",
          "old_text": null,
          "new_text": "replication->add_replicated_at_leaf_buffers(replicated)",
          "old_line_content": "    }",
          "new_line_content": "        replication->add_replicated_at_leaf_buffers(replicated);",
          "content_same": false
        },
        {
          "line": 1403,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kParameter);",
          "content_same": false
        },
        {
          "line": 5500,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "lhs.builder()->BinaryOp(HloOpcode::kPower, lhs, rhs,\n                                 broadcast_dimensions)",
          "old_line_content": "}",
          "new_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kPower, lhs, rhs,",
          "content_same": false
        },
        {
          "line": 3453,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "operands.size()",
          "old_line_content": "    const auto& init_value = init_values[i];",
          "new_line_content": "  for (int i = 0; i < operands.size(); ++i) {",
          "content_same": false
        },
        {
          "line": 1409,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(\n        const Shape& shape,\n        ShapeInference::InferBroadcastShape(*operand_shape, broadcast_sizes));\n\n    // The client-level broadcast op just appends dimensions on the left (adds\n    // lowest numbered dimensions). The HLO broadcast instruction is more\n    // flexible and can add new dimensions anywhere. The instruction's\n    // dimensions field maps operand dimensions to dimensions in the broadcast\n    // output, so to append dimensions on the left the instruction's dimensions\n    // should just be the n highest dimension numbers of the output shape where\n    // n is the number of input dimensions.\n    const int64_t operand_rank = operand_shape->rank();\n    std::vector<int64_t> dimensions(operand_rank);\n    for (int i = 0; i < operand_rank; ++i) {\n      dimensions[i] = i + shape.rank() - operand_rank;\n    }\n    return InDimBroadcast(shape, operand, dimensions);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5505,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kIsFinite, operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kIsFinite, operand);",
          "content_same": false
        },
        {
          "line": 3459,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "init_shapes.push_back(init_shape)",
          "old_line_content": "  TF_ASSIGN_OR_RETURN(const ProgramShape& to_apply_shape,",
          "new_line_content": "    init_shapes.push_back(init_shape);",
          "content_same": false
        },
        {
          "line": 5509,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->ConvertElementType(operand, new_element_type)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->ConvertElementType(operand, new_element_type);",
          "content_same": false
        },
        {
          "line": 5513,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->BitcastConvertType(operand, new_element_type)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->BitcastConvertType(operand, new_element_type);",
          "content_same": false
        },
        {
          "line": 1422,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "operand_shape->rank()",
          "old_line_content": "    for (int i = 0; i < operand_rank; ++i) {",
          "new_line_content": "    const int64_t operand_rank = operand_shape->rank();",
          "content_same": false
        },
        {
          "line": 5518,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->StochasticConvertType(operand, random,\n                                                  new_element_type)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->StochasticConvertType(operand, random,",
          "content_same": false
        },
        {
          "line": 3474,
          "old_api": null,
          "new_api": "std::move(window)",
          "old_text": null,
          "new_text": "std::move(window)",
          "old_line_content": "  return instr;",
          "new_line_content": "  *instr.mutable_window() = std::move(window);",
          "content_same": false
        },
        {
          "line": 1427,
          "old_api": null,
          "new_api": "InDimBroadcast",
          "old_text": null,
          "new_text": "InDimBroadcast(shape, operand, dimensions)",
          "old_line_content": "}",
          "new_line_content": "    return InDimBroadcast(shape, operand, dimensions);",
          "content_same": false
        },
        {
          "line": 3475,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(computation, &instr)",
          "old_line_content": "}",
          "new_line_content": "  AddCalledComputation(computation, &instr);",
          "content_same": false
        },
        {
          "line": 5523,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->UnaryOp(HloOpcode::kNegate, operand)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kNegate, operand);",
          "content_same": false
        },
        {
          "line": 5527,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Transpose(operand, permutation)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Transpose(operand, permutation);",
          "content_same": false
        },
        {
          "line": 1434,
          "old_api": null,
          "new_api": "is_unbounded_dynamic",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    // Output shape, in the case of degenerate broadcast, the out_dim_size is\n    // not necessarily the same as the dimension sizes of the output shape.\n    TF_ASSIGN_OR_RETURN(auto output_shape,\n                        ShapeUtil::MakeValidatedShape(\n                            operand_shape->element_type(), out_dim_size));\n    TF_RET_CHECK(!output_shape.is_unbounded_dynamic())\n        << \"BroadcastInDim output must shape be static or bounded dynamic \"\n        << output_shape.ToString();\n    int64_t broadcast_rank = broadcast_dimensions.size();\n    if (operand_shape->rank() != broadcast_rank) {\n      return InvalidArgument(\n          \"Size of broadcast_dimensions has to match operand's rank; operand \"\n          \"rank: %lld, size of broadcast_dimensions %u.\",\n          operand_shape->rank(), broadcast_dimensions.size());\n    }\n    for (int i = 0; i < broadcast_rank; i++) {\n      const int64_t num_dims = out_dim_size.size();\n      if (broadcast_dimensions[i] < 0 || broadcast_dimensions[i] > num_dims) {\n        return InvalidArgument(\"Broadcast dimension %lld is out of bound\",\n                               broadcast_dimensions[i]);\n      }\n      output_shape.set_dynamic_dimension(\n          broadcast_dimensions[i],\n          operand_shape->is_bounded_dynamic_dimension(i));\n    }\n\n    TF_RETURN_IF_ERROR(ShapeInference::InferBroadcastShape(\n                           *operand_shape, output_shape, broadcast_dimensions)\n                           .status());\n    std::vector<int64_t> in_dim_size(out_dim_size.begin(), out_dim_size.end());\n    std::vector<bool> in_dim_dynamic(out_dim_size.size(), false);\n    for (int i = 0; i < broadcast_rank; i++) {\n      in_dim_size[broadcast_dimensions[i]] =\n          (operand_shape->is_unbounded_dynamic_dimension(i))\n              ? out_dim_size[broadcast_dimensions[i]]\n              : operand_shape->dimensions(i);\n      in_dim_dynamic[broadcast_dimensions[i]] =\n          operand_shape->is_bounded_dynamic_dimension(i);\n    }\n    const auto& in_dim_shape = ShapeUtil::MakeShape(\n        operand_shape->element_type(), in_dim_size, in_dim_dynamic);\n    TF_ASSIGN_OR_RETURN(\n        XlaOp in_dim_broadcast,\n        InDimBroadcast(in_dim_shape, operand, broadcast_dimensions));\n\n    // If broadcast is not degenerate, return broadcasted result.\n    if (ShapeUtil::Equal(in_dim_shape, output_shape)) {\n      return in_dim_broadcast;\n    }\n\n    // Otherwise handle degenerate broadcast case.\n    return AddBroadcastSequence(output_shape, in_dim_broadcast);\n  })",
          "old_line_content": "    // Output shape, in the case of degenerate broadcast, the out_dim_size is",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3483,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 5531,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Rev(operand, dimensions)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Rev(operand, dimensions);",
          "content_same": false
        },
        {
          "line": 3486,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(computation, &instr)",
          "old_line_content": "                        {operand, init_value});",
          "new_line_content": "  AddCalledComputation(computation, &instr);",
          "content_same": false
        },
        {
          "line": 3487,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kReduceWindow,",
          "content_same": false
        },
        {
          "line": 5536,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operands[0].builder()->Sort(operands, comparator, dimension,\n                                     is_stable)",
          "old_line_content": "}",
          "new_line_content": "  return operands[0].builder()->Sort(operands, comparator, dimension,",
          "content_same": false
        },
        {
          "line": 1445,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "operand_shape->rank()",
          "old_line_content": "          \"Size of broadcast_dimensions has to match operand's rank; operand \"",
          "new_line_content": "    if (operand_shape->rank() != broadcast_rank) {",
          "content_same": false
        },
        {
          "line": 1446,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Size of broadcast_dimensions has to match operand's rank; operand \"\n          \"rank: %lld, size of broadcast_dimensions %u.\",\n          operand_shape->rank(), broadcast_dimensions.size())",
          "old_line_content": "          \"rank: %lld, size of broadcast_dimensions %u.\",",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 3493,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* scale_shape, GetShapePtr(scale));\n    TF_ASSIGN_OR_RETURN(const Shape* offset_shape, GetShapePtr(offset));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferBatchNormTrainingShape(\n            *operand_shape, *scale_shape, *offset_shape, feature_index));\n    *instr.mutable_shape() = shape.ToProto();\n\n    instr.set_epsilon(epsilon);\n    instr.set_feature_index(feature_index);\n\n    return AddInstruction(std::move(instr), HloOpcode::kBatchNormTraining,\n                          {operand, scale, offset});\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5541,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->TopK(operand, k, largest)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->TopK(operand, k, largest);",
          "content_same": false
        },
        {
          "line": 1449,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "broadcast_dimensions.size()",
          "old_line_content": "    for (int i = 0; i < broadcast_rank; i++) {",
          "new_line_content": "          operand_shape->rank(), broadcast_dimensions.size());",
          "content_same": false
        },
        {
          "line": 5545,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "min.builder()->Clamp(min, operand, max)",
          "old_line_content": "",
          "new_line_content": "  return min.builder()->Clamp(min, operand, max);",
          "content_same": false
        },
        {
          "line": 1454,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Broadcast dimension %lld is out of bound\",\n                               broadcast_dimensions[i])",
          "old_line_content": "      }",
          "new_line_content": "        return InvalidArgument(\"Broadcast dimension %lld is out of bound\",",
          "content_same": false
        },
        {
          "line": 5552,
          "old_api": null,
          "new_api": "Map",
          "old_text": null,
          "new_text": "builder->Map(operands, computation, dimensions, static_operands)",
          "old_line_content": "",
          "new_line_content": "  return builder->Map(operands, computation, dimensions, static_operands);",
          "content_same": false
        },
        {
          "line": 3505,
          "old_api": null,
          "new_api": "set_epsilon",
          "old_text": null,
          "new_text": "instr.set_epsilon(epsilon)",
          "old_line_content": "",
          "new_line_content": "    instr.set_epsilon(epsilon);",
          "content_same": false
        },
        {
          "line": 1459,
          "old_api": null,
          "new_api": "is_bounded_dynamic_dimension",
          "old_text": null,
          "new_text": "operand_shape->is_bounded_dynamic_dimension(i)",
          "old_line_content": "",
          "new_line_content": "          operand_shape->is_bounded_dynamic_dimension(i));",
          "content_same": false
        },
        {
          "line": 3508,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kBatchNormTraining,",
          "content_same": false
        },
        {
          "line": 5556,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "mu.builder()->RngNormal(mu, sigma, shape)",
          "old_line_content": "",
          "new_line_content": "  return mu.builder()->RngNormal(mu, sigma, shape);",
          "content_same": false
        },
        {
          "line": 1462,
          "old_api": null,
          "new_api": "status",
          "old_text": null,
          "new_text": "ShapeInference::InferBroadcastShape(\n                           *operand_shape, output_shape, broadcast_dimensions)\n                           .status()",
          "old_line_content": "                           .status());",
          "new_line_content": "    TF_RETURN_IF_ERROR(ShapeInference::InferBroadcastShape(",
          "content_same": false
        },
        {
          "line": 5560,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "a.builder()->RngUniform(a, b, shape)",
          "old_line_content": "",
          "new_line_content": "  return a.builder()->RngUniform(a, b, shape);",
          "content_same": false
        },
        {
          "line": 1465,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "out_dim_size.end()",
          "old_line_content": "    for (int i = 0; i < broadcast_rank; i++) {",
          "new_line_content": "    std::vector<int64_t> in_dim_size(out_dim_size.begin(), out_dim_size.end());",
          "content_same": false
        },
        {
          "line": 1466,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "out_dim_size.size()",
          "old_line_content": "      in_dim_size[broadcast_dimensions[i]] =",
          "new_line_content": "    std::vector<bool> in_dim_dynamic(out_dim_size.size(), false);",
          "content_same": false
        },
        {
          "line": 3516,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* scale_shape, GetShapePtr(scale));\n    TF_ASSIGN_OR_RETURN(const Shape* offset_shape, GetShapePtr(offset));\n    TF_ASSIGN_OR_RETURN(const Shape* mean_shape, GetShapePtr(mean));\n    TF_ASSIGN_OR_RETURN(const Shape* variance_shape, GetShapePtr(variance));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferBatchNormInferenceShape(\n                            *operand_shape, *scale_shape, *offset_shape,\n                            *mean_shape, *variance_shape, feature_index));\n    *instr.mutable_shape() = shape.ToProto();\n\n    instr.set_epsilon(epsilon);\n    instr.set_feature_index(feature_index);\n\n    return AddInstruction(std::move(instr), HloOpcode::kBatchNormInference,\n                          {operand, scale, offset, mean, variance});\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5565,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "initial_state.builder()->RngBitGenerator(algorithm, initial_state,\n                                                  shape)",
          "old_line_content": "}",
          "new_line_content": "  return initial_state.builder()->RngBitGenerator(algorithm, initial_state,",
          "content_same": false
        },
        {
          "line": 1475,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "ShapeUtil::MakeShape(\n        operand_shape->element_type(), in_dim_size, in_dim_dynamic)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(",
          "new_line_content": "    const auto& in_dim_shape = ShapeUtil::MakeShape(",
          "content_same": false
        },
        {
          "line": 1476,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "operand_shape->element_type()",
          "old_line_content": "        XlaOp in_dim_broadcast,",
          "new_line_content": "        operand_shape->element_type(), in_dim_size, in_dim_dynamic);",
          "content_same": false
        },
        {
          "line": 5571,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "init.builder()->While(condition, body, init)",
          "old_line_content": "",
          "new_line_content": "  return init.builder()->While(condition, body, init);",
          "content_same": false
        },
        {
          "line": 1482,
          "old_api": null,
          "new_api": "ShapeUtil::Equal(in_dim_shape, output_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::Equal(in_dim_shape, output_shape)",
          "old_line_content": "    }",
          "new_line_content": "    if (ShapeUtil::Equal(in_dim_shape, output_shape)) {",
          "content_same": false
        },
        {
          "line": 3530,
          "old_api": null,
          "new_api": "set_epsilon",
          "old_text": null,
          "new_text": "instr.set_epsilon(epsilon)",
          "old_line_content": "",
          "new_line_content": "    instr.set_epsilon(epsilon);",
          "content_same": false
        },
        {
          "line": 5578,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "predicate.builder()->Conditional(predicate, true_operand,\n                                          true_computation, false_operand,\n                                          false_computation)",
          "old_line_content": "                                          false_computation);",
          "new_line_content": "  return predicate.builder()->Conditional(predicate, true_operand,",
          "content_same": false
        },
        {
          "line": 3533,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kBatchNormInference,",
          "content_same": false
        },
        {
          "line": 1487,
          "old_api": null,
          "new_api": "AddBroadcastSequence",
          "old_text": null,
          "new_text": "AddBroadcastSequence(output_shape, in_dim_broadcast)",
          "old_line_content": "}",
          "new_line_content": "    return AddBroadcastSequence(output_shape, in_dim_broadcast);",
          "content_same": false
        },
        {
          "line": 5586,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "branch_index.builder()->Conditional(branch_index, branch_computations,\n                                             branch_operands)",
          "old_line_content": "}",
          "new_line_content": "  return branch_index.builder()->Conditional(branch_index, branch_computations,",
          "content_same": false
        },
        {
          "line": 3541,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* scale_shape, GetShapePtr(scale));\n    TF_ASSIGN_OR_RETURN(const Shape* batch_mean_shape, GetShapePtr(batch_mean));\n    TF_ASSIGN_OR_RETURN(const Shape* batch_var_shape, GetShapePtr(batch_var));\n    TF_ASSIGN_OR_RETURN(const Shape* grad_output_shape,\n                        GetShapePtr(grad_output));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape, ShapeInference::InferBatchNormGradShape(\n                         *operand_shape, *scale_shape, *batch_mean_shape,\n                         *batch_var_shape, *grad_output_shape, feature_index));\n    *instr.mutable_shape() = shape.ToProto();\n\n    instr.set_epsilon(epsilon);\n    instr.set_feature_index(feature_index);\n\n    return AddInstruction(std::move(instr), HloOpcode::kBatchNormGrad,\n                          {operand, scale, batch_mean, batch_var, grad_output});\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1494,
          "old_api": null,
          "new_api": "is_unbounded_dynamic",
          "old_text": null,
          "new_text": "shape.is_unbounded_dynamic()",
          "old_line_content": "        \"Reshaping with unbounded result shape is not supported.\");",
          "new_line_content": "  if (shape.is_unbounded_dynamic()) {",
          "content_same": false
        },
        {
          "line": 1495,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n        \"Reshaping with unbounded result shape is not supported.\")",
          "old_line_content": "  }",
          "new_line_content": "    return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 5592,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->ReducePrecision(operand, exponent_bits,\n                                            mantissa_bits)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->ReducePrecision(operand, exponent_bits,",
          "content_same": false
        },
        {
          "line": 5599,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "input.builder()->Gather(input, start_indices, dimension_numbers,\n                                 slice_sizes, indices_are_sorted)",
          "old_line_content": "}",
          "new_line_content": "  return input.builder()->Gather(input, start_indices, dimension_numbers,",
          "content_same": false
        },
        {
          "line": 1504,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kReshape, {operand});",
          "content_same": false
        },
        {
          "line": 3556,
          "old_api": null,
          "new_api": "set_epsilon",
          "old_text": null,
          "new_text": "instr.set_epsilon(epsilon)",
          "old_line_content": "",
          "new_line_content": "    instr.set_epsilon(epsilon);",
          "content_same": false
        },
        {
          "line": 1510,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferSliceShape(\n                                         *operand_shape, start_indices,\n                                         limit_indices, strides));\n    return SliceInternal(shape, operand, start_indices, limit_indices, strides);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferSliceShape(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3559,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kBatchNormGrad,",
          "content_same": false
        },
        {
          "line": 5607,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "input.builder()->Scatter(input, scatter_indices, updates,\n                                  update_computation, dimension_numbers,\n                                  indices_are_sorted, unique_indices)",
          "old_line_content": "                                  indices_are_sorted, unique_indices);",
          "new_line_content": "  return input.builder()->Scatter(input, scatter_indices, updates,",
          "content_same": false
        },
        {
          "line": 1515,
          "old_api": null,
          "new_api": "SliceInternal",
          "old_text": null,
          "new_text": "SliceInternal(shape, operand, start_indices, limit_indices, strides)",
          "old_line_content": "}",
          "new_line_content": "    return SliceInternal(shape, operand, start_indices, limit_indices, strides);",
          "content_same": false
        },
        {
          "line": 5617,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "scatter_indices.builder()->Scatter(\n      inputs, scatter_indices, updates, update_computation, dimension_numbers,\n      indices_are_sorted, unique_indices)",
          "old_line_content": "      indices_are_sorted, unique_indices);",
          "new_line_content": "  return scatter_indices.builder()->Scatter(",
          "content_same": false
        },
        {
          "line": 3570,
          "old_api": null,
          "new_api": "AllGatherImpl",
          "old_text": null,
          "new_text": "AllGatherImpl(operand, all_gather_dimension, shard_count,\n                       replica_groups, channel_id, layout,\n                       use_global_device_ids, /*async=*/false)",
          "old_line_content": "                       use_global_device_ids, /*async=*/false);",
          "new_line_content": "  return AllGatherImpl(operand, all_gather_dimension, shard_count,",
          "content_same": false
        },
        {
          "line": 5623,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->Send(operand, handle)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->Send(operand, handle);",
          "content_same": false
        },
        {
          "line": 1528,
          "old_api": null,
          "new_api": "set_limit",
          "old_text": null,
          "new_text": "slice_config->set_limit(limit_indices[i])",
          "old_line_content": "  }",
          "new_line_content": "    slice_config->set_limit(limit_indices[i]);",
          "content_same": false
        },
        {
          "line": 3577,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));\n    const Shape* element_shape;\n    if (shape->IsTuple()) {\n      if (shape->tuple_shapes_size() == 0) {\n        return Unimplemented(\n            \"0 element tuple CrossReplicaSum is not supported\");\n      }\n      element_shape = &shape->tuple_shapes(0);\n    } else {\n      element_shape = shape;\n    }\n    const Shape scalar_shape =\n        ShapeUtil::MakeShape(element_shape->element_type(), {});\n    auto b = CreateSubBuilder(\"sum\");\n    auto x = b->Parameter(/*parameter_number=*/0, scalar_shape, \"x\");\n    auto y = b->Parameter(/*parameter_number=*/1, scalar_shape, \"y\");\n    if (scalar_shape.element_type() == PRED) {\n      Or(x, y);\n    } else {\n      Add(x, y);\n    }\n    TF_ASSIGN_OR_RETURN(auto computation, b->Build());\n    return AllReduce(operand, computation, replica_groups,\n                     /*channel_id=*/std::nullopt);\n  })",
          "old_line_content": "    const Shape* element_shape;",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1531,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kSlice, {operand});",
          "content_same": false
        },
        {
          "line": 5628,
          "old_api": null,
          "new_api": "Recv",
          "old_text": null,
          "new_text": "builder->Recv(shape, handle)",
          "old_line_content": "",
          "new_line_content": "  return builder->Recv(shape, handle);",
          "content_same": false
        },
        {
          "line": 3581,
          "old_api": null,
          "new_api": "tuple_shapes_size",
          "old_text": null,
          "new_text": "shape->tuple_shapes_size()",
          "old_line_content": "            \"0 element tuple CrossReplicaSum is not supported\");",
          "new_line_content": "      if (shape->tuple_shapes_size() == 0) {",
          "content_same": false
        },
        {
          "line": 3582,
          "old_api": null,
          "new_api": "Unimplemented",
          "old_text": null,
          "new_text": "Unimplemented(\n            \"0 element tuple CrossReplicaSum is not supported\")",
          "old_line_content": "      }",
          "new_line_content": "        return Unimplemented(",
          "content_same": false
        },
        {
          "line": 3585,
          "old_api": null,
          "new_api": "tuple_shapes",
          "old_text": null,
          "new_text": "shape->tuple_shapes(0)",
          "old_line_content": "      element_shape = shape;",
          "new_line_content": "      element_shape = &shape->tuple_shapes(0);",
          "content_same": false
        },
        {
          "line": 5633,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->SendWithToken(operand, token, handle)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->SendWithToken(operand, token, handle);",
          "content_same": false
        },
        {
          "line": 1541,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "shape->dimensions().end()",
          "old_line_content": "    starts[dimno] = start_index;",
          "new_line_content": "                                shape->dimensions().end());",
          "content_same": false
        },
        {
          "line": 1542,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "shape->rank()",
          "old_line_content": "    limits[dimno] = limit_index;",
          "new_line_content": "    std::vector<int64_t> strides(shape->rank(), 1);",
          "content_same": false
        },
        {
          "line": 5638,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "token.builder()->RecvWithToken(token, shape, handle)",
          "old_line_content": "",
          "new_line_content": "  return token.builder()->RecvWithToken(token, shape, handle);",
          "content_same": false
        },
        {
          "line": 1546,
          "old_api": null,
          "new_api": "Slice",
          "old_text": null,
          "new_text": "Slice(operand, starts, limits, strides)",
          "old_line_content": "}",
          "new_line_content": "    return Slice(operand, starts, limits, strides);",
          "content_same": false
        },
        {
          "line": 3594,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "scalar_shape.element_type()",
          "old_line_content": "    } else {",
          "new_line_content": "    if (scalar_shape.element_type() == PRED) {",
          "content_same": false
        },
        {
          "line": 5643,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->SendToHost(operand, token, shape_with_layout,\n                                       handle)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->SendToHost(operand, token, shape_with_layout,",
          "content_same": false
        },
        {
          "line": 3597,
          "old_api": null,
          "new_api": "Add",
          "old_text": null,
          "new_text": "Add(x, y)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(auto computation, b->Build());",
          "new_line_content": "      Add(x, y);",
          "content_same": false
        },
        {
          "line": 3600,
          "old_api": null,
          "new_api": "AllReduce",
          "old_text": null,
          "new_text": "AllReduce(operand, computation, replica_groups,\n                     /*channel_id=*/std::nullopt)",
          "old_line_content": "  });",
          "new_line_content": "    return AllReduce(operand, computation, replica_groups,",
          "content_same": false
        },
        {
          "line": 1553,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<const Shape*> start_indices_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& start_indices_shapes,\n                        GetOperandShapes(start_indices));\n    absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferDynamicSliceShape(\n                            *operand_shape, start_indices_shapes, slice_sizes));\n    return DynamicSliceInternal(shape, operand, start_indices, slice_sizes);\n  })",
          "old_line_content": "    std::vector<const Shape*> start_indices_shape_ptrs;",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5649,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "token.builder()->RecvFromHost(token, shape, handle)",
          "old_line_content": "",
          "new_line_content": "  return token.builder()->RecvFromHost(token, shape, handle);",
          "content_same": false
        },
        {
          "line": 1558,
          "old_api": null,
          "new_api": "absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; })",
          "old_text": null,
          "new_text": "absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; })",
          "old_line_content": "                      [](const Shape& shape) { return &shape; });",
          "new_line_content": "    absl::c_transform(start_indices_shapes,",
          "content_same": false
        },
        {
          "line": 1559,
          "old_api": null,
          "new_api": "std::back_inserter(start_indices_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(start_indices_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape,",
          "new_line_content": "                      std::back_inserter(start_indices_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 5654,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "token.builder()->InfeedWithToken(token, shape, config)",
          "old_line_content": "",
          "new_line_content": "  return token.builder()->InfeedWithToken(token, shape, config);",
          "content_same": false
        },
        {
          "line": 3610,
          "old_api": null,
          "new_api": "AllReduceImpl",
          "old_text": null,
          "new_text": "AllReduceImpl(operand, computation, replica_groups, channel_id,\n                       shape_with_layout, use_global_device_ids,\n                       /*async =*/false)",
          "old_line_content": "                       /*async =*/false);",
          "new_line_content": "  return AllReduceImpl(operand, computation, replica_groups, channel_id,",
          "content_same": false
        },
        {
          "line": 1564,
          "old_api": null,
          "new_api": "DynamicSliceInternal",
          "old_text": null,
          "new_text": "DynamicSliceInternal(shape, operand, start_indices, slice_sizes)",
          "old_line_content": "}",
          "new_line_content": "    return DynamicSliceInternal(shape, operand, start_indices, slice_sizes);",
          "content_same": false
        },
        {
          "line": 5660,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->OutfeedWithToken(operand, token, shape_with_layout,\n                                             outfeed_config)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->OutfeedWithToken(operand, token, shape_with_layout,",
          "content_same": false
        },
        {
          "line": 5664,
          "old_api": null,
          "new_api": "CreateToken",
          "old_text": null,
          "new_text": "builder->CreateToken()",
          "old_line_content": "XlaOp AfterAll(XlaBuilder* builder, absl::Span<const XlaOp> tokens) {",
          "new_line_content": "XlaOp CreateToken(XlaBuilder* builder) { return builder->CreateToken(); }",
          "content_same": false
        },
        {
          "line": 5667,
          "old_api": null,
          "new_api": "AfterAll",
          "old_text": null,
          "new_text": "builder->AfterAll(tokens)",
          "old_line_content": "",
          "new_line_content": "  return builder->AfterAll(tokens);",
          "content_same": false
        },
        {
          "line": 1572,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "  for (int64_t size : slice_sizes) {",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3621,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<const Shape*> operand_shapes;\n    std::vector<XlaOp> operands;\n    if (operand_shape->IsTuple()) {\n      if (operand_shape->tuple_shapes_size() == 0) {\n        return Unimplemented(\"0 element tuple ReduceScatter is not supported\");\n      }\n      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {\n        if (operand_shape->tuple_shapes(i).element_type() !=\n            operand_shape->tuple_shapes(0).element_type()) {\n          return Unimplemented(\n              \"All the shapes of a tuple input of ReduceScatter must have \"\n              \"the same element type\");\n        }\n        operand_shapes.push_back(&operand_shape->tuple_shapes(i));\n        operands.push_back(GetTupleElement(operand, i));\n      }\n    } else {\n      operand_shapes.push_back(operand_shape);\n      operands.push_back(operand);\n    }\n\n    TF_ASSIGN_OR_RETURN(Shape inferred_shape,\n                        ShapeInference::InferReduceScatterShape(\n                            operand_shapes, scatter_dimension, shard_count));\n    if (layout) {\n      *inferred_shape.mutable_layout() = *layout;\n      instr.set_constrain_layout(true);\n    }\n    *instr.mutable_shape() = inferred_shape.ToProto();\n\n    AddCalledComputation(computation, &instr);\n\n    instr.add_dimensions(scatter_dimension);\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n    if (use_global_device_ids.has_value()) {\n      instr.set_use_global_device_ids(use_global_device_ids.value());\n    }\n\n    TF_ASSIGN_OR_RETURN(\n        auto reduce_scatter,\n        AddInstruction(std::move(instr), HloOpcode::kReduceScatter, operands));\n    return reduce_scatter;\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1575,
          "old_api": null,
          "new_api": "add_dynamic_slice_sizes",
          "old_text": null,
          "new_text": "instr.add_dynamic_slice_sizes(size)",
          "old_line_content": "",
          "new_line_content": "    instr.add_dynamic_slice_sizes(size);",
          "content_same": false
        },
        {
          "line": 5673,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->BatchNormTraining(operand, scale, offset, epsilon,\n                                              feature_index)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->BatchNormTraining(operand, scale, offset, epsilon,",
          "content_same": false
        },
        {
          "line": 1579,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "start_indices.end()",
          "old_line_content": "}",
          "new_line_content": "  operands.insert(operands.end(), start_indices.begin(), start_indices.end());",
          "content_same": false
        },
        {
          "line": 1580,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kDynamicSlice, operands);",
          "content_same": false
        },
        {
          "line": 3627,
          "old_api": null,
          "new_api": "tuple_shapes_size",
          "old_text": null,
          "new_text": "operand_shape->tuple_shapes_size()",
          "old_line_content": "      }",
          "new_line_content": "      if (operand_shape->tuple_shapes_size() == 0) {",
          "content_same": false
        },
        {
          "line": 3632,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "operand_shape->tuple_shapes(0).element_type()",
          "old_line_content": "              \"All the shapes of a tuple input of ReduceScatter must have \"",
          "new_line_content": "            operand_shape->tuple_shapes(0).element_type()) {",
          "content_same": false
        },
        {
          "line": 1585,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* update_shape, GetShapePtr(update));\n    std::vector<const Shape*> start_indices_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& start_indices_shapes,\n                        GetOperandShapes(start_indices));\n    absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(\n        Shape shape, ShapeInference::InferDynamicUpdateSliceShape(\n                         *operand_shape, *update_shape, start_indices_shapes));\n    return DynamicUpdateSliceInternal(shape, operand, update, start_indices);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* update_shape, GetShapePtr(update));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3633,
          "old_api": null,
          "new_api": "Unimplemented",
          "old_text": null,
          "new_text": "Unimplemented(\n              \"All the shapes of a tuple input of ReduceScatter must have \"\n              \"the same element type\")",
          "old_line_content": "              \"the same element type\");",
          "new_line_content": "          return Unimplemented(",
          "content_same": false
        },
        {
          "line": 5681,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->BatchNormInference(\n      operand, scale, offset, mean, variance, epsilon, feature_index)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->BatchNormInference(",
          "content_same": false
        },
        {
          "line": 3637,
          "old_api": null,
          "new_api": "tuple_shapes",
          "old_text": null,
          "new_text": "operand_shape->tuple_shapes(i)",
          "old_line_content": "      }",
          "new_line_content": "        operand_shapes.push_back(&operand_shape->tuple_shapes(i));",
          "content_same": false
        },
        {
          "line": 3638,
          "old_api": null,
          "new_api": "GetTupleElement",
          "old_text": null,
          "new_text": "GetTupleElement(operand, i)",
          "old_line_content": "    } else {",
          "new_line_content": "        operands.push_back(GetTupleElement(operand, i));",
          "content_same": false
        },
        {
          "line": 1591,
          "old_api": null,
          "new_api": "absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; })",
          "old_text": null,
          "new_text": "absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; })",
          "old_line_content": "                      [](const Shape& shape) { return &shape; });",
          "new_line_content": "    absl::c_transform(start_indices_shapes,",
          "content_same": false
        },
        {
          "line": 1592,
          "old_api": null,
          "new_api": "std::back_inserter(start_indices_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(start_indices_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(",
          "new_line_content": "                      std::back_inserter(start_indices_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 3641,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operand_shapes.push_back(operand_shape)",
          "old_line_content": "    }",
          "new_line_content": "      operand_shapes.push_back(operand_shape);",
          "content_same": false
        },
        {
          "line": 3642,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operands.push_back(operand)",
          "old_line_content": "",
          "new_line_content": "      operands.push_back(operand);",
          "content_same": false
        },
        {
          "line": 5689,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->BatchNormGrad(operand, scale, batch_mean, batch_var,\n                                          grad_output, epsilon, feature_index)",
          "old_line_content": "}",
          "new_line_content": "  return operand.builder()->BatchNormGrad(operand, scale, batch_mean, batch_var,",
          "content_same": false
        },
        {
          "line": 1597,
          "old_api": null,
          "new_api": "DynamicUpdateSliceInternal",
          "old_text": null,
          "new_text": "DynamicUpdateSliceInternal(shape, operand, update, start_indices)",
          "old_line_content": "}",
          "new_line_content": "    return DynamicUpdateSliceInternal(shape, operand, update, start_indices);",
          "content_same": false
        },
        {
          "line": 5694,
          "old_api": null,
          "new_api": "Iota",
          "old_text": null,
          "new_text": "builder->Iota(type, size)",
          "old_line_content": "",
          "new_line_content": "  return builder->Iota(type, size);",
          "content_same": false
        },
        {
          "line": 3649,
          "old_api": null,
          "new_api": "mutable_layout",
          "old_text": null,
          "new_text": "inferred_shape.mutable_layout()",
          "old_line_content": "    }",
          "new_line_content": "      *inferred_shape.mutable_layout() = *layout;",
          "content_same": false
        },
        {
          "line": 5698,
          "old_api": null,
          "new_api": "Iota",
          "old_text": null,
          "new_text": "builder->Iota(shape, iota_dimension)",
          "old_line_content": "",
          "new_line_content": "  return builder->Iota(shape, iota_dimension);",
          "content_same": false
        },
        {
          "line": 1605,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "  std::vector<XlaOp> operands = {operand, update};",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 5702,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->GetDimensionSize(operand, dimension)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->GetDimensionSize(operand, dimension);",
          "content_same": false
        },
        {
          "line": 1608,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "start_indices.end()",
          "old_line_content": "                        operands);",
          "new_line_content": "  operands.insert(operands.end(), start_indices.begin(), start_indices.end());",
          "content_same": false
        },
        {
          "line": 1609,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kDynamicUpdateSlice,",
          "content_same": false
        },
        {
          "line": 5707,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->SetDimensionSize(operand, val, dimension)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->SetDimensionSize(operand, val, dimension);",
          "content_same": false
        },
        {
          "line": 3660,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "channel_id.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (channel_id.has_value()) {",
          "content_same": false
        },
        {
          "line": 1615,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferConcatOpShape(\n                                         operand_shape_ptrs, dimension));\n    return ConcatInDimInternal(shape, operands, dimension);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(operands));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3663,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "use_global_device_ids.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (use_global_device_ids.has_value()) {",
          "content_same": false
        },
        {
          "line": 3664,
          "old_api": null,
          "new_api": "value",
          "old_text": null,
          "new_text": "use_global_device_ids.value()",
          "old_line_content": "",
          "new_line_content": "      instr.set_use_global_device_ids(use_global_device_ids.value());",
          "content_same": false
        },
        {
          "line": 1618,
          "old_api": null,
          "new_api": "std::back_inserter(operand_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(operand_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferConcatOpShape(",
          "new_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 5711,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "operand.builder()->RemoveDynamicDimension(operand, dimension)",
          "old_line_content": "",
          "new_line_content": "  return operand.builder()->RemoveDynamicDimension(operand, dimension);",
          "content_same": false
        },
        {
          "line": 5716,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "original.type()",
          "old_line_content": "    return manual;",
          "new_line_content": "  if (single_dim < 0 || original.type() != OpSharding::OTHER) {",
          "content_same": false
        },
        {
          "line": 5717,
          "old_api": null,
          "new_api": "set_type",
          "old_text": null,
          "new_text": "manual.set_type(OpSharding::MANUAL)",
          "old_line_content": "  }",
          "new_line_content": "    manual.set_type(OpSharding::MANUAL);",
          "content_same": false
        },
        {
          "line": 1622,
          "old_api": null,
          "new_api": "ConcatInDimInternal",
          "old_text": null,
          "new_text": "ConcatInDimInternal(shape, operands, dimension)",
          "old_line_content": "}",
          "new_line_content": "    return ConcatInDimInternal(shape, operands, dimension);",
          "content_same": false
        },
        {
          "line": 5723,
          "old_api": null,
          "new_api": "tile_assignment_dimensions",
          "old_text": null,
          "new_text": "original.tile_assignment_dimensions().end()",
          "old_line_content": "  new_tile_shape[single_dim] = 1;",
          "new_line_content": "      original.tile_assignment_dimensions().end());",
          "content_same": false
        },
        {
          "line": 5724,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "new_tile_shape.push_back(new_tile_shape[single_dim])",
          "old_line_content": "  Array<int64_t> new_tile(new_tile_shape);",
          "new_line_content": "  new_tile_shape.push_back(new_tile_shape[single_dim]);",
          "content_same": false
        },
        {
          "line": 1633,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kConcatenate, operands);",
          "content_same": false
        },
        {
          "line": 3681,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "layout.has_value()",
          "old_line_content": "                         split_count, replica_groups, layout, channel_id);",
          "new_line_content": "  if (layout.has_value()) {",
          "content_same": false
        },
        {
          "line": 3682,
          "old_api": null,
          "new_api": "AllToAllTuple",
          "old_text": null,
          "new_text": "AllToAllTuple(operand, split_dimension, concat_dimension,\n                         split_count, replica_groups, layout, channel_id)",
          "old_line_content": "  }",
          "new_line_content": "    return AllToAllTuple(operand, split_dimension, concat_dimension,",
          "content_same": false
        },
        {
          "line": 5729,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "indices.size()",
          "old_line_content": "        src_index *= new_tile_shape[i];",
          "new_line_content": "    for (int64_t i = 0; i < indices.size() - 1; ++i) {",
          "content_same": false
        },
        {
          "line": 3685,
          "old_api": null,
          "new_api": "AllToAllArray",
          "old_text": null,
          "new_text": "AllToAllArray(operand, split_dimension, concat_dimension, split_count,\n                       replica_groups, channel_id)",
          "old_line_content": "}",
          "new_line_content": "  return AllToAllArray(operand, split_dimension, concat_dimension, split_count,",
          "content_same": false
        },
        {
          "line": 1638,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* padding_value_shape,\n                        GetShapePtr(padding_value));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape, ShapeInference::InferPadShape(\n                         *operand_shape, *padding_value_shape, padding_config));\n    return PadInternal(shape, operand, padding_value, padding_config);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* padding_value_shape,",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5735,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "indices.back()",
          "old_line_content": "      src_index += index;",
          "new_line_content": "        index = indices.back();",
          "content_same": false
        },
        {
          "line": 5739,
          "old_api": null,
          "new_api": "tile_assignment_devices",
          "old_text": null,
          "new_text": "original.tile_assignment_devices(src_index)",
          "old_line_content": "  for (int64_t dim : new_tile_shape) {",
          "new_line_content": "    *v = original.tile_assignment_devices(src_index);",
          "content_same": false
        },
        {
          "line": 1645,
          "old_api": null,
          "new_api": "PadInternal",
          "old_text": null,
          "new_text": "PadInternal(shape, operand, padding_value, padding_config)",
          "old_line_content": "}",
          "new_line_content": "    return PadInternal(shape, operand, padding_value, padding_config);",
          "content_same": false
        },
        {
          "line": 3693,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(\n        const Shape all_to_all_shape,\n        ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,\n                                           concat_dimension, split_count));\n    HloInstructionProto instr;\n    *instr.mutable_shape() = operand_shape->ToProto();\n    if (replica_groups.empty()) {\n      auto* group = instr.add_replica_groups();\n      for (int64_t i = 0; i < split_count; ++i) {\n        group->add_replica_ids(i);\n      }\n    } else {\n      for (const ReplicaGroup& group : replica_groups) {\n        *instr.add_replica_groups() = group;\n      }\n    }\n    instr.add_dimensions(split_dimension);\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n    TF_ASSIGN_OR_RETURN(\n        XlaOp all_to_all,\n        AddInstruction(std::move(instr), HloOpcode::kAllToAll, {operand}));\n    if (split_dimension == concat_dimension) {\n      return all_to_all;\n    }\n    DimensionVector sizes;\n    for (int64_t i = 0; i < operand_shape->rank(); ++i) {\n      if (i != split_dimension) {\n        sizes.push_back(operand_shape->dimensions(i));\n        continue;\n      }\n      sizes.push_back(split_count);\n      sizes.push_back(operand_shape->dimensions(i) / split_count);\n    }\n    all_to_all = Reshape(all_to_all, sizes);\n\n    std::vector<int64_t> permutation;\n    const auto rank = operand_shape->rank();\n    permutation.reserve(rank + 1);\n    for (int64_t i = 0; i < rank; ++i) {\n      int64_t dim_after_reshape = i >= split_dimension ? i + 1 : i;\n      if (i == concat_dimension) {\n        permutation.push_back(split_dimension);\n      }\n      permutation.push_back(dim_after_reshape);\n    }\n    all_to_all = Transpose(all_to_all, permutation);\n    return Reshape(all_to_all_shape, all_to_all);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5742,
          "old_api": null,
          "new_api": "add_tile_assignment_dimensions",
          "old_text": null,
          "new_text": "manual.add_tile_assignment_dimensions(dim)",
          "old_line_content": "  for (int64_t device : new_tile) {",
          "new_line_content": "    manual.add_tile_assignment_dimensions(dim);",
          "content_same": false
        },
        {
          "line": 5747,
          "old_api": null,
          "new_api": "replicate_on_last_tile_dim",
          "old_text": null,
          "new_text": "original.replicate_on_last_tile_dim()",
          "old_line_content": "  }",
          "new_line_content": "  if (original.replicate_on_last_tile_dim()) {",
          "content_same": false
        },
        {
          "line": 3701,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "replica_groups.empty()",
          "old_line_content": "      for (int64_t i = 0; i < split_count; ++i) {",
          "new_line_content": "    if (replica_groups.empty()) {",
          "content_same": false
        },
        {
          "line": 5750,
          "old_api": null,
          "new_api": "last_tile_dims",
          "old_text": null,
          "new_text": "original.last_tile_dims()",
          "old_line_content": "  }",
          "new_line_content": "  for (int64_t type : original.last_tile_dims()) {",
          "content_same": false
        },
        {
          "line": 1656,
          "old_api": null,
          "new_api": "set_edge_padding_high",
          "old_text": null,
          "new_text": "dims->set_edge_padding_high(pad_hi)",
          "old_line_content": "  });",
          "new_line_content": "    dims->set_edge_padding_high(pad_hi);",
          "content_same": false
        },
        {
          "line": 1657,
          "old_api": null,
          "new_api": "Pad",
          "old_text": null,
          "new_text": "Pad(operand, padding_value, padding_config)",
          "old_line_content": "}",
          "new_line_content": "    return Pad(operand, padding_value, padding_config);",
          "content_same": false
        },
        {
          "line": 3704,
          "old_api": null,
          "new_api": "add_replica_ids",
          "old_text": null,
          "new_text": "group->add_replica_ids(i)",
          "old_line_content": "    } else {",
          "new_line_content": "        group->add_replica_ids(i);",
          "content_same": false
        },
        {
          "line": 5753,
          "old_api": null,
          "new_api": "add_last_tile_dims",
          "old_text": null,
          "new_text": "manual.add_last_tile_dims(OpSharding::MANUAL)",
          "old_line_content": "}",
          "new_line_content": "  manual.add_last_tile_dims(OpSharding::MANUAL);",
          "content_same": false
        },
        {
          "line": 3708,
          "old_api": null,
          "new_api": "add_replica_groups",
          "old_text": null,
          "new_text": "instr.add_replica_groups()",
          "old_line_content": "    }",
          "new_line_content": "        *instr.add_replica_groups() = group;",
          "content_same": false
        },
        {
          "line": 3712,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "channel_id.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (channel_id.has_value()) {",
          "content_same": false
        },
        {
          "line": 3713,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "channel_id->handle()",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(",
          "new_line_content": "      instr.set_channel_id(channel_id->handle());",
          "content_same": false
        },
        {
          "line": 1666,
          "old_api": null,
          "new_api": "mutable_padding_config",
          "old_text": null,
          "new_text": "instr.mutable_padding_config()",
          "old_line_content": "                        {operand, padding_value});",
          "new_line_content": "  *instr.mutable_padding_config() = padding_config;",
          "content_same": false
        },
        {
          "line": 1667,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kPad,",
          "content_same": false
        },
        {
          "line": 5764,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "output_shape.rank()",
          "old_line_content": "    for (int64_t i = 0; i < rank; ++i) {",
          "new_line_content": "  const int64_t rank = output_shape.rank();",
          "content_same": false
        },
        {
          "line": 5765,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "manual_sharding.type()",
          "old_line_content": "      if (single_dim >= 0 && i != single_dim) {",
          "new_line_content": "  if (manual_sharding.type() == OpSharding::OTHER) {",
          "content_same": false
        },
        {
          "line": 1674,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape shape, ShapeInference::InferReshapeShape(\n                                               *operand_shape, dimensions,\n                                               new_sizes, inferred_dimension));\n    XlaOp transposed = IsIdentityPermutation(dimensions)\n                           ? operand\n                           : Transpose(operand, dimensions);\n    return ReshapeInternal(shape, transposed, inferred_dimension);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape shape, ShapeInference::InferReshapeShape(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 5771,
          "old_api": null,
          "new_api": "tile_assignment_dimensions",
          "old_text": null,
          "new_text": "manual_sharding.tile_assignment_dimensions(i)",
          "old_line_content": "      const int64_t dim_size =",
          "new_line_content": "          manual_sharding.tile_assignment_dimensions(i);",
          "content_same": false
        },
        {
          "line": 3724,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "operand_shape->dimensions(i)",
          "old_line_content": "      }",
          "new_line_content": "        sizes.push_back(operand_shape->dimensions(i));",
          "content_same": false
        },
        {
          "line": 5774,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "output_shape.dimensions(i)",
          "old_line_content": "    }",
          "new_line_content": "          CeilOfRatio(output_shape.dimensions(i), partitions_i);",
          "content_same": false
        },
        {
          "line": 3727,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "sizes.push_back(split_count)",
          "old_line_content": "    }",
          "new_line_content": "      sizes.push_back(split_count);",
          "content_same": false
        },
        {
          "line": 5775,
          "old_api": null,
          "new_api": "set_dimensions",
          "old_text": null,
          "new_text": "output_shape.set_dimensions(i, dim_size)",
          "old_line_content": "  }",
          "new_line_content": "      output_shape.set_dimensions(i, dim_size);",
          "content_same": false
        },
        {
          "line": 1681,
          "old_api": null,
          "new_api": "Transpose",
          "old_text": null,
          "new_text": "Transpose(operand, dimensions)",
          "old_line_content": "  });",
          "new_line_content": "                           : Transpose(operand, dimensions);",
          "content_same": false
        },
        {
          "line": 1682,
          "old_api": null,
          "new_api": "ReshapeInternal",
          "old_text": null,
          "new_text": "ReshapeInternal(shape, transposed, inferred_dimension)",
          "old_line_content": "}",
          "new_line_content": "    return ReshapeInternal(shape, transposed, inferred_dimension);",
          "content_same": false
        },
        {
          "line": 3730,
          "old_api": null,
          "new_api": "Reshape",
          "old_text": null,
          "new_text": "Reshape(all_to_all, sizes)",
          "old_line_content": "    std::vector<int64_t> permutation;",
          "new_line_content": "    all_to_all = Reshape(all_to_all, sizes);",
          "content_same": false
        },
        {
          "line": 3733,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "operand_shape->rank()",
          "old_line_content": "    for (int64_t i = 0; i < rank; ++i) {",
          "new_line_content": "    const auto rank = operand_shape->rank();",
          "content_same": false
        },
        {
          "line": 3734,
          "old_api": null,
          "new_api": "reserve",
          "old_text": null,
          "new_text": "permutation.reserve(rank + 1)",
          "old_line_content": "      int64_t dim_after_reshape = i >= split_dimension ? i + 1 : i;",
          "new_line_content": "    permutation.reserve(rank + 1);",
          "content_same": false
        },
        {
          "line": 5783,
          "old_api": null,
          "new_api": "CustomCall",
          "old_text": null,
          "new_text": "CustomCall(\n        builder, /*call_target_name=*/\"Sharding\", {input}, input_shape,\n        /*opaque=*/\n        sharding_op_util::EncodeAttributes(unspecified_dims))",
          "old_line_content": "        /*opaque=*/",
          "new_line_content": "    input_annotation = CustomCall(",
          "content_same": false
        },
        {
          "line": 5786,
          "old_api": null,
          "new_api": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "old_text": null,
          "new_text": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "old_line_content": "",
          "new_line_content": "        sharding_op_util::EncodeAttributes(unspecified_dims));",
          "content_same": false
        },
        {
          "line": 1691,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "dimensions.end()",
          "old_line_content": "  });",
          "new_line_content": "    std::iota(dimensions.begin(), dimensions.end(), 0);",
          "content_same": false
        },
        {
          "line": 1692,
          "old_api": null,
          "new_api": "Reshape",
          "old_text": null,
          "new_text": "Reshape(operand, dimensions, new_sizes, inferred_dimension)",
          "old_line_content": "}",
          "new_line_content": "    return Reshape(operand, dimensions, new_sizes, inferred_dimension);",
          "content_same": false
        },
        {
          "line": 3742,
          "old_api": null,
          "new_api": "Transpose",
          "old_text": null,
          "new_text": "Transpose(all_to_all, permutation)",
          "old_line_content": "  });",
          "new_line_content": "    all_to_all = Transpose(all_to_all, permutation);",
          "content_same": false
        },
        {
          "line": 3743,
          "old_api": null,
          "new_api": "Reshape",
          "old_text": null,
          "new_text": "Reshape(all_to_all_shape, all_to_all)",
          "old_line_content": "}",
          "new_line_content": "    return Reshape(all_to_all_shape, all_to_all);",
          "content_same": false
        },
        {
          "line": 1698,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    return ReshapeInternal(shape, operand, inferred_dimension);\n  })",
          "old_line_content": "  });",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1699,
          "old_api": null,
          "new_api": "ReshapeInternal",
          "old_text": null,
          "new_text": "ReshapeInternal(shape, operand, inferred_dimension)",
          "old_line_content": "}",
          "new_line_content": "    return ReshapeInternal(shape, operand, inferred_dimension);",
          "content_same": false
        },
        {
          "line": 5794,
          "old_api": null,
          "new_api": "CustomCall",
          "old_text": null,
          "new_text": "CustomCall(builder,\n                      /*call_target_name=*/\"SPMDFullToShardShape\",\n                      {input_annotation}, output_shape,\n                      /*opaque=*/\n                      sharding_op_util::EncodeAttributes(unspecified_dims))",
          "old_line_content": "                      {input_annotation}, output_shape,",
          "new_line_content": "    return CustomCall(builder,",
          "content_same": false
        },
        {
          "line": 5798,
          "old_api": null,
          "new_api": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "old_text": null,
          "new_text": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "old_line_content": "}",
          "new_line_content": "                      sharding_op_util::EncodeAttributes(unspecified_dims));",
          "content_same": false
        },
        {
          "line": 3752,
          "old_api": null,
          "new_api": "reserve",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(auto operand_shapes, this->GetOperandShapes(operands));\n    std::vector<const Shape*> operand_shape_ptrs;\n    operand_shape_ptrs.reserve(operand_shapes.size());\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferAllToAllTupleShape(\n                                         operand_shape_ptrs));\n\n    if (layout) {\n      TF_RET_CHECK(shape.IsTuple() && !ShapeUtil::IsNestedTuple(shape));\n      for (int64_t i = 0; i < ShapeUtil::TupleElementCount(shape); ++i) {\n        const int64_t layout_minor_to_major_size =\n            layout->minor_to_major().size();\n        if (layout_minor_to_major_size != shape.tuple_shapes(i).rank()) {\n          return InvalidArgument(\n              \"Provided layout must be compatible with the operands' shape. \"\n              \"The layout is %s, but operand %d has shape %s.\",\n              layout->ToString(), i, shape.tuple_shapes(i).ToString());\n        }\n        *(shape.mutable_tuple_shapes(i)->mutable_layout()) = *layout;\n      }\n      instr.set_constrain_layout(true);\n    }\n    *instr.mutable_shape() = shape.ToProto();\n\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n\n    return AddInstruction(std::move(instr), HloOpcode::kAllToAll, operands);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(auto operand_shapes, this->GetOperandShapes(operands));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1707,
          "old_api": null,
          "new_api": "reserve",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<const Shape*> dim_size_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& dim_size_shapes,\n                        GetOperandShapes(dim_sizes));\n\n    absl::c_transform(dim_size_shapes, std::back_inserter(dim_size_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(const Shape shape,\n                        ShapeInference::InferDynamicReshapeShape(\n                            *operand_shape, dim_size_shape_ptrs,\n                            new_size_bounds, dims_are_dynamic));\n    TF_RETURN_IF_ERROR(first_error_);\n    std::vector<XlaOp> operands;\n    operands.reserve(1 + dim_sizes.size());\n    operands.push_back(operand);\n    for (const XlaOp& dim_size : dim_sizes) {\n      operands.push_back(dim_size);\n    }\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kDynamicReshape,\n                          operands);\n  })",
          "old_line_content": "    std::vector<const Shape*> dim_size_shape_ptrs;",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3756,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "operand_shapes.size()",
          "old_line_content": "                      [](const Shape& shape) { return &shape; });",
          "new_line_content": "    operand_shape_ptrs.reserve(operand_shapes.size());",
          "content_same": false
        },
        {
          "line": 3757,
          "old_api": null,
          "new_api": "std::back_inserter(operand_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(operand_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferAllToAllTupleShape(",
          "new_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 1713,
          "old_api": null,
          "new_api": "std::back_inserter(dim_size_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(dim_size_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape shape,",
          "new_line_content": "    absl::c_transform(dim_size_shapes, std::back_inserter(dim_size_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 3763,
          "old_api": null,
          "new_api": "ShapeUtil::IsNestedTuple(shape)",
          "old_text": null,
          "new_text": "ShapeUtil::IsNestedTuple(shape)",
          "old_line_content": "        const int64_t layout_minor_to_major_size =",
          "new_line_content": "      TF_RET_CHECK(shape.IsTuple() && !ShapeUtil::IsNestedTuple(shape));",
          "content_same": false
        },
        {
          "line": 3767,
          "old_api": null,
          "new_api": "tuple_shapes",
          "old_text": null,
          "new_text": "shape.tuple_shapes(i).rank()",
          "old_line_content": "              \"Provided layout must be compatible with the operands' shape. \"",
          "new_line_content": "        if (layout_minor_to_major_size != shape.tuple_shapes(i).rank()) {",
          "content_same": false
        },
        {
          "line": 3768,
          "old_api": null,
          "new_api": "tuple_shapes",
          "old_text": null,
          "new_text": "InvalidArgument(\n              \"Provided layout must be compatible with the operands' shape. \"\n              \"The layout is %s, but operand %d has shape %s.\",\n              layout->ToString(), i, shape.tuple_shapes(i).ToString())",
          "old_line_content": "              \"The layout is %s, but operand %d has shape %s.\",",
          "new_line_content": "          return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 1721,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "dim_sizes.size()",
          "old_line_content": "    for (const XlaOp& dim_size : dim_sizes) {",
          "new_line_content": "    operands.reserve(1 + dim_sizes.size());",
          "content_same": false
        },
        {
          "line": 5816,
          "old_api": null,
          "new_api": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "old_text": null,
          "new_text": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "old_line_content": "",
          "new_line_content": "        sharding_op_util::EncodeAttributes(unspecified_dims));",
          "content_same": false
        },
        {
          "line": 1724,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "operands.push_back(dim_size)",
          "old_line_content": "    HloInstructionProto instr;",
          "new_line_content": "      operands.push_back(dim_size);",
          "content_same": false
        },
        {
          "line": 5822,
          "old_api": null,
          "new_api": "CustomCall",
          "old_text": null,
          "new_text": "CustomCall(builder,\n                      /*call_target_name=*/\"SPMDShardToFullShape\",\n                      {input_annotation}, output_shape,\n                      sharding_op_util::EncodeAttributes(unspecified_dims))",
          "old_line_content": "                      {input_annotation}, output_shape,",
          "new_line_content": "    return CustomCall(builder,",
          "content_same": false
        },
        {
          "line": 1727,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "                          operands);",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1728,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kDynamicReshape,",
          "content_same": false
        },
        {
          "line": 3777,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "    for (const ReplicaGroup& group : replica_groups) {",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 5825,
          "old_api": null,
          "new_api": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "old_text": null,
          "new_text": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "old_line_content": "}",
          "new_line_content": "                      sharding_op_util::EncodeAttributes(unspecified_dims));",
          "content_same": false
        },
        {
          "line": 3782,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "channel_id.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (channel_id.has_value()) {",
          "content_same": false
        },
        {
          "line": 1735,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (dimensions.size() <= 1) {\n      // Not collapsing anything, trivially we can return the operand versus\n      // enqueueing a trivial reshape.\n      return operand;\n    }\n\n    // Out-of-order collapse is not supported.\n    // Checks that the collapsed dimensions are in order and consecutive.\n    for (absl::Span<const int64_t>::size_type i = 1; i < dimensions.size();\n         ++i) {\n      if (dimensions[i] - 1 != dimensions[i - 1]) {\n        return InvalidArgument(\n            \"Collapsed dimensions are not in consecutive order.\");\n      }\n    }\n\n    // Create a new sizes vector from the old shape, replacing the collapsed\n    // dimensions by the product of their sizes.\n    TF_ASSIGN_OR_RETURN(const Shape* original_shape, GetShapePtr(operand));\n\n    VLOG(3) << \"original shape: \" << ShapeUtil::HumanString(*original_shape);\n    VLOG(3) << \"dims to collapse: \" << absl::StrJoin(dimensions, \",\");\n\n    std::vector<int64_t> new_sizes;\n    for (int i = 0; i < original_shape->rank(); ++i) {\n      if (i <= dimensions.front() || i > dimensions.back()) {\n        new_sizes.push_back(original_shape->dimensions(i));\n      } else {\n        new_sizes.back() *= original_shape->dimensions(i);\n      }\n    }\n\n    VLOG(3) << \"new sizes: [\" << absl::StrJoin(new_sizes, \",\") << \"]\";\n\n    return Reshape(operand, new_sizes);\n  })",
          "old_line_content": "      // Not collapsing anything, trivially we can return the operand versus",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1736,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "dimensions.size()",
          "old_line_content": "      // enqueueing a trivial reshape.",
          "new_line_content": "    if (dimensions.size() <= 1) {",
          "content_same": false
        },
        {
          "line": 3783,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "channel_id->handle()",
          "old_line_content": "",
          "new_line_content": "      instr.set_channel_id(channel_id->handle());",
          "content_same": false
        },
        {
          "line": 3786,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kAllToAll, operands);",
          "content_same": false
        },
        {
          "line": 1744,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "dimensions.size()",
          "old_line_content": "      if (dimensions[i] - 1 != dimensions[i - 1]) {",
          "new_line_content": "    for (absl::Span<const int64_t>::size_type i = 1; i < dimensions.size();",
          "content_same": false
        },
        {
          "line": 1747,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n            \"Collapsed dimensions are not in consecutive order.\")",
          "old_line_content": "      }",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 3795,
          "old_api": null,
          "new_api": "status",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n\n    // The HloInstruction for Alltoall currently only handles the data\n    // communication: it accepts N already split parts and scatters them to N\n    // cores, and each core gathers the N received parts into a tuple as the\n    // output. So here we explicitly split the operand before the hlo alltoall,\n    // and concat the tuple elements.\n    //\n    // First, run shape inference to make sure the shapes are valid.\n    TF_RETURN_IF_ERROR(\n        ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,\n                                           concat_dimension, split_count)\n            .status());\n\n    // Split into N parts.\n    std::vector<XlaOp> slices;\n    slices.reserve(split_count);\n    const int64_t block_size =\n        operand_shape->dimensions(split_dimension) / split_count;\n    for (int i = 0; i < split_count; i++) {\n      slices.push_back(SliceInDim(operand, /*start_index=*/i * block_size,\n                                  /*limit_index=*/(i + 1) * block_size,\n                                  /*stride=*/1, /*dimno=*/split_dimension));\n    }\n\n    // Handle data communication.\n    XlaOp alltoall =\n        this->AllToAllTuple(slices, replica_groups, layout, channel_id);\n\n    // Concat the N received parts.\n    std::vector<XlaOp> received;\n    received.reserve(split_count);\n    for (int i = 0; i < split_count; i++) {\n      received.push_back(this->GetTupleElement(alltoall, i));\n    }\n    return this->ConcatInDim(received, concat_dimension);\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1756,
          "old_api": null,
          "new_api": "ShapeUtil::HumanString(*original_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanString(*original_shape)",
          "old_line_content": "",
          "new_line_content": "    VLOG(3) << \"original shape: \" << ShapeUtil::HumanString(*original_shape);",
          "content_same": false
        },
        {
          "line": 1757,
          "old_api": null,
          "new_api": "absl::StrJoin(dimensions, \",\")",
          "old_text": null,
          "new_text": "absl::StrJoin(dimensions, \",\")",
          "old_line_content": "    std::vector<int64_t> new_sizes;",
          "new_line_content": "    VLOG(3) << \"dims to collapse: \" << absl::StrJoin(dimensions, \",\");",
          "content_same": false
        },
        {
          "line": 3805,
          "old_api": null,
          "new_api": "status",
          "old_text": null,
          "new_text": "TF_RETURN_IF_ERROR(\n        ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,\n                                           concat_dimension, split_count)\n            .status())",
          "old_line_content": "                                           concat_dimension, split_count)",
          "new_line_content": "    TF_RETURN_IF_ERROR(",
          "content_same": false
        },
        {
          "line": 3806,
          "old_api": null,
          "new_api": "status",
          "old_text": null,
          "new_text": "ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,\n                                           concat_dimension, split_count)\n            .status()",
          "old_line_content": "            .status());",
          "new_line_content": "        ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,",
          "content_same": false
        },
        {
          "line": 1761,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "dimensions.back()",
          "old_line_content": "      } else {",
          "new_line_content": "      if (i <= dimensions.front() || i > dimensions.back()) {",
          "content_same": false
        },
        {
          "line": 1764,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "original_shape->dimensions(i)",
          "old_line_content": "    }",
          "new_line_content": "        new_sizes.back() *= original_shape->dimensions(i);",
          "content_same": false
        },
        {
          "line": 3816,
          "old_api": null,
          "new_api": "SliceInDim",
          "old_text": null,
          "new_text": "SliceInDim(operand, /*start_index=*/i * block_size,\n                                  /*limit_index=*/(i + 1) * block_size,\n                                  /*stride=*/1, /*dimno=*/split_dimension)",
          "old_line_content": "                                  /*stride=*/1, /*dimno=*/split_dimension));",
          "new_line_content": "      slices.push_back(SliceInDim(operand, /*start_index=*/i * block_size,",
          "content_same": false
        },
        {
          "line": 1770,
          "old_api": null,
          "new_api": "Reshape",
          "old_text": null,
          "new_text": "Reshape(operand, new_sizes)",
          "old_line_content": "}",
          "new_line_content": "    return Reshape(operand, new_sizes);",
          "content_same": false
        },
        {
          "line": 3823,
          "old_api": null,
          "new_api": "AllToAllTuple",
          "old_text": null,
          "new_text": "this->AllToAllTuple(slices, replica_groups, layout, channel_id)",
          "old_line_content": "    // Concat the N received parts.",
          "new_line_content": "        this->AllToAllTuple(slices, replica_groups, layout, channel_id);",
          "content_same": false
        },
        {
          "line": 1777,
          "old_api": null,
          "new_api": "Parameter",
          "old_text": null,
          "new_text": "Parameter(&builder, 0, shape, \"p\")",
          "old_line_content": "}",
          "new_line_content": "  XlaOp out = Parameter(&builder, 0, shape, \"p\");",
          "content_same": false
        },
        {
          "line": 1778,
          "old_api": null,
          "new_api": "Build",
          "old_text": null,
          "new_text": "builder.Build(out)",
          "old_line_content": "",
          "new_line_content": "  return builder.Build(out);",
          "content_same": false
        },
        {
          "line": 1782,
          "old_api": null,
          "new_api": "IsTuple",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* true_shape, GetShapePtr(on_true));\n    TF_ASSIGN_OR_RETURN(const Shape* false_shape, GetShapePtr(on_false));\n    TF_RET_CHECK(true_shape->IsTuple() == false_shape->IsTuple());\n    if (true_shape->IsTuple()) {\n      TF_ASSIGN_OR_RETURN(XlaComputation passthrough_true,\n                          PassthroughComputation(*true_shape));\n      TF_ASSIGN_OR_RETURN(XlaComputation passthrough_false,\n                          PassthroughComputation(*false_shape));\n      return Conditional(pred, on_true, passthrough_true, on_false,\n                         passthrough_false);\n    }\n    return TernaryOp(HloOpcode::kSelect, pred, on_true, on_false);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* false_shape, GetShapePtr(on_false));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3831,
          "old_api": null,
          "new_api": "ConcatInDim",
          "old_text": null,
          "new_text": "this->ConcatInDim(received, concat_dimension)",
          "old_line_content": "}",
          "new_line_content": "    return this->ConcatInDim(received, concat_dimension);",
          "content_same": false
        },
        {
          "line": 1785,
          "old_api": null,
          "new_api": "IsTuple",
          "old_text": null,
          "new_text": "false_shape->IsTuple()",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(XlaComputation passthrough_true,",
          "new_line_content": "    TF_RET_CHECK(true_shape->IsTuple() == false_shape->IsTuple());",
          "content_same": false
        },
        {
          "line": 1786,
          "old_api": null,
          "new_api": "IsTuple",
          "old_text": null,
          "new_text": "true_shape->IsTuple()",
          "old_line_content": "                          PassthroughComputation(*true_shape));",
          "new_line_content": "    if (true_shape->IsTuple()) {",
          "content_same": false
        },
        {
          "line": 3838,
          "old_api": null,
          "new_api": "CollectiveBroadcastImpl",
          "old_text": null,
          "new_text": "CollectiveBroadcastImpl(operand, replica_groups, channel_id)",
          "old_line_content": "",
          "new_line_content": "  return CollectiveBroadcastImpl(operand, replica_groups, channel_id);",
          "content_same": false
        },
        {
          "line": 1791,
          "old_api": null,
          "new_api": "Conditional",
          "old_text": null,
          "new_text": "Conditional(pred, on_true, passthrough_true, on_false,\n                         passthrough_false)",
          "old_line_content": "    }",
          "new_line_content": "      return Conditional(pred, on_true, passthrough_true, on_false,",
          "content_same": false
        },
        {
          "line": 1794,
          "old_api": null,
          "new_api": "TernaryOp",
          "old_text": null,
          "new_text": "TernaryOp(HloOpcode::kSelect, pred, on_true, on_false)",
          "old_line_content": "}",
          "new_line_content": "    return TernaryOp(HloOpcode::kSelect, pred, on_true, on_false);",
          "content_same": false
        },
        {
          "line": 3844,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> absl::StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferCollectiveBroadcastShape({operand_shape}));\n    *instr.mutable_shape() = shape.ToProto();\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n\n    return AddInstruction(std::move(instr), HloOpcode::kCollectiveBroadcast,\n                          {operand});\n  })",
          "old_line_content": "    HloInstructionProto instr;",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> absl::StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1799,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(elements));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(const Shape shape,\n                        ShapeInference::InferVariadicOpShape(\n                            HloOpcode::kTuple, operand_shape_ptrs));\n    return TupleInternal(shape, elements);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(elements));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1802,
          "old_api": null,
          "new_api": "std::back_inserter(operand_shape_ptrs)",
          "old_text": null,
          "new_text": "std::back_inserter(operand_shape_ptrs)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape shape,",
          "new_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "content_same": false
        },
        {
          "line": 3854,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "channel_id.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (channel_id.has_value()) {",
          "content_same": false
        },
        {
          "line": 1807,
          "old_api": null,
          "new_api": "TupleInternal",
          "old_text": null,
          "new_text": "TupleInternal(shape, elements)",
          "old_line_content": "}",
          "new_line_content": "    return TupleInternal(shape, elements);",
          "content_same": false
        },
        {
          "line": 3855,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "channel_id->handle()",
          "old_line_content": "",
          "new_line_content": "      instr.set_channel_id(channel_id->handle());",
          "content_same": false
        },
        {
          "line": 3858,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCollectiveBroadcast,",
          "content_same": false
        },
        {
          "line": 1814,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "}",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1815,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kTuple, elements);",
          "content_same": false
        },
        {
          "line": 3867,
          "old_api": null,
          "new_api": "CollectivePermuteImpl",
          "old_text": null,
          "new_text": "CollectivePermuteImpl(operand, source_target_pairs, channel_id,\n                               /*async=*/false)",
          "old_line_content": "}",
          "new_line_content": "  return CollectivePermuteImpl(operand, source_target_pairs, channel_id,",
          "content_same": false
        },
        {
          "line": 1821,
          "old_api": null,
          "new_api": "IsTuple",
          "old_text": null,
          "new_text": "tuple_shape->IsTuple()",
          "old_line_content": "          \"Operand to GetTupleElement() is not a tuple; got %s\",",
          "new_line_content": "    if (!tuple_shape->IsTuple()) {",
          "content_same": false
        },
        {
          "line": 1826,
          "old_api": null,
          "new_api": "ShapeUtil::TupleElementCount(*tuple_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::TupleElementCount(*tuple_shape)",
          "old_line_content": "          \"GetTupleElement() index (%d) out of range for tuple shape %s\", index,",
          "new_line_content": "    if (index < 0 || index >= ShapeUtil::TupleElementCount(*tuple_shape)) {",
          "content_same": false
        },
        {
          "line": 3875,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferCollectivePermuteShape({operand_shape}));\n    *instr.mutable_shape() = shape.ToProto();\n\n    for (const auto& pair : source_target_pairs) {\n      auto* proto_pair = instr.add_source_target_pairs();\n      proto_pair->set_source(pair.first);\n      proto_pair->set_target(pair.second);\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n\n    return AddInstruction(std::move(instr),\n                          async ? HloOpcode::kCollectivePermuteStart\n                                : HloOpcode::kCollectivePermute,\n                          {operand});\n  })",
          "old_line_content": "    HloInstructionProto instr;",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1831,
          "old_api": null,
          "new_api": "GetTupleElementInternal",
          "old_text": null,
          "new_text": "GetTupleElementInternal(\n        ShapeUtil::GetTupleElementShape(*tuple_shape, index), tuple_data,\n        index)",
          "old_line_content": "        index);",
          "new_line_content": "    return GetTupleElementInternal(",
          "content_same": false
        },
        {
          "line": 1832,
          "old_api": null,
          "new_api": "ShapeUtil::GetTupleElementShape(*tuple_shape, index)",
          "old_text": null,
          "new_text": "ShapeUtil::GetTupleElementShape(*tuple_shape, index)",
          "old_line_content": "  });",
          "new_line_content": "        ShapeUtil::GetTupleElementShape(*tuple_shape, index), tuple_data,",
          "content_same": false
        },
        {
          "line": 3881,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "    for (const auto& pair : source_target_pairs) {",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 3885,
          "old_api": null,
          "new_api": "set_source",
          "old_text": null,
          "new_text": "proto_pair->set_source(pair.first)",
          "old_line_content": "    }",
          "new_line_content": "      proto_pair->set_source(pair.first);",
          "content_same": false
        },
        {
          "line": 3888,
          "old_api": null,
          "new_api": "has_value",
          "old_text": null,
          "new_text": "channel_id.has_value()",
          "old_line_content": "    }",
          "new_line_content": "    if (channel_id.has_value()) {",
          "content_same": false
        },
        {
          "line": 3889,
          "old_api": null,
          "new_api": "handle",
          "old_text": null,
          "new_text": "channel_id->handle()",
          "old_line_content": "",
          "new_line_content": "      instr.set_channel_id(channel_id->handle());",
          "content_same": false
        },
        {
          "line": 1842,
          "old_api": null,
          "new_api": "set_tuple_index",
          "old_text": null,
          "new_text": "instr.set_tuple_index(index)",
          "old_line_content": "                        {tuple_data});",
          "new_line_content": "  instr.set_tuple_index(index);",
          "content_same": false
        },
        {
          "line": 1843,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kGetTupleElement,",
          "content_same": false
        },
        {
          "line": 3892,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "                                : HloOpcode::kCollectivePermute,",
          "new_line_content": "    return AddInstruction(std::move(instr),",
          "content_same": false
        },
        {
          "line": 1850,
          "old_api": null,
          "new_api": "add_lhs_contracting_dimensions",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n\n    DotDimensionNumbers dimension_numbers;\n    dimension_numbers.add_lhs_contracting_dimensions(\n        lhs_shape->dimensions_size() == 1 ? 0 : 1);\n    dimension_numbers.add_rhs_contracting_dimensions(0);\n    return DotGeneral(lhs, rhs, dimension_numbers, precision_config,\n                      preferred_element_type);\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3902,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeShape(U32, {}).ToProto()",
          "old_line_content": "  });",
          "new_line_content": "    *instr.mutable_shape() = ShapeUtil::MakeShape(U32, {}).ToProto();",
          "content_same": false
        },
        {
          "line": 3903,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kReplicaId, {});",
          "content_same": false
        },
        {
          "line": 1856,
          "old_api": null,
          "new_api": "add_rhs_contracting_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.add_rhs_contracting_dimensions(0)",
          "old_line_content": "                      preferred_element_type);",
          "new_line_content": "    dimension_numbers.add_rhs_contracting_dimensions(0);",
          "content_same": false
        },
        {
          "line": 1857,
          "old_api": null,
          "new_api": "DotGeneral",
          "old_text": null,
          "new_text": "DotGeneral(lhs, rhs, dimension_numbers, precision_config,\n                      preferred_element_type)",
          "old_line_content": "  });",
          "new_line_content": "    return DotGeneral(lhs, rhs, dimension_numbers, precision_config,",
          "content_same": false
        },
        {
          "line": 3913,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n\n    std::vector<std::pair<int64_t, int64_t>> padding_values =\n        MakePadding(operand_shape->dimensions(), window_dimensions,\n                    window_strides, padding);\n\n    TF_ASSIGN_OR_RETURN(auto window,\n                        ShapeInference::InferWindowFromDimensions(\n                            window_dimensions, window_strides, padding_values,\n                            /*lhs_dilation=*/{},\n                            /*rhs_dilation=*/{}));\n    PaddingType padding_type = PADDING_INVALID;\n    for (int64_t i = 0; i < operand_shape->rank(); ++i) {\n      if (operand_shape->is_dynamic_dimension(i) &&\n          !window_util::IsTrivialWindowDimension(window.dimensions(i)) &&\n          padding == Padding::kSame) {\n        // SAME padding can create dynamic padding sizes. The padding size\n        // need to be rewritten by dynamic padder using HloInstructions. We\n        // create a CustomCall to handle this.\n        padding_type = PADDING_SAME;\n      }\n    }\n    if (padding_type == PADDING_SAME) {\n      TF_ASSIGN_OR_RETURN(\n          HloInstructionProto instr,\n          SelectAndScatterInternal(operand, select, window_dimensions,\n                                   window_strides, padding_values, source,\n                                   init_value, scatter));\n      instr.set_custom_call_target(\"DynamicSelectAndScatterSamePadding\");\n      return AddInstruction(std::move(instr), HloOpcode::kCustomCall,\n                            {operand, source, init_value});\n    }\n    return SelectAndScatterWithGeneralPadding(\n        operand, select, window_dimensions, window_strides, padding_values,\n        source, init_value, scatter);\n  })",
          "old_line_content": "",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1866,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferDotOpShape(\n            *lhs_shape, *rhs_shape, dimension_numbers, preferred_element_type));\n    return DotGeneralInternal(shape, lhs, rhs, dimension_numbers,\n                              precision_config);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3917,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "operand_shape->dimensions()",
          "old_line_content": "",
          "new_line_content": "        MakePadding(operand_shape->dimensions(), window_dimensions,",
          "content_same": false
        },
        {
          "line": 1873,
          "old_api": null,
          "new_api": "DotGeneralInternal",
          "old_text": null,
          "new_text": "DotGeneralInternal(shape, lhs, rhs, dimension_numbers,\n                              precision_config)",
          "old_line_content": "  });",
          "new_line_content": "    return DotGeneralInternal(shape, lhs, rhs, dimension_numbers,",
          "content_same": false
        },
        {
          "line": 3927,
          "old_api": null,
          "new_api": "is_dynamic_dimension",
          "old_text": null,
          "new_text": "operand_shape->is_dynamic_dimension(i)",
          "old_line_content": "          padding == Padding::kSame) {",
          "new_line_content": "      if (operand_shape->is_dynamic_dimension(i) &&",
          "content_same": false
        },
        {
          "line": 3928,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "window.dimensions(i)",
          "old_line_content": "        // SAME padding can create dynamic padding sizes. The padding size",
          "new_line_content": "          !window_util::IsTrivialWindowDimension(window.dimensions(i)) &&",
          "content_same": false
        },
        {
          "line": 1883,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "  if (precision_config != nullptr) {",
          "new_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1888,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kDot, {lhs, rhs});",
          "content_same": false
        },
        {
          "line": 3942,
          "old_api": null,
          "new_api": "set_custom_call_target",
          "old_text": null,
          "new_text": "instr.set_custom_call_target(\"DynamicSelectAndScatterSamePadding\")",
          "old_line_content": "                            {operand, source, init_value});",
          "new_line_content": "      instr.set_custom_call_target(\"DynamicSelectAndScatterSamePadding\");",
          "content_same": false
        },
        {
          "line": 3943,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "    }",
          "new_line_content": "      return AddInstruction(std::move(instr), HloOpcode::kCustomCall,",
          "content_same": false
        },
        {
          "line": 1897,
          "old_api": null,
          "new_api": "insert",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferDotOpShape(\n                            *lhs_shape, *rhs_shape, dimension_numbers,\n                            preferred_element_type, sparsity));\n    std::vector<XlaOp> operands{lhs, rhs};\n    operands.insert(operands.end(), sparse_meta.begin(), sparse_meta.end());\n\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    *instr.mutable_dot_dimension_numbers() = dimension_numbers;\n    if (precision_config != nullptr) {\n      *instr.mutable_precision_config() = *precision_config;\n    }\n    for (const SparsityDescriptor& descriptor : sparsity) {\n      *instr.add_dot_sparsity() = descriptor;\n    }\n    return AddInstruction(std::move(instr), HloOpcode::kDot, operands);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 3946,
          "old_api": null,
          "new_api": "SelectAndScatterWithGeneralPadding",
          "old_text": null,
          "new_text": "SelectAndScatterWithGeneralPadding(\n        operand, select, window_dimensions, window_strides, padding_values,\n        source, init_value, scatter)",
          "old_line_content": "        source, init_value, scatter);",
          "new_line_content": "    return SelectAndScatterWithGeneralPadding(",
          "content_same": false
        },
        {
          "line": 1905,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "sparse_meta.end()",
          "old_line_content": "    HloInstructionProto instr;",
          "new_line_content": "    operands.insert(operands.end(), sparse_meta.begin(), sparse_meta.end());",
          "content_same": false
        },
        {
          "line": 1908,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "shape.ToProto()",
          "old_line_content": "    if (precision_config != nullptr) {",
          "new_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "content_same": false
        },
        {
          "line": 1911,
          "old_api": null,
          "new_api": "mutable_precision_config",
          "old_text": null,
          "new_text": "instr.mutable_precision_config()",
          "old_line_content": "    for (const SparsityDescriptor& descriptor : sparsity) {",
          "new_line_content": "      *instr.mutable_precision_config() = *precision_config;",
          "content_same": false
        },
        {
          "line": 1916,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kDot, operands);",
          "content_same": false
        },
        {
          "line": 3967,
          "old_api": null,
          "new_api": "mutable_window",
          "old_text": null,
          "new_text": "instr.mutable_window()",
          "old_line_content": "                          window_dimensions, window_strides, padding,",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(*instr.mutable_window(),",
          "content_same": false
        },
        {
          "line": 3968,
          "old_api": null,
          "new_api": "ShapeInference::InferWindowFromDimensions(\n                          window_dimensions, window_strides, padding,\n                          /*lhs_dilation=*/{}, /*rhs_dilation=*/{})",
          "old_text": null,
          "new_text": "ShapeInference::InferWindowFromDimensions(\n                          window_dimensions, window_strides, padding,\n                          /*lhs_dilation=*/{}, /*rhs_dilation=*/{})",
          "old_line_content": "                          /*lhs_dilation=*/{}, /*rhs_dilation=*/{}));",
          "new_line_content": "                      ShapeInference::InferWindowFromDimensions(",
          "content_same": false
        },
        {
          "line": 1923,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "rhs_shape.rank()",
          "old_line_content": "        \"Convolution arguments must have same number of \"",
          "new_line_content": "  if (lhs_shape.rank() != rhs_shape.rank()) {",
          "content_same": false
        },
        {
          "line": 1924,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n        \"Convolution arguments must have same number of \"\n        \"dimensions. Got: %s and %s\",\n        ShapeUtil::HumanString(lhs_shape), ShapeUtil::HumanString(rhs_shape))",
          "old_line_content": "        \"dimensions. Got: %s and %s\",",
          "new_line_content": "    return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 3977,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(select, &instr)",
          "old_line_content": "  return instr;",
          "new_line_content": "  AddCalledComputation(select, &instr);",
          "content_same": false
        },
        {
          "line": 3978,
          "old_api": null,
          "new_api": "AddCalledComputation",
          "old_text": null,
          "new_text": "AddCalledComputation(scatter, &instr)",
          "old_line_content": "}",
          "new_line_content": "  AddCalledComputation(scatter, &instr);",
          "content_same": false
        },
        {
          "line": 1931,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n        \"Convolution expects argument arrays with >= 3 dimensions. \"\n        \"Got: %s and %s\",\n        ShapeUtil::HumanString(lhs_shape), ShapeUtil::HumanString(rhs_shape))",
          "old_line_content": "        \"Got: %s and %s\",",
          "new_line_content": "    return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 1934,
          "old_api": null,
          "new_api": "ShapeUtil::HumanString(rhs_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanString(rhs_shape)",
          "old_line_content": "  int num_spatial_dims = num_dims - 2;",
          "new_line_content": "        ShapeUtil::HumanString(lhs_shape), ShapeUtil::HumanString(rhs_shape));",
          "content_same": false
        },
        {
          "line": 3988,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(HloInstructionProto instr,\n                        SelectAndScatterInternal(\n                            operand, select, window_dimensions, window_strides,\n                            padding, source, init_value, scatter));\n\n    return AddInstruction(std::move(instr), HloOpcode::kSelectAndScatter,\n                          {operand, source, init_value});\n  })",
          "old_line_content": "                        SelectAndScatterInternal(",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1941,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "InvalidArgument(\"Expected %d elements for %s, but got %d.\",\n                             num_spatial_dims, field_name, numbers.size())",
          "old_line_content": "    }",
          "new_line_content": "      return InvalidArgument(\"Expected %d elements for %s, but got %d.\",",
          "content_same": false
        },
        {
          "line": 1946,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Convolution %s[%d] is out of bounds: %d\",\n                               field_name, i, numbers[i])",
          "old_line_content": "      }",
          "new_line_content": "        return InvalidArgument(\"Convolution %s[%d] is out of bounds: %d\",",
          "content_same": false
        },
        {
          "line": 3994,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kSelectAndScatter,",
          "content_same": false
        },
        {
          "line": 4001,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferReducePrecisionShape(\n                            *operand_shape, exponent_bits, mantissa_bits));\n    return ReducePrecisionInternal(shape, operand, exponent_bits,\n                                   mantissa_bits);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape,",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1957,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.kernel_spatial_dimensions()",
          "old_line_content": "      \"output_spatial_dimensions\",",
          "new_line_content": "                               dimension_numbers.kernel_spatial_dimensions()));",
          "content_same": false
        },
        {
          "line": 4006,
          "old_api": null,
          "new_api": "ReducePrecisionInternal",
          "old_text": null,
          "new_text": "ReducePrecisionInternal(shape, operand, exponent_bits,\n                                   mantissa_bits)",
          "old_line_content": "  });",
          "new_line_content": "    return ReducePrecisionInternal(shape, operand, exponent_bits,",
          "content_same": false
        },
        {
          "line": 1960,
          "old_api": null,
          "new_api": "output_spatial_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.output_spatial_dimensions()",
          "old_line_content": "",
          "new_line_content": "      dimension_numbers.output_spatial_dimensions());",
          "content_same": false
        },
        {
          "line": 4018,
          "old_api": null,
          "new_api": "set_mantissa_bits",
          "old_text": null,
          "new_text": "instr.set_mantissa_bits(mantissa_bits)",
          "old_line_content": "                        {operand});",
          "new_line_content": "  instr.set_mantissa_bits(mantissa_bits);",
          "content_same": false
        },
        {
          "line": 1971,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "window_strides.size()",
          "old_line_content": "      preferred_element_type);",
          "new_line_content": "      CreateDefaultConvDimensionNumbers(window_strides.size()),",
          "content_same": false
        },
        {
          "line": 4019,
          "old_api": null,
          "new_api": "std::move(instr)",
          "old_text": null,
          "new_text": "std::move(instr)",
          "old_line_content": "}",
          "new_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kReducePrecision,",
          "content_same": false
        },
        {
          "line": 4024,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Send HLO takes two operands: a data operand and a token. Generate the\n    // token to pass into the send.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto token_instr;\n    *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    TF_ASSIGN_OR_RETURN(XlaOp token, AddInstruction(std::move(token_instr),\n                                                    HloOpcode::kAfterAll, {}));\n\n    return SendWithToken(operand, token, handle);\n  })",
          "old_line_content": "    // token to pass into the send.",
          "new_line_content": "  ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 1982,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "ConvGeneral(lhs, rhs, window_strides, padding,\n                     CreateDefaultConvDimensionNumbers(window_strides.size()),\n                     feature_group_count, batch_group_count, precision_config,\n                     preferred_element_type)",
          "old_line_content": "                     feature_group_count, batch_group_count, precision_config,",
          "new_line_content": "  return ConvGeneral(lhs, rhs, window_strides, padding,",
          "content_same": false
        },
        {
          "line": 1983,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "window_strides.size()",
          "old_line_content": "                     preferred_element_type);",
          "new_line_content": "                     CreateDefaultConvDimensionNumbers(window_strides.size()),",
          "content_same": false
        },
        {
          "line": 4030,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "                                                    HloOpcode::kAfterAll, {}));",
          "new_line_content": "    *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 4034,
          "old_api": null,
          "new_api": "SendWithToken",
          "old_text": null,
          "new_text": "SendWithToken(operand, token, handle)",
          "old_line_content": "}",
          "new_line_content": "    return SendWithToken(operand, token, handle);",
          "content_same": false
        },
        {
          "line": 4041,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "handle.type()",
          "old_line_content": "    }",
          "new_line_content": "    if (handle.type() != ChannelHandle::DEVICE_TO_DEVICE) {",
          "content_same": false
        },
        {
          "line": 1994,
          "old_api": null,
          "new_api": "input_spatial_dimensions_size",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n\n    TF_RETURN_IF_ERROR(\n        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers));\n\n    std::vector<int64_t> base_area_dimensions(\n        dimension_numbers.input_spatial_dimensions_size());\n    for (std::vector<int64_t>::size_type i = 0; i < base_area_dimensions.size();\n         ++i) {\n      base_area_dimensions[i] =\n          lhs_shape->dimensions(dimension_numbers.input_spatial_dimensions(i));\n    }\n\n    std::vector<int64_t> window_dimensions(\n        dimension_numbers.kernel_spatial_dimensions_size());\n    for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();\n         ++i) {\n      window_dimensions[i] =\n          rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));\n    }\n\n    return ConvGeneral(lhs, rhs, window_strides,\n                       MakePadding(base_area_dimensions, window_dimensions,\n                                   window_strides, padding),\n                       dimension_numbers, feature_group_count,\n                       batch_group_count, precision_config,\n                       preferred_element_type);\n  })",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 4042,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Send must use a device-to-device channel\")",
          "old_line_content": "",
          "new_line_content": "      return InvalidArgument(\"Send must use a device-to-device channel\");",
          "content_same": false
        },
        {
          "line": 1998,
          "old_api": null,
          "new_api": "TF_RETURN_IF_ERROR",
          "old_text": null,
          "new_text": "TF_RETURN_IF_ERROR(\n        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers))",
          "old_line_content": "",
          "new_line_content": "    TF_RETURN_IF_ERROR(",
          "content_same": false
        },
        {
          "line": 1999,
          "old_api": null,
          "new_api": "VerifyConvolution",
          "old_text": null,
          "new_text": "VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers)",
          "old_line_content": "    std::vector<int64_t> base_area_dimensions(",
          "new_line_content": "        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers));",
          "content_same": false
        },
        {
          "line": 4047,
          "old_api": null,
          "new_api": "internal::XlaBuilderFriend::BuildSendDone(this, send_op, handle,\n                                                     false)",
          "old_text": null,
          "new_text": "internal::XlaBuilderFriend::BuildSendDone(this, send_op, handle,\n                                                     false)",
          "old_line_content": "  });",
          "new_line_content": "    return internal::XlaBuilderFriend::BuildSendDone(this, send_op, handle,",
          "content_same": false
        },
        {
          "line": 2002,
          "old_api": null,
          "new_api": "input_spatial_dimensions_size",
          "old_text": null,
          "new_text": "dimension_numbers.input_spatial_dimensions_size()",
          "old_line_content": "         ++i) {",
          "new_line_content": "        dimension_numbers.input_spatial_dimensions_size());",
          "content_same": false
        },
        {
          "line": 2003,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "base_area_dimensions.size()",
          "old_line_content": "      base_area_dimensions[i] =",
          "new_line_content": "    for (std::vector<int64_t>::size_type i = 0; i < base_area_dimensions.size();",
          "content_same": false
        },
        {
          "line": 4053,
          "old_api": null,
          "new_api": "mutable_shape",
          "old_text": null,
          "new_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Recv HLO takes a single token operand. Generate the token to pass into\n    // the Recv and RecvDone instructions.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto token_instr;\n    *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    TF_ASSIGN_OR_RETURN(XlaOp token, AddInstruction(std::move(token_instr),\n                                                    HloOpcode::kAfterAll, {}));\n\n    XlaOp recv = RecvWithToken(token, shape, handle);\n\n    // The RecvDone instruction produces a tuple of the data and a token\n    // type. Return XLA op containing the data.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto recv_data;\n    *recv_data.mutable_shape() = shape.ToProto();\n    recv_data.set_tuple_index(0);\n    return AddInstruction(std::move(recv_data), HloOpcode::kGetTupleElement,\n                          {recv});\n  })",
          "old_line_content": "    // the Recv and RecvDone instructions.",
          "new_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 2006,
          "old_api": null,
          "new_api": "input_spatial_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.input_spatial_dimensions(i)",
          "old_line_content": "",
          "new_line_content": "          lhs_shape->dimensions(dimension_numbers.input_spatial_dimensions(i));",
          "content_same": false
        },
        {
          "line": 2010,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions_size",
          "old_text": null,
          "new_text": "dimension_numbers.kernel_spatial_dimensions_size()",
          "old_line_content": "         ++i) {",
          "new_line_content": "        dimension_numbers.kernel_spatial_dimensions_size());",
          "content_same": false
        },
        {
          "line": 2011,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "window_dimensions.size()",
          "old_line_content": "      window_dimensions[i] =",
          "new_line_content": "    for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();",
          "content_same": false
        },
        {
          "line": 4059,
          "old_api": null,
          "new_api": "ToProto",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "old_line_content": "                                                    HloOpcode::kAfterAll, {}));",
          "new_line_content": "    *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "content_same": false
        },
        {
          "line": 2014,
          "old_api": null,
          "new_api": "kernel_spatial_dimensions",
          "old_text": null,
          "new_text": "dimension_numbers.kernel_spatial_dimensions(i)",
          "old_line_content": "",
          "new_line_content": "          rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));",
          "content_same": false
        },
        {
          "line": 4063,
          "old_api": null,
          "new_api": "RecvWithToken",
          "old_text": null,
          "new_text": "RecvWithToken(token, shape, handle)",
          "old_line_content": "    // The RecvDone instruction produces a tuple of the data and a token",
          "new_line_content": "    XlaOp recv = RecvWithToken(token, shape, handle);",
          "content_same": false
        },
        {
          "line": 2017,
          "old_api": null,
          "new_api": "ConvGeneral",
          "old_text": null,
          "new_text": "ConvGeneral(lhs, rhs, window_strides,\n                       MakePadding(base_area_dimensions, window_dimensions,\n                                   window_strides, padding),\n                       dimension_numbers, feature_group_count,\n                       batch_group_count, precision_config,\n                       preferred_element_type)",
          "old_line_content": "                                   window_strides, padding),",
          "new_line_content": "    return ConvGeneral(lhs, rhs, window_strides,",
          "content_same": false
        },
        {
          "line": 2018,
          "old_api": null,
          "new_api": "MakePadding",
          "old_text": null,
          "new_text": "MakePadding(base_area_dimensions, window_dimensions,\n                                   window_strides, padding)",
          "old_line_content": "                       dimension_numbers, feature_group_count,",
          "new_line_content": "                       MakePadding(base_area_dimensions, window_dimensions,",
          "content_same": false
        },
        {
          "line": 4071,
          "old_api": null,
          "new_api": "set_tuple_index",
          "old_text": null,
          "new_text": "recv_data.set_tuple_index(0)",
          "old_line_content": "                          {recv});",
          "new_line_content": "    recv_data.set_tuple_index(0);",
          "content_same": false
        },
        {
          "line": 4072,
          "old_api": null,
          "new_api": "std::move(recv_data)",
          "old_text": null,
          "new_text": "std::move(recv_data)",
          "old_line_content": "  });",
          "new_line_content": "    return AddInstruction(std::move(recv_data), HloOpcode::kGetTupleElement,",
          "content_same": false
        },
        {
          "line": 4080,
          "old_api": null,
          "new_api": "type",
          "old_text": null,
          "new_text": "handle.type()",
          "old_line_content": "    }",
          "new_line_content": "    if (handle.type() != ChannelHandle::DEVICE_TO_DEVICE) {",
          "content_same": false
        },
        {
          "line": 2033,
          "old_api": null,
          "new_api": "ConvGeneralDilated",
          "old_text": null,
          "new_text": "ConvGeneralDilated(lhs, rhs, window_strides, padding, {}, {},\n                            dimension_numbers, feature_group_count,\n                            batch_group_count, precision_config,\n                            preferred_element_type)",
          "old_line_content": "                            batch_group_count, precision_config,",
          "new_line_content": "  return ConvGeneralDilated(lhs, rhs, window_strides, padding, {}, {},",
          "content_same": false
        },
        {
          "line": 4081,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\"Recv must use a device-to-device channel\")",
          "old_line_content": "",
          "new_line_content": "      return InvalidArgument(\"Recv must use a device-to-device channel\");",
          "content_same": false
        },
        {
          "line": 4086,
          "old_api": null,
          "new_api": "internal::XlaBuilderFriend::BuildRecvDone(this, recv_op, shape,\n                                                     handle, false)",
          "old_text": null,
          "new_text": "internal::XlaBuilderFriend::BuildRecvDone(this, recv_op, shape,\n                                                     handle, false)",
          "old_line_content": "  });",
          "new_line_content": "    return internal::XlaBuilderFriend::BuildRecvDone(this, recv_op, shape,",
          "content_same": false
        },
        {
          "line": 4095,
          "old_api": null,
          "new_api": "LayoutUtil::HasLayout(shape_with_layout)",
          "old_text": null,
          "new_text": "LayoutUtil::HasLayout(shape_with_layout)",
          "old_line_content": "    }",
          "new_line_content": "    if (!LayoutUtil::HasLayout(shape_with_layout)) {",
          "content_same": false
        }
      ],
      "deletions": [
        {
          "line": 4097,
          "old_api": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "new_api": null,
          "old_text": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "new_text": null,
          "old_line_content": "    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2050,
          "old_api": "kernel_spatial_dimensions_size",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n    TF_RETURN_IF_ERROR(\n        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers));\n\n    std::vector<int64_t> window_dimensions(\n        dimension_numbers.kernel_spatial_dimensions_size());\n    for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();\n         ++i) {\n      window_dimensions[i] =\n          rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));\n    }\n\n    TF_ASSIGN_OR_RETURN(Window window,\n                        ShapeInference::InferWindowFromDimensions(\n                            window_dimensions, window_strides, padding,\n                            lhs_dilation, rhs_dilation, window_reversal));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferConvolveShape(\n            *lhs_shape, *rhs_shape, feature_group_count, batch_group_count,\n            window, dimension_numbers, preferred_element_type));\n    return ConvGeneralDilatedInternal(shape, lhs, rhs, window, window_strides,\n                                      padding, lhs_dilation, rhs_dilation,\n                                      dimension_numbers, feature_group_count,\n                                      batch_group_count, precision_config);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    std::optional<PrimitiveType> preferred_element_type,",
          "content_same": false
        },
        {
          "line": 4098,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"SendToHost shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 2053,
          "old_api": "TF_RETURN_IF_ERROR",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(\n        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers))",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));",
          "content_same": false
        },
        {
          "line": 2054,
          "old_api": "VerifyConvolution",
          "new_api": null,
          "old_text": "VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers)",
          "new_text": null,
          "old_line_content": "        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers));",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));",
          "content_same": false
        },
        {
          "line": 4101,
          "old_api": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "new_text": null,
          "old_line_content": "          ShapeUtil::HumanStringWithLayout(*operand_shape));",
          "new_line_content": "          \"SendToHost shape %s must be compatible with operand shape %s\",",
          "content_same": false
        },
        {
          "line": 4104,
          "old_api": "IsArray",
          "new_api": null,
          "old_text": "operand_shape->IsArray()",
          "new_text": null,
          "old_line_content": "    if (!operand_shape->IsArray()) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2057,
          "old_api": "kernel_spatial_dimensions_size",
          "new_api": null,
          "old_text": "dimension_numbers.kernel_spatial_dimensions_size()",
          "new_text": null,
          "old_line_content": "        dimension_numbers.kernel_spatial_dimensions_size());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2058,
          "old_api": "size",
          "new_api": null,
          "old_text": "window_dimensions.size()",
          "new_text": null,
          "old_line_content": "    for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();",
          "new_line_content": "    std::vector<int64_t> window_dimensions(",
          "content_same": false
        },
        {
          "line": 4105,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\"SendToHost only supports array shapes, shape: %s\",\n                             ShapeUtil::HumanString(*operand_shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"SendToHost only supports array shapes, shape: %s\",",
          "new_line_content": "    // TODO(b/111544877): Support tuple shapes.",
          "content_same": false
        },
        {
          "line": 2061,
          "old_api": "kernel_spatial_dimensions",
          "new_api": null,
          "old_text": "dimension_numbers.kernel_spatial_dimensions(i)",
          "new_text": null,
          "old_line_content": "          rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));",
          "new_line_content": "         ++i) {",
          "content_same": false
        },
        {
          "line": 4109,
          "old_api": "type",
          "new_api": null,
          "old_text": "handle.type()",
          "new_text": null,
          "old_line_content": "    if (handle.type() != ChannelHandle::DEVICE_TO_HOST) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4110,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\"SendToHost must use a device-to-host channel\")",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"SendToHost must use a device-to-host channel\");",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4116,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "send_instr.mutable_shape()",
          "new_text": null,
          "old_line_content": "    *send_instr.mutable_shape() =",
          "new_line_content": "    // token}.",
          "content_same": false
        },
        {
          "line": 4117,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTupleShape({shape_with_layout,\n                                   ShapeUtil::MakeShape(U32, {}),\n                                   ShapeUtil::MakeTokenShape()})\n            .ToProto()",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeTupleShape({shape_with_layout,",
          "new_line_content": "    HloInstructionProto send_instr;",
          "content_same": false
        },
        {
          "line": 2073,
          "old_api": "ConvGeneralDilatedInternal",
          "new_api": null,
          "old_text": "ConvGeneralDilatedInternal(shape, lhs, rhs, window, window_strides,\n                                      padding, lhs_dilation, rhs_dilation,\n                                      dimension_numbers, feature_group_count,\n                                      batch_group_count, precision_config)",
          "new_text": null,
          "old_line_content": "    return ConvGeneralDilatedInternal(shape, lhs, rhs, window, window_strides,",
          "new_line_content": "            *lhs_shape, *rhs_shape, feature_group_count, batch_group_count,",
          "content_same": false
        },
        {
          "line": 4122,
          "old_api": "set_is_host_transfer",
          "new_api": null,
          "old_text": "send_instr.set_is_host_transfer(true)",
          "new_text": null,
          "old_line_content": "    send_instr.set_is_host_transfer(true);",
          "new_line_content": "            .ToProto();",
          "content_same": false
        },
        {
          "line": 4128,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": null,
          "old_line_content": "    *send_done_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4129,
          "old_api": "handle",
          "new_api": null,
          "old_text": "handle.handle()",
          "new_text": null,
          "old_line_content": "    send_done_instr.set_channel_id(handle.handle());",
          "new_line_content": "    HloInstructionProto send_done_instr;",
          "content_same": false
        },
        {
          "line": 2092,
          "old_api": "kernel_spatial_dimensions_size",
          "new_api": null,
          "old_text": "dimension_numbers.kernel_spatial_dimensions_size()",
          "new_text": null,
          "old_line_content": "      dimension_numbers.kernel_spatial_dimensions_size());",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));",
          "content_same": false
        },
        {
          "line": 2093,
          "old_api": "size",
          "new_api": null,
          "old_text": "window_dimensions.size()",
          "new_text": null,
          "old_line_content": "  for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();",
          "new_line_content": "  std::vector<int64_t> window_dimensions(",
          "content_same": false
        },
        {
          "line": 4140,
          "old_api": "IsArray",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!LayoutUtil::HasLayout(shape)) {\n      return InvalidArgument(\"Shape passed to RecvFromHost must have a layout\");\n    }\n\n    // TODO(b/111544877): Support tuple shapes.\n    if (!shape.IsArray()) {\n      return InvalidArgument(\n          \"RecvFromHost only supports array shapes, shape: %s\",\n          ShapeUtil::HumanString(shape));\n    }\n\n    if (handle.type() != ChannelHandle::HOST_TO_DEVICE) {\n      return InvalidArgument(\"RecvFromHost must use a host-to-device channel\");\n    }\n\n    // Recv instruction produces a tuple of {receive buffer, U32 context,\n    // token}.\n    HloInstructionProto recv_instr;\n    *recv_instr.mutable_shape() =\n        ShapeUtil::MakeTupleShape(\n            {shape, ShapeUtil::MakeShape(U32, {}), ShapeUtil::MakeTokenShape()})\n            .ToProto();\n    recv_instr.set_channel_id(handle.handle());\n    recv_instr.set_is_host_transfer(true);\n    TF_ASSIGN_OR_RETURN(XlaOp recv, AddInstruction(std::move(recv_instr),\n                                                   HloOpcode::kRecv, {token}));\n\n    HloInstructionProto recv_done_instr;\n    *recv_done_instr.mutable_shape() =\n        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()})\n            .ToProto();\n    recv_done_instr.set_channel_id(handle.handle());\n    recv_done_instr.set_is_host_transfer(true);\n    return AddInstruction(std::move(recv_done_instr), HloOpcode::kRecvDone,\n                          {recv});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::RecvFromHost(XlaOp token, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 4141,
          "old_api": "LayoutUtil::HasLayout(shape)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(shape)",
          "new_text": null,
          "old_line_content": "    if (!LayoutUtil::HasLayout(shape)) {",
          "new_line_content": "                               const ChannelHandle& handle) {",
          "content_same": false
        },
        {
          "line": 2096,
          "old_api": "kernel_spatial_dimensions",
          "new_api": null,
          "old_text": "dimension_numbers.kernel_spatial_dimensions(i)",
          "new_text": null,
          "old_line_content": "        rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));",
          "new_line_content": "       ++i) {",
          "content_same": false
        },
        {
          "line": 4146,
          "old_api": "IsArray",
          "new_api": null,
          "old_text": "shape.IsArray()",
          "new_text": null,
          "old_line_content": "    if (!shape.IsArray()) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4147,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"RecvFromHost only supports array shapes, shape: %s\",\n          ShapeUtil::HumanString(shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    // TODO(b/111544877): Support tuple shapes.",
          "content_same": false
        },
        {
          "line": 4152,
          "old_api": "type",
          "new_api": null,
          "old_text": "handle.type()",
          "new_text": null,
          "old_line_content": "    if (handle.type() != ChannelHandle::HOST_TO_DEVICE) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4153,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\"RecvFromHost must use a host-to-device channel\")",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"RecvFromHost must use a host-to-device channel\");",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2109,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4159,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "recv_instr.mutable_shape()",
          "new_text": null,
          "old_line_content": "    *recv_instr.mutable_shape() =",
          "new_line_content": "    // token}.",
          "content_same": false
        },
        {
          "line": 2112,
          "old_api": "mutable_convolution_dimension_numbers",
          "new_api": null,
          "old_text": "instr.mutable_convolution_dimension_numbers()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_convolution_dimension_numbers() = dimension_numbers;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4160,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTupleShape(\n            {shape, ShapeUtil::MakeShape(U32, {}), ShapeUtil::MakeTokenShape()})\n            .ToProto()",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeTupleShape(",
          "new_line_content": "    HloInstructionProto recv_instr;",
          "content_same": false
        },
        {
          "line": 4164,
          "old_api": "set_is_host_transfer",
          "new_api": null,
          "old_text": "recv_instr.set_is_host_transfer(true)",
          "new_text": null,
          "old_line_content": "    recv_instr.set_is_host_transfer(true);",
          "new_line_content": "            .ToProto();",
          "content_same": false
        },
        {
          "line": 2118,
          "old_api": "mutable_precision_config",
          "new_api": null,
          "old_text": "instr.mutable_precision_config()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_precision_config() = *precision_config;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4169,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "recv_done_instr.mutable_shape()",
          "new_text": null,
          "old_line_content": "    *recv_done_instr.mutable_shape() =",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4170,
          "old_api": "ShapeUtil::MakeTokenShape()",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape()",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()})",
          "new_line_content": "    HloInstructionProto recv_done_instr;",
          "content_same": false
        },
        {
          "line": 4173,
          "old_api": "set_is_host_transfer",
          "new_api": null,
          "old_text": "recv_done_instr.set_is_host_transfer(true)",
          "new_text": null,
          "old_line_content": "    recv_done_instr.set_is_host_transfer(true);",
          "new_line_content": "            .ToProto();",
          "content_same": false
        },
        {
          "line": 4180,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferGetDimensionSizeShape(\n                                         *operand_shape, dimension));\n    // Calling GetDimensionSize on a static dimension returns a constant\n    // instruction.\n    if (operand_shape->is_static_dimension(dimension)) {\n      return ConstantR0<int32_t>(this, operand_shape->dimensions(dimension));\n    }\n    *instr.mutable_shape() = shape.ToProto();\n    instr.add_dimensions(dimension);\n    return AddInstruction(std::move(instr), HloOpcode::kGetDimensionSize,\n                          {operand});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2133,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(\n        HloInstructionProto instr,\n        DynamicConvInstruction(\n            lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\n            dimension_numbers, feature_group_count, batch_group_count,\n            precision_config, padding_type, preferred_element_type));\n\n    instr.set_custom_call_target(\"DynamicConvolutionInputGrad\");\n\n    return AddInstruction(std::move(instr), HloOpcode::kCustomCall,\n                          {input_sizes, lhs, rhs});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const PrecisionConfig* precision_config, PaddingType padding_type,",
          "content_same": false
        },
        {
          "line": 4187,
          "old_api": "is_static_dimension",
          "new_api": null,
          "old_text": "operand_shape->is_static_dimension(dimension)",
          "new_text": null,
          "old_line_content": "    if (operand_shape->is_static_dimension(dimension)) {",
          "new_line_content": "    // Calling GetDimensionSize on a static dimension returns a constant",
          "content_same": false
        },
        {
          "line": 4188,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "operand_shape->dimensions(dimension)",
          "new_text": null,
          "old_line_content": "      return ConstantR0<int32_t>(this, operand_shape->dimensions(dimension));",
          "new_line_content": "    // instruction.",
          "content_same": false
        },
        {
          "line": 2141,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "instr.set_custom_call_target(\"DynamicConvolutionInputGrad\")",
          "new_text": null,
          "old_line_content": "    instr.set_custom_call_target(\"DynamicConvolutionInputGrad\");",
          "new_line_content": "            precision_config, padding_type, preferred_element_type));",
          "content_same": false
        },
        {
          "line": 4191,
          "old_api": "add_dimensions",
          "new_api": null,
          "old_text": "instr.add_dimensions(dimension)",
          "new_text": null,
          "old_line_content": "    instr.add_dimensions(dimension);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4198,
          "old_api": "set_dynamic_dimension",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n\n    Shape shape = *operand_shape;\n    shape.set_dynamic_dimension(dimension, false);\n    // Setting an op's dynamic dimension to its static size removes the dynamic\n    // dimension.\n    XlaOp static_size =\n        ConstantR0<int32_t>(this, operand_shape->dimensions(dimension));\n    return SetDimensionSizeInternal(shape, operand, static_size, dimension);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4202,
          "old_api": "set_dynamic_dimension",
          "new_api": null,
          "old_text": "shape.set_dynamic_dimension(dimension, false)",
          "new_text": null,
          "old_line_content": "    shape.set_dynamic_dimension(dimension, false);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2158,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(\n        HloInstructionProto instr,\n        DynamicConvInstruction(activations, gradients, window_strides, padding,\n                               lhs_dilation, rhs_dilation, dimension_numbers,\n                               feature_group_count, batch_group_count,\n                               precision_config, padding_type,\n                               preferred_element_type));\n\n    instr.set_custom_call_target(\"DynamicConvolutionKernelGrad\");\n    // The gradient of kernel has kernel shape and shouldn't have any dynamic\n    // sizes.\n    instr.mutable_shape()->clear_is_dynamic_dimension();\n    return AddInstruction(std::move(instr), HloOpcode::kCustomCall,\n                          {activations, gradients});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const PrecisionConfig* precision_config, PaddingType padding_type,",
          "content_same": false
        },
        {
          "line": 4206,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "operand_shape->dimensions(dimension)",
          "new_text": null,
          "old_line_content": "        ConstantR0<int32_t>(this, operand_shape->dimensions(dimension));",
          "new_line_content": "    // dimension.",
          "content_same": false
        },
        {
          "line": 4207,
          "old_api": "SetDimensionSizeInternal",
          "new_api": null,
          "old_text": "SetDimensionSizeInternal(shape, operand, static_size, dimension)",
          "new_text": null,
          "old_line_content": "    return SetDimensionSizeInternal(shape, operand, static_size, dimension);",
          "new_line_content": "    XlaOp static_size =",
          "content_same": false
        },
        {
          "line": 4213,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* val_shape, GetShapePtr(val));\n\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferSetDimensionSizeShape(\n                            *operand_shape, *val_shape, dimension));\n    return SetDimensionSizeInternal(shape, operand, val, dimension);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::SetDimensionSize(XlaOp operand, XlaOp val,",
          "content_same": false
        },
        {
          "line": 2167,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "instr.set_custom_call_target(\"DynamicConvolutionKernelGrad\")",
          "new_text": null,
          "old_line_content": "    instr.set_custom_call_target(\"DynamicConvolutionKernelGrad\");",
          "new_line_content": "                               preferred_element_type));",
          "content_same": false
        },
        {
          "line": 2170,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "instr.mutable_shape()->clear_is_dynamic_dimension()",
          "new_text": null,
          "old_line_content": "    instr.mutable_shape()->clear_is_dynamic_dimension();",
          "new_line_content": "    // The gradient of kernel has kernel shape and shouldn't have any dynamic",
          "content_same": false
        },
        {
          "line": 2171,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCustomCall,",
          "new_line_content": "    // sizes.",
          "content_same": false
        },
        {
          "line": 4220,
          "old_api": "SetDimensionSizeInternal",
          "new_api": null,
          "old_text": "SetDimensionSizeInternal(shape, operand, val, dimension)",
          "new_text": null,
          "old_line_content": "    return SetDimensionSizeInternal(shape, operand, val, dimension);",
          "new_line_content": "                        ShapeInference::InferSetDimensionSizeShape(",
          "content_same": false
        },
        {
          "line": 2185,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(\n        HloInstructionProto instr,\n        DynamicConvInstruction(\n            lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\n            dimension_numbers, feature_group_count, batch_group_count,\n            precision_config, padding_type, preferred_element_type));\n    instr.set_custom_call_target(\"DynamicConvolutionForward\");\n\n    return AddInstruction(std::move(instr), HloOpcode::kCustomCall, {lhs, rhs});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const PrecisionConfig* precision_config, PaddingType padding_type,",
          "content_same": false
        },
        {
          "line": 4237,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "  // adding a separate RemoveDynamicDimension opcode.",
          "content_same": false
        },
        {
          "line": 4238,
          "old_api": "add_dimensions",
          "new_api": null,
          "old_text": "instr.add_dimensions(dimension)",
          "new_text": null,
          "old_line_content": "  instr.add_dimensions(dimension);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 2192,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "instr.set_custom_call_target(\"DynamicConvolutionForward\")",
          "new_text": null,
          "old_line_content": "    instr.set_custom_call_target(\"DynamicConvolutionForward\");",
          "new_line_content": "            dimension_numbers, feature_group_count, batch_group_count,",
          "content_same": false
        },
        {
          "line": 4244,
          "old_api": "TF_RETURN_IF_ERROR",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(first_error_)",
          "new_text": null,
          "old_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4247,
          "old_api": "status",
          "new_api": null,
          "old_text": "LookUpInstruction(operand).status()",
          "new_text": null,
          "old_line_content": "  TF_RETURN_IF_ERROR(LookUpInstruction(operand).status());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4251,
          "old_api": "handle",
          "new_api": null,
          "old_text": "operand.handle()",
          "new_text": null,
          "old_line_content": "  IsConstantVisitor(operand.handle(), /*depth=*/0, &visited, &is_constant);",
          "new_line_content": "  bool is_constant = true;",
          "content_same": false
        },
        {
          "line": 2208,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    const PrecisionConfig* precision_config) {",
          "content_same": false
        },
        {
          "line": 2211,
          "old_api": "mutable_convolution_dimension_numbers",
          "new_api": null,
          "old_text": "instr.mutable_convolution_dimension_numbers()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_convolution_dimension_numbers() = dimension_numbers;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4259,
          "old_api": "LookUpInstruction",
          "new_api": null,
          "old_text": "LookUpInstruction(root_op)",
          "new_text": null,
          "old_line_content": "    auto op_status = LookUpInstruction(root_op);",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(bool is_constant, IsConstant(root_op));",
          "content_same": false
        },
        {
          "line": 4262,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n        \"Operand to BuildConstantSubGraph depends on a parameter.\\n\\n\"\n        \"  op requested for constant subgraph: %s\\n\\n\"\n        \"This is an internal error that typically happens when the XLA user \"\n        \"(e.g. TensorFlow) is attempting to determine a value that must be a \"\n        \"compile-time constant (e.g. an array dimension) but it is not capable \"\n        \"of being evaluated at XLA compile time.\\n\\n\"\n        \"Please file a usability bug with the framework being used (e.g. \"\n        \"TensorFlow).\",\n        op_string)",
          "new_text": null,
          "old_line_content": "    return InvalidArgument(",
          "new_line_content": "    std::string op_string =",
          "content_same": false
        },
        {
          "line": 2216,
          "old_api": "mutable_precision_config",
          "new_api": null,
          "old_text": "instr.mutable_precision_config()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_precision_config() = *precision_config;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2219,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kConvolution, {lhs, rhs});",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 2224,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferFftShape(\n                                         *operand_shape, fft_type, fft_length));\n    return FftInternal(shape, operand, fft_type, fft_length);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::Fft(XlaOp operand, const FftType fft_type,",
          "content_same": false
        },
        {
          "line": 2228,
          "old_api": "FftInternal",
          "new_api": null,
          "old_text": "FftInternal(shape, operand, fft_type, fft_length)",
          "new_text": null,
          "old_line_content": "    return FftInternal(shape, operand, fft_type, fft_length);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferFftShape(",
          "content_same": false
        },
        {
          "line": 4276,
          "old_api": "VLOG_IS_ON",
          "new_api": null,
          "old_text": "VLOG_IS_ON(4)",
          "new_text": null,
          "old_line_content": "  if (VLOG_IS_ON(4)) {",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(const HloInstructionProto* root,",
          "content_same": false
        },
        {
          "line": 4277,
          "old_api": "OpToString",
          "new_api": null,
          "old_text": "OpToString(root_op)",
          "new_text": null,
          "old_line_content": "    VLOG(4) << \"Build constant subgraph for:\\n\" << OpToString(root_op);",
          "new_line_content": "                      LookUpInstruction(root_op));",
          "content_same": false
        },
        {
          "line": 4281,
          "old_api": "StrCat",
          "new_api": null,
          "old_text": "StrCat(name_, \"_compute_constant\")",
          "new_text": null,
          "old_line_content": "  SetProtoIdAndName(&entry, StrCat(name_, \"_compute_constant\"), kNameSeparator,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4282,
          "old_api": "GetNextId",
          "new_api": null,
          "old_text": "GetNextId()",
          "new_text": null,
          "old_line_content": "                    GetNextId());",
          "new_line_content": "  HloComputationProto entry;",
          "content_same": false
        },
        {
          "line": 2236,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    const absl::Span<const int64_t> fft_length) {",
          "content_same": false
        },
        {
          "line": 2237,
          "old_api": "set_fft_type",
          "new_api": null,
          "old_text": "instr.set_fft_type(fft_type)",
          "new_text": null,
          "old_line_content": "  instr.set_fft_type(fft_type);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 2242,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kFft, {operand});",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4293,
          "old_api": "id",
          "new_api": null,
          "old_text": "root->id()",
          "new_text": null,
          "old_line_content": "  worklist.push(root->id());",
          "new_line_content": "  absl::flat_hash_set<int64_t> related_calls;  // Related computations.",
          "content_same": false
        },
        {
          "line": 4294,
          "old_api": "id",
          "new_api": null,
          "old_text": "root->id()",
          "new_text": null,
          "old_line_content": "  related_ops.insert(root->id());",
          "new_line_content": "  std::queue<int64_t> worklist;",
          "content_same": false
        },
        {
          "line": 2248,
          "old_api": "std::move(options)",
          "new_api": null,
          "old_text": "std::move(options)",
          "new_text": null,
          "old_line_content": "  *instr.mutable_triangular_solve_options() = std::move(options);",
          "new_line_content": "    const Shape& shape, XlaOp a, XlaOp b, TriangularSolveOptions options) {",
          "content_same": false
        },
        {
          "line": 2249,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4297,
          "old_api": "front",
          "new_api": null,
          "old_text": "worklist.front()",
          "new_text": null,
          "old_line_content": "    int64_t handle = worklist.front();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4304,
          "old_api": "operand_ids",
          "new_api": null,
          "old_text": "instr_proto->operand_ids()",
          "new_text": null,
          "old_line_content": "      for (int64_t id : instr_proto->operand_ids()) {",
          "new_line_content": "    auto default_behavior = [&related_ops, &worklist, &related_calls,",
          "content_same": false
        },
        {
          "line": 2257,
          "old_api": "mutable_cholesky_options",
          "new_api": null,
          "old_text": "instr.mutable_cholesky_options()",
          "new_text": null,
          "old_line_content": "  CholeskyOptions& options = *instr.mutable_cholesky_options();",
          "new_line_content": "                                             bool lower) {",
          "content_same": false
        },
        {
          "line": 2258,
          "old_api": "set_lower",
          "new_api": null,
          "old_text": "options.set_lower(lower)",
          "new_text": null,
          "old_line_content": "  options.set_lower(lower);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4305,
          "old_api": "insert",
          "new_api": null,
          "old_text": "related_ops.insert(id)",
          "new_text": null,
          "old_line_content": "        if (related_ops.insert(id).second) {",
          "new_line_content": "                             instr_proto]() {",
          "content_same": false
        },
        {
          "line": 4309,
          "old_api": "called_computation_ids",
          "new_api": null,
          "old_text": "instr_proto->called_computation_ids()",
          "new_text": null,
          "old_line_content": "      for (int64_t called_id : instr_proto->called_computation_ids()) {",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 4310,
          "old_api": "insert",
          "new_api": null,
          "old_text": "related_calls.insert(called_id)",
          "new_text": null,
          "old_line_content": "        related_calls.insert(called_id);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 2265,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    if (!LayoutUtil::HasLayout(shape)) {\n      return InvalidArgument(\"Given shape to Infeed must have a layout\");\n    }\n    const Shape infeed_instruction_shape =\n        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()});\n    *instr.mutable_shape() = infeed_instruction_shape.ToProto();\n    instr.set_infeed_config(config);\n\n    if (shape.IsArray() && sharding() &&\n        sharding()->type() == OpSharding::OTHER) {\n      // TODO(b/110793772): Support tiled array-shaped infeeds.\n      return InvalidArgument(\n          \"Tiled sharding is not yet supported for array-shaped infeeds\");\n    }\n\n    if (sharding() && sharding()->type() == OpSharding::REPLICATED) {\n      return InvalidArgument(\n          \"Replicated sharding is not yet supported for infeeds\");\n    }\n\n    // Infeed takes a single token operand. Generate the token to pass to the\n    // infeed.\n    XlaOp token;\n    auto make_token = [&]() {\n      HloInstructionProto token_instr;\n      *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n      return AddInstruction(std::move(token_instr), HloOpcode::kAfterAll, {});\n    };\n    if (sharding()) {\n      // Arbitrarily assign token to device 0.\n      OpSharding sharding = sharding_builder::AssignDevice(0);\n      XlaScopedShardingAssignment scoped_sharding(this, sharding);\n      TF_ASSIGN_OR_RETURN(token, make_token());\n    } else {\n      TF_ASSIGN_OR_RETURN(token, make_token());\n    }\n\n    // The sharding is set by the client according to the data tuple shape.\n    // However, the shape of the infeed instruction is a tuple containing the\n    // data and a token. For tuple sharding type, the sharding must be changed\n    // to accommodate the token.\n    XlaOp infeed;\n    if (sharding() && sharding()->type() == OpSharding::TUPLE) {\n      // TODO(b/80000000): Remove this when clients have been updated to handle\n      // tokens.\n      OpSharding infeed_instruction_sharding = *sharding();\n      // Arbitrarily assign the token to device 0.\n      *infeed_instruction_sharding.add_tuple_shardings() =\n          sharding_builder::AssignDevice(0);\n      XlaScopedShardingAssignment scoped_sharding(this,\n                                                  infeed_instruction_sharding);\n      TF_ASSIGN_OR_RETURN(infeed, AddInstruction(std::move(instr),\n                                                 HloOpcode::kInfeed, {token}));\n    } else {\n      TF_ASSIGN_OR_RETURN(infeed, AddInstruction(std::move(instr),\n                                                 HloOpcode::kInfeed, {token}));\n    }\n\n    // The infeed instruction produces a tuple of the infed data and a token\n    // type. Return XLA op containing the data.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto infeed_data;\n    *infeed_data.mutable_shape() = shape.ToProto();\n    infeed_data.set_tuple_index(0);\n    return AddInstruction(std::move(infeed_data), HloOpcode::kGetTupleElement,\n                          {infeed});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4314,
          "old_api": "opcode",
          "new_api": null,
          "old_text": "instr_proto->opcode()",
          "new_text": null,
          "old_line_content": "    if (instr_proto->opcode() ==",
          "new_line_content": "    };",
          "content_same": false
        },
        {
          "line": 4315,
          "old_api": "HloOpcodeString",
          "new_api": null,
          "old_text": "HloOpcodeString(HloOpcode::kGetDimensionSize)",
          "new_text": null,
          "old_line_content": "            HloOpcodeString(HloOpcode::kGetDimensionSize) ||",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2268,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\"Given shape to Infeed must have a layout\")",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"Given shape to Infeed must have a layout\");",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 2271,
          "old_api": "ShapeUtil::MakeTokenShape()",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape()",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()});",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2272,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "infeed_instruction_shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = infeed_instruction_shape.ToProto();",
          "new_line_content": "    const Shape infeed_instruction_shape =",
          "content_same": false
        },
        {
          "line": 4320,
          "old_api": "opcode",
          "new_api": null,
          "old_text": "instr_proto->opcode()",
          "new_text": null,
          "old_line_content": "      if (instr_proto->opcode() ==",
          "new_line_content": "      HloInstructionProto const_instr;",
          "content_same": false
        },
        {
          "line": 4321,
          "old_api": "HloOpcodeString",
          "new_api": null,
          "old_text": "HloOpcodeString(HloOpcode::kGetDimensionSize)",
          "new_text": null,
          "old_line_content": "          HloOpcodeString(HloOpcode::kGetDimensionSize)) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2276,
          "old_api": "type",
          "new_api": null,
          "old_text": "sharding()->type()",
          "new_text": null,
          "old_line_content": "        sharding()->type() == OpSharding::OTHER) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4328,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "instr_proto->dimensions(0)",
          "new_text": null,
          "old_line_content": "        int64_t dimension = instr_proto->dimensions(0);",
          "new_line_content": "        // Replace GetDimensionSize with a Constant representing the static",
          "content_same": false
        },
        {
          "line": 4329,
          "old_api": "operand_ids",
          "new_api": null,
          "old_text": "instr_proto->operand_ids(0)",
          "new_text": null,
          "old_line_content": "        int64_t operand_handle = instr_proto->operand_ids(0);",
          "new_line_content": "        // bound of the shape.",
          "content_same": false
        },
        {
          "line": 2282,
          "old_api": "type",
          "new_api": null,
          "old_text": "sharding()->type()",
          "new_text": null,
          "old_line_content": "    if (sharding() && sharding()->type() == OpSharding::REPLICATED) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2283,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Replicated sharding is not yet supported for infeeds\")",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4333,
          "old_api": "is_dynamic_dimension",
          "new_api": null,
          "old_text": "operand_proto->shape().is_dynamic_dimension(dimension)",
          "new_text": null,
          "old_line_content": "        if (!(operand_proto->shape().is_dynamic_dimension(dimension) &&",
          "new_line_content": "                            LookUpInstructionByHandle(operand_handle));",
          "content_same": false
        },
        {
          "line": 4336,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "operand_proto->shape().dimensions(dimension)",
          "new_text": null,
          "old_line_content": "              operand_proto->shape().dimensions(dimension));",
          "new_line_content": "              dynamic_dimension_is_minus_one)) {",
          "content_same": false
        },
        {
          "line": 4339,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "literal.ToProto()",
          "new_text": null,
          "old_line_content": "        *const_instr.mutable_literal() = literal.ToProto();",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 2292,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": null,
          "old_line_content": "      *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "    auto make_token = [&]() {",
          "content_same": false
        },
        {
          "line": 2293,
          "old_api": "std::move(token_instr)",
          "new_api": null,
          "old_text": "std::move(token_instr)",
          "new_text": null,
          "old_line_content": "      return AddInstruction(std::move(token_instr), HloOpcode::kAfterAll, {});",
          "new_line_content": "      HloInstructionProto token_instr;",
          "content_same": false
        },
        {
          "line": 4343,
          "old_api": "mutable_literal",
          "new_api": null,
          "old_text": "const_instr.mutable_literal()",
          "new_text": null,
          "old_line_content": "          *const_instr.mutable_literal() =",
          "new_line_content": "      } else {",
          "content_same": false
        },
        {
          "line": 4346,
          "old_api": "tuple_literals",
          "new_api": null,
          "old_text": "instr_proto->literal().tuple_literals(0)",
          "new_text": null,
          "old_line_content": "              instr_proto->literal().tuple_literals(0);",
          "new_line_content": "              // First literal of SetBound contains bounds, second literal",
          "content_same": false
        },
        {
          "line": 4351,
          "old_api": "shape",
          "new_api": null,
          "old_text": "instr_proto->shape()",
          "new_text": null,
          "old_line_content": "        *const_instr.mutable_shape() = instr_proto->shape();",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 4354,
          "old_api": "HloOpcodeString",
          "new_api": null,
          "old_text": "HloOpcodeString(HloOpcode::kConstant)",
          "new_text": null,
          "old_line_content": "          std::string(HloOpcodeString(HloOpcode::kConstant));",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 2309,
          "old_api": "type",
          "new_api": null,
          "old_text": "sharding()->type()",
          "new_text": null,
          "old_line_content": "    if (sharding() && sharding()->type() == OpSharding::TUPLE) {",
          "new_line_content": "    // to accommodate the token.",
          "content_same": false
        },
        {
          "line": 2312,
          "old_api": "sharding",
          "new_api": null,
          "old_text": "sharding()",
          "new_text": null,
          "old_line_content": "      OpSharding infeed_instruction_sharding = *sharding();",
          "new_line_content": "      // TODO(b/80000000): Remove this when clients have been updated to handle",
          "content_same": false
        },
        {
          "line": 4361,
          "old_api": "opcode",
          "new_api": null,
          "old_text": "instr_proto->opcode()",
          "new_text": null,
          "old_line_content": "    } else if (instr_proto->opcode() ==",
          "new_line_content": "          const_instr;  // Add to the result constant graph.",
          "content_same": false
        },
        {
          "line": 4362,
          "old_api": "HloOpcodeString",
          "new_api": null,
          "old_text": "HloOpcodeString(HloOpcode::kGetTupleElement)",
          "new_text": null,
          "old_line_content": "               HloOpcodeString(HloOpcode::kGetTupleElement)) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2315,
          "old_api": "sharding_builder::AssignDevice(0)",
          "new_api": null,
          "old_text": "sharding_builder::AssignDevice(0)",
          "new_text": null,
          "old_line_content": "          sharding_builder::AssignDevice(0);",
          "new_line_content": "      // Arbitrarily assign the token to device 0.",
          "content_same": false
        },
        {
          "line": 2318,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "      TF_ASSIGN_OR_RETURN(infeed, AddInstruction(std::move(instr),",
          "new_line_content": "      XlaScopedShardingAssignment scoped_sharding(this,",
          "content_same": false
        },
        {
          "line": 4368,
          "old_api": "HloOpcodeString",
          "new_api": null,
          "old_text": "HloOpcodeString(HloOpcode::kTuple)",
          "new_text": null,
          "old_line_content": "      if (maybe_tuple_instr->opcode() == HloOpcodeString(HloOpcode::kTuple)) {",
          "new_line_content": "          LookUpInstructionByHandle(instr_proto->operand_ids(0)));",
          "content_same": false
        },
        {
          "line": 2321,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "      TF_ASSIGN_OR_RETURN(infeed, AddInstruction(std::move(instr),",
          "new_line_content": "                                                 HloOpcode::kInfeed, {token}));",
          "content_same": false
        },
        {
          "line": 4369,
          "old_api": "tuple_index",
          "new_api": null,
          "old_text": "instr_proto->tuple_index()",
          "new_text": null,
          "old_line_content": "        int64_t id = maybe_tuple_instr->operand_ids(instr_proto->tuple_index());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4372,
          "old_api": "push",
          "new_api": null,
          "old_text": "worklist.push(id)",
          "new_text": null,
          "old_line_content": "          worklist.push(id);",
          "new_line_content": "        // Enqueue any dependencies of `id`.",
          "content_same": false
        },
        {
          "line": 4377,
          "old_api": "default_behavior",
          "new_api": null,
          "old_text": "default_behavior()",
          "new_text": null,
          "old_line_content": "        default_behavior();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2330,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *infeed_data.mutable_shape() = shape.ToProto();",
          "new_line_content": "    // tokens.",
          "content_same": false
        },
        {
          "line": 2331,
          "old_api": "set_tuple_index",
          "new_api": null,
          "old_text": "infeed_data.set_tuple_index(0)",
          "new_text": null,
          "old_line_content": "    infeed_data.set_tuple_index(0);",
          "new_line_content": "    HloInstructionProto infeed_data;",
          "content_same": false
        },
        {
          "line": 4381,
          "old_api": "default_behavior",
          "new_api": null,
          "old_text": "default_behavior()",
          "new_text": null,
          "old_line_content": "      default_behavior();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4386,
          "old_api": "id",
          "new_api": null,
          "old_text": "root->id()",
          "new_text": null,
          "old_line_content": "  int64_t root_id = root->id();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2339,
          "old_api": "IsArray",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!LayoutUtil::HasLayout(shape)) {\n      return InvalidArgument(\"Given shape to Infeed must have a layout\");\n    }\n    const Shape infeed_instruction_shape =\n        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()});\n\n    if (shape.IsArray() && sharding() &&\n        sharding()->type() == OpSharding::OTHER) {\n      // TODO(b/110793772): Support tiled array-shaped infeeds.\n      return InvalidArgument(\n          \"Tiled sharding is not yet supported for array-shaped infeeds\");\n    }\n\n    if (sharding() && sharding()->type() == OpSharding::REPLICATED) {\n      return InvalidArgument(\n          \"Replicated sharding is not yet supported for infeeds\");\n    }\n    return InfeedWithTokenInternal(infeed_instruction_shape, token, config);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::InfeedWithToken(XlaOp token, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 2340,
          "old_api": "LayoutUtil::HasLayout(shape)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(shape)",
          "new_text": null,
          "old_line_content": "    if (!LayoutUtil::HasLayout(shape)) {",
          "new_line_content": "                                  const std::string& config) {",
          "content_same": false
        },
        {
          "line": 4387,
          "old_api": "find",
          "new_api": null,
          "old_text": "substitutions.find(root_id)",
          "new_text": null,
          "old_line_content": "  auto it = substitutions.find(root_id);",
          "new_line_content": "  // Resolve any substitutions for the root id.",
          "content_same": false
        },
        {
          "line": 2344,
          "old_api": "ShapeUtil::MakeTokenShape()",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape()",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeTupleShape({shape, ShapeUtil::MakeTokenShape()});",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2347,
          "old_api": "type",
          "new_api": null,
          "old_text": "sharding()->type()",
          "new_text": null,
          "old_line_content": "        sharding()->type() == OpSharding::OTHER) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4396,
          "old_api": "end",
          "new_api": null,
          "old_text": "substitutions.end()",
          "new_text": null,
          "old_line_content": "    if (substitutions.find(id) != substitutions.end()) {",
          "new_line_content": "  // Add related ops to the computation.",
          "content_same": false
        },
        {
          "line": 2353,
          "old_api": "type",
          "new_api": null,
          "old_text": "sharding()->type()",
          "new_text": null,
          "old_line_content": "    if (sharding() && sharding()->type() == OpSharding::REPLICATED) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2354,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Replicated sharding is not yet supported for infeeds\")",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4404,
          "old_api": "HloOpcodeString",
          "new_api": null,
          "old_text": "HloOpcodeString(HloOpcode::kGetDimensionSize)",
          "new_text": null,
          "old_line_content": "    if (instr_src->opcode() == HloOpcodeString(HloOpcode::kGetDimensionSize) ||",
          "new_line_content": "                        LookUpInstructionByHandle(id));",
          "content_same": false
        },
        {
          "line": 2357,
          "old_api": "InfeedWithTokenInternal",
          "new_api": null,
          "old_text": "InfeedWithTokenInternal(infeed_instruction_shape, token, config)",
          "new_text": null,
          "old_line_content": "    return InfeedWithTokenInternal(infeed_instruction_shape, token, config);",
          "new_line_content": "          \"Replicated sharding is not yet supported for infeeds\");",
          "content_same": false
        },
        {
          "line": 4405,
          "old_api": "InstrIsSetBound",
          "new_api": null,
          "old_text": "InstrIsSetBound(instr_src)",
          "new_text": null,
          "old_line_content": "        InstrIsSetBound(instr_src)) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4408,
          "old_api": "add_instructions",
          "new_api": null,
          "old_text": "entry.add_instructions()",
          "new_text": null,
          "old_line_content": "    HloInstructionProto* instr = entry.add_instructions();",
          "new_line_content": "      continue;",
          "content_same": false
        },
        {
          "line": 4411,
          "old_api": "clear_operand_ids",
          "new_api": null,
          "old_text": "instr->clear_operand_ids()",
          "new_text": null,
          "old_line_content": "    instr->clear_operand_ids();",
          "new_line_content": "    *instr = *instr_src;",
          "content_same": false
        },
        {
          "line": 4412,
          "old_api": "operand_ids",
          "new_api": null,
          "old_text": "instr_src->operand_ids()",
          "new_text": null,
          "old_line_content": "    for (int64_t operand_id : instr_src->operand_ids()) {",
          "new_line_content": "    // Replace operands in case we have substitutions mapped.",
          "content_same": false
        },
        {
          "line": 2365,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "infeed_instruction_shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = infeed_instruction_shape.ToProto();",
          "new_line_content": "    const std::string& config) {",
          "content_same": false
        },
        {
          "line": 2366,
          "old_api": "set_infeed_config",
          "new_api": null,
          "old_text": "instr.set_infeed_config(config)",
          "new_text": null,
          "old_line_content": "  instr.set_infeed_config(config);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 2372,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n\n    // Check and set outfeed shape.\n    if (!LayoutUtil::HasLayout(shape_with_layout)) {\n      return InvalidArgument(\"Given shape to Outfeed must have a layout\");\n    }\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {\n      return InvalidArgument(\n          \"Outfeed shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape));\n    }\n    *instr.mutable_outfeed_shape() = shape_with_layout.ToProto();\n\n    instr.set_outfeed_config(outfeed_config);\n\n    // Outfeed takes a token as its second operand. Generate the token to pass\n    // to the outfeed.\n    XlaOp token;\n    auto make_token = [&]() {\n      HloInstructionProto token_instr;\n      *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n      return AddInstruction(std::move(token_instr), HloOpcode::kAfterAll, {});\n    };\n    auto make_outfeed = [&](XlaOp token) {\n      return AddInstruction(std::move(instr), HloOpcode::kOutfeed,\n                            {operand, token});\n    };\n    if (sharding()) {\n      XlaScopedShardingAssignment scoped_sharding(\n          this, sharding_builder::AssignDevice(0));\n      TF_ASSIGN_OR_RETURN(token, make_token());\n    } else {\n      TF_ASSIGN_OR_RETURN(token, make_token());\n    }\n    if (sharding()) {\n      OpSharding tuple_sharding = *sharding();\n      if (tuple_sharding.type() != OpSharding::TUPLE) {\n        tuple_sharding = sharding_builder::Tuple({});\n        *tuple_sharding.add_tuple_shardings() = *sharding();\n      }\n      *tuple_sharding.add_tuple_shardings() = sharding_builder::AssignDevice(0);\n      XlaScopedShardingAssignment scoped_sharding(this, tuple_sharding);\n      TF_RETURN_IF_ERROR(make_outfeed(token).status());\n    } else {\n      TF_RETURN_IF_ERROR(make_outfeed(token).status());\n    }\n    // The outfeed instruction produces a token. However, existing users expect\n    // a nil shape (empty tuple). This should only be relevant if the outfeed is\n    // the root of a computation.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto tuple_instr;\n    *tuple_instr.mutable_shape() = ShapeUtil::MakeNil().ToProto();\n\n    // The dummy tuple should have no sharding.\n    {\n      XlaScopedShardingAssignment scoped_sharding(this, std::nullopt);\n      TF_ASSIGN_OR_RETURN(\n          XlaOp empty_tuple,\n          AddInstruction(std::move(tuple_instr), HloOpcode::kTuple, {}));\n      return empty_tuple;\n    }\n  })",
          "new_text": null,
          "old_line_content": "  ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "void XlaBuilder::Outfeed(XlaOp operand, const Shape& shape_with_layout,",
          "content_same": false
        },
        {
          "line": 4422,
          "old_api": "id",
          "new_api": null,
          "old_text": "instr->id()",
          "new_text": null,
          "old_line_content": "        StrCat(instr->name(), \".\", entry.id(), \".\", instr->id());",
          "new_line_content": "    // Ensures that the instruction names are unique among the graph.",
          "content_same": false
        },
        {
          "line": 2375,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4423,
          "old_api": "set_name",
          "new_api": null,
          "old_text": "instr->set_name(new_name)",
          "new_text": null,
          "old_line_content": "    instr->set_name(new_name);",
          "new_line_content": "    const std::string& new_name =",
          "content_same": false
        },
        {
          "line": 2378,
          "old_api": "LayoutUtil::HasLayout(shape_with_layout)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(shape_with_layout)",
          "new_text": null,
          "old_line_content": "    if (!LayoutUtil::HasLayout(shape_with_layout)) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2379,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\"Given shape to Outfeed must have a layout\")",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"Given shape to Outfeed must have a layout\");",
          "new_line_content": "    // Check and set outfeed shape.",
          "content_same": false
        },
        {
          "line": 4426,
          "old_api": "id",
          "new_api": null,
          "old_text": "entry.id()",
          "new_text": null,
          "old_line_content": "  XlaComputation computation(entry.id());",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4427,
          "old_api": "mutable_proto",
          "new_api": null,
          "old_text": "computation.mutable_proto()",
          "new_text": null,
          "old_line_content": "  HloModuleProto* module = computation.mutable_proto();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2382,
          "old_api": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "new_api": null,
          "old_text": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "new_text": null,
          "old_line_content": "    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2383,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Outfeed shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 2386,
          "old_api": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "new_text": null,
          "old_line_content": "          ShapeUtil::HumanStringWithLayout(*operand_shape));",
          "new_line_content": "          \"Outfeed shape %s must be compatible with operand shape %s\",",
          "content_same": false
        },
        {
          "line": 4435,
          "old_api": "add_computations",
          "new_api": null,
          "old_text": "module->add_computations()",
          "new_text": null,
          "old_line_content": "      *module->add_computations() = e.second;",
          "new_line_content": "  for (auto& e : embedded_) {",
          "content_same": false
        },
        {
          "line": 4438,
          "old_api": "std::move(entry)",
          "new_api": null,
          "old_text": "std::move(entry)",
          "new_text": null,
          "old_line_content": "  *module->add_computations() = std::move(entry);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4439,
          "old_api": "VLOG_IS_ON",
          "new_api": null,
          "old_text": "VLOG_IS_ON(4)",
          "new_text": null,
          "old_line_content": "  if (VLOG_IS_ON(4)) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 2397,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": null,
          "old_line_content": "      *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "    auto make_token = [&]() {",
          "content_same": false
        },
        {
          "line": 2398,
          "old_api": "std::move(token_instr)",
          "new_api": null,
          "old_text": "std::move(token_instr)",
          "new_text": null,
          "old_line_content": "      return AddInstruction(std::move(token_instr), HloOpcode::kAfterAll, {});",
          "new_line_content": "      HloInstructionProto token_instr;",
          "content_same": false
        },
        {
          "line": 4447,
          "old_api": "std::make_unique<XlaBuilder>(computation_name)",
          "new_api": null,
          "old_text": "std::make_unique<XlaBuilder>(computation_name)",
          "new_text": null,
          "old_line_content": "  auto sub_builder = std::make_unique<XlaBuilder>(computation_name);",
          "new_line_content": "std::unique_ptr<XlaBuilder> XlaBuilder::CreateSubBuilder(",
          "content_same": false
        },
        {
          "line": 2401,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "      return AddInstruction(std::move(instr), HloOpcode::kOutfeed,",
          "new_line_content": "    };",
          "content_same": false
        },
        {
          "line": 2404,
          "old_api": "sharding",
          "new_api": null,
          "old_text": "sharding()",
          "new_text": null,
          "old_line_content": "    if (sharding()) {",
          "new_line_content": "                            {operand, token});",
          "content_same": false
        },
        {
          "line": 2407,
          "old_api": "make_token",
          "new_api": null,
          "old_text": "make_token()",
          "new_text": null,
          "old_line_content": "      TF_ASSIGN_OR_RETURN(token, make_token());",
          "new_line_content": "      XlaScopedShardingAssignment scoped_sharding(",
          "content_same": false
        },
        {
          "line": 4456,
          "old_api": "set_input_batch_dimension",
          "new_api": null,
          "old_text": "dimension_numbers.set_input_batch_dimension(kConvBatchDimension)",
          "new_text": null,
          "old_line_content": "  dimension_numbers.set_input_batch_dimension(kConvBatchDimension);",
          "new_line_content": "XlaBuilder::CreateDefaultConvDimensionNumbers(int num_spatial_dims) {",
          "content_same": false
        },
        {
          "line": 4457,
          "old_api": "set_input_feature_dimension",
          "new_api": null,
          "old_text": "dimension_numbers.set_input_feature_dimension(kConvFeatureDimension)",
          "new_text": null,
          "old_line_content": "  dimension_numbers.set_input_feature_dimension(kConvFeatureDimension);",
          "new_line_content": "  ConvolutionDimensionNumbers dimension_numbers;",
          "content_same": false
        },
        {
          "line": 2412,
          "old_api": "sharding",
          "new_api": null,
          "old_text": "sharding()",
          "new_text": null,
          "old_line_content": "      OpSharding tuple_sharding = *sharding();",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4465,
          "old_api": "add_input_spatial_dimensions",
          "new_api": null,
          "old_text": "dimension_numbers.add_input_spatial_dimensions(i + 2)",
          "new_text": null,
          "old_line_content": "    dimension_numbers.add_input_spatial_dimensions(i + 2);",
          "new_line_content": "      kConvKernelInputDimension);",
          "content_same": false
        },
        {
          "line": 4466,
          "old_api": "add_kernel_spatial_dimensions",
          "new_api": null,
          "old_text": "dimension_numbers.add_kernel_spatial_dimensions(i + 2)",
          "new_text": null,
          "old_line_content": "    dimension_numbers.add_kernel_spatial_dimensions(i + 2);",
          "new_line_content": "  for (int i = 0; i < num_spatial_dims; ++i) {",
          "content_same": false
        },
        {
          "line": 4474,
          "old_api": "input_spatial_dimensions_size",
          "new_api": null,
          "old_text": "dnum.input_spatial_dimensions_size()",
          "new_text": null,
          "old_line_content": "  if (dnum.input_spatial_dimensions_size() < 2) {",
          "new_line_content": "/* static */ Status XlaBuilder::Validate(",
          "content_same": false
        },
        {
          "line": 4475,
          "old_api": "input_spatial_dimensions_size",
          "new_api": null,
          "old_text": "FailedPrecondition(\"input spacial dimension < 2: %d\",\n                              dnum.input_spatial_dimensions_size())",
          "new_text": null,
          "old_line_content": "    return FailedPrecondition(\"input spacial dimension < 2: %d\",",
          "new_line_content": "    const ConvolutionDimensionNumbers& dnum) {",
          "content_same": false
        },
        {
          "line": 2429,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeNil().ToProto()",
          "new_text": null,
          "old_line_content": "    *tuple_instr.mutable_shape() = ShapeUtil::MakeNil().ToProto();",
          "new_line_content": "    // tokens.",
          "content_same": false
        },
        {
          "line": 4479,
          "old_api": "kernel_spatial_dimensions_size",
          "new_api": null,
          "old_text": "FailedPrecondition(\"kernel spacial dimension < 2: %d\",\n                              dnum.kernel_spatial_dimensions_size())",
          "new_text": null,
          "old_line_content": "    return FailedPrecondition(\"kernel spacial dimension < 2: %d\",",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4483,
          "old_api": "output_spatial_dimensions_size",
          "new_api": null,
          "old_text": "FailedPrecondition(\"output spacial dimension < 2: %d\",\n                              dnum.output_spatial_dimensions_size())",
          "new_text": null,
          "old_line_content": "    return FailedPrecondition(\"output spacial dimension < 2: %d\",",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4487,
          "old_api": "input_batch_dimension",
          "new_api": null,
          "old_text": "std::set<int64_t>(\n          {dnum.input_batch_dimension(), dnum.input_feature_dimension(),\n           dnum.input_spatial_dimensions(0), dnum.input_spatial_dimensions(1)})\n          .size()",
          "new_text": null,
          "old_line_content": "  if (std::set<int64_t>(",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4488,
          "old_api": "input_feature_dimension",
          "new_api": null,
          "old_text": "dnum.input_feature_dimension()",
          "new_text": null,
          "old_line_content": "          {dnum.input_batch_dimension(), dnum.input_feature_dimension(),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2445,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Check and set outfeed shape.\n    if (!LayoutUtil::HasLayout(shape_with_layout)) {\n      return InvalidArgument(\"Given shape to Outfeed must have a layout\");\n    }\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {\n      return InvalidArgument(\n          \"Outfeed shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape));\n    }\n    return OutfeedWithTokenInternal(operand, token, shape_with_layout,\n                                    outfeed_config);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                                   const Shape& shape_with_layout,",
          "content_same": false
        },
        {
          "line": 4494,
          "old_api": "input_feature_dimension",
          "new_api": null,
          "old_text": "dnum.input_feature_dimension()",
          "new_text": null,
          "old_line_content": "        dnum.input_batch_dimension(), dnum.input_feature_dimension(),",
          "new_line_content": "        \"dimension numbers for the input are not unique: (%d, %d, %d, \"",
          "content_same": false
        },
        {
          "line": 4495,
          "old_api": "input_spatial_dimensions",
          "new_api": null,
          "old_text": "dnum.input_spatial_dimensions(1)",
          "new_text": null,
          "old_line_content": "        dnum.input_spatial_dimensions(0), dnum.input_spatial_dimensions(1));",
          "new_line_content": "        \"%d)\",",
          "content_same": false
        },
        {
          "line": 2448,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\"Given shape to Outfeed must have a layout\")",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"Given shape to Outfeed must have a layout\");",
          "new_line_content": "    // Check and set outfeed shape.",
          "content_same": false
        },
        {
          "line": 4498,
          "old_api": "kernel_input_feature_dimension",
          "new_api": null,
          "old_text": "dnum.kernel_input_feature_dimension()",
          "new_text": null,
          "old_line_content": "                         dnum.kernel_input_feature_dimension(),",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 2451,
          "old_api": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "new_api": null,
          "old_text": "ShapeUtil::Compatible(*operand_shape, shape_with_layout)",
          "new_text": null,
          "old_line_content": "    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2452,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Outfeed shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 2455,
          "old_api": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanStringWithLayout(*operand_shape)",
          "new_text": null,
          "old_line_content": "          ShapeUtil::HumanStringWithLayout(*operand_shape));",
          "new_line_content": "          \"Outfeed shape %s must be compatible with operand shape %s\",",
          "content_same": false
        },
        {
          "line": 4505,
          "old_api": "kernel_output_feature_dimension",
          "new_api": null,
          "old_text": "dnum.kernel_output_feature_dimension()",
          "new_text": null,
          "old_line_content": "        dnum.kernel_output_feature_dimension(),",
          "new_line_content": "        \"dimension numbers for the weight are not unique: (%d, %d, %d, \"",
          "content_same": false
        },
        {
          "line": 4506,
          "old_api": "kernel_input_feature_dimension",
          "new_api": null,
          "old_text": "dnum.kernel_input_feature_dimension()",
          "new_text": null,
          "old_line_content": "        dnum.kernel_input_feature_dimension(),",
          "new_line_content": "        \"%d)\",",
          "content_same": false
        },
        {
          "line": 4510,
          "old_api": "output_feature_dimension",
          "new_api": null,
          "old_text": "dnum.output_feature_dimension()",
          "new_text": null,
          "old_line_content": "                         dnum.output_feature_dimension(),",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 2466,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "    const std::string& outfeed_config) {",
          "content_same": false
        },
        {
          "line": 2467,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape_with_layout.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_outfeed_shape() = shape_with_layout.ToProto();",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4517,
          "old_api": "output_feature_dimension",
          "new_api": null,
          "old_text": "dnum.output_feature_dimension()",
          "new_text": null,
          "old_line_content": "        dnum.output_batch_dimension(), dnum.output_feature_dimension(),",
          "new_line_content": "        \"dimension numbers for the output are not unique: (%d, %d, %d, \"",
          "content_same": false
        },
        {
          "line": 4518,
          "old_api": "output_spatial_dimensions",
          "new_api": null,
          "old_text": "dnum.output_spatial_dimensions(1)",
          "new_text": null,
          "old_line_content": "        dnum.output_spatial_dimensions(0), dnum.output_spatial_dimensions(1));",
          "new_line_content": "        \"%d)\",",
          "content_same": false
        },
        {
          "line": 2474,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kAfterAll);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2477,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kAfterAll);",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4526,
          "old_api": "TF_RETURN_IF_ERROR",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(first_error_)",
          "new_text": null,
          "old_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "new_line_content": "                                           HloOpcode opcode,",
          "content_same": false
        },
        {
          "line": 4529,
          "old_api": "set_id",
          "new_api": null,
          "old_text": "instr.set_id(handle)",
          "new_text": null,
          "old_line_content": "  instr.set_id(handle);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2482,
          "old_api": "empty",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (tokens.empty()) {\n      return InvalidArgument(\"AfterAll requires at least one operand\");\n    }\n    for (int i = 0, end = tokens.size(); i < end; ++i) {\n      XlaOp operand = tokens[i];\n      TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n      if (!operand_shape->IsToken()) {\n        return InvalidArgument(\n            \"All operands to AfterAll must be tokens; operand %d has shape %s\",\n            i, ShapeUtil::HumanString(*operand_shape));\n      }\n    }\n    HloInstructionProto instr;\n    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kAfterAll, tokens);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2483,
          "old_api": "empty",
          "new_api": null,
          "old_text": "tokens.empty()",
          "new_text": null,
          "old_line_content": "    if (tokens.empty()) {",
          "new_line_content": "XlaOp XlaBuilder::AfterAll(absl::Span<const XlaOp> tokens) {",
          "content_same": false
        },
        {
          "line": 4536,
          "old_api": "handle",
          "new_api": null,
          "old_text": "operand.handle()",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"invalid XlaOp with handle %d\", operand.handle());",
          "new_line_content": "  for (const auto& operand : operands) {",
          "content_same": false
        },
        {
          "line": 2489,
          "old_api": "IsToken",
          "new_api": null,
          "old_text": "operand_shape->IsToken()",
          "new_text": null,
          "old_line_content": "      if (!operand_shape->IsToken()) {",
          "new_line_content": "      XlaOp operand = tokens[i];",
          "content_same": false
        },
        {
          "line": 2490,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n            \"All operands to AfterAll must be tokens; operand %d has shape %s\",\n            i, ShapeUtil::HumanString(*operand_shape))",
          "new_text": null,
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 4539,
          "old_api": "name",
          "new_api": null,
          "old_text": "InvalidArgument(\"Do not add XlaOp from builder %s to builder %s\",\n                             operand.builder_->name(), this->name())",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"Do not add XlaOp from builder %s to builder %s\",",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4540,
          "old_api": "name",
          "new_api": null,
          "old_text": "this->name()",
          "new_text": null,
          "old_line_content": "                             operand.builder_->name(), this->name());",
          "new_line_content": "    if (operand.builder_ != this) {",
          "content_same": false
        },
        {
          "line": 2496,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2497,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kAfterAll, tokens);",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4545,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "one_shot_metadata_.has_value()",
          "new_text": null,
          "old_line_content": "  if (one_shot_metadata_.has_value()) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4546,
          "old_api": "value",
          "new_api": null,
          "old_text": "one_shot_metadata_.value()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_metadata() = one_shot_metadata_.value();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4553,
          "old_api": "shape",
          "new_api": null,
          "old_text": "instr.shape()",
          "new_text": null,
          "old_line_content": "    Shape shape(instr.shape());",
          "new_line_content": "  if (sharding_) {",
          "content_same": false
        },
        {
          "line": 4556,
          "old_api": "NormalizeTupleSharding",
          "new_api": null,
          "old_text": "sharding.NormalizeTupleSharding(shape)",
          "new_text": null,
          "old_line_content": "    sharding = sharding.NormalizeTupleSharding(shape);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(HloSharding sharding,",
          "content_same": false
        },
        {
          "line": 4557,
          "old_api": "Validate",
          "new_api": null,
          "old_text": "sharding.Validate(shape)",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(sharding.Validate(shape));",
          "new_line_content": "                        HloSharding::FromProto(*sharding_));",
          "content_same": false
        },
        {
          "line": 2511,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (absl::StartsWith(call_target_name, \"$\")) {\n      return InvalidArgument(\n          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"\n          \"are reserved for internal use.\",\n          call_target_name);\n    }\n    if (operand_shapes_with_layout.has_value()) {\n      if (!LayoutUtil::HasLayout(shape)) {\n        return InvalidArgument(\n            \"Result shape must have layout for custom call with constrained \"\n            \"layout.\");\n      }\n      if (operands.size() != operand_shapes_with_layout->size()) {\n        return InvalidArgument(\n            \"Must specify a shape with layout for each operand for custom call \"\n            \"with constrained layout; given %d shapes, expected %d\",\n            operand_shapes_with_layout->size(), operands.size());\n      }\n      int64_t operand_num = 0;\n      for (const Shape& operand_shape : *operand_shapes_with_layout) {\n        if (!LayoutUtil::HasLayout(operand_shape)) {\n          return InvalidArgument(\n              \"No layout specified for operand %d for custom call with \"\n              \"constrained layout.\",\n              operand_num);\n        }\n        ++operand_num;\n      }\n    }\n    return CustomCallInternal(\n        call_target_name, operands, /*computation=*/nullptr, shape, opaque,\n        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,\n        literal, window, dnums, schedule, api_version);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    std::optional<ConvolutionDimensionNumbers> dnums,",
          "content_same": false
        },
        {
          "line": 2512,
          "old_api": "absl::StartsWith(call_target_name, \"$\")",
          "new_api": null,
          "old_text": "absl::StartsWith(call_target_name, \"$\")",
          "new_text": null,
          "old_line_content": "    if (absl::StartsWith(call_target_name, \"$\")) {",
          "new_line_content": "    CustomCallSchedule schedule, CustomCallApiVersion api_version) {",
          "content_same": false
        },
        {
          "line": 4563,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  instructions_.push_back(std::move(instr));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2518,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "operand_shapes_with_layout.has_value()",
          "new_text": null,
          "old_line_content": "    if (operand_shapes_with_layout.has_value()) {",
          "new_line_content": "          call_target_name);",
          "content_same": false
        },
        {
          "line": 2519,
          "old_api": "LayoutUtil::HasLayout(shape)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(shape)",
          "new_text": null,
          "old_line_content": "      if (!LayoutUtil::HasLayout(shape)) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2524,
          "old_api": "size",
          "new_api": null,
          "old_text": "operand_shapes_with_layout->size()",
          "new_text": null,
          "old_line_content": "      if (operands.size() != operand_shapes_with_layout->size()) {",
          "new_line_content": "            \"layout.\");",
          "content_same": false
        },
        {
          "line": 2525,
          "old_api": "size",
          "new_api": null,
          "old_text": "InvalidArgument(\n            \"Must specify a shape with layout for each operand for custom call \"\n            \"with constrained layout; given %d shapes, expected %d\",\n            operand_shapes_with_layout->size(), operands.size())",
          "new_text": null,
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 4574,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                           absl::Span<const XlaOp> operands) {",
          "content_same": false
        },
        {
          "line": 4575,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), opcode, operands);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 2528,
          "old_api": "size",
          "new_api": null,
          "old_text": "operands.size()",
          "new_text": null,
          "old_line_content": "            operand_shapes_with_layout->size(), operands.size());",
          "new_line_content": "            \"Must specify a shape with layout for each operand for custom call \"",
          "content_same": false
        },
        {
          "line": 2532,
          "old_api": "LayoutUtil::HasLayout(operand_shape)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(operand_shape)",
          "new_text": null,
          "old_line_content": "        if (!LayoutUtil::HasLayout(operand_shape)) {",
          "new_line_content": "      int64_t operand_num = 0;",
          "content_same": false
        },
        {
          "line": 2533,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n              \"No layout specified for operand %d for custom call with \"\n              \"constrained layout.\",\n              operand_num)",
          "new_text": null,
          "old_line_content": "          return InvalidArgument(",
          "new_line_content": "      for (const Shape& operand_shape : *operand_shapes_with_layout) {",
          "content_same": false
        },
        {
          "line": 4582,
          "old_api": "proto",
          "new_api": null,
          "old_text": "computation.proto().computations_size()",
          "new_text": null,
          "old_line_content": "  imported_computations.reserve(computation.proto().computations_size());",
          "new_line_content": "  absl::flat_hash_map<int64_t, int64_t> remapped_ids;",
          "content_same": false
        },
        {
          "line": 4585,
          "old_api": "proto",
          "new_api": null,
          "old_text": "computation.proto().computations()",
          "new_text": null,
          "old_line_content": "  for (const HloComputationProto& e : computation.proto().computations()) {",
          "new_line_content": "  // Before we import the computations by remapping IDs, and capturing the",
          "content_same": false
        },
        {
          "line": 4588,
          "old_api": "id",
          "new_api": null,
          "old_text": "new_computation.id()",
          "new_text": null,
          "old_line_content": "    remapped_ids[new_computation.id()] = computation_id;",
          "new_line_content": "    HloComputationProto new_computation(e);",
          "content_same": false
        },
        {
          "line": 2541,
          "old_api": "CustomCallInternal",
          "new_api": null,
          "old_text": "CustomCallInternal(\n        call_target_name, operands, /*computation=*/nullptr, shape, opaque,\n        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,\n        literal, window, dnums, schedule, api_version)",
          "new_text": null,
          "old_line_content": "    return CustomCallInternal(",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 4593,
          "old_api": "GetNextId",
          "new_api": null,
          "old_text": "GetNextId()",
          "new_text": null,
          "old_line_content": "      int64_t instruction_id = GetNextId();",
          "new_line_content": "                      kNameSeparator, computation_id);",
          "content_same": false
        },
        {
          "line": 4599,
          "old_api": "root_id",
          "new_api": null,
          "old_text": "new_computation.root_id()",
          "new_text": null,
          "old_line_content": "    new_computation.set_root_id(remapped_ids.at(new_computation.root_id()));",
          "new_line_content": "                        kNameSeparator, instruction_id);",
          "content_same": false
        },
        {
          "line": 4605,
          "old_api": "at",
          "new_api": null,
          "old_text": "instr->add_called_computation_ids(\n      remapped_ids.at(computation.proto().entry_computation_id()))",
          "new_text": null,
          "old_line_content": "  instr->add_called_computation_ids(",
          "new_line_content": "  // Once we have imported all the computations, and captured all the ID",
          "content_same": false
        },
        {
          "line": 4606,
          "old_api": "proto",
          "new_api": null,
          "old_text": "computation.proto().entry_computation_id()",
          "new_text": null,
          "old_line_content": "      remapped_ids.at(computation.proto().entry_computation_id()));",
          "new_line_content": "  // mappings, we go back and fixup the IDs in the imported computations.",
          "content_same": false
        },
        {
          "line": 4609,
          "old_api": "mutable_operand_ids",
          "new_api": null,
          "old_text": "instruction.mutable_operand_ids()",
          "new_text": null,
          "old_line_content": "      for (auto& operand_id : *instruction.mutable_operand_ids()) {",
          "new_line_content": "  for (auto& imported_computation : imported_computations) {",
          "content_same": false
        },
        {
          "line": 2564,
          "old_api": "set_name",
          "new_api": null,
          "old_text": "instr.set_name(\"cudnn-conv\")",
          "new_text": null,
          "old_line_content": "    instr.set_name(\"cudnn-conv\");",
          "new_line_content": "  // cosmetic.)",
          "content_same": false
        },
        {
          "line": 4613,
          "old_api": "mutable_control_predecessor_ids",
          "new_api": null,
          "old_text": "instruction.mutable_control_predecessor_ids()",
          "new_text": null,
          "old_line_content": "           *instruction.mutable_control_predecessor_ids()) {",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 4614,
          "old_api": "at",
          "new_api": null,
          "old_text": "remapped_ids.at(control_predecessor_id)",
          "new_text": null,
          "old_line_content": "        control_predecessor_id = remapped_ids.at(control_predecessor_id);",
          "new_line_content": "      for (auto& control_predecessor_id :",
          "content_same": false
        },
        {
          "line": 4617,
          "old_api": "mutable_called_computation_ids",
          "new_api": null,
          "old_text": "instruction.mutable_called_computation_ids()",
          "new_text": null,
          "old_line_content": "           *instruction.mutable_called_computation_ids()) {",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 4618,
          "old_api": "at",
          "new_api": null,
          "old_text": "remapped_ids.at(called_computation_id)",
          "new_text": null,
          "old_line_content": "        called_computation_id = remapped_ids.at(called_computation_id);",
          "new_line_content": "      for (auto& called_computation_id :",
          "content_same": false
        },
        {
          "line": 2573,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "instr.set_custom_call_target(call_target_name)",
          "new_text": null,
          "old_line_content": "  instr.set_custom_call_target(call_target_name);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4622,
          "old_api": "id",
          "new_api": null,
          "old_text": "imported_computation.id()",
          "new_text": null,
          "old_line_content": "    int64_t computation_id = imported_computation.id();",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4623,
          "old_api": "instructions_size",
          "new_api": null,
          "old_text": "imported_computation.instructions_size()",
          "new_text": null,
          "old_line_content": "    for (int64_t i = 0; i < imported_computation.instructions_size(); ++i) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4627,
          "old_api": "insert",
          "new_api": null,
          "old_text": "handle_to_imported_index_.insert(\n          {imported_computation.instructions(i).id(), imported_instruction})",
          "new_text": null,
          "old_line_content": "      handle_to_imported_index_.insert(",
          "new_line_content": "      imported_instruction.computation_id = computation_id;",
          "content_same": false
        },
        {
          "line": 4628,
          "old_api": "instructions",
          "new_api": null,
          "old_text": "imported_computation.instructions(i).id()",
          "new_text": null,
          "old_line_content": "          {imported_computation.instructions(i).id(), imported_instruction});",
          "new_line_content": "      imported_instruction.instruction_index = i;",
          "content_same": false
        },
        {
          "line": 2582,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "literal->ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_literal() = literal->ToProto();",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 2585,
          "old_api": "IsNull",
          "new_api": null,
          "old_text": "computation->IsNull()",
          "new_text": null,
          "old_line_content": "  if (computation != nullptr && !computation->IsNull()) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4636,
          "old_api": "TF_RETURN_IF_ERROR",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(first_error_)",
          "new_text": null,
          "old_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "new_line_content": "StatusOr<const HloInstructionProto*> XlaBuilder::LookUpInstruction(",
          "content_same": false
        },
        {
          "line": 2589,
          "old_api": "add_output_operand_aliasing",
          "new_api": null,
          "old_text": "instr.add_output_operand_aliasing()",
          "new_text": null,
          "old_line_content": "    auto aliasing = instr.add_output_operand_aliasing();",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 2590,
          "old_api": "set_operand_index",
          "new_api": null,
          "old_text": "aliasing->set_operand_index(pair.second.first)",
          "new_text": null,
          "old_line_content": "    aliasing->set_operand_index(pair.second.first);",
          "new_line_content": "  for (const auto& pair : output_operand_aliasing) {",
          "content_same": false
        },
        {
          "line": 4637,
          "old_api": "LookUpInstructionInternal<const HloInstructionProto*>(op)",
          "new_api": null,
          "old_text": "LookUpInstructionInternal<const HloInstructionProto*>(op)",
          "new_text": null,
          "old_line_content": "  return LookUpInstructionInternal<const HloInstructionProto*>(op);",
          "new_line_content": "    const XlaOp op) const {",
          "content_same": false
        },
        {
          "line": 4642,
          "old_api": "LookUpInstructionByHandleInternal<const HloInstructionProto*>(handle)",
          "new_api": null,
          "old_text": "LookUpInstructionByHandleInternal<const HloInstructionProto*>(handle)",
          "new_text": null,
          "old_line_content": "  return LookUpInstructionByHandleInternal<const HloInstructionProto*>(handle);",
          "new_line_content": "StatusOr<const HloInstructionProto*> XlaBuilder::LookUpInstructionByHandle(",
          "content_same": false
        },
        {
          "line": 2595,
          "old_api": "add_output_shape_index",
          "new_api": null,
          "old_text": "aliasing->add_output_shape_index(index)",
          "new_text": null,
          "old_line_content": "      aliasing->add_output_shape_index(index);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2598,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "window.has_value()",
          "new_text": null,
          "old_line_content": "  if (window.has_value()) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2599,
          "old_api": "mutable_window",
          "new_api": null,
          "old_text": "instr.mutable_window()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_window() = *window;",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4647,
          "old_api": "TF_RETURN_IF_ERROR",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(first_error_)",
          "new_text": null,
          "old_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "new_line_content": "StatusOr<HloInstructionProto*> XlaBuilder::LookUpMutableInstruction(",
          "content_same": false
        },
        {
          "line": 4648,
          "old_api": "LookUpInstructionInternal<HloInstructionProto*>(op)",
          "new_api": null,
          "old_text": "LookUpInstructionInternal<HloInstructionProto*>(op)",
          "new_text": null,
          "old_line_content": "  return LookUpInstructionInternal<HloInstructionProto*>(op);",
          "new_line_content": "    const XlaOp op) {",
          "content_same": false
        },
        {
          "line": 2602,
          "old_api": "mutable_convolution_dimension_numbers",
          "new_api": null,
          "old_text": "instr.mutable_convolution_dimension_numbers()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_convolution_dimension_numbers() = *dnums;",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 2605,
          "old_api": "set_custom_call_api_version",
          "new_api": null,
          "old_text": "instr.set_custom_call_api_version(api_version)",
          "new_text": null,
          "old_line_content": "  instr.set_custom_call_api_version(api_version);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4653,
          "old_api": "LookUpInstructionByHandleInternal<HloInstructionProto*>(handle)",
          "new_api": null,
          "old_text": "LookUpInstructionByHandleInternal<HloInstructionProto*>(handle)",
          "new_text": null,
          "old_line_content": "  return LookUpInstructionByHandleInternal<HloInstructionProto*>(handle);",
          "new_line_content": "StatusOr<HloInstructionProto*> XlaBuilder::LookUpMutableInstructionByHandle(",
          "content_same": false
        },
        {
          "line": 4661,
          "old_api": "Parameter",
          "new_api": null,
          "old_text": "Parameter(builder, parameter_number, shape, name, empty_bools)",
          "new_text": null,
          "old_line_content": "  return Parameter(builder, parameter_number, shape, name, empty_bools);",
          "new_line_content": "                const Shape& shape, const std::string& name) {",
          "content_same": false
        },
        {
          "line": 2619,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (absl::StartsWith(call_target_name, \"$\")) {\n      return InvalidArgument(\n          \"Invalid custom_call_target \\\"%s\\\": Call targets that start with '$' \"\n          \"are reserved for internal use.\",\n          call_target_name);\n    }\n    if (operand_shapes_with_layout.has_value()) {\n      if (!LayoutUtil::HasLayout(shape)) {\n        return InvalidArgument(\n            \"Result shape must have layout for custom call with constrained \"\n            \"layout.\");\n      }\n      if (operands.size() != operand_shapes_with_layout->size()) {\n        return InvalidArgument(\n            \"Must specify a shape with layout for each operand for custom call \"\n            \"with constrained layout; given %d shapes, expected %d\",\n            operand_shapes_with_layout->size(), operands.size());\n      }\n      int64_t operand_num = 0;\n      for (const Shape& operand_shape : *operand_shapes_with_layout) {\n        if (!LayoutUtil::HasLayout(operand_shape)) {\n          return InvalidArgument(\n              \"No layout specified for operand %d for custom call with \"\n              \"constrained layout.\",\n              operand_num);\n        }\n        ++operand_num;\n      }\n    }\n    return CustomCallInternal(\n        call_target_name, operands, &computation, shape, opaque,\n        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,\n        literal, /*window=*/{}, /*dnums=*/{}, schedule, api_version);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const Literal* literal, CustomCallSchedule schedule,",
          "content_same": false
        },
        {
          "line": 2620,
          "old_api": "absl::StartsWith(call_target_name, \"$\")",
          "new_api": null,
          "old_text": "absl::StartsWith(call_target_name, \"$\")",
          "new_text": null,
          "old_line_content": "    if (absl::StartsWith(call_target_name, \"$\")) {",
          "new_line_content": "    CustomCallApiVersion api_version) {",
          "content_same": false
        },
        {
          "line": 4667,
          "old_api": "Parameter",
          "new_api": null,
          "old_text": "builder->Parameter(parameter_number, shape, name,\n                            replicated_at_leaf_buffers)",
          "new_text": null,
          "old_line_content": "  return builder->Parameter(parameter_number, shape, name,",
          "new_line_content": "                const Shape& shape, const std::string& name,",
          "content_same": false
        },
        {
          "line": 2626,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "operand_shapes_with_layout.has_value()",
          "new_text": null,
          "old_line_content": "    if (operand_shapes_with_layout.has_value()) {",
          "new_line_content": "          call_target_name);",
          "content_same": false
        },
        {
          "line": 2627,
          "old_api": "LayoutUtil::HasLayout(shape)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(shape)",
          "new_text": null,
          "old_line_content": "      if (!LayoutUtil::HasLayout(shape)) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4674,
          "old_api": "ConstantLiteral",
          "new_api": null,
          "old_text": "builder->ConstantLiteral(literal)",
          "new_text": null,
          "old_line_content": "  return builder->ConstantLiteral(literal);",
          "new_line_content": "// computation.",
          "content_same": false
        },
        {
          "line": 4679,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Broadcast(operand, broadcast_sizes)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Broadcast(operand, broadcast_sizes);",
          "new_line_content": "XlaOp Broadcast(const XlaOp operand,",
          "content_same": false
        },
        {
          "line": 2632,
          "old_api": "size",
          "new_api": null,
          "old_text": "operand_shapes_with_layout->size()",
          "new_text": null,
          "old_line_content": "      if (operands.size() != operand_shapes_with_layout->size()) {",
          "new_line_content": "            \"layout.\");",
          "content_same": false
        },
        {
          "line": 2633,
          "old_api": "size",
          "new_api": null,
          "old_text": "InvalidArgument(\n            \"Must specify a shape with layout for each operand for custom call \"\n            \"with constrained layout; given %d shapes, expected %d\",\n            operand_shapes_with_layout->size(), operands.size())",
          "new_text": null,
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 2636,
          "old_api": "size",
          "new_api": null,
          "old_text": "operands.size()",
          "new_text": null,
          "old_line_content": "            operand_shapes_with_layout->size(), operands.size());",
          "new_line_content": "            \"Must specify a shape with layout for each operand for custom call \"",
          "content_same": false
        },
        {
          "line": 4685,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->BroadcastInDim(operand, out_dim_size,\n                                           broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->BroadcastInDim(operand, out_dim_size,",
          "new_line_content": "                     const absl::Span<const int64_t> out_dim_size,",
          "content_same": false
        },
        {
          "line": 2640,
          "old_api": "LayoutUtil::HasLayout(operand_shape)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(operand_shape)",
          "new_text": null,
          "old_line_content": "        if (!LayoutUtil::HasLayout(operand_shape)) {",
          "new_line_content": "      int64_t operand_num = 0;",
          "content_same": false
        },
        {
          "line": 2641,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n              \"No layout specified for operand %d for custom call with \"\n              \"constrained layout.\",\n              operand_num)",
          "new_text": null,
          "old_line_content": "          return InvalidArgument(",
          "new_line_content": "      for (const Shape& operand_shape : *operand_shapes_with_layout) {",
          "content_same": false
        },
        {
          "line": 4692,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->DynamicBroadcastInDim(\n      operand, output_dimensions, broadcast_dimensions, output_shape)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->DynamicBroadcastInDim(",
          "new_line_content": "                            absl::Span<const int64_t> broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2649,
          "old_api": "CustomCallInternal",
          "new_api": null,
          "old_text": "CustomCallInternal(\n        call_target_name, operands, &computation, shape, opaque,\n        operand_shapes_with_layout, has_side_effect, output_operand_aliasing,\n        literal, /*window=*/{}, /*dnums=*/{}, schedule, api_version)",
          "new_text": null,
          "old_line_content": "    return CustomCallInternal(",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 4697,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kCopy, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kCopy, operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4702,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Pad(operand, padding_value, padding_config)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Pad(operand, padding_value, padding_config);",
          "new_line_content": "XlaOp Pad(const XlaOp operand, const XlaOp padding_value,",
          "content_same": false
        },
        {
          "line": 2657,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    Shape shape = *operand_shape;\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kOptimizationBarrier,\n                          {operand});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4707,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->PadInDim(operand, padding_value, dimno, pad_lo,\n                                     pad_hi)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->PadInDim(operand, padding_value, dimno, pad_lo,",
          "new_line_content": "XlaOp PadInDim(XlaOp operand, XlaOp padding_value, int64_t dimno,",
          "content_same": false
        },
        {
          "line": 2661,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    Shape shape = *operand_shape;",
          "content_same": false
        },
        {
          "line": 2662,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kOptimizationBarrier,",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4713,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Reshape(operand, dimensions, new_sizes)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Reshape(operand, dimensions, new_sizes);",
          "new_line_content": "XlaOp Reshape(const XlaOp operand, absl::Span<const int64_t> dimensions,",
          "content_same": false
        },
        {
          "line": 2669,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferTransposeShape(\n                                         *operand_shape, permutation));\n    return TransposeInternal(shape, operand, permutation);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::Transpose(XlaOp operand,",
          "content_same": false
        },
        {
          "line": 4717,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Reshape(operand, new_sizes)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Reshape(operand, new_sizes);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2673,
          "old_api": "TransposeInternal",
          "new_api": null,
          "old_text": "TransposeInternal(shape, operand, permutation)",
          "new_text": null,
          "old_line_content": "    return TransposeInternal(shape, operand, permutation);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferTransposeShape(",
          "content_same": false
        },
        {
          "line": 4721,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Reshape(shape, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Reshape(shape, operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4727,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->DynamicReshape(operand, dim_sizes, new_size_bounds,\n                                           dims_are_dynamic)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->DynamicReshape(operand, dim_sizes, new_size_bounds,",
          "new_line_content": "                     absl::Span<const int64_t> new_size_bounds,",
          "content_same": false
        },
        {
          "line": 2680,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    const Shape& shape, XlaOp operand, absl::Span<const int64_t> permutation) {",
          "content_same": false
        },
        {
          "line": 4734,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Reshape(operand, new_sizes, inferred_dimension)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Reshape(operand, new_sizes, inferred_dimension);",
          "new_line_content": "                                   absl::Span<const int64_t> new_sizes,",
          "content_same": false
        },
        {
          "line": 2688,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferReverseShape(\n                                         *operand_shape, dimensions));\n    return RevInternal(shape, operand, dimensions);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4738,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Collapse(operand, dimensions)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Collapse(operand, dimensions);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2692,
          "old_api": "RevInternal",
          "new_api": null,
          "old_text": "RevInternal(shape, operand, dimensions)",
          "new_text": null,
          "old_line_content": "    return RevInternal(shape, operand, dimensions);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferReverseShape(",
          "content_same": false
        },
        {
          "line": 4744,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Slice(operand, start_indices, limit_indices,\n                                  strides)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Slice(operand, start_indices, limit_indices,",
          "new_line_content": "            absl::Span<const int64_t> limit_indices,",
          "content_same": false
        },
        {
          "line": 2699,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                        absl::Span<const int64_t> dimensions) {",
          "content_same": false
        },
        {
          "line": 4750,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->SliceInDim(operand, start_index, limit_index,\n                                       stride, dimno)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->SliceInDim(operand, start_index, limit_index,",
          "new_line_content": "XlaOp SliceInDim(const XlaOp operand, int64_t start_index, int64_t limit_index,",
          "content_same": false
        },
        {
          "line": 4756,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->DynamicSlice(operand, start_indices, slice_sizes)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->DynamicSlice(operand, start_indices, slice_sizes);",
          "new_line_content": "XlaOp DynamicSlice(const XlaOp operand, absl::Span<const XlaOp> start_indices,",
          "content_same": false
        },
        {
          "line": 2709,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(std::vector<Shape> operand_shapes,\n                        GetOperandShapes(operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferVariadicOpShape(\n                                         HloOpcode::kSort, operand_shape_ptrs));\n    return SortInternal(shape, operands, comparator, dimension, is_stable);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                       const XlaComputation& comparator, int64_t dimension,",
          "content_same": false
        },
        {
          "line": 2712,
          "old_api": "GetOperandShapes",
          "new_api": null,
          "old_text": "GetOperandShapes(operands)",
          "new_text": null,
          "old_line_content": "                        GetOperandShapes(operands));",
          "new_line_content": "    std::vector<const Shape*> operand_shape_ptrs;",
          "content_same": false
        },
        {
          "line": 4761,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->DynamicUpdateSlice(operand, update, start_indices)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->DynamicUpdateSlice(operand, update, start_indices);",
          "new_line_content": "XlaOp DynamicUpdateSlice(const XlaOp operand, const XlaOp update,",
          "content_same": false
        },
        {
          "line": 2717,
          "old_api": "SortInternal",
          "new_api": null,
          "old_text": "SortInternal(shape, operands, comparator, dimension, is_stable)",
          "new_text": null,
          "old_line_content": "    return SortInternal(shape, operands, comparator, dimension, is_stable);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferVariadicOpShape(",
          "content_same": false
        },
        {
          "line": 4766,
          "old_api": "ConcatInDim",
          "new_api": null,
          "old_text": "builder->ConcatInDim(operands, dimension)",
          "new_text": null,
          "old_line_content": "  return builder->ConcatInDim(operands, dimension);",
          "new_line_content": "XlaOp ConcatInDim(XlaBuilder* builder, absl::Span<const XlaOp> operands,",
          "content_same": false
        },
        {
          "line": 4770,
          "old_api": "builder",
          "new_api": null,
          "old_text": "pred.builder()->Select(pred, on_true, on_false)",
          "new_text": null,
          "old_line_content": "  return pred.builder()->Select(pred, on_true, on_false);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2726,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                         int64_t dimension, bool is_stable) {",
          "content_same": false
        },
        {
          "line": 2727,
          "old_api": "set_is_stable",
          "new_api": null,
          "old_text": "instr.set_is_stable(is_stable)",
          "new_text": null,
          "old_line_content": "  instr.set_is_stable(is_stable);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4774,
          "old_api": "Tuple",
          "new_api": null,
          "old_text": "builder->Tuple(elements)",
          "new_text": null,
          "old_line_content": "  return builder->Tuple(elements);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2730,
          "old_api": "rank",
          "new_api": null,
          "old_text": "keys_shape->rank()",
          "new_text": null,
          "old_line_content": "    dimension = keys_shape->rank() - 1;",
          "new_line_content": "  if (dimension == -1) {",
          "content_same": false
        },
        {
          "line": 4778,
          "old_api": "builder",
          "new_api": null,
          "old_text": "tuple_data.builder()->GetTupleElement(tuple_data, index)",
          "new_text": null,
          "old_line_content": "  return tuple_data.builder()->GetTupleElement(tuple_data, index);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2733,
          "old_api": "AddCalledComputation",
          "new_api": null,
          "old_text": "AddCalledComputation(comparator, &instr)",
          "new_text": null,
          "old_line_content": "  AddCalledComputation(comparator, &instr);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 4783,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kEq)",
          "new_text": null,
          "old_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kEq);",
          "new_line_content": "XlaOp Eq(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2738,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferTopKShape(*operand_shape, k));\n    return TopKInternal(shape, operand, k, largest);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4789,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()",
          "new_text": null,
          "old_line_content": "  auto b = lhs.builder();",
          "new_line_content": "                               absl::Span<const int64_t> broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 4790,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "b->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(auto operand_shape, b->GetShape(lhs));\n    auto operand_element_type = operand_shape.element_type();\n    auto compare_type =\n        primitive_util::IsFloatingPointType(operand_element_type)\n            ? Comparison::Type::kFloatTotalOrder\n            : Comparison::DefaultComparisonType(operand_element_type);\n    return Compare(lhs, rhs, broadcast_dimensions, comparison_direction,\n                   compare_type);\n  })",
          "new_text": null,
          "old_line_content": "  return b->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                               ComparisonDirection comparison_direction) {",
          "content_same": false
        },
        {
          "line": 2743,
          "old_api": "TopKInternal",
          "new_api": null,
          "old_text": "TopKInternal(shape, operand, k, largest)",
          "new_text": null,
          "old_line_content": "    return TopKInternal(shape, operand, k, largest);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape,",
          "content_same": false
        },
        {
          "line": 4797,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(lhs, rhs, broadcast_dimensions, comparison_direction,\n                   compare_type)",
          "new_text": null,
          "old_line_content": "    return Compare(lhs, rhs, broadcast_dimensions, comparison_direction,",
          "new_line_content": "            ? Comparison::Type::kFloatTotalOrder",
          "content_same": false
        },
        {
          "line": 2750,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                         int64_t k, bool largest) {",
          "content_same": false
        },
        {
          "line": 2751,
          "old_api": "set_k",
          "new_api": null,
          "old_text": "instr.set_k(k)",
          "new_text": null,
          "old_line_content": "  instr.set_k(k);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4804,
          "old_api": "CompareTotalOrder",
          "new_api": null,
          "old_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kEq)",
          "new_text": null,
          "old_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "new_line_content": "XlaOp EqTotalOrder(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2758,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferConvertShape(\n                                         *operand_shape, new_element_type));\n    if (primitive_util::IsComplexType(operand_shape->element_type()) &&\n        !primitive_util::IsComplexType(new_element_type)) {\n      operand = Real(operand);\n    }\n    return AddOpWithShape(HloOpcode::kConvert, shape, {operand});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::ConvertElementType(XlaOp operand,",
          "content_same": false
        },
        {
          "line": 2762,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "operand_shape->element_type()",
          "new_text": null,
          "old_line_content": "    if (primitive_util::IsComplexType(operand_shape->element_type()) &&",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferConvertShape(",
          "content_same": false
        },
        {
          "line": 2763,
          "old_api": "primitive_util::IsComplexType(new_element_type)",
          "new_api": null,
          "old_text": "primitive_util::IsComplexType(new_element_type)",
          "new_text": null,
          "old_line_content": "        !primitive_util::IsComplexType(new_element_type)) {",
          "new_line_content": "                                         *operand_shape, new_element_type));",
          "content_same": false
        },
        {
          "line": 4810,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kNe)",
          "new_text": null,
          "old_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kNe);",
          "new_line_content": "XlaOp Ne(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 4815,
          "old_api": "CompareTotalOrder",
          "new_api": null,
          "old_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kNe)",
          "new_text": null,
          "old_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "new_line_content": "XlaOp NeTotalOrder(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2772,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferBitcastConvertShape(\n                                         *operand_shape, new_element_type));\n    return BitcastConvertTypeInternal(shape, operand);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::BitcastConvertType(XlaOp operand,",
          "content_same": false
        },
        {
          "line": 4821,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kGe)",
          "new_text": null,
          "old_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kGe);",
          "new_line_content": "XlaOp Ge(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2776,
          "old_api": "BitcastConvertTypeInternal",
          "new_api": null,
          "old_text": "BitcastConvertTypeInternal(shape, operand)",
          "new_text": null,
          "old_line_content": "    return BitcastConvertTypeInternal(shape, operand);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferBitcastConvertShape(",
          "content_same": false
        },
        {
          "line": 4826,
          "old_api": "CompareTotalOrder",
          "new_api": null,
          "old_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kGe)",
          "new_text": null,
          "old_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "new_line_content": "XlaOp GeTotalOrder(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2783,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                                       XlaOp operand) {",
          "content_same": false
        },
        {
          "line": 2784,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kBitcastConvert,",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4832,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kGt)",
          "new_text": null,
          "old_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kGt);",
          "new_line_content": "XlaOp Gt(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 4837,
          "old_api": "CompareTotalOrder",
          "new_api": null,
          "old_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kGt)",
          "new_text": null,
          "old_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "new_line_content": "XlaOp GtTotalOrder(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2790,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* random_shape, GetShapePtr(random));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferStochasticConvertShape(\n                            *operand_shape, *random_shape, new_element_type));\n    return AddOpWithShape(HloOpcode::kStochasticConvert, shape,\n                          {operand, random});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::StochasticConvertType(XlaOp operand, XlaOp random,",
          "content_same": false
        },
        {
          "line": 4843,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kLe)",
          "new_text": null,
          "old_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kLe);",
          "new_line_content": "XlaOp Le(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2796,
          "old_api": "AddOpWithShape",
          "new_api": null,
          "old_text": "AddOpWithShape(HloOpcode::kStochasticConvert, shape,\n                          {operand, random})",
          "new_text": null,
          "old_line_content": "    return AddOpWithShape(HloOpcode::kStochasticConvert, shape,",
          "new_line_content": "                        ShapeInference::InferStochasticConvertShape(",
          "content_same": false
        },
        {
          "line": 4848,
          "old_api": "CompareTotalOrder",
          "new_api": null,
          "old_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kLe)",
          "new_text": null,
          "old_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "new_line_content": "XlaOp LeTotalOrder(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2802,
          "old_api": "TernaryOp",
          "new_api": null,
          "old_text": "TernaryOp(HloOpcode::kClamp, min, operand, max)",
          "new_text": null,
          "old_line_content": "  return TernaryOp(HloOpcode::kClamp, min, operand, max);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4854,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kLt)",
          "new_text": null,
          "old_line_content": "  return Compare(lhs, rhs, broadcast_dimensions, ComparisonDirection::kLt);",
          "new_line_content": "XlaOp Lt(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2809,
          "old_api": "empty",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!static_operands.empty()) {\n      return Unimplemented(\"static_operands is not supported in Map\");\n    }\n\n    HloInstructionProto instr;\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(const ProgramShape& called_program_shape,\n                        computation.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(\n        Shape shape, ShapeInference::InferMapShape(\n                         operand_shape_ptrs, called_program_shape, dimensions));\n    *instr.mutable_shape() = shape.ToProto();\n\n    Shape output_shape(instr.shape());\n    const int64_t output_rank = output_shape.rank();\n    AddCalledComputation(computation, &instr);\n    std::vector<XlaOp> new_operands(operands.begin(), operands.end());\n    for (XlaOp& new_operand : new_operands) {\n      TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(new_operand));\n      const int64_t rank = shape->rank();\n      if (rank != output_rank) {\n        TF_ASSIGN_OR_RETURN(new_operand,\n                            InDimBroadcast(output_shape, new_operand, {}));\n        TF_ASSIGN_OR_RETURN(shape, GetShapePtr(new_operand));\n      }\n      if (!ShapeUtil::SameDimensions(output_shape, *shape)) {\n        TF_ASSIGN_OR_RETURN(new_operand,\n                            AddBroadcastSequence(output_shape, new_operand));\n      }\n    }\n\n    return AddInstruction(std::move(instr), HloOpcode::kMap, new_operands);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                      absl::Span<const int64_t> dimensions,",
          "content_same": false
        },
        {
          "line": 2810,
          "old_api": "empty",
          "new_api": null,
          "old_text": "static_operands.empty()",
          "new_text": null,
          "old_line_content": "    if (!static_operands.empty()) {",
          "new_line_content": "                      absl::Span<const XlaOp> static_operands) {",
          "content_same": false
        },
        {
          "line": 4859,
          "old_api": "CompareTotalOrder",
          "new_api": null,
          "old_text": "CompareTotalOrder(lhs, rhs, broadcast_dimensions,\n                           ComparisonDirection::kLt)",
          "new_text": null,
          "old_line_content": "  return CompareTotalOrder(lhs, rhs, broadcast_dimensions,",
          "new_line_content": "XlaOp LtTotalOrder(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 2817,
          "old_api": "std::back_inserter(operand_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(operand_shape_ptrs)",
          "new_text": null,
          "old_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "new_line_content": "    std::vector<const Shape*> operand_shape_ptrs;",
          "content_same": false
        },
        {
          "line": 4866,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kCompare, lhs, rhs,\n                                 broadcast_dimensions, direction)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kCompare, lhs, rhs,",
          "new_line_content": "              absl::Span<const int64_t> broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2824,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "        Shape shape, ShapeInference::InferMapShape(",
          "content_same": false
        },
        {
          "line": 4873,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kCompare, lhs, rhs,\n                                 broadcast_dimensions, direction, compare_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kCompare, lhs, rhs,",
          "new_line_content": "              absl::Span<const int64_t> broadcast_dimensions,",
          "content_same": false
        },
        {
          "line": 2827,
          "old_api": "rank",
          "new_api": null,
          "old_text": "output_shape.rank()",
          "new_text": null,
          "old_line_content": "    const int64_t output_rank = output_shape.rank();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4878,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(lhs, rhs, {}, direction)",
          "new_text": null,
          "old_line_content": "  return Compare(lhs, rhs, {}, direction);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2832,
          "old_api": "rank",
          "new_api": null,
          "old_text": "shape->rank()",
          "new_text": null,
          "old_line_content": "      const int64_t rank = shape->rank();",
          "new_line_content": "    for (XlaOp& new_operand : new_operands) {",
          "content_same": false
        },
        {
          "line": 2835,
          "old_api": "InDimBroadcast",
          "new_api": null,
          "old_text": "InDimBroadcast(output_shape, new_operand, {})",
          "new_text": null,
          "old_line_content": "                            InDimBroadcast(output_shape, new_operand, {}));",
          "new_line_content": "      if (rank != output_rank) {",
          "content_same": false
        },
        {
          "line": 4884,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->Dot(lhs, rhs, precision_config, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->Dot(lhs, rhs, precision_config, preferred_element_type);",
          "new_line_content": "          const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 2839,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": null,
          "old_text": "TF_ASSIGN_OR_RETURN(new_operand,\n                            AddBroadcastSequence(output_shape, new_operand))",
          "new_text": null,
          "old_line_content": "        TF_ASSIGN_OR_RETURN(new_operand,",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 4891,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->DotGeneral(lhs, rhs, dimension_numbers,\n                                   precision_config, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->DotGeneral(lhs, rhs, dimension_numbers,",
          "new_line_content": "                 const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 2844,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kMap, new_operands);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2851,
          "old_api": "size",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Check the number of parameters per RNG distribution.\n    switch (distribution) {\n      case RandomDistribution::RNG_NORMAL:\n      case RandomDistribution::RNG_UNIFORM:\n        if (parameters.size() != 2) {\n          return InvalidArgument(\n              \"RNG distribution (%s) expects 2 parameters, but got %ld\",\n              RandomDistribution_Name(distribution), parameters.size());\n        }\n        break;\n      default:\n        LOG(FATAL) << \"unhandled distribution \" << distribution;\n    }\n\n    TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));\n    return RngOpInternal(distribution, parameters, shape);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                        absl::Span<const XlaOp> parameters,",
          "content_same": false
        },
        {
          "line": 4901,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->SparseDot(lhs, rhs, sparse_meta, sparsity,\n                                  dimension_numbers, precision_config,\n                                  preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->SparseDot(lhs, rhs, sparse_meta, sparsity,",
          "new_line_content": "                const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 2856,
          "old_api": "size",
          "new_api": null,
          "old_text": "parameters.size()",
          "new_text": null,
          "old_line_content": "        if (parameters.size() != 2) {",
          "new_line_content": "      case RandomDistribution::RNG_NORMAL:",
          "content_same": false
        },
        {
          "line": 2857,
          "old_api": "size",
          "new_api": null,
          "old_text": "InvalidArgument(\n              \"RNG distribution (%s) expects 2 parameters, but got %ld\",\n              RandomDistribution_Name(distribution), parameters.size())",
          "new_text": null,
          "old_line_content": "          return InvalidArgument(",
          "new_line_content": "      case RandomDistribution::RNG_UNIFORM:",
          "content_same": false
        },
        {
          "line": 2863,
          "old_api": "LOG",
          "new_api": null,
          "old_text": "LOG(FATAL)",
          "new_text": null,
          "old_line_content": "        LOG(FATAL) << \"unhandled distribution \" << distribution;",
          "new_line_content": "        break;",
          "content_same": false
        },
        {
          "line": 4911,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->Conv(lhs, rhs, window_strides, padding,\n                             feature_group_count, batch_group_count,\n                             precision_config, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->Conv(lhs, rhs, window_strides, padding,",
          "new_line_content": "           const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 2866,
          "old_api": "ShapeUtil::ValidateShapeWithOptionalLayout(shape)",
          "new_api": null,
          "old_text": "ShapeUtil::ValidateShapeWithOptionalLayout(shape)",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2867,
          "old_api": "RngOpInternal",
          "new_api": null,
          "old_text": "RngOpInternal(distribution, parameters, shape)",
          "new_text": null,
          "old_line_content": "    return RngOpInternal(distribution, parameters, shape);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4922,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->ConvWithGeneralPadding(\n      lhs, rhs, window_strides, padding, feature_group_count, batch_group_count,\n      precision_config, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->ConvWithGeneralPadding(",
          "new_line_content": "    const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 2875,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                          const Shape& shape) {",
          "content_same": false
        },
        {
          "line": 2876,
          "old_api": "set_distribution",
          "new_api": null,
          "old_text": "instr.set_distribution(distribution)",
          "new_text": null,
          "old_line_content": "  instr.set_distribution(distribution);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 2882,
          "old_api": "RngOp",
          "new_api": null,
          "old_text": "RngOp(RandomDistribution::RNG_NORMAL, {mu, sigma}, shape)",
          "new_text": null,
          "old_line_content": "  return RngOp(RandomDistribution::RNG_NORMAL, {mu, sigma}, shape);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4933,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->ConvWithGeneralDimensions(\n      lhs, rhs, window_strides, padding, dimension_numbers, feature_group_count,\n      batch_group_count, precision_config, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->ConvWithGeneralDimensions(",
          "new_line_content": "    const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 2886,
          "old_api": "RngOp",
          "new_api": null,
          "old_text": "RngOp(RandomDistribution::RNG_UNIFORM, {a, b}, shape)",
          "new_text": null,
          "old_line_content": "  return RngOp(RandomDistribution::RNG_UNIFORM, {a, b}, shape);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2891,
          "old_api": "set_element_type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));\n    TF_ASSIGN_OR_RETURN(Shape state_shape, GetShape(initial_state));\n    Shape output_shape = shape;\n    output_shape.set_element_type(PRIMITIVE_TYPE_INVALID);\n    if (primitive_util::IsArrayType(shape.element_type())) {\n      output_shape.set_element_type(\n          primitive_util::UnsignedIntegralTypeForBitWidth(\n              primitive_util::BitWidth(shape.element_type())));\n    }\n    if (!primitive_util::IsUnsignedIntegralType(output_shape.element_type())) {\n      return InvalidArgument(\"Unsupported shape for RngBitGenerator: %s\",\n                             PrimitiveType_Name(shape.element_type()));\n    }\n    return RngBitGeneratorInternal(\n        ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape}),\n        algorithm, initial_state);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::RngBitGenerator(RandomAlgorithm algorithm,",
          "content_same": false
        },
        {
          "line": 2892,
          "old_api": "ShapeUtil::ValidateShapeWithOptionalLayout(shape)",
          "new_api": null,
          "old_text": "ShapeUtil::ValidateShapeWithOptionalLayout(shape)",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(ShapeUtil::ValidateShapeWithOptionalLayout(shape));",
          "new_line_content": "                                  XlaOp initial_state, const Shape& shape) {",
          "content_same": false
        },
        {
          "line": 2895,
          "old_api": "set_element_type",
          "new_api": null,
          "old_text": "output_shape.set_element_type(PRIMITIVE_TYPE_INVALID)",
          "new_text": null,
          "old_line_content": "    output_shape.set_element_type(PRIMITIVE_TYPE_INVALID);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape state_shape, GetShape(initial_state));",
          "content_same": false
        },
        {
          "line": 2896,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape.element_type()",
          "new_text": null,
          "old_line_content": "    if (primitive_util::IsArrayType(shape.element_type())) {",
          "new_line_content": "    Shape output_shape = shape;",
          "content_same": false
        },
        {
          "line": 4945,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->ConvGeneral(\n      lhs, rhs, window_strides, padding, dimension_numbers, feature_group_count,\n      batch_group_count, precision_config, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->ConvGeneral(",
          "new_line_content": "                  const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 2902,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "InvalidArgument(\"Unsupported shape for RngBitGenerator: %s\",\n                             PrimitiveType_Name(shape.element_type()))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"Unsupported shape for RngBitGenerator: %s\",",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2906,
          "old_api": "ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape})",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape})",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeTupleShapeWithPtrs({&state_shape, &output_shape}),",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4960,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->ConvGeneralDilated(\n      lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\n      dimension_numbers, feature_group_count, batch_group_count,\n      precision_config, preferred_element_type, window_reversal)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->ConvGeneralDilated(",
          "new_line_content": "                         std::optional<PrimitiveType> preferred_element_type,",
          "content_same": false
        },
        {
          "line": 2915,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "full_result_shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = full_result_shape.ToProto();",
          "new_line_content": "    XlaOp initial_state) {",
          "content_same": false
        },
        {
          "line": 2916,
          "old_api": "set_rng_algorithm",
          "new_api": null,
          "old_text": "instr.set_rng_algorithm(algorithm)",
          "new_text": null,
          "old_line_content": "  instr.set_rng_algorithm(algorithm);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 2923,
          "old_api": "GetProgramShape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Infer shape.\n    TF_ASSIGN_OR_RETURN(const auto& body_program_shape, body.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(const auto& condition_program_shape,\n                        condition.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(const Shape* init_shape, GetShapePtr(init));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferWhileShape(\n                                         condition_program_shape,\n                                         body_program_shape, *init_shape));\n    return WhileInternal(shape, condition, body, init);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::While(const XlaComputation& condition,",
          "content_same": false
        },
        {
          "line": 4976,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->DynamicConvInputGrad(\n      input_sizes, lhs, rhs, window_strides, padding, lhs_dilation,\n      rhs_dilation, dimension_numbers, feature_group_count, batch_group_count,\n      precision_config, padding_type, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->DynamicConvInputGrad(",
          "new_line_content": "    const PrecisionConfig* precision_config, PaddingType padding_type,",
          "content_same": false
        },
        {
          "line": 2932,
          "old_api": "WhileInternal",
          "new_api": null,
          "old_text": "WhileInternal(shape, condition, body, init)",
          "new_text": null,
          "old_line_content": "    return WhileInternal(shape, condition, body, init);",
          "new_line_content": "                                         condition_program_shape,",
          "content_same": false
        },
        {
          "line": 2941,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                          XlaOp init) {",
          "content_same": false
        },
        {
          "line": 2944,
          "old_api": "AddCalledComputation",
          "new_api": null,
          "old_text": "AddCalledComputation(condition, &instr)",
          "new_text": null,
          "old_line_content": "  AddCalledComputation(condition, &instr);",
          "new_line_content": "  // Body comes before condition computation in the vector.",
          "content_same": false
        },
        {
          "line": 4992,
          "old_api": "builder",
          "new_api": null,
          "old_text": "activations.builder()->DynamicConvKernelGrad(\n      activations, gradients, window_strides, padding, lhs_dilation,\n      rhs_dilation, dimension_numbers, feature_group_count, batch_group_count,\n      precision_config, padding_type, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return activations.builder()->DynamicConvKernelGrad(",
          "new_line_content": "    const PrecisionConfig* precision_config, PaddingType padding_type,",
          "content_same": false
        },
        {
          "line": 2952,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* input_shape, GetShapePtr(input));\n    TF_ASSIGN_OR_RETURN(const Shape* start_indices_shape,\n                        GetShapePtr(start_indices));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferGatherShape(\n                                         *input_shape, *start_indices_shape,\n                                         dimension_numbers, slice_sizes));\n    return GatherInternal(shape, input, start_indices, dimension_numbers,\n                          slice_sizes, indices_are_sorted);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                         absl::Span<const int64_t> slice_sizes,",
          "content_same": false
        },
        {
          "line": 2959,
          "old_api": "GatherInternal",
          "new_api": null,
          "old_text": "GatherInternal(shape, input, start_indices, dimension_numbers,\n                          slice_sizes, indices_are_sorted)",
          "new_text": null,
          "old_line_content": "    return GatherInternal(shape, input, start_indices, dimension_numbers,",
          "new_line_content": "                                         *input_shape, *start_indices_shape,",
          "content_same": false
        },
        {
          "line": 5008,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->DynamicConvForward(\n      lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\n      dimension_numbers, feature_group_count, batch_group_count,\n      precision_config, padding_type, preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->DynamicConvForward(",
          "new_line_content": "                         PaddingType padding_type,",
          "content_same": false
        },
        {
          "line": 5016,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Fft(operand, fft_type, fft_length)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Fft(operand, fft_type, fft_length);",
          "new_line_content": "XlaOp Fft(const XlaOp operand, FftType fft_type,",
          "content_same": false
        },
        {
          "line": 2969,
          "old_api": "set_indices_are_sorted",
          "new_api": null,
          "old_text": "instr.set_indices_are_sorted(indices_are_sorted)",
          "new_text": null,
          "old_line_content": "  instr.set_indices_are_sorted(indices_are_sorted);",
          "new_line_content": "    absl::Span<const int64_t> slice_sizes, bool indices_are_sorted) {",
          "content_same": false
        },
        {
          "line": 2970,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5022,
          "old_api": "builder",
          "new_api": null,
          "old_text": "a.builder()",
          "new_text": null,
          "old_line_content": "  XlaBuilder* builder = a.builder();",
          "new_line_content": "                      bool unit_diagonal,",
          "content_same": false
        },
        {
          "line": 5023,
          "old_api": "set_left_side",
          "new_api": null,
          "old_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* a_shape, builder->GetShapePtr(a));\n    TF_ASSIGN_OR_RETURN(const Shape* b_shape, builder->GetShapePtr(b));\n    TriangularSolveOptions options;\n    options.set_left_side(left_side);\n    options.set_lower(lower);\n    options.set_unit_diagonal(unit_diagonal);\n    options.set_transpose_a(transpose_a);\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferTriangularSolveShape(\n                                         *a_shape, *b_shape, options));\n    return builder->TriangularSolveInternal(shape, a, b, std::move(options));\n  })",
          "new_text": null,
          "old_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                      TriangularSolveOptions::Transpose transpose_a) {",
          "content_same": false
        },
        {
          "line": 2976,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kGather,",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5027,
          "old_api": "set_left_side",
          "new_api": null,
          "old_text": "options.set_left_side(left_side)",
          "new_text": null,
          "old_line_content": "    options.set_left_side(left_side);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* b_shape, builder->GetShapePtr(b));",
          "content_same": false
        },
        {
          "line": 5028,
          "old_api": "set_lower",
          "new_api": null,
          "old_text": "options.set_lower(lower)",
          "new_text": null,
          "old_line_content": "    options.set_lower(lower);",
          "new_line_content": "    TriangularSolveOptions options;",
          "content_same": false
        },
        {
          "line": 2984,
          "old_api": "absl::MakeConstSpan(&input, 1)",
          "new_api": null,
          "old_text": "absl::MakeConstSpan(&input, 1)",
          "new_text": null,
          "old_line_content": "  return Scatter(absl::MakeConstSpan(&input, 1), scatter_indices,",
          "new_line_content": "                          const ScatterDimensionNumbers& dimension_numbers,",
          "content_same": false
        },
        {
          "line": 2985,
          "old_api": "absl::MakeConstSpan(&updates, 1)",
          "new_api": null,
          "old_text": "absl::MakeConstSpan(&updates, 1)",
          "new_text": null,
          "old_line_content": "                 absl::MakeConstSpan(&updates, 1), update_computation,",
          "new_line_content": "                          bool indices_are_sorted, bool unique_indices) {",
          "content_same": false
        },
        {
          "line": 5033,
          "old_api": "std::move(options)",
          "new_api": null,
          "old_text": "std::move(options)",
          "new_text": null,
          "old_line_content": "    return builder->TriangularSolveInternal(shape, a, b, std::move(options));",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferTriangularSolveShape(",
          "content_same": false
        },
        {
          "line": 5038,
          "old_api": "builder",
          "new_api": null,
          "old_text": "a.builder()",
          "new_text": null,
          "old_line_content": "  XlaBuilder* builder = a.builder();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5039,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* a_shape, builder->GetShapePtr(a));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferCholeskyShape(*a_shape));\n    return builder->CholeskyInternal(shape, a, lower);\n  })",
          "new_text": null,
          "old_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp Cholesky(XlaOp a, bool lower) {",
          "content_same": false
        },
        {
          "line": 2994,
          "old_api": "empty",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (inputs.empty()) {\n      return InvalidArgument(\"Scatter inputs cannot be empty.\");\n    }\n    if (inputs.size() != updates.size()) {\n      return InvalidArgument(\n          \"Scatter should have same number of inputs and updates: %d vs %d.\",\n          inputs.size(), updates.size());\n    }\n    absl::InlinedVector<const Shape*, 3> operand_shapes;\n    operand_shapes.reserve(inputs.size() + 1 + updates.size());\n    for (const XlaOp& input : inputs) {\n      TF_ASSIGN_OR_RETURN(const Shape* input_shape, GetShapePtr(input));\n      operand_shapes.push_back(input_shape);\n    }\n    TF_ASSIGN_OR_RETURN(const Shape* scatter_indices_shape,\n                        GetShapePtr(scatter_indices));\n    operand_shapes.push_back(scatter_indices_shape);\n    for (const XlaOp& update : updates) {\n      TF_ASSIGN_OR_RETURN(const Shape* update_shape, GetShapePtr(update));\n      operand_shapes.push_back(update_shape);\n    }\n    TF_ASSIGN_OR_RETURN(const ProgramShape& to_apply_shape,\n                        update_computation.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferScatterShape(\n                            operand_shapes, to_apply_shape, dimension_numbers));\n    return ScatterInternal(shape, inputs, scatter_indices, updates,\n                           update_computation, dimension_numbers,\n                           indices_are_sorted, unique_indices);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                          const ScatterDimensionNumbers& dimension_numbers,",
          "content_same": false
        },
        {
          "line": 2995,
          "old_api": "empty",
          "new_api": null,
          "old_text": "inputs.empty()",
          "new_text": null,
          "old_line_content": "    if (inputs.empty()) {",
          "new_line_content": "                          bool indices_are_sorted, bool unique_indices) {",
          "content_same": false
        },
        {
          "line": 5043,
          "old_api": "CholeskyInternal",
          "new_api": null,
          "old_text": "builder->CholeskyInternal(shape, a, lower)",
          "new_text": null,
          "old_line_content": "    return builder->CholeskyInternal(shape, a, lower);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape,",
          "content_same": false
        },
        {
          "line": 2999,
          "old_api": "size",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Scatter should have same number of inputs and updates: %d vs %d.\",\n          inputs.size(), updates.size())",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5049,
          "old_api": "Infeed",
          "new_api": null,
          "old_text": "builder->Infeed(shape, config)",
          "new_text": null,
          "old_line_content": "  return builder->Infeed(shape, config);",
          "new_line_content": "XlaOp Infeed(XlaBuilder* builder, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 3004,
          "old_api": "size",
          "new_api": null,
          "old_text": "updates.size()",
          "new_text": null,
          "old_line_content": "    operand_shapes.reserve(inputs.size() + 1 + updates.size());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5054,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Outfeed(operand, shape_with_layout, outfeed_config)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Outfeed(operand, shape_with_layout, outfeed_config);",
          "new_line_content": "void Outfeed(const XlaOp operand, const Shape& shape_with_layout,",
          "content_same": false
        },
        {
          "line": 3007,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operand_shapes.push_back(input_shape)",
          "new_text": null,
          "old_line_content": "      operand_shapes.push_back(input_shape);",
          "new_line_content": "    for (const XlaOp& input : inputs) {",
          "content_same": false
        },
        {
          "line": 3011,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operand_shapes.push_back(scatter_indices_shape)",
          "new_text": null,
          "old_line_content": "    operand_shapes.push_back(scatter_indices_shape);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* scatter_indices_shape,",
          "content_same": false
        },
        {
          "line": 5059,
          "old_api": "Call",
          "new_api": null,
          "old_text": "builder->Call(computation, operands)",
          "new_text": null,
          "old_line_content": "  return builder->Call(computation, operands);",
          "new_line_content": "XlaOp Call(XlaBuilder* builder, const XlaComputation& computation,",
          "content_same": false
        },
        {
          "line": 3014,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operand_shapes.push_back(update_shape)",
          "new_text": null,
          "old_line_content": "      operand_shapes.push_back(update_shape);",
          "new_line_content": "    for (const XlaOp& update : updates) {",
          "content_same": false
        },
        {
          "line": 3021,
          "old_api": "ScatterInternal",
          "new_api": null,
          "old_text": "ScatterInternal(shape, inputs, scatter_indices, updates,\n                           update_computation, dimension_numbers,\n                           indices_are_sorted, unique_indices)",
          "new_text": null,
          "old_line_content": "    return ScatterInternal(shape, inputs, scatter_indices, updates,",
          "new_line_content": "                        ShapeInference::InferScatterShape(",
          "content_same": false
        },
        {
          "line": 5070,
          "old_api": "CustomCall",
          "new_api": null,
          "old_text": "builder->CustomCall(call_target_name, operands, shape, opaque,\n                             /*operand_shapes_with_layout=*/std::nullopt,\n                             has_side_effect, output_operand_aliasing, literal,\n                             /*window=*/std::nullopt, /*dnums=*/std::nullopt,\n                             schedule, api_version)",
          "new_text": null,
          "old_line_content": "  return builder->CustomCall(call_target_name, operands, shape, opaque,",
          "new_line_content": "    const Literal* literal, CustomCallSchedule schedule,",
          "content_same": false
        },
        {
          "line": 3032,
          "old_api": "set_indices_are_sorted",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    instr.set_indices_are_sorted(indices_are_sorted);\n    instr.set_unique_indices(unique_indices);\n    *instr.mutable_shape() = shape.ToProto();\n    *instr.mutable_scatter_dimension_numbers() = dimension_numbers;\n\n    AddCalledComputation(update_computation, &instr);\n    absl::InlinedVector<XlaOp, 3> operands;\n    operands.reserve(inputs.size() + 1 + updates.size());\n    absl::c_copy(inputs, std::back_inserter(operands));\n    operands.push_back(scatter_indices);\n    absl::c_copy(updates, std::back_inserter(operands));\n    return AddInstruction(std::move(instr), HloOpcode::kScatter, operands);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const ScatterDimensionNumbers& dimension_numbers, bool indices_are_sorted,",
          "content_same": false
        },
        {
          "line": 3035,
          "old_api": "set_unique_indices",
          "new_api": null,
          "old_text": "instr.set_unique_indices(unique_indices)",
          "new_text": null,
          "old_line_content": "    instr.set_unique_indices(unique_indices);",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5085,
          "old_api": "CustomCall",
          "new_api": null,
          "old_text": "builder->CustomCall(\n      call_target_name, operands, computation, shape, opaque,\n      /*operand_shapes_with_layout=*/std::nullopt, has_side_effect,\n      output_operand_aliasing, literal, schedule, api_version)",
          "new_text": null,
          "old_line_content": "  return builder->CustomCall(",
          "new_line_content": "    const Literal* literal, CustomCallSchedule schedule,",
          "content_same": false
        },
        {
          "line": 3042,
          "old_api": "std::back_inserter(operands)",
          "new_api": null,
          "old_text": "std::back_inserter(operands)",
          "new_text": null,
          "old_line_content": "    absl::c_copy(inputs, std::back_inserter(operands));",
          "new_line_content": "    absl::InlinedVector<XlaOp, 3> operands;",
          "content_same": false
        },
        {
          "line": 5100,
          "old_api": "CustomCall",
          "new_api": null,
          "old_text": "builder->CustomCall(\n      call_target_name, operands, shape, opaque, operand_shapes_with_layout,\n      has_side_effect, output_operand_aliasing, literal,\n      /*window=*/std::nullopt, /*dnums=*/std::nullopt, schedule, api_version)",
          "new_text": null,
          "old_line_content": "  return builder->CustomCall(",
          "new_line_content": "    const Literal* literal, CustomCallSchedule schedule,",
          "content_same": false
        },
        {
          "line": 3053,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(predicate));\n\n    if (!ShapeUtil::IsScalar(*shape) || shape->element_type() != PRED) {\n      return InvalidArgument(\n          \"Argument to predicated-Conditional is not a scalar of PRED type \"\n          \"(%s).\",\n          ShapeUtil::HumanString(*shape));\n    }\n    // The index of true_computation must be 0 and that of false computation\n    // must be 1.\n    return ConditionalImpl(predicate, {&true_computation, &false_computation},\n                           {true_operand, false_operand});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                              XlaOp false_operand,",
          "content_same": false
        },
        {
          "line": 3056,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape->element_type()",
          "new_text": null,
          "old_line_content": "    if (!ShapeUtil::IsScalar(*shape) || shape->element_type() != PRED) {",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(predicate));",
          "content_same": false
        },
        {
          "line": 3057,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Argument to predicated-Conditional is not a scalar of PRED type \"\n          \"(%s).\",\n          ShapeUtil::HumanString(*shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3060,
          "old_api": "ShapeUtil::HumanString(*shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanString(*shape)",
          "new_text": null,
          "old_line_content": "          ShapeUtil::HumanString(*shape));",
          "new_line_content": "          \"Argument to predicated-Conditional is not a scalar of PRED type \"",
          "content_same": false
        },
        {
          "line": 3064,
          "old_api": "ConditionalImpl",
          "new_api": null,
          "old_text": "ConditionalImpl(predicate, {&true_computation, &false_computation},\n                           {true_operand, false_operand})",
          "new_text": null,
          "old_line_content": "    return ConditionalImpl(predicate, {&true_computation, &false_computation},",
          "new_line_content": "    // The index of true_computation must be 0 and that of false computation",
          "content_same": false
        },
        {
          "line": 5116,
          "old_api": "empty",
          "new_api": null,
          "old_text": "operand_shapes_with_layout.empty()",
          "new_text": null,
          "old_line_content": "  if (!operand_shapes_with_layout.empty()) {",
          "new_line_content": "    CustomCallSchedule schedule, CustomCallApiVersion api_version) {",
          "content_same": false
        },
        {
          "line": 5119,
          "old_api": "CustomCall",
          "new_api": null,
          "old_text": "builder->CustomCall(call_target_name, operands, shape, opaque,\n                             maybe_operand_shapes, has_side_effect,\n                             output_operand_aliasing, literal, window, dnums,\n                             schedule, api_version)",
          "new_text": null,
          "old_line_content": "  return builder->CustomCall(call_target_name, operands, shape, opaque,",
          "new_line_content": "    maybe_operand_shapes = operand_shapes_with_layout;",
          "content_same": false
        },
        {
          "line": 3073,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(branch_index));\n\n    if (!ShapeUtil::IsScalar(*shape) || shape->element_type() != S32) {\n      return InvalidArgument(\n          \"Argument to indexed-Conditional is not a scalar of S32 type (%s).\",\n          ShapeUtil::HumanString(*shape));\n    }\n    return ConditionalImpl(branch_index, branch_computations, branch_operands);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    absl::Span<const XlaComputation* const> branch_computations,",
          "content_same": false
        },
        {
          "line": 3076,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape->element_type()",
          "new_text": null,
          "old_line_content": "    if (!ShapeUtil::IsScalar(*shape) || shape->element_type() != S32) {",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(branch_index));",
          "content_same": false
        },
        {
          "line": 3077,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Argument to indexed-Conditional is not a scalar of S32 type (%s).\",\n          ShapeUtil::HumanString(*shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5126,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->OptimizationBarrier(operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->OptimizationBarrier(operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5131,
          "old_api": "builder",
          "new_api": null,
          "old_text": "real.builder()->BinaryOp(HloOpcode::kComplex, real, imag,\n                                  broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return real.builder()->BinaryOp(HloOpcode::kComplex, real, imag,",
          "new_line_content": "XlaOp Complex(const XlaOp real, const XlaOp imag,",
          "content_same": false
        },
        {
          "line": 5136,
          "old_api": "Imag",
          "new_api": null,
          "old_text": "Imag(operand)",
          "new_text": null,
          "old_line_content": "  return Complex(Real(operand), Neg(Imag(operand)));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3092,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<const Shape*> operand_shapes;\n    std::vector<XlaOp> operands;\n    if (operand_shape->IsTuple()) {\n      if (operand_shape->tuple_shapes_size() == 0) {\n        return Unimplemented(\"0 element tuple AllReduce is not supported\");\n      }\n      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {\n        if (operand_shape->tuple_shapes(i).element_type() !=\n            operand_shape->tuple_shapes(0).element_type()) {\n          return Unimplemented(\n              \"All the shapes of a tuple input of AllReduce must have the same \"\n              \"element type\");\n        }\n        operand_shapes.push_back(&operand_shape->tuple_shapes(i));\n        operands.push_back(GetTupleElement(operand, i));\n      }\n    } else {\n      operand_shapes.push_back(operand_shape);\n      operands.push_back(operand);\n    }\n\n    TF_ASSIGN_OR_RETURN(Shape inferred_shape,\n                        ShapeInference::InferAllReduceShape(operand_shapes));\n    if (layout) {\n      if (!LayoutUtil::HasLayout(*layout)) {\n        return InvalidArgument(\"shape_with_layout must have the layout set: %s\",\n                               layout->ToString());\n      }\n      if (!ShapeUtil::Compatible(*layout, *operand_shape)) {\n        return InvalidArgument(\n            \"Provided shape_with_layout must be compatible with the \"\n            \"operand shape: %s vs %s\",\n            layout->ToString(), operand_shape->ToString());\n      }\n      instr.set_constrain_layout(true);\n      if (operand_shape->IsTuple() && !inferred_shape.IsTuple()) {\n        // For a single-element tuple, take the tuple element shape.\n        TF_RET_CHECK(layout->tuple_shapes_size() == 1);\n        *instr.mutable_shape() = layout->tuple_shapes(0).ToProto();\n      } else {\n        *instr.mutable_shape() = layout->ToProto();\n      }\n    } else {\n      *instr.mutable_shape() = inferred_shape.ToProto();\n    }\n\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n\n    if (use_global_device_ids.has_value()) {\n      instr.set_use_global_device_ids(*use_global_device_ids);\n    }\n\n    AddCalledComputation(computation, &instr);\n\n    TF_ASSIGN_OR_RETURN(auto all_reduce,\n                        AddInstruction(std::move(instr),\n                                       async ? HloOpcode::kAllReduceStart\n                                             : HloOpcode::kAllReduce,\n                                       operands));\n    if (operand_shape->IsTuple() && !inferred_shape.IsTuple()) {\n      // For a single-element tuple, wrap the result into a tuple.\n      TF_RET_CHECK(operand_shapes.size() == 1);\n      TF_RET_CHECK(ShapeUtil::Compatible(*operand_shapes[0], inferred_shape));\n      return Tuple({all_reduce});\n    }\n    return all_reduce;\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                                const std::optional<bool> use_global_device_ids,",
          "content_same": false
        },
        {
          "line": 5141,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kAdd, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kAdd, lhs, rhs,",
          "new_line_content": "XlaOp Add(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3097,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "operand_shape->IsTuple()",
          "new_text": null,
          "old_line_content": "    if (operand_shape->IsTuple()) {",
          "new_line_content": "    std::vector<const Shape*> operand_shapes;",
          "content_same": false
        },
        {
          "line": 3098,
          "old_api": "tuple_shapes_size",
          "new_api": null,
          "old_text": "operand_shape->tuple_shapes_size()",
          "new_text": null,
          "old_line_content": "      if (operand_shape->tuple_shapes_size() == 0) {",
          "new_line_content": "    std::vector<XlaOp> operands;",
          "content_same": false
        },
        {
          "line": 5147,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kSubtract, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kSubtract, lhs, rhs,",
          "new_line_content": "XlaOp Sub(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3102,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "operand_shape->tuple_shapes(i).element_type()",
          "new_text": null,
          "old_line_content": "        if (operand_shape->tuple_shapes(i).element_type() !=",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 5153,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kMultiply, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kMultiply, lhs, rhs,",
          "new_line_content": "XlaOp Mul(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3108,
          "old_api": "tuple_shapes",
          "new_api": null,
          "old_text": "operand_shape->tuple_shapes(i)",
          "new_text": null,
          "old_line_content": "        operand_shapes.push_back(&operand_shape->tuple_shapes(i));",
          "new_line_content": "              \"element type\");",
          "content_same": false
        },
        {
          "line": 3109,
          "old_api": "GetTupleElement",
          "new_api": null,
          "old_text": "GetTupleElement(operand, i)",
          "new_text": null,
          "old_line_content": "        operands.push_back(GetTupleElement(operand, i));",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 5159,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kDivide, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kDivide, lhs, rhs,",
          "new_line_content": "XlaOp Div(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3112,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operand_shapes.push_back(operand_shape)",
          "new_text": null,
          "old_line_content": "      operand_shapes.push_back(operand_shape);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 3113,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operands.push_back(operand)",
          "new_text": null,
          "old_line_content": "      operands.push_back(operand);",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 5165,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kRemainder, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kRemainder, lhs, rhs,",
          "new_line_content": "XlaOp Rem(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3119,
          "old_api": "LayoutUtil::HasLayout(*layout)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(*layout)",
          "new_text": null,
          "old_line_content": "      if (!LayoutUtil::HasLayout(*layout)) {",
          "new_line_content": "                        ShapeInference::InferAllReduceShape(operand_shapes));",
          "content_same": false
        },
        {
          "line": 3120,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "InvalidArgument(\"shape_with_layout must have the layout set: %s\",\n                               layout->ToString())",
          "new_text": null,
          "old_line_content": "        return InvalidArgument(\"shape_with_layout must have the layout set: %s\",",
          "new_line_content": "    if (layout) {",
          "content_same": false
        },
        {
          "line": 5171,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kMaximum, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kMaximum, lhs, rhs,",
          "new_line_content": "XlaOp Max(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3124,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "InvalidArgument(\n            \"Provided shape_with_layout must be compatible with the \"\n            \"operand shape: %s vs %s\",\n            layout->ToString(), operand_shape->ToString())",
          "new_text": null,
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 3127,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "operand_shape->ToString()",
          "new_text": null,
          "old_line_content": "            layout->ToString(), operand_shape->ToString());",
          "new_line_content": "            \"Provided shape_with_layout must be compatible with the \"",
          "content_same": false
        },
        {
          "line": 5177,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kMinimum, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kMinimum, lhs, rhs,",
          "new_line_content": "XlaOp Min(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3130,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "inferred_shape.IsTuple()",
          "new_text": null,
          "old_line_content": "      if (operand_shape->IsTuple() && !inferred_shape.IsTuple()) {",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 3133,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "layout->tuple_shapes(0).ToProto()",
          "new_text": null,
          "old_line_content": "        *instr.mutable_shape() = layout->tuple_shapes(0).ToProto();",
          "new_line_content": "        // For a single-element tuple, take the tuple element shape.",
          "content_same": false
        },
        {
          "line": 5183,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kAnd, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kAnd, lhs, rhs,",
          "new_line_content": "XlaOp And(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3138,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "inferred_shape.ToProto()",
          "new_text": null,
          "old_line_content": "      *instr.mutable_shape() = inferred_shape.ToProto();",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 5189,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kOr, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kOr, lhs, rhs,",
          "new_line_content": "XlaOp Or(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3142,
          "old_api": "add_replica_groups",
          "new_api": null,
          "old_text": "instr.add_replica_groups()",
          "new_text": null,
          "old_line_content": "      *instr.add_replica_groups() = group;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3145,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "channel_id.has_value()",
          "new_text": null,
          "old_line_content": "    if (channel_id.has_value()) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3146,
          "old_api": "handle",
          "new_api": null,
          "old_text": "channel_id->handle()",
          "new_text": null,
          "old_line_content": "      instr.set_channel_id(channel_id->handle());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5195,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kXor, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kXor, lhs, rhs,",
          "new_line_content": "XlaOp Xor(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3149,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "use_global_device_ids.has_value()",
          "new_text": null,
          "old_line_content": "    if (use_global_device_ids.has_value()) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3150,
          "old_api": "set_use_global_device_ids",
          "new_api": null,
          "old_text": "instr.set_use_global_device_ids(*use_global_device_ids)",
          "new_text": null,
          "old_line_content": "      instr.set_use_global_device_ids(*use_global_device_ids);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5200,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kNot, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kNot, operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3153,
          "old_api": "AddCalledComputation",
          "new_api": null,
          "old_text": "AddCalledComputation(computation, &instr)",
          "new_text": null,
          "old_line_content": "    AddCalledComputation(computation, &instr);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5204,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kPopulationCount, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kPopulationCount, operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5209,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kShiftLeft, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kShiftLeft, lhs, rhs,",
          "new_line_content": "XlaOp ShiftLeft(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3162,
          "old_api": "size",
          "new_api": null,
          "old_text": "operand_shapes.size()",
          "new_text": null,
          "old_line_content": "      TF_RET_CHECK(operand_shapes.size() == 1);",
          "new_line_content": "    if (operand_shape->IsTuple() && !inferred_shape.IsTuple()) {",
          "content_same": false
        },
        {
          "line": 3163,
          "old_api": "ShapeUtil::Compatible(*operand_shapes[0], inferred_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::Compatible(*operand_shapes[0], inferred_shape)",
          "new_text": null,
          "old_line_content": "      TF_RET_CHECK(ShapeUtil::Compatible(*operand_shapes[0], inferred_shape));",
          "new_line_content": "      // For a single-element tuple, wrap the result into a tuple.",
          "content_same": false
        },
        {
          "line": 5215,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kShiftRightArithmetic, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kShiftRightArithmetic, lhs, rhs,",
          "new_line_content": "XlaOp ShiftRightArithmetic(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 5221,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kShiftRightLogical, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kShiftRightLogical, lhs, rhs,",
          "new_line_content": "XlaOp ShiftRightLogical(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3178,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n\n    std::vector<const Shape*> operand_shapes;\n    std::vector<XlaOp> operands;\n    if (operand_shape->IsTuple()) {\n      if (operand_shape->tuple_shapes_size() == 0) {\n        return Unimplemented(\"0 element tuple AllGather is not supported\");\n      }\n      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {\n        operand_shapes.push_back(&operand_shape->tuple_shapes(i));\n        operands.push_back(GetTupleElement(operand, i));\n      }\n    } else {\n      operand_shapes.push_back(operand_shape);\n      operands.push_back(operand);\n    }\n\n    TF_ASSIGN_OR_RETURN(Shape inferred_shape,\n                        ShapeInference::InferAllGatherShape(\n                            operand_shapes, all_gather_dimension, shard_count));\n    if (layout) {\n      *inferred_shape.mutable_layout() = *layout;\n      instr.set_constrain_layout(true);\n    }\n    *instr.mutable_shape() = inferred_shape.ToProto();\n\n    instr.add_dimensions(all_gather_dimension);\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n    if (use_global_device_ids.has_value()) {\n      instr.set_use_global_device_ids(use_global_device_ids.value());\n    }\n\n    TF_ASSIGN_OR_RETURN(auto all_gather,\n                        AddInstruction(std::move(instr),\n                                       async ? HloOpcode::kAllGatherStart\n                                             : HloOpcode::kAllGather,\n                                       operands));\n    return all_gather;\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                                const std::optional<bool> use_global_device_ids,",
          "content_same": false
        },
        {
          "line": 5228,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Reduce(operand, init_value, computation,\n                                   dimensions_to_reduce)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Reduce(operand, init_value, computation,",
          "new_line_content": "             const XlaComputation& computation,",
          "content_same": false
        },
        {
          "line": 3184,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "operand_shape->IsTuple()",
          "new_text": null,
          "old_line_content": "    if (operand_shape->IsTuple()) {",
          "new_line_content": "    std::vector<const Shape*> operand_shapes;",
          "content_same": false
        },
        {
          "line": 3185,
          "old_api": "tuple_shapes_size",
          "new_api": null,
          "old_text": "operand_shape->tuple_shapes_size()",
          "new_text": null,
          "old_line_content": "      if (operand_shape->tuple_shapes_size() == 0) {",
          "new_line_content": "    std::vector<XlaOp> operands;",
          "content_same": false
        },
        {
          "line": 3189,
          "old_api": "tuple_shapes",
          "new_api": null,
          "old_text": "operand_shape->tuple_shapes(i)",
          "new_text": null,
          "old_line_content": "        operand_shapes.push_back(&operand_shape->tuple_shapes(i));",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 5238,
          "old_api": "Reduce",
          "new_api": null,
          "old_text": "builder->Reduce(operands, init_values, computation,\n                         dimensions_to_reduce)",
          "new_text": null,
          "old_line_content": "  return builder->Reduce(operands, init_values, computation,",
          "new_line_content": "             const XlaComputation& computation,",
          "content_same": false
        },
        {
          "line": 3193,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operand_shapes.push_back(operand_shape)",
          "new_text": null,
          "old_line_content": "      operand_shapes.push_back(operand_shape);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 3194,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operands.push_back(operand)",
          "new_text": null,
          "old_line_content": "      operands.push_back(operand);",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 5244,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->ReduceAll(operand, init_value, computation)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->ReduceAll(operand, init_value, computation);",
          "new_line_content": "XlaOp ReduceAll(const XlaOp operand, const XlaOp init_value,",
          "content_same": false
        },
        {
          "line": 3201,
          "old_api": "mutable_layout",
          "new_api": null,
          "old_text": "inferred_shape.mutable_layout()",
          "new_text": null,
          "old_line_content": "      *inferred_shape.mutable_layout() = *layout;",
          "new_line_content": "                            operand_shapes, all_gather_dimension, shard_count));",
          "content_same": false
        },
        {
          "line": 3202,
          "old_api": "set_constrain_layout",
          "new_api": null,
          "old_text": "instr.set_constrain_layout(true)",
          "new_text": null,
          "old_line_content": "      instr.set_constrain_layout(true);",
          "new_line_content": "    if (layout) {",
          "content_same": false
        },
        {
          "line": 5251,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->ReduceWindow(operand, init_value, computation,\n                                         window_dimensions, window_strides,\n                                         padding)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->ReduceWindow(operand, init_value, computation,",
          "new_line_content": "                   absl::Span<const int64_t> window_dimensions,",
          "content_same": false
        },
        {
          "line": 3211,
          "old_api": "handle",
          "new_api": null,
          "old_text": "channel_id->handle()",
          "new_text": null,
          "old_line_content": "      instr.set_channel_id(channel_id->handle());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5261,
          "old_api": "empty",
          "new_api": null,
          "old_text": "operands.empty()",
          "new_text": null,
          "old_line_content": "  CHECK(!operands.empty());",
          "new_line_content": "                   absl::Span<const int64_t> window_dimensions,",
          "content_same": false
        },
        {
          "line": 3214,
          "old_api": "value",
          "new_api": null,
          "old_text": "use_global_device_ids.value()",
          "new_text": null,
          "old_line_content": "      instr.set_use_global_device_ids(use_global_device_ids.value());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5262,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operands[0].builder()->ReduceWindow(operands, init_values, computation,\n                                             window_dimensions, window_strides,\n                                             padding)",
          "new_text": null,
          "old_line_content": "  return operands[0].builder()->ReduceWindow(operands, init_values, computation,",
          "new_line_content": "                   absl::Span<const int64_t> window_strides, Padding padding) {",
          "content_same": false
        },
        {
          "line": 5275,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->ReduceWindowWithGeneralPadding(\n      absl::MakeSpan(&operand, 1), absl::MakeSpan(&init_value, 1), computation,\n      window_dimensions, window_strides, base_dilations, window_dilations,\n      padding)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->ReduceWindowWithGeneralPadding(",
          "new_line_content": "    absl::Span<const int64_t> window_dilations,",
          "content_same": false
        },
        {
          "line": 5276,
          "old_api": "absl::MakeSpan(&init_value, 1)",
          "new_api": null,
          "old_text": "absl::MakeSpan(&init_value, 1)",
          "new_text": null,
          "old_line_content": "      absl::MakeSpan(&operand, 1), absl::MakeSpan(&init_value, 1), computation,",
          "new_line_content": "    absl::Span<const std::pair<int64_t, int64_t>> padding) {",
          "content_same": false
        },
        {
          "line": 3230,
          "old_api": "size",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    TF_ASSIGN_OR_RETURN(const Shape* branch_index_shape,\n                        GetShapePtr(branch_index));\n    std::vector<Shape> branch_operand_shapes(branch_operands.size());\n    std::vector<ProgramShape> branch_computation_shapes(\n        branch_computations.size());\n    for (int j = 0, end = branch_operands.size(); j < end; ++j) {\n      TF_ASSIGN_OR_RETURN(branch_operand_shapes[j],\n                          GetShape(branch_operands[j]));\n      TF_ASSIGN_OR_RETURN(branch_computation_shapes[j],\n                          branch_computations[j]->GetProgramShape());\n    }\n    TF_ASSIGN_OR_RETURN(const Shape shape,\n                        ShapeInference::InferConditionalShape(\n                            *branch_index_shape, branch_computation_shapes,\n                            branch_operand_shapes));\n    *instr.mutable_shape() = shape.ToProto();\n\n    for (const XlaComputation* branch_computation : branch_computations) {\n      AddCalledComputation(*branch_computation, &instr);\n    }\n\n    std::vector<XlaOp> operands(1, branch_index);\n    for (const XlaOp branch_operand : branch_operands) {\n      operands.emplace_back(branch_operand);\n    }\n    return AddInstruction(std::move(instr), HloOpcode::kConditional,\n                          absl::MakeSpan(operands));\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    absl::Span<const XlaComputation* const> branch_computations,",
          "content_same": false
        },
        {
          "line": 1186,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "lhs_shape->element_type()",
          "new_text": null,
          "old_line_content": "        output_shape.set_element_type(lhs_shape->element_type());",
          "new_line_content": "          Shape output_shape = shape;",
          "content_same": false
        },
        {
          "line": 3235,
          "old_api": "size",
          "new_api": null,
          "old_text": "branch_operands.size()",
          "new_text": null,
          "old_line_content": "    std::vector<Shape> branch_operand_shapes(branch_operands.size());",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* branch_index_shape,",
          "content_same": false
        },
        {
          "line": 3238,
          "old_api": "size",
          "new_api": null,
          "old_text": "branch_operands.size()",
          "new_text": null,
          "old_line_content": "    for (int j = 0, end = branch_operands.size(); j < end; ++j) {",
          "new_line_content": "    std::vector<ProgramShape> branch_computation_shapes(",
          "content_same": false
        },
        {
          "line": 5289,
          "old_api": "empty",
          "new_api": null,
          "old_text": "operands.empty()",
          "new_text": null,
          "old_line_content": "  CHECK(!operands.empty());",
          "new_line_content": "    absl::Span<const int64_t> window_dilations,",
          "content_same": false
        },
        {
          "line": 5290,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operands[0].builder()->ReduceWindowWithGeneralPadding(\n      operands, init_values, computation, window_dimensions, window_strides,\n      base_dilations, window_dilations, padding)",
          "new_text": null,
          "old_line_content": "  return operands[0].builder()->ReduceWindowWithGeneralPadding(",
          "new_line_content": "    absl::Span<const std::pair<int64_t, int64_t>> padding) {",
          "content_same": false
        },
        {
          "line": 1197,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "direction.has_value()",
          "new_text": null,
          "old_line_content": "      if (!direction.has_value()) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1198,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n            \"kCompare expects a ComparisonDirection, but none provided.\")",
          "new_text": null,
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "    if (binop == HloOpcode::kCompare) {",
          "content_same": false
        },
        {
          "line": 3248,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                            *branch_index_shape, branch_computation_shapes,",
          "content_same": false
        },
        {
          "line": 1202,
          "old_api": "Compare",
          "new_api": null,
          "old_text": "Compare(shape, updated_lhs, updated_rhs, *direction)",
          "new_text": null,
          "old_line_content": "        return Compare(shape, updated_lhs, updated_rhs, *direction);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 3251,
          "old_api": "AddCalledComputation",
          "new_api": null,
          "old_text": "AddCalledComputation(*branch_computation, &instr)",
          "new_text": null,
          "old_line_content": "      AddCalledComputation(*branch_computation, &instr);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5301,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->AllGather(operand, all_gather_dimension,\n                                      shard_count, replica_groups, channel_id,\n                                      layout, use_global_device_ids)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->AllGather(operand, all_gather_dimension,",
          "new_line_content": "                const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 1208,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "direction.has_value()",
          "new_text": null,
          "old_line_content": "    if (direction.has_value()) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1209,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"A comparison direction is provided for a non-compare opcode: %s.\",\n          HloOpcodeString(binop))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3256,
          "old_api": "emplace_back",
          "new_api": null,
          "old_text": "operands.emplace_back(branch_operand)",
          "new_text": null,
          "old_line_content": "      operands.emplace_back(branch_operand);",
          "new_line_content": "    std::vector<XlaOp> operands(1, branch_index);",
          "content_same": false
        },
        {
          "line": 3259,
          "old_api": "absl::MakeSpan(operands)",
          "new_api": null,
          "old_text": "absl::MakeSpan(operands)",
          "new_text": null,
          "old_line_content": "                          absl::MakeSpan(operands));",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3264,
          "old_api": "builder",
          "new_api": null,
          "old_text": "op.builder()",
          "new_text": null,
          "old_line_content": "  if (this != op.builder()) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3265,
          "old_api": "handle",
          "new_api": null,
          "old_text": "InvalidArgument(\n        \"XlaOp with handle %d is built by builder '%s', but is trying to use \"\n        \"it in builder '%s'\",\n        op.handle(), op.builder()->name(), name())",
          "new_text": null,
          "old_line_content": "    return InvalidArgument(",
          "new_line_content": "Status XlaBuilder::CheckOpBuilder(XlaOp op) const {",
          "content_same": false
        },
        {
          "line": 5312,
          "old_api": "empty",
          "new_api": null,
          "old_text": "operands.empty()",
          "new_text": null,
          "old_line_content": "  CHECK(!operands.empty());",
          "new_line_content": "                     const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 1219,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    return AddInstruction(std::move(instr), binop, {lhs, rhs});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::BinaryOpNoBroadcast(HloOpcode binop, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 3268,
          "old_api": "name",
          "new_api": null,
          "old_text": "name()",
          "new_text": null,
          "old_line_content": "        op.handle(), op.builder()->name(), name());",
          "new_line_content": "        \"XlaOp with handle %d is built by builder '%s', but is trying to use \"",
          "content_same": false
        },
        {
          "line": 5313,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operands[0].builder()->AllGather(\n      operands[0].builder()->Tuple(operands), all_gather_dimension, shard_count,\n      replica_groups, channel_id, layout, use_global_device_ids)",
          "new_text": null,
          "old_line_content": "  return operands[0].builder()->AllGather(",
          "new_line_content": "                     const std::optional<bool> use_global_device_ids) {",
          "content_same": false
        },
        {
          "line": 1222,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), binop, {lhs, rhs});",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5320,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->CrossReplicaSum(operand, replica_groups)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->CrossReplicaSum(operand, replica_groups);",
          "new_line_content": "XlaOp CrossReplicaSum(const XlaOp operand,",
          "content_same": false
        },
        {
          "line": 3276,
          "old_api": "absl::Span<const XlaOp>({operand})",
          "new_api": null,
          "old_text": "absl::Span<const XlaOp>({operand})",
          "new_text": null,
          "old_line_content": "  return Reduce(absl::Span<const XlaOp>({operand}),",
          "new_line_content": "                         const XlaComputation& computation,",
          "content_same": false
        },
        {
          "line": 1229,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "Compare(\n      shape, lhs, rhs, direction,\n      Comparison::DefaultComparisonType(operand_shape.element_type()))",
          "new_text": null,
          "old_line_content": "  return Compare(",
          "new_line_content": "                                    ComparisonDirection direction) {",
          "content_same": false
        },
        {
          "line": 3277,
          "old_api": "absl::Span<const XlaOp>({init_value})",
          "new_api": null,
          "old_text": "absl::Span<const XlaOp>({init_value})",
          "new_text": null,
          "old_line_content": "                absl::Span<const XlaOp>({init_value}), computation,",
          "new_line_content": "                         absl::Span<const int64_t> dimensions_to_reduce) {",
          "content_same": false
        },
        {
          "line": 5328,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->AllReduce(operand, computation, replica_groups,\n                                      channel_id, shape_with_layout,\n                                      use_global_device_ids)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->AllReduce(operand, computation, replica_groups,",
          "new_line_content": "                const std::optional<Shape>& shape_with_layout,",
          "content_same": false
        },
        {
          "line": 3285,
          "old_api": "GetProgramShape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const ProgramShape& called_program_shape,\n                        computation.GetProgramShape());\n\n    std::vector<XlaOp> all_operands;\n    all_operands.insert(all_operands.end(), operands.begin(), operands.end());\n    all_operands.insert(all_operands.end(), init_values.begin(),\n                        init_values.end());\n\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes,\n                        GetOperandShapes(all_operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferReduceShape(\n            operand_shape_ptrs, dimensions_to_reduce, called_program_shape));\n    return ReduceInternal(shape, all_operands, computation,\n                          dimensions_to_reduce);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                         const XlaComputation& computation,",
          "content_same": false
        },
        {
          "line": 1238,
          "old_api": "ComparisonDirectionToString",
          "new_api": null,
          "old_text": "ComparisonDirectionToString(direction)",
          "new_text": null,
          "old_line_content": "  instr.set_comparison_direction(ComparisonDirectionToString(direction));",
          "new_line_content": "                                    Comparison::Type type) {",
          "content_same": false
        },
        {
          "line": 1239,
          "old_api": "ComparisonTypeToString",
          "new_api": null,
          "old_text": "ComparisonTypeToString(type)",
          "new_text": null,
          "old_line_content": "  instr.set_comparison_type(ComparisonTypeToString(type));",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 3290,
          "old_api": "end",
          "new_api": null,
          "old_text": "operands.end()",
          "new_text": null,
          "old_line_content": "    all_operands.insert(all_operands.end(), operands.begin(), operands.end());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3291,
          "old_api": "begin",
          "new_api": null,
          "old_text": "init_values.begin()",
          "new_text": null,
          "old_line_content": "    all_operands.insert(all_operands.end(), init_values.begin(),",
          "new_line_content": "    std::vector<XlaOp> all_operands;",
          "content_same": false
        },
        {
          "line": 5339,
          "old_api": "empty",
          "new_api": null,
          "old_text": "operands.empty()",
          "new_text": null,
          "old_line_content": "  CHECK(!operands.empty());",
          "new_line_content": "                     const std::optional<Shape>& shape_with_layout,",
          "content_same": false
        },
        {
          "line": 5340,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operands[0].builder()->AllReduce(\n      operands[0].builder()->Tuple(operands), computation, replica_groups,\n      channel_id, shape_with_layout, use_global_device_ids)",
          "new_text": null,
          "old_line_content": "  return operands[0].builder()->AllReduce(",
          "new_line_content": "                     const std::optional<bool> use_global_device_ids) {",
          "content_same": false
        },
        {
          "line": 3297,
          "old_api": "std::back_inserter(operand_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(operand_shape_ptrs)",
          "new_text": null,
          "old_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const auto& operand_shapes,",
          "content_same": false
        },
        {
          "line": 1250,
          "old_api": "is_unbounded_dynamic",
          "new_api": null,
          "old_text": "output_shape->is_unbounded_dynamic()",
          "new_text": null,
          "old_line_content": "  if (output_shape->is_unbounded_dynamic()) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1253,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": null,
          "old_text": "TF_ASSIGN_OR_RETURN(updated_output,\n                        BroadcastScalarToOutputShapeWithUnbounded(\n                            this, scalar, output, output_shape_copy))",
          "new_text": null,
          "old_line_content": "    TF_ASSIGN_OR_RETURN(updated_output,",
          "new_line_content": "    Shape output_shape_copy = *output_shape;",
          "content_same": false
        },
        {
          "line": 5351,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->ReduceScatter(\n      operand, computation, scatter_dimension, shard_count, replica_groups,\n      channel_id, layout, use_global_device_ids)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->ReduceScatter(",
          "new_line_content": "                    const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 3304,
          "old_api": "ReduceInternal",
          "new_api": null,
          "old_text": "ReduceInternal(shape, all_operands, computation,\n                          dimensions_to_reduce)",
          "new_text": null,
          "old_line_content": "    return ReduceInternal(shape, all_operands, computation,",
          "new_line_content": "        ShapeInference::InferReduceShape(",
          "content_same": false
        },
        {
          "line": 1259,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": null,
          "old_text": "TF_ASSIGN_OR_RETURN(updated_output,\n                      AddBroadcastSequence(*output_shape, updated_output))",
          "new_text": null,
          "old_line_content": "  TF_ASSIGN_OR_RETURN(updated_output,",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 1260,
          "old_api": "AddBroadcastSequence",
          "new_api": null,
          "old_text": "AddBroadcastSequence(*output_shape, updated_output)",
          "new_text": null,
          "old_line_content": "                      AddBroadcastSequence(*output_shape, updated_output));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1265,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    XlaOp updated_lhs = lhs;\n    XlaOp updated_rhs = rhs;\n    XlaOp updated_ehs = ehs;\n\n    // The client API supports implicit broadcast for kSelect and kClamp, but\n    // XLA does not support implicit broadcast. Make implicit broadcast explicit\n    // and update the operands.\n    if (triop == HloOpcode::kSelect || triop == HloOpcode::kClamp) {\n      TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n      TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n      TF_ASSIGN_OR_RETURN(const Shape* ehs_shape, GetShapePtr(ehs));\n      TF_ASSIGN_OR_RETURN(\n          std::optional<Shape> output_shape,\n          ShapeInference::InferScalarBroadcastShape(\n              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})));\n\n      // Scalar broadcast if mix of scalars and non-scalars\n      if (output_shape.has_value()) {\n        if (ShapeUtil::IsScalar(*lhs_shape)) {\n          TF_ASSIGN_OR_RETURN(\n              updated_lhs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/lhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *rhs_shape) ? rhs : ehs));\n        }\n        if (ShapeUtil::IsScalar(*rhs_shape)) {\n          TF_ASSIGN_OR_RETURN(\n              updated_rhs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/rhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : ehs));\n        }\n        if (ShapeUtil::IsScalar(*ehs_shape)) {\n          TF_ASSIGN_OR_RETURN(\n              updated_ehs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/ehs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : rhs));\n        }\n      }\n    }\n\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(updated_lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(updated_rhs));\n    TF_ASSIGN_OR_RETURN(const Shape* ehs_shape, GetShapePtr(updated_ehs));\n    TF_ASSIGN_OR_RETURN(const Shape inferred_shape,\n                        ShapeInference::InferTernaryOpShape(\n                            triop, *lhs_shape, *rhs_shape, *ehs_shape));\n\n    return AddOpWithShape(triop, inferred_shape,\n                          {updated_lhs, updated_rhs, updated_ehs});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3313,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n\n    for (int64_t dim : dimensions_to_reduce) {\n      instr.add_dimensions(dim);\n    }\n\n    AddCalledComputation(computation, &instr);\n    return AddInstruction(std::move(instr), HloOpcode::kReduce, all_operands);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const XlaComputation& computation,",
          "content_same": false
        },
        {
          "line": 5361,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->AllToAll(operand, split_dimension, concat_dimension,\n                                     split_count, replica_groups, layout,\n                                     channel_id)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->AllToAll(operand, split_dimension, concat_dimension,",
          "new_line_content": "               const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 3318,
          "old_api": "add_dimensions",
          "new_api": null,
          "old_text": "instr.add_dimensions(dim)",
          "new_text": null,
          "old_line_content": "      instr.add_dimensions(dim);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3321,
          "old_api": "AddCalledComputation",
          "new_api": null,
          "old_text": "AddCalledComputation(computation, &instr)",
          "new_text": null,
          "old_line_content": "    AddCalledComputation(computation, &instr);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3322,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kReduce, all_operands);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5370,
          "old_api": "empty",
          "new_api": null,
          "old_text": "operands.empty()",
          "new_text": null,
          "old_line_content": "  CHECK(!operands.empty());",
          "new_line_content": "                    const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 5371,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operands[0].builder()->AllToAllTuple(operands, replica_groups, layout,\n                                              channel_id)",
          "new_text": null,
          "old_line_content": "  return operands[0].builder()->AllToAllTuple(operands, replica_groups, layout,",
          "new_line_content": "                    const std::optional<ChannelHandle>& channel_id) {",
          "content_same": false
        },
        {
          "line": 1277,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": null,
          "old_text": "TF_ASSIGN_OR_RETURN(\n          std::optional<Shape> output_shape,\n          ShapeInference::InferScalarBroadcastShape(\n              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})))",
          "new_text": null,
          "old_line_content": "      TF_ASSIGN_OR_RETURN(",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));",
          "content_same": false
        },
        {
          "line": 1280,
          "old_api": "absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})",
          "new_api": null,
          "old_text": "absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})",
          "new_text": null,
          "old_line_content": "              absl::Span<const Shape>({*lhs_shape, *rhs_shape, *ehs_shape})));",
          "new_line_content": "          std::optional<Shape> output_shape,",
          "content_same": false
        },
        {
          "line": 3328,
          "old_api": "begin",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<int64_t> all_dimnos(operand_shape->rank());\n    std::iota(all_dimnos.begin(), all_dimnos.end(), 0);\n    return Reduce(operand, init_value, computation, all_dimnos);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::ReduceAll(XlaOp operand, XlaOp init_value,",
          "content_same": false
        },
        {
          "line": 1283,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "output_shape.has_value()",
          "new_text": null,
          "old_line_content": "      if (output_shape.has_value()) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1284,
          "old_api": "ShapeUtil::IsScalar(*lhs_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::IsScalar(*lhs_shape)",
          "new_text": null,
          "old_line_content": "        if (ShapeUtil::IsScalar(*lhs_shape)) {",
          "new_line_content": "      // Scalar broadcast if mix of scalars and non-scalars",
          "content_same": false
        },
        {
          "line": 3331,
          "old_api": "end",
          "new_api": null,
          "old_text": "all_dimnos.end()",
          "new_text": null,
          "old_line_content": "    std::iota(all_dimnos.begin(), all_dimnos.end(), 0);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 5380,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->AllToAllTuple(operand, split_dimension,\n                                          concat_dimension, split_count,\n                                          replica_groups, layout, channel_id)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->AllToAllTuple(operand, split_dimension,",
          "new_line_content": "                    const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 1290,
          "old_api": "ShapeUtil::Equal(*output_shape, *rhs_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::Equal(*output_shape, *rhs_shape)",
          "new_text": null,
          "old_line_content": "                  ShapeUtil::Equal(*output_shape, *rhs_shape) ? rhs : ehs));",
          "new_line_content": "                  /*scalar=*/lhs,",
          "content_same": false
        },
        {
          "line": 5388,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->CollectiveBroadcast(operand, replica_groups,\n                                                channel_id)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->CollectiveBroadcast(operand, replica_groups,",
          "new_line_content": "                          absl::Span<const ReplicaGroup> replica_groups,",
          "content_same": false
        },
        {
          "line": 1293,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": null,
          "old_text": "TF_ASSIGN_OR_RETURN(\n              updated_rhs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/rhs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : ehs))",
          "new_text": null,
          "old_line_content": "          TF_ASSIGN_OR_RETURN(",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 3341,
          "old_api": "absl::MakeSpan(&operand, 1)",
          "new_api": null,
          "old_text": "absl::MakeSpan(&operand, 1)",
          "new_text": null,
          "old_line_content": "  return ReduceWindow(absl::MakeSpan(&operand, 1),",
          "new_line_content": "                               absl::Span<const int64_t> window_strides,",
          "content_same": false
        },
        {
          "line": 3342,
          "old_api": "absl::MakeSpan(&init_value, 1)",
          "new_api": null,
          "old_text": "absl::MakeSpan(&init_value, 1)",
          "new_text": null,
          "old_line_content": "                      absl::MakeSpan(&init_value, 1), computation,",
          "new_line_content": "                               Padding padding) {",
          "content_same": false
        },
        {
          "line": 1298,
          "old_api": "ShapeUtil::Equal(*output_shape, *lhs_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::Equal(*output_shape, *lhs_shape)",
          "new_text": null,
          "old_line_content": "                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : ehs));",
          "new_line_content": "                  /*scalar=*/rhs,",
          "content_same": false
        },
        {
          "line": 5396,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->CollectivePermute(operand, source_target_pairs,\n                                              channel_id)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->CollectivePermute(operand, source_target_pairs,",
          "new_line_content": "    const std::vector<std::pair<int64_t, int64_t>>& source_target_pairs,",
          "content_same": false
        },
        {
          "line": 1301,
          "old_api": "TF_ASSIGN_OR_RETURN",
          "new_api": null,
          "old_text": "TF_ASSIGN_OR_RETURN(\n              updated_ehs,\n              BroadcastScalarToOutputShape(\n                  /*scalar=*/ehs,\n                  /*output=*/\n                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : rhs))",
          "new_text": null,
          "old_line_content": "          TF_ASSIGN_OR_RETURN(",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 3352,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    const Shape* operand_shape = nullptr;\n    for (const auto& operand : operands) {\n      TF_ASSIGN_OR_RETURN(operand_shape, GetShapePtr(operand));\n      TF_RETURN_IF_ERROR(ValidatePaddingValues(\n          operand_shape->dimensions(), window_dimensions, window_strides));\n    }\n    CHECK(operand_shape != nullptr);\n    std::vector<std::pair<int64_t, int64_t>> padding_values =\n        MakePadding(operand_shape->dimensions(), window_dimensions,\n                    window_strides, padding);\n    TF_ASSIGN_OR_RETURN(auto window,\n                        ShapeInference::InferWindowFromDimensions(\n                            window_dimensions, window_strides, padding_values,\n                            /*lhs_dilation=*/{},\n                            /*rhs_dilation=*/{}));\n    PaddingType padding_type = PADDING_INVALID;\n    for (int64_t i = 0; i < operand_shape->rank(); ++i) {\n      if (operand_shape->is_dynamic_dimension(i) &&\n          !window_util::IsTrivialWindowDimension(window.dimensions(i)) &&\n          padding == Padding::kSame) {\n        // SAME padding can create dynamic padding sizes. The padding size\n        // need to be rewritten by dynamic padder using HloInstructions. We\n        // create a CustomCall to handle this.\n        padding_type = PADDING_SAME;\n      }\n    }\n    if (padding_type == PADDING_SAME) {\n      TF_ASSIGN_OR_RETURN(\n          HloInstructionProto instr,\n          ReduceWindowInternal(operands, init_values, computation,\n                               window_dimensions, window_strides, {}, {},\n                               padding_values));\n      instr.set_custom_call_target(\"DynamicReduceWindowSamePadding\");\n      std::vector<XlaOp> args;\n      args.insert(args.end(), operands.begin(), operands.end());\n      args.insert(args.end(), init_values.begin(), init_values.end());\n      return AddInstruction(std::move(instr), HloOpcode::kCustomCall, args);\n    }\n    return ReduceWindowWithGeneralPadding(\n        operands, init_values, computation, window_dimensions, window_strides,\n        /*base_dilations=*/{}, /*window_dilations=*/{}, padding_values);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                               absl::Span<const int64_t> window_strides,",
          "content_same": false
        },
        {
          "line": 5400,
          "old_api": "ReplicaId",
          "new_api": null,
          "old_text": "builder->ReplicaId()",
          "new_text": null,
          "old_line_content": "XlaOp ReplicaId(XlaBuilder* builder) { return builder->ReplicaId(); }",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1306,
          "old_api": "ShapeUtil::Equal(*output_shape, *lhs_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::Equal(*output_shape, *lhs_shape)",
          "new_text": null,
          "old_line_content": "                  ShapeUtil::Equal(*output_shape, *lhs_shape) ? lhs : rhs));",
          "new_line_content": "                  /*scalar=*/ehs,",
          "content_same": false
        },
        {
          "line": 3355,
          "old_api": "GetShapePtr",
          "new_api": null,
          "old_text": "GetShapePtr(operand)",
          "new_text": null,
          "old_line_content": "      TF_ASSIGN_OR_RETURN(operand_shape, GetShapePtr(operand));",
          "new_line_content": "    const Shape* operand_shape = nullptr;",
          "content_same": false
        },
        {
          "line": 3356,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "ValidatePaddingValues(\n          operand_shape->dimensions(), window_dimensions, window_strides)",
          "new_text": null,
          "old_line_content": "      TF_RETURN_IF_ERROR(ValidatePaddingValues(",
          "new_line_content": "    for (const auto& operand : operands) {",
          "content_same": false
        },
        {
          "line": 5407,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->SelectAndScatter(operand, select, window_dimensions,\n                                             window_strides, padding, source,\n                                             init_value, scatter)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->SelectAndScatter(operand, select, window_dimensions,",
          "new_line_content": "                       Padding padding, const XlaOp source,",
          "content_same": false
        },
        {
          "line": 1318,
          "old_api": "AddOpWithShape",
          "new_api": null,
          "old_text": "AddOpWithShape(triop, inferred_shape,\n                          {updated_lhs, updated_rhs, updated_ehs})",
          "new_text": null,
          "old_line_content": "    return AddOpWithShape(triop, inferred_shape,",
          "new_line_content": "                            triop, *lhs_shape, *rhs_shape, *ehs_shape));",
          "content_same": false
        },
        {
          "line": 3369,
          "old_api": "rank",
          "new_api": null,
          "old_text": "operand_shape->rank()",
          "new_text": null,
          "old_line_content": "    for (int64_t i = 0; i < operand_shape->rank(); ++i) {",
          "new_line_content": "                            /*rhs_dilation=*/{}));",
          "content_same": false
        },
        {
          "line": 3370,
          "old_api": "is_dynamic_dimension",
          "new_api": null,
          "old_text": "operand_shape->is_dynamic_dimension(i)",
          "new_text": null,
          "old_line_content": "      if (operand_shape->is_dynamic_dimension(i) &&",
          "new_line_content": "    PaddingType padding_type = PADDING_INVALID;",
          "content_same": false
        },
        {
          "line": 5418,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->SelectAndScatterWithGeneralPadding(\n      operand, select, window_dimensions, window_strides, padding, source,\n      init_value, scatter)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->SelectAndScatterWithGeneralPadding(",
          "new_line_content": "    absl::Span<const std::pair<int64_t, int64_t>> padding, const XlaOp source,",
          "content_same": false
        },
        {
          "line": 1324,
          "old_api": "shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (literal.shape().IsArray() && literal.element_count() > 1 &&\n        literal.IsAllFirst()) {\n      Literal scalar = LiteralUtil::GetFirstScalarLiteral(literal);\n      HloInstructionProto instr;\n      *instr.mutable_shape() = scalar.shape().ToProto();\n      *instr.mutable_literal() = scalar.ToProto();\n      TF_ASSIGN_OR_RETURN(\n          XlaOp scalar_op,\n          AddInstruction(std::move(instr), HloOpcode::kConstant));\n      return Broadcast(scalar_op, literal.shape().dimensions());\n    } else {\n      HloInstructionProto instr;\n      *instr.mutable_shape() = literal.shape().ToProto();\n      *instr.mutable_literal() = literal.ToProto();\n      return AddInstruction(std::move(instr), HloOpcode::kConstant);\n    }\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1325,
          "old_api": "element_count",
          "new_api": null,
          "old_text": "literal.element_count()",
          "new_text": null,
          "old_line_content": "    if (literal.shape().IsArray() && literal.element_count() > 1 &&",
          "new_line_content": "XlaOp XlaBuilder::ConstantLiteral(const LiteralSlice& literal) {",
          "content_same": false
        },
        {
          "line": 5424,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kAbs, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kAbs, operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1330,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "scalar.ToProto()",
          "new_text": null,
          "old_line_content": "      *instr.mutable_literal() = scalar.ToProto();",
          "new_line_content": "      HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5429,
          "old_api": "builder",
          "new_api": null,
          "old_text": "y.builder()->BinaryOp(HloOpcode::kAtan2, y, x, broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return y.builder()->BinaryOp(HloOpcode::kAtan2, y, x, broadcast_dimensions);",
          "new_line_content": "XlaOp Atan2(const XlaOp y, const XlaOp x,",
          "content_same": false
        },
        {
          "line": 1334,
          "old_api": "shape",
          "new_api": null,
          "old_text": "literal.shape().dimensions()",
          "new_text": null,
          "old_line_content": "      return Broadcast(scalar_op, literal.shape().dimensions());",
          "new_line_content": "          XlaOp scalar_op,",
          "content_same": false
        },
        {
          "line": 1337,
          "old_api": "shape",
          "new_api": null,
          "old_text": "literal.shape().ToProto()",
          "new_text": null,
          "old_line_content": "      *instr.mutable_shape() = literal.shape().ToProto();",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 1338,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "literal.ToProto()",
          "new_text": null,
          "old_line_content": "      *instr.mutable_literal() = literal.ToProto();",
          "new_line_content": "      HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 3385,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "instr.set_custom_call_target(\"DynamicReduceWindowSamePadding\")",
          "new_text": null,
          "old_line_content": "      instr.set_custom_call_target(\"DynamicReduceWindowSamePadding\");",
          "new_line_content": "                               window_dimensions, window_strides, {}, {},",
          "content_same": false
        },
        {
          "line": 3388,
          "old_api": "end",
          "new_api": null,
          "old_text": "init_values.end()",
          "new_text": null,
          "old_line_content": "      args.insert(args.end(), init_values.begin(), init_values.end());",
          "new_line_content": "      std::vector<XlaOp> args;",
          "content_same": false
        },
        {
          "line": 5433,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kExp, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kExp, operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5436,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kExpm1, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kExpm1, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5439,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kFloor, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kFloor, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1345,
          "old_api": "is_static",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!shape.is_static()) {\n      return InvalidArgument(\n          \"The output of iota must not have dynamic dimensions: %s\",\n          shape.ToString());\n    }\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    instr.add_dimensions(iota_dimension);\n    return AddInstruction(std::move(instr), HloOpcode::kIota);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1346,
          "old_api": "is_static",
          "new_api": null,
          "old_text": "shape.is_static()",
          "new_text": null,
          "old_line_content": "    if (!shape.is_static()) {",
          "new_line_content": "XlaOp XlaBuilder::Iota(const Shape& shape, int64_t iota_dimension) {",
          "content_same": false
        },
        {
          "line": 5442,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kCeil, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kCeil, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5445,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kRoundNearestAfz, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kRoundNearestAfz, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1352,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1353,
          "old_api": "add_dimensions",
          "new_api": null,
          "old_text": "instr.add_dimensions(iota_dimension)",
          "new_text": null,
          "old_line_content": "    instr.add_dimensions(iota_dimension);",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5448,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kRoundNearestEven, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kRoundNearestEven, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5451,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kLog, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kLog, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 3406,
          "old_api": "size",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (operands.size() == 1) {\n      const auto& operand = operands[0];\n      const auto& init_value = init_values[0];\n      TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n      operand_shapes.push_back(operand_shape);\n      TF_ASSIGN_OR_RETURN(const Shape* init_shape, GetShapePtr(init_value));\n      init_shapes.push_back(init_shape);\n\n      TF_ASSIGN_OR_RETURN(const ProgramShape& to_apply_shape,\n                          computation.GetProgramShape());\n      TF_ASSIGN_OR_RETURN(auto window,\n                          ShapeInference::InferWindowFromDimensions(\n                              window_dimensions, window_strides, padding,\n                              /*lhs_dilation=*/base_dilations,\n                              /*rhs_dilation=*/window_dilations));\n      TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferReduceWindowShape(\n                                           absl::MakeSpan(operand_shapes),\n                                           absl::MakeSpan(init_shapes), window,\n                                           to_apply_shape));\n      return ReduceWindowInternal(shape, operands[0], init_values[0],\n                                  computation, window);\n    }\n\n    TF_ASSIGN_OR_RETURN(\n        HloInstructionProto instr,\n        ReduceWindowInternal(operands, init_values, computation,\n                             window_dimensions, window_strides, base_dilations,\n                             window_dilations, padding));\n    std::vector<XlaOp> args;\n    args.insert(args.end(), operands.begin(), operands.end());\n    args.insert(args.end(), init_values.begin(), init_values.end());\n    return AddInstruction(std::move(instr), HloOpcode::kReduceWindow, args);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    absl::Span<const std::pair<int64_t, int64_t>> padding) {",
          "content_same": false
        },
        {
          "line": 1359,
          "old_api": "ShapeUtil::MakeShape(type, {size})",
          "new_api": null,
          "old_text": "ShapeUtil::MakeShape(type, {size})",
          "new_text": null,
          "old_line_content": "  return Iota(ShapeUtil::MakeShape(type, {size}), /*iota_dimension=*/0);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3407,
          "old_api": "size",
          "new_api": null,
          "old_text": "operands.size()",
          "new_text": null,
          "old_line_content": "    if (operands.size() == 1) {",
          "new_line_content": "  std::vector<const Shape*> operand_shapes, init_shapes;",
          "content_same": false
        },
        {
          "line": 5454,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kLog1p, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kLog1p, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5457,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kErf, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kErf, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 3411,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operand_shapes.push_back(operand_shape)",
          "new_text": null,
          "old_line_content": "      operand_shapes.push_back(operand_shape);",
          "new_line_content": "      const auto& init_value = init_values[0];",
          "content_same": false
        },
        {
          "line": 1364,
          "old_api": "GetProgramShape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(const ProgramShape& called_program_shape,\n                        computation.GetProgramShape());\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferCallShape(\n                                         operand_shape_ptrs,\n                                         /*to_apply=*/called_program_shape));\n    *instr.mutable_shape() = shape.ToProto();\n\n    AddCalledComputation(computation, &instr);\n\n    return AddInstruction(std::move(instr), HloOpcode::kCall, operands);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::Call(const XlaComputation& computation,",
          "content_same": false
        },
        {
          "line": 5460,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kLogistic, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kLogistic, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5463,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kSign, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kSign, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1368,
          "old_api": "std::back_inserter(operand_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(operand_shape_ptrs)",
          "new_text": null,
          "old_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "new_line_content": "    std::vector<const Shape*> operand_shape_ptrs;",
          "content_same": false
        },
        {
          "line": 5466,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kClz, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kClz, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5469,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kCos, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kCos, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1375,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                         operand_shape_ptrs,",
          "content_same": false
        },
        {
          "line": 5472,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kSin, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kSin, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 3426,
          "old_api": "ReduceWindowInternal",
          "new_api": null,
          "old_text": "ReduceWindowInternal(shape, operands[0], init_values[0],\n                                  computation, window)",
          "new_text": null,
          "old_line_content": "      return ReduceWindowInternal(shape, operands[0], init_values[0],",
          "new_line_content": "                                           absl::MakeSpan(init_shapes), window,",
          "content_same": false
        },
        {
          "line": 5475,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kTan, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kTan, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5478,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kTanh, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kTanh, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5481,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kReal, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kReal, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1386,
          "old_api": "insert",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    if (!parameter_numbers_.insert(parameter_number).second) {\n      return InvalidArgument(\"parameter %d already registered\",\n                             parameter_number);\n    }\n    instr.set_parameter_number(parameter_number);\n    instr.set_name(name);\n    *instr.mutable_shape() = shape.ToProto();\n    if (!replicated_at_leaf_buffers.empty()) {\n      auto replication = instr.mutable_parameter_replication();\n      for (bool replicated : replicated_at_leaf_buffers) {\n        replication->add_replicated_at_leaf_buffers(replicated);\n      }\n    }\n    return AddInstruction(std::move(instr), HloOpcode::kParameter);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    int64_t parameter_number, const Shape& shape, const std::string& name,",
          "content_same": false
        },
        {
          "line": 3436,
          "old_api": "end",
          "new_api": null,
          "old_text": "operands.end()",
          "new_text": null,
          "old_line_content": "    args.insert(args.end(), operands.begin(), operands.end());",
          "new_line_content": "                             window_dilations, padding));",
          "content_same": false
        },
        {
          "line": 1389,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\"parameter %d already registered\",\n                             parameter_number)",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"parameter %d already registered\",",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 3437,
          "old_api": "end",
          "new_api": null,
          "old_text": "init_values.end()",
          "new_text": null,
          "old_line_content": "    args.insert(args.end(), init_values.begin(), init_values.end());",
          "new_line_content": "    std::vector<XlaOp> args;",
          "content_same": false
        },
        {
          "line": 5484,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kImag, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kImag, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1392,
          "old_api": "set_parameter_number",
          "new_api": null,
          "old_text": "instr.set_parameter_number(parameter_number)",
          "new_text": null,
          "old_line_content": "    instr.set_parameter_number(parameter_number);",
          "new_line_content": "                             parameter_number);",
          "content_same": false
        },
        {
          "line": 1393,
          "old_api": "set_name",
          "new_api": null,
          "old_text": "instr.set_name(name)",
          "new_text": null,
          "old_line_content": "    instr.set_name(name);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5487,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kSqrt, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kSqrt, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5490,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kCbrt, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kCbrt, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5493,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kRsqrt, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kRsqrt, operand);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1401,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kParameter);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 5498,
          "old_api": "builder",
          "new_api": null,
          "old_text": "lhs.builder()->BinaryOp(HloOpcode::kPower, lhs, rhs,\n                                 broadcast_dimensions)",
          "new_text": null,
          "old_line_content": "  return lhs.builder()->BinaryOp(HloOpcode::kPower, lhs, rhs,",
          "new_line_content": "XlaOp Pow(const XlaOp lhs, const XlaOp rhs,",
          "content_same": false
        },
        {
          "line": 3451,
          "old_api": "size",
          "new_api": null,
          "old_text": "operands.size()",
          "new_text": null,
          "old_line_content": "  for (int i = 0; i < operands.size(); ++i) {",
          "new_line_content": "    absl::Span<const std::pair<int64_t, int64_t>> padding) {",
          "content_same": false
        },
        {
          "line": 1407,
          "old_api": "rank",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(\n        const Shape& shape,\n        ShapeInference::InferBroadcastShape(*operand_shape, broadcast_sizes));\n\n    // The client-level broadcast op just appends dimensions on the left (adds\n    // lowest numbered dimensions). The HLO broadcast instruction is more\n    // flexible and can add new dimensions anywhere. The instruction's\n    // dimensions field maps operand dimensions to dimensions in the broadcast\n    // output, so to append dimensions on the left the instruction's dimensions\n    // should just be the n highest dimension numbers of the output shape where\n    // n is the number of input dimensions.\n    const int64_t operand_rank = operand_shape->rank();\n    std::vector<int64_t> dimensions(operand_rank);\n    for (int i = 0; i < operand_rank; ++i) {\n      dimensions[i] = i + shape.rank() - operand_rank;\n    }\n    return InDimBroadcast(shape, operand, dimensions);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::Broadcast(XlaOp operand,",
          "content_same": false
        },
        {
          "line": 3455,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operand_shapes.push_back(operand_shape)",
          "new_text": null,
          "old_line_content": "    operand_shapes.push_back(operand_shape);",
          "new_line_content": "    const auto& init_value = init_values[i];",
          "content_same": false
        },
        {
          "line": 5503,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kIsFinite, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kIsFinite, operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5507,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->ConvertElementType(operand, new_element_type)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->ConvertElementType(operand, new_element_type);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5511,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->BitcastConvertType(operand, new_element_type)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->BitcastConvertType(operand, new_element_type);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1420,
          "old_api": "rank",
          "new_api": null,
          "old_text": "operand_shape->rank()",
          "new_text": null,
          "old_line_content": "    const int64_t operand_rank = operand_shape->rank();",
          "new_line_content": "    // should just be the n highest dimension numbers of the output shape where",
          "content_same": false
        },
        {
          "line": 5516,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->StochasticConvertType(operand, random,\n                                                  new_element_type)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->StochasticConvertType(operand, random,",
          "new_line_content": "XlaOp StochasticConvertType(const XlaOp operand, const XlaOp random,",
          "content_same": false
        },
        {
          "line": 1423,
          "old_api": "rank",
          "new_api": null,
          "old_text": "shape.rank()",
          "new_text": null,
          "old_line_content": "      dimensions[i] = i + shape.rank() - operand_rank;",
          "new_line_content": "    std::vector<int64_t> dimensions(operand_rank);",
          "content_same": false
        },
        {
          "line": 3471,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                          absl::MakeSpan(init_shapes), window, to_apply_shape));",
          "content_same": false
        },
        {
          "line": 3472,
          "old_api": "std::move(window)",
          "new_api": null,
          "old_text": "std::move(window)",
          "new_text": null,
          "old_line_content": "  *instr.mutable_window() = std::move(window);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5521,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->UnaryOp(HloOpcode::kNegate, operand)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->UnaryOp(HloOpcode::kNegate, operand);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5525,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Transpose(operand, permutation)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Transpose(operand, permutation);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1432,
          "old_api": "is_unbounded_dynamic",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    // Output shape, in the case of degenerate broadcast, the out_dim_size is\n    // not necessarily the same as the dimension sizes of the output shape.\n    TF_ASSIGN_OR_RETURN(auto output_shape,\n                        ShapeUtil::MakeValidatedShape(\n                            operand_shape->element_type(), out_dim_size));\n    TF_RET_CHECK(!output_shape.is_unbounded_dynamic())\n        << \"BroadcastInDim output must shape be static or bounded dynamic \"\n        << output_shape.ToString();\n    int64_t broadcast_rank = broadcast_dimensions.size();\n    if (operand_shape->rank() != broadcast_rank) {\n      return InvalidArgument(\n          \"Size of broadcast_dimensions has to match operand's rank; operand \"\n          \"rank: %lld, size of broadcast_dimensions %u.\",\n          operand_shape->rank(), broadcast_dimensions.size());\n    }\n    for (int i = 0; i < broadcast_rank; i++) {\n      const int64_t num_dims = out_dim_size.size();\n      if (broadcast_dimensions[i] < 0 || broadcast_dimensions[i] > num_dims) {\n        return InvalidArgument(\"Broadcast dimension %lld is out of bound\",\n                               broadcast_dimensions[i]);\n      }\n      output_shape.set_dynamic_dimension(\n          broadcast_dimensions[i],\n          operand_shape->is_bounded_dynamic_dimension(i));\n    }\n\n    TF_RETURN_IF_ERROR(ShapeInference::InferBroadcastShape(\n                           *operand_shape, output_shape, broadcast_dimensions)\n                           .status());\n    std::vector<int64_t> in_dim_size(out_dim_size.begin(), out_dim_size.end());\n    std::vector<bool> in_dim_dynamic(out_dim_size.size(), false);\n    for (int i = 0; i < broadcast_rank; i++) {\n      in_dim_size[broadcast_dimensions[i]] =\n          (operand_shape->is_unbounded_dynamic_dimension(i))\n              ? out_dim_size[broadcast_dimensions[i]]\n              : operand_shape->dimensions(i);\n      in_dim_dynamic[broadcast_dimensions[i]] =\n          operand_shape->is_bounded_dynamic_dimension(i);\n    }\n    const auto& in_dim_shape = ShapeUtil::MakeShape(\n        operand_shape->element_type(), in_dim_size, in_dim_dynamic);\n    TF_ASSIGN_OR_RETURN(\n        XlaOp in_dim_broadcast,\n        InDimBroadcast(in_dim_shape, operand, broadcast_dimensions));\n\n    // If broadcast is not degenerate, return broadcasted result.\n    if (ShapeUtil::Equal(in_dim_shape, output_shape)) {\n      return in_dim_broadcast;\n    }\n\n    // Otherwise handle degenerate broadcast case.\n    return AddBroadcastSequence(output_shape, in_dim_broadcast);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    XlaOp operand, const absl::Span<const int64_t> out_dim_size,",
          "content_same": false
        },
        {
          "line": 3481,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    const XlaComputation& computation, Window window) {",
          "content_same": false
        },
        {
          "line": 3482,
          "old_api": "std::move(window)",
          "new_api": null,
          "old_text": "std::move(window)",
          "new_text": null,
          "old_line_content": "  *instr.mutable_window() = std::move(window);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5529,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Rev(operand, dimensions)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Rev(operand, dimensions);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3485,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kReduceWindow,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5534,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operands[0].builder()->Sort(operands, comparator, dimension,\n                                     is_stable)",
          "new_text": null,
          "old_line_content": "  return operands[0].builder()->Sort(operands, comparator, dimension,",
          "new_line_content": "XlaOp Sort(absl::Span<const XlaOp> operands, const XlaComputation& comparator,",
          "content_same": false
        },
        {
          "line": 1439,
          "old_api": "is_unbounded_dynamic",
          "new_api": null,
          "old_text": "output_shape.is_unbounded_dynamic()",
          "new_text": null,
          "old_line_content": "    TF_RET_CHECK(!output_shape.is_unbounded_dynamic())",
          "new_line_content": "                        ShapeUtil::MakeValidatedShape(",
          "content_same": false
        },
        {
          "line": 1442,
          "old_api": "size",
          "new_api": null,
          "old_text": "broadcast_dimensions.size()",
          "new_text": null,
          "old_line_content": "    int64_t broadcast_rank = broadcast_dimensions.size();",
          "new_line_content": "        << \"BroadcastInDim output must shape be static or bounded dynamic \"",
          "content_same": false
        },
        {
          "line": 3491,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* scale_shape, GetShapePtr(scale));\n    TF_ASSIGN_OR_RETURN(const Shape* offset_shape, GetShapePtr(offset));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferBatchNormTrainingShape(\n            *operand_shape, *scale_shape, *offset_shape, feature_index));\n    *instr.mutable_shape() = shape.ToProto();\n\n    instr.set_epsilon(epsilon);\n    instr.set_feature_index(feature_index);\n\n    return AddInstruction(std::move(instr), HloOpcode::kBatchNormTraining,\n                          {operand, scale, offset});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::BatchNormTraining(XlaOp operand, XlaOp scale, XlaOp offset,",
          "content_same": false
        },
        {
          "line": 5539,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->TopK(operand, k, largest)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->TopK(operand, k, largest);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1447,
          "old_api": "size",
          "new_api": null,
          "old_text": "broadcast_dimensions.size()",
          "new_text": null,
          "old_line_content": "          operand_shape->rank(), broadcast_dimensions.size());",
          "new_line_content": "          \"Size of broadcast_dimensions has to match operand's rank; operand \"",
          "content_same": false
        },
        {
          "line": 5543,
          "old_api": "builder",
          "new_api": null,
          "old_text": "min.builder()->Clamp(min, operand, max)",
          "new_text": null,
          "old_line_content": "  return min.builder()->Clamp(min, operand, max);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1450,
          "old_api": "size",
          "new_api": null,
          "old_text": "out_dim_size.size()",
          "new_text": null,
          "old_line_content": "      const int64_t num_dims = out_dim_size.size();",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3501,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "        ShapeInference::InferBatchNormTrainingShape(",
          "content_same": false
        },
        {
          "line": 5550,
          "old_api": "Map",
          "new_api": null,
          "old_text": "builder->Map(operands, computation, dimensions, static_operands)",
          "new_text": null,
          "old_line_content": "  return builder->Map(operands, computation, dimensions, static_operands);",
          "new_line_content": "          absl::Span<const int64_t> dimensions,",
          "content_same": false
        },
        {
          "line": 1455,
          "old_api": "set_dynamic_dimension",
          "new_api": null,
          "old_text": "output_shape.set_dynamic_dimension(\n          broadcast_dimensions[i],\n          operand_shape->is_bounded_dynamic_dimension(i))",
          "new_text": null,
          "old_line_content": "      output_shape.set_dynamic_dimension(",
          "new_line_content": "                               broadcast_dimensions[i]);",
          "content_same": false
        },
        {
          "line": 3504,
          "old_api": "set_feature_index",
          "new_api": null,
          "old_text": "instr.set_feature_index(feature_index)",
          "new_text": null,
          "old_line_content": "    instr.set_feature_index(feature_index);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5554,
          "old_api": "builder",
          "new_api": null,
          "old_text": "mu.builder()->RngNormal(mu, sigma, shape)",
          "new_text": null,
          "old_line_content": "  return mu.builder()->RngNormal(mu, sigma, shape);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1460,
          "old_api": "status",
          "new_api": null,
          "old_text": "ShapeInference::InferBroadcastShape(\n                           *operand_shape, output_shape, broadcast_dimensions)\n                           .status()",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(ShapeInference::InferBroadcastShape(",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5558,
          "old_api": "builder",
          "new_api": null,
          "old_text": "a.builder()->RngUniform(a, b, shape)",
          "new_text": null,
          "old_line_content": "  return a.builder()->RngUniform(a, b, shape);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1463,
          "old_api": "end",
          "new_api": null,
          "old_text": "out_dim_size.end()",
          "new_text": null,
          "old_line_content": "    std::vector<int64_t> in_dim_size(out_dim_size.begin(), out_dim_size.end());",
          "new_line_content": "                           *operand_shape, output_shape, broadcast_dimensions)",
          "content_same": false
        },
        {
          "line": 1464,
          "old_api": "size",
          "new_api": null,
          "old_text": "out_dim_size.size()",
          "new_text": null,
          "old_line_content": "    std::vector<bool> in_dim_dynamic(out_dim_size.size(), false);",
          "new_line_content": "                           .status());",
          "content_same": false
        },
        {
          "line": 3514,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* scale_shape, GetShapePtr(scale));\n    TF_ASSIGN_OR_RETURN(const Shape* offset_shape, GetShapePtr(offset));\n    TF_ASSIGN_OR_RETURN(const Shape* mean_shape, GetShapePtr(mean));\n    TF_ASSIGN_OR_RETURN(const Shape* variance_shape, GetShapePtr(variance));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferBatchNormInferenceShape(\n                            *operand_shape, *scale_shape, *offset_shape,\n                            *mean_shape, *variance_shape, feature_index));\n    *instr.mutable_shape() = shape.ToProto();\n\n    instr.set_epsilon(epsilon);\n    instr.set_feature_index(feature_index);\n\n    return AddInstruction(std::move(instr), HloOpcode::kBatchNormInference,\n                          {operand, scale, offset, mean, variance});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                                     XlaOp mean, XlaOp variance, float epsilon,",
          "content_same": false
        },
        {
          "line": 1467,
          "old_api": "is_unbounded_dynamic_dimension",
          "new_api": null,
          "old_text": "operand_shape->is_unbounded_dynamic_dimension(i)",
          "new_text": null,
          "old_line_content": "          (operand_shape->is_unbounded_dynamic_dimension(i))",
          "new_line_content": "    for (int i = 0; i < broadcast_rank; i++) {",
          "content_same": false
        },
        {
          "line": 5563,
          "old_api": "builder",
          "new_api": null,
          "old_text": "initial_state.builder()->RngBitGenerator(algorithm, initial_state,\n                                                  shape)",
          "new_text": null,
          "old_line_content": "  return initial_state.builder()->RngBitGenerator(algorithm, initial_state,",
          "new_line_content": "XlaOp RngBitGenerator(RandomAlgorithm algorithm, const XlaOp initial_state,",
          "content_same": false
        },
        {
          "line": 5569,
          "old_api": "builder",
          "new_api": null,
          "old_text": "init.builder()->While(condition, body, init)",
          "new_text": null,
          "old_line_content": "  return init.builder()->While(condition, body, init);",
          "new_line_content": "XlaOp While(const XlaComputation& condition, const XlaComputation& body,",
          "content_same": false
        },
        {
          "line": 1474,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "operand_shape->element_type()",
          "new_text": null,
          "old_line_content": "        operand_shape->element_type(), in_dim_size, in_dim_dynamic);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3526,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                            *operand_shape, *scale_shape, *offset_shape,",
          "content_same": false
        },
        {
          "line": 1480,
          "old_api": "ShapeUtil::Equal(in_dim_shape, output_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::Equal(in_dim_shape, output_shape)",
          "new_text": null,
          "old_line_content": "    if (ShapeUtil::Equal(in_dim_shape, output_shape)) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3529,
          "old_api": "set_feature_index",
          "new_api": null,
          "old_text": "instr.set_feature_index(feature_index)",
          "new_text": null,
          "old_line_content": "    instr.set_feature_index(feature_index);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5576,
          "old_api": "builder",
          "new_api": null,
          "old_text": "predicate.builder()->Conditional(predicate, true_operand,\n                                          true_computation, false_operand,\n                                          false_computation)",
          "new_text": null,
          "old_line_content": "  return predicate.builder()->Conditional(predicate, true_operand,",
          "new_line_content": "                  const XlaOp false_operand,",
          "content_same": false
        },
        {
          "line": 1485,
          "old_api": "AddBroadcastSequence",
          "new_api": null,
          "old_text": "AddBroadcastSequence(output_shape, in_dim_broadcast)",
          "new_text": null,
          "old_line_content": "    return AddBroadcastSequence(output_shape, in_dim_broadcast);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5584,
          "old_api": "builder",
          "new_api": null,
          "old_text": "branch_index.builder()->Conditional(branch_index, branch_computations,\n                                             branch_operands)",
          "new_text": null,
          "old_line_content": "  return branch_index.builder()->Conditional(branch_index, branch_computations,",
          "new_line_content": "                  absl::Span<const XlaComputation* const> branch_computations,",
          "content_same": false
        },
        {
          "line": 1491,
          "old_api": "TF_RETURN_IF_ERROR",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(first_error_)",
          "new_text": null,
          "old_line_content": "  TF_RETURN_IF_ERROR(first_error_);",
          "new_line_content": "StatusOr<XlaOp> XlaBuilder::ReshapeInternal(const Shape& shape, XlaOp operand,",
          "content_same": false
        },
        {
          "line": 1492,
          "old_api": "is_unbounded_dynamic",
          "new_api": null,
          "old_text": "shape.is_unbounded_dynamic()",
          "new_text": null,
          "old_line_content": "  if (shape.is_unbounded_dynamic()) {",
          "new_line_content": "                                            int64_t inferred_dimension) {",
          "content_same": false
        },
        {
          "line": 3539,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* scale_shape, GetShapePtr(scale));\n    TF_ASSIGN_OR_RETURN(const Shape* batch_mean_shape, GetShapePtr(batch_mean));\n    TF_ASSIGN_OR_RETURN(const Shape* batch_var_shape, GetShapePtr(batch_var));\n    TF_ASSIGN_OR_RETURN(const Shape* grad_output_shape,\n                        GetShapePtr(grad_output));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape, ShapeInference::InferBatchNormGradShape(\n                         *operand_shape, *scale_shape, *batch_mean_shape,\n                         *batch_var_shape, *grad_output_shape, feature_index));\n    *instr.mutable_shape() = shape.ToProto();\n\n    instr.set_epsilon(epsilon);\n    instr.set_feature_index(feature_index);\n\n    return AddInstruction(std::move(instr), HloOpcode::kBatchNormGrad,\n                          {operand, scale, batch_mean, batch_var, grad_output});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                                XlaOp batch_var, XlaOp grad_output,",
          "content_same": false
        },
        {
          "line": 5590,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->ReducePrecision(operand, exponent_bits,\n                                            mantissa_bits)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->ReducePrecision(operand, exponent_bits,",
          "new_line_content": "XlaOp ReducePrecision(const XlaOp operand, const int exponent_bits,",
          "content_same": false
        },
        {
          "line": 1498,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5597,
          "old_api": "builder",
          "new_api": null,
          "old_text": "input.builder()->Gather(input, start_indices, dimension_numbers,\n                                 slice_sizes, indices_are_sorted)",
          "new_text": null,
          "old_line_content": "  return input.builder()->Gather(input, start_indices, dimension_numbers,",
          "new_line_content": "             const GatherDimensionNumbers& dimension_numbers,",
          "content_same": false
        },
        {
          "line": 3552,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                         *operand_shape, *scale_shape, *batch_mean_shape,",
          "content_same": false
        },
        {
          "line": 3555,
          "old_api": "set_feature_index",
          "new_api": null,
          "old_text": "instr.set_feature_index(feature_index)",
          "new_text": null,
          "old_line_content": "    instr.set_feature_index(feature_index);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1508,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferSliceShape(\n                                         *operand_shape, start_indices,\n                                         limit_indices, strides));\n    return SliceInternal(shape, operand, start_indices, limit_indices, strides);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                        absl::Span<const int64_t> limit_indices,",
          "content_same": false
        },
        {
          "line": 5605,
          "old_api": "builder",
          "new_api": null,
          "old_text": "input.builder()->Scatter(input, scatter_indices, updates,\n                                  update_computation, dimension_numbers,\n                                  indices_are_sorted, unique_indices)",
          "new_text": null,
          "old_line_content": "  return input.builder()->Scatter(input, scatter_indices, updates,",
          "new_line_content": "              const ScatterDimensionNumbers& dimension_numbers,",
          "content_same": false
        },
        {
          "line": 1513,
          "old_api": "SliceInternal",
          "new_api": null,
          "old_text": "SliceInternal(shape, operand, start_indices, limit_indices, strides)",
          "new_text": null,
          "old_line_content": "    return SliceInternal(shape, operand, start_indices, limit_indices, strides);",
          "new_line_content": "                                         *operand_shape, start_indices,",
          "content_same": false
        },
        {
          "line": 5615,
          "old_api": "builder",
          "new_api": null,
          "old_text": "scatter_indices.builder()->Scatter(\n      inputs, scatter_indices, updates, update_computation, dimension_numbers,\n      indices_are_sorted, unique_indices)",
          "new_text": null,
          "old_line_content": "  return scatter_indices.builder()->Scatter(",
          "new_line_content": "              const ScatterDimensionNumbers& dimension_numbers,",
          "content_same": false
        },
        {
          "line": 3568,
          "old_api": "AllGatherImpl",
          "new_api": null,
          "old_text": "AllGatherImpl(operand, all_gather_dimension, shard_count,\n                       replica_groups, channel_id, layout,\n                       use_global_device_ids, /*async=*/false)",
          "new_text": null,
          "old_line_content": "  return AllGatherImpl(operand, all_gather_dimension, shard_count,",
          "new_line_content": "                            const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 1522,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    absl::Span<const int64_t> strides) {",
          "content_same": false
        },
        {
          "line": 1523,
          "old_api": "size",
          "new_api": null,
          "old_text": "start_indices.size()",
          "new_text": null,
          "old_line_content": "  for (int i = 0, end = start_indices.size(); i < end; i++) {",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5621,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->Send(operand, handle)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->Send(operand, handle);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3575,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));\n    const Shape* element_shape;\n    if (shape->IsTuple()) {\n      if (shape->tuple_shapes_size() == 0) {\n        return Unimplemented(\n            \"0 element tuple CrossReplicaSum is not supported\");\n      }\n      element_shape = &shape->tuple_shapes(0);\n    } else {\n      element_shape = shape;\n    }\n    const Shape scalar_shape =\n        ShapeUtil::MakeShape(element_shape->element_type(), {});\n    auto b = CreateSubBuilder(\"sum\");\n    auto x = b->Parameter(/*parameter_number=*/0, scalar_shape, \"x\");\n    auto y = b->Parameter(/*parameter_number=*/1, scalar_shape, \"y\");\n    if (scalar_shape.element_type() == PRED) {\n      Or(x, y);\n    } else {\n      Add(x, y);\n    }\n    TF_ASSIGN_OR_RETURN(auto computation, b->Build());\n    return AllReduce(operand, computation, replica_groups,\n                     /*channel_id=*/std::nullopt);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::CrossReplicaSum(",
          "content_same": false
        },
        {
          "line": 3578,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "shape->IsTuple()",
          "new_text": null,
          "old_line_content": "    if (shape->IsTuple()) {",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 3579,
          "old_api": "tuple_shapes_size",
          "new_api": null,
          "old_text": "shape->tuple_shapes_size()",
          "new_text": null,
          "old_line_content": "      if (shape->tuple_shapes_size() == 0) {",
          "new_line_content": "    const Shape* element_shape;",
          "content_same": false
        },
        {
          "line": 5626,
          "old_api": "Recv",
          "new_api": null,
          "old_text": "builder->Recv(shape, handle)",
          "new_text": null,
          "old_line_content": "  return builder->Recv(shape, handle);",
          "new_line_content": "XlaOp Recv(XlaBuilder* builder, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 1535,
          "old_api": "begin",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));\n    std::vector<int64_t> starts(shape->rank(), 0);\n    std::vector<int64_t> limits(shape->dimensions().begin(),\n                                shape->dimensions().end());\n    std::vector<int64_t> strides(shape->rank(), 1);\n    starts[dimno] = start_index;\n    limits[dimno] = limit_index;\n    strides[dimno] = stride;\n    return Slice(operand, starts, limits, strides);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                             int64_t limit_index, int64_t stride,",
          "content_same": false
        },
        {
          "line": 3583,
          "old_api": "tuple_shapes",
          "new_api": null,
          "old_text": "shape->tuple_shapes(0)",
          "new_text": null,
          "old_line_content": "      element_shape = &shape->tuple_shapes(0);",
          "new_line_content": "            \"0 element tuple CrossReplicaSum is not supported\");",
          "content_same": false
        },
        {
          "line": 5631,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->SendWithToken(operand, token, handle)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->SendWithToken(operand, token, handle);",
          "new_line_content": "XlaOp SendWithToken(const XlaOp operand, const XlaOp token,",
          "content_same": false
        },
        {
          "line": 1538,
          "old_api": "begin",
          "new_api": null,
          "old_text": "shape->dimensions().begin()",
          "new_text": null,
          "old_line_content": "    std::vector<int64_t> limits(shape->dimensions().begin(),",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 3588,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "element_shape->element_type()",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeShape(element_shape->element_type(), {});",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3589,
          "old_api": "CreateSubBuilder",
          "new_api": null,
          "old_text": "CreateSubBuilder(\"sum\")",
          "new_text": null,
          "old_line_content": "    auto b = CreateSubBuilder(\"sum\");",
          "new_line_content": "    const Shape scalar_shape =",
          "content_same": false
        },
        {
          "line": 5636,
          "old_api": "builder",
          "new_api": null,
          "old_text": "token.builder()->RecvWithToken(token, shape, handle)",
          "new_text": null,
          "old_line_content": "  return token.builder()->RecvWithToken(token, shape, handle);",
          "new_line_content": "XlaOp RecvWithToken(const XlaOp token, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 1544,
          "old_api": "Slice",
          "new_api": null,
          "old_text": "Slice(operand, starts, limits, strides)",
          "new_text": null,
          "old_line_content": "    return Slice(operand, starts, limits, strides);",
          "new_line_content": "    limits[dimno] = limit_index;",
          "content_same": false
        },
        {
          "line": 5641,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->SendToHost(operand, token, shape_with_layout,\n                                       handle)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->SendToHost(operand, token, shape_with_layout,",
          "new_line_content": "XlaOp SendToHost(const XlaOp operand, const XlaOp token,",
          "content_same": false
        },
        {
          "line": 3598,
          "old_api": "AllReduce",
          "new_api": null,
          "old_text": "AllReduce(operand, computation, replica_groups,\n                     /*channel_id=*/std::nullopt)",
          "new_text": null,
          "old_line_content": "    return AllReduce(operand, computation, replica_groups,",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1551,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<const Shape*> start_indices_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& start_indices_shapes,\n                        GetOperandShapes(start_indices));\n    absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferDynamicSliceShape(\n                            *operand_shape, start_indices_shapes, slice_sizes));\n    return DynamicSliceInternal(shape, operand, start_indices, slice_sizes);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                               absl::Span<const XlaOp> start_indices,",
          "content_same": false
        },
        {
          "line": 5647,
          "old_api": "builder",
          "new_api": null,
          "old_text": "token.builder()->RecvFromHost(token, shape, handle)",
          "new_text": null,
          "old_line_content": "  return token.builder()->RecvFromHost(token, shape, handle);",
          "new_line_content": "XlaOp RecvFromHost(const XlaOp token, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 1556,
          "old_api": "absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; })",
          "new_api": null,
          "old_text": "absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; })",
          "new_text": null,
          "old_line_content": "    absl::c_transform(start_indices_shapes,",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const auto& start_indices_shapes,",
          "content_same": false
        },
        {
          "line": 1557,
          "old_api": "std::back_inserter(start_indices_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(start_indices_shape_ptrs)",
          "new_text": null,
          "old_line_content": "                      std::back_inserter(start_indices_shape_ptrs),",
          "new_line_content": "                        GetOperandShapes(start_indices));",
          "content_same": false
        },
        {
          "line": 5652,
          "old_api": "builder",
          "new_api": null,
          "old_text": "token.builder()->InfeedWithToken(token, shape, config)",
          "new_text": null,
          "old_line_content": "  return token.builder()->InfeedWithToken(token, shape, config);",
          "new_line_content": "XlaOp InfeedWithToken(const XlaOp token, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 3608,
          "old_api": "AllReduceImpl",
          "new_api": null,
          "old_text": "AllReduceImpl(operand, computation, replica_groups, channel_id,\n                       shape_with_layout, use_global_device_ids,\n                       /*async =*/false)",
          "new_text": null,
          "old_line_content": "  return AllReduceImpl(operand, computation, replica_groups, channel_id,",
          "new_line_content": "                            const std::optional<Shape>& shape_with_layout,",
          "content_same": false
        },
        {
          "line": 1562,
          "old_api": "DynamicSliceInternal",
          "new_api": null,
          "old_text": "DynamicSliceInternal(shape, operand, start_indices, slice_sizes)",
          "new_text": null,
          "old_line_content": "    return DynamicSliceInternal(shape, operand, start_indices, slice_sizes);",
          "new_line_content": "                        ShapeInference::InferDynamicSliceShape(",
          "content_same": false
        },
        {
          "line": 5658,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->OutfeedWithToken(operand, token, shape_with_layout,\n                                             outfeed_config)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->OutfeedWithToken(operand, token, shape_with_layout,",
          "new_line_content": "                       const Shape& shape_with_layout,",
          "content_same": false
        },
        {
          "line": 5662,
          "old_api": "CreateToken",
          "new_api": null,
          "old_text": "builder->CreateToken()",
          "new_text": null,
          "old_line_content": "XlaOp CreateToken(XlaBuilder* builder) { return builder->CreateToken(); }",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 5665,
          "old_api": "AfterAll",
          "new_api": null,
          "old_text": "builder->AfterAll(tokens)",
          "new_text": null,
          "old_line_content": "  return builder->AfterAll(tokens);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1570,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    absl::Span<const int64_t> slice_sizes) {",
          "content_same": false
        },
        {
          "line": 3619,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<const Shape*> operand_shapes;\n    std::vector<XlaOp> operands;\n    if (operand_shape->IsTuple()) {\n      if (operand_shape->tuple_shapes_size() == 0) {\n        return Unimplemented(\"0 element tuple ReduceScatter is not supported\");\n      }\n      for (int i = 0; i < operand_shape->tuple_shapes_size(); ++i) {\n        if (operand_shape->tuple_shapes(i).element_type() !=\n            operand_shape->tuple_shapes(0).element_type()) {\n          return Unimplemented(\n              \"All the shapes of a tuple input of ReduceScatter must have \"\n              \"the same element type\");\n        }\n        operand_shapes.push_back(&operand_shape->tuple_shapes(i));\n        operands.push_back(GetTupleElement(operand, i));\n      }\n    } else {\n      operand_shapes.push_back(operand_shape);\n      operands.push_back(operand);\n    }\n\n    TF_ASSIGN_OR_RETURN(Shape inferred_shape,\n                        ShapeInference::InferReduceScatterShape(\n                            operand_shapes, scatter_dimension, shard_count));\n    if (layout) {\n      *inferred_shape.mutable_layout() = *layout;\n      instr.set_constrain_layout(true);\n    }\n    *instr.mutable_shape() = inferred_shape.ToProto();\n\n    AddCalledComputation(computation, &instr);\n\n    instr.add_dimensions(scatter_dimension);\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n    if (use_global_device_ids.has_value()) {\n      instr.set_use_global_device_ids(use_global_device_ids.value());\n    }\n\n    TF_ASSIGN_OR_RETURN(\n        auto reduce_scatter,\n        AddInstruction(std::move(instr), HloOpcode::kReduceScatter, operands));\n    return reduce_scatter;\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 1573,
          "old_api": "add_dynamic_slice_sizes",
          "new_api": null,
          "old_text": "instr.add_dynamic_slice_sizes(size)",
          "new_text": null,
          "old_line_content": "    instr.add_dynamic_slice_sizes(size);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5671,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->BatchNormTraining(operand, scale, offset, epsilon,\n                                              feature_index)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->BatchNormTraining(operand, scale, offset, epsilon,",
          "new_line_content": "                        const XlaOp offset, float epsilon,",
          "content_same": false
        },
        {
          "line": 3624,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "operand_shape->IsTuple()",
          "new_text": null,
          "old_line_content": "    if (operand_shape->IsTuple()) {",
          "new_line_content": "    std::vector<const Shape*> operand_shapes;",
          "content_same": false
        },
        {
          "line": 1577,
          "old_api": "end",
          "new_api": null,
          "old_text": "start_indices.end()",
          "new_text": null,
          "old_line_content": "  operands.insert(operands.end(), start_indices.begin(), start_indices.end());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1578,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kDynamicSlice, operands);",
          "new_line_content": "  std::vector<XlaOp> operands = {operand};",
          "content_same": false
        },
        {
          "line": 3625,
          "old_api": "tuple_shapes_size",
          "new_api": null,
          "old_text": "operand_shape->tuple_shapes_size()",
          "new_text": null,
          "old_line_content": "      if (operand_shape->tuple_shapes_size() == 0) {",
          "new_line_content": "    std::vector<XlaOp> operands;",
          "content_same": false
        },
        {
          "line": 3629,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "operand_shape->tuple_shapes(i).element_type()",
          "new_text": null,
          "old_line_content": "        if (operand_shape->tuple_shapes(i).element_type() !=",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 1583,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* update_shape, GetShapePtr(update));\n    std::vector<const Shape*> start_indices_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& start_indices_shapes,\n                        GetOperandShapes(start_indices));\n    absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(\n        Shape shape, ShapeInference::InferDynamicUpdateSliceShape(\n                         *operand_shape, *update_shape, start_indices_shapes));\n    return DynamicUpdateSliceInternal(shape, operand, update, start_indices);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::DynamicUpdateSlice(XlaOp operand, XlaOp update,",
          "content_same": false
        },
        {
          "line": 5679,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->BatchNormInference(\n      operand, scale, offset, mean, variance, epsilon, feature_index)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->BatchNormInference(",
          "new_line_content": "                         const XlaOp variance, float epsilon,",
          "content_same": false
        },
        {
          "line": 3635,
          "old_api": "tuple_shapes",
          "new_api": null,
          "old_text": "operand_shape->tuple_shapes(i)",
          "new_text": null,
          "old_line_content": "        operand_shapes.push_back(&operand_shape->tuple_shapes(i));",
          "new_line_content": "              \"the same element type\");",
          "content_same": false
        },
        {
          "line": 3636,
          "old_api": "GetTupleElement",
          "new_api": null,
          "old_text": "GetTupleElement(operand, i)",
          "new_text": null,
          "old_line_content": "        operands.push_back(GetTupleElement(operand, i));",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 1589,
          "old_api": "absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; })",
          "new_api": null,
          "old_text": "absl::c_transform(start_indices_shapes,\n                      std::back_inserter(start_indices_shape_ptrs),\n                      [](const Shape& shape) { return &shape; })",
          "new_text": null,
          "old_line_content": "    absl::c_transform(start_indices_shapes,",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const auto& start_indices_shapes,",
          "content_same": false
        },
        {
          "line": 1590,
          "old_api": "std::back_inserter(start_indices_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(start_indices_shape_ptrs)",
          "new_text": null,
          "old_line_content": "                      std::back_inserter(start_indices_shape_ptrs),",
          "new_line_content": "                        GetOperandShapes(start_indices));",
          "content_same": false
        },
        {
          "line": 3639,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operand_shapes.push_back(operand_shape)",
          "new_text": null,
          "old_line_content": "      operand_shapes.push_back(operand_shape);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 3640,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operands.push_back(operand)",
          "new_text": null,
          "old_line_content": "      operands.push_back(operand);",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 5687,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->BatchNormGrad(operand, scale, batch_mean, batch_var,\n                                          grad_output, epsilon, feature_index)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->BatchNormGrad(operand, scale, batch_mean, batch_var,",
          "new_line_content": "                    const XlaOp grad_output, float epsilon,",
          "content_same": false
        },
        {
          "line": 1595,
          "old_api": "DynamicUpdateSliceInternal",
          "new_api": null,
          "old_text": "DynamicUpdateSliceInternal(shape, operand, update, start_indices)",
          "new_text": null,
          "old_line_content": "    return DynamicUpdateSliceInternal(shape, operand, update, start_indices);",
          "new_line_content": "        Shape shape, ShapeInference::InferDynamicUpdateSliceShape(",
          "content_same": false
        },
        {
          "line": 5692,
          "old_api": "Iota",
          "new_api": null,
          "old_text": "builder->Iota(type, size)",
          "new_text": null,
          "old_line_content": "  return builder->Iota(type, size);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3647,
          "old_api": "mutable_layout",
          "new_api": null,
          "old_text": "inferred_shape.mutable_layout()",
          "new_text": null,
          "old_line_content": "      *inferred_shape.mutable_layout() = *layout;",
          "new_line_content": "                            operand_shapes, scatter_dimension, shard_count));",
          "content_same": false
        },
        {
          "line": 3648,
          "old_api": "set_constrain_layout",
          "new_api": null,
          "old_text": "instr.set_constrain_layout(true)",
          "new_text": null,
          "old_line_content": "      instr.set_constrain_layout(true);",
          "new_line_content": "    if (layout) {",
          "content_same": false
        },
        {
          "line": 5696,
          "old_api": "Iota",
          "new_api": null,
          "old_text": "builder->Iota(shape, iota_dimension)",
          "new_text": null,
          "old_line_content": "  return builder->Iota(shape, iota_dimension);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1603,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    absl::Span<const XlaOp> start_indices) {",
          "content_same": false
        },
        {
          "line": 5700,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->GetDimensionSize(operand, dimension)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->GetDimensionSize(operand, dimension);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1606,
          "old_api": "end",
          "new_api": null,
          "old_text": "start_indices.end()",
          "new_text": null,
          "old_line_content": "  operands.insert(operands.end(), start_indices.begin(), start_indices.end());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1607,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kDynamicUpdateSlice,",
          "new_line_content": "  std::vector<XlaOp> operands = {operand, update};",
          "content_same": false
        },
        {
          "line": 5705,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->SetDimensionSize(operand, val, dimension)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->SetDimensionSize(operand, val, dimension);",
          "new_line_content": "XlaOp SetDimensionSize(const XlaOp operand, const XlaOp val,",
          "content_same": false
        },
        {
          "line": 3659,
          "old_api": "handle",
          "new_api": null,
          "old_text": "channel_id->handle()",
          "new_text": null,
          "old_line_content": "      instr.set_channel_id(channel_id->handle());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1613,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(operands));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferConcatOpShape(\n                                         operand_shape_ptrs, dimension));\n    return ConcatInDimInternal(shape, operands, dimension);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::ConcatInDim(absl::Span<const XlaOp> operands,",
          "content_same": false
        },
        {
          "line": 3662,
          "old_api": "value",
          "new_api": null,
          "old_text": "use_global_device_ids.value()",
          "new_text": null,
          "old_line_content": "      instr.set_use_global_device_ids(use_global_device_ids.value());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5709,
          "old_api": "builder",
          "new_api": null,
          "old_text": "operand.builder()->RemoveDynamicDimension(operand, dimension)",
          "new_text": null,
          "old_line_content": "  return operand.builder()->RemoveDynamicDimension(operand, dimension);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1616,
          "old_api": "std::back_inserter(operand_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(operand_shape_ptrs)",
          "new_text": null,
          "old_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "new_line_content": "    std::vector<const Shape*> operand_shape_ptrs;",
          "content_same": false
        },
        {
          "line": 5714,
          "old_api": "type",
          "new_api": null,
          "old_text": "original.type()",
          "new_text": null,
          "old_line_content": "  if (single_dim < 0 || original.type() != OpSharding::OTHER) {",
          "new_line_content": "OpSharding GetManualSharding(const OpSharding& original, int64_t single_dim) {",
          "content_same": false
        },
        {
          "line": 5715,
          "old_api": "set_type",
          "new_api": null,
          "old_text": "manual.set_type(OpSharding::MANUAL)",
          "new_text": null,
          "old_line_content": "    manual.set_type(OpSharding::MANUAL);",
          "new_line_content": "  OpSharding manual;",
          "content_same": false
        },
        {
          "line": 1620,
          "old_api": "ConcatInDimInternal",
          "new_api": null,
          "old_text": "ConcatInDimInternal(shape, operands, dimension)",
          "new_text": null,
          "old_line_content": "    return ConcatInDimInternal(shape, operands, dimension);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferConcatOpShape(",
          "content_same": false
        },
        {
          "line": 5718,
          "old_api": "set_type",
          "new_api": null,
          "old_text": "manual.set_type(OpSharding::OTHER)",
          "new_text": null,
          "old_line_content": "  manual.set_type(OpSharding::OTHER);",
          "new_line_content": "    return manual;",
          "content_same": false
        },
        {
          "line": 5721,
          "old_api": "tile_assignment_dimensions",
          "new_api": null,
          "old_text": "original.tile_assignment_dimensions().end()",
          "new_text": null,
          "old_line_content": "      original.tile_assignment_dimensions().end());",
          "new_line_content": "  std::vector<int64_t> new_tile_shape(",
          "content_same": false
        },
        {
          "line": 1627,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    const Shape& shape, absl::Span<const XlaOp> operands, int64_t dimension) {",
          "content_same": false
        },
        {
          "line": 5725,
          "old_api": "Each",
          "new_api": null,
          "old_text": "new_tile.Each([&](absl::Span<const int64_t> indices, int64_t* v) {\n    int64_t src_index = 0;\n    for (int64_t i = 0; i < indices.size() - 1; ++i) {\n      if (i > 0) {\n        src_index *= new_tile_shape[i];\n      }\n      int64_t index = indices[i];\n      if (i == single_dim) {\n        index = indices.back();\n      }\n      src_index += index;\n    }\n    *v = original.tile_assignment_devices(src_index);\n  })",
          "new_text": null,
          "old_line_content": "  new_tile.Each([&](absl::Span<const int64_t> indices, int64_t* v) {",
          "new_line_content": "  new_tile_shape[single_dim] = 1;",
          "content_same": false
        },
        {
          "line": 3679,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "layout.has_value()",
          "new_text": null,
          "old_line_content": "  if (layout.has_value()) {",
          "new_line_content": "  // Array all_to_all may need to violate layout constraint to be legal so use",
          "content_same": false
        },
        {
          "line": 3680,
          "old_api": "AllToAllTuple",
          "new_api": null,
          "old_text": "AllToAllTuple(operand, split_dimension, concat_dimension,\n                         split_count, replica_groups, layout, channel_id)",
          "new_text": null,
          "old_line_content": "    return AllToAllTuple(operand, split_dimension, concat_dimension,",
          "new_line_content": "  // the tuple version.",
          "content_same": false
        },
        {
          "line": 3683,
          "old_api": "AllToAllArray",
          "new_api": null,
          "old_text": "AllToAllArray(operand, split_dimension, concat_dimension, split_count,\n                       replica_groups, channel_id)",
          "new_text": null,
          "old_line_content": "  return AllToAllArray(operand, split_dimension, concat_dimension, split_count,",
          "new_line_content": "                         split_count, replica_groups, layout, channel_id);",
          "content_same": false
        },
        {
          "line": 1636,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape* padding_value_shape,\n                        GetShapePtr(padding_value));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape, ShapeInference::InferPadShape(\n                         *operand_shape, *padding_value_shape, padding_config));\n    return PadInternal(shape, operand, padding_value, padding_config);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::Pad(XlaOp operand, XlaOp padding_value,",
          "content_same": false
        },
        {
          "line": 5733,
          "old_api": "back",
          "new_api": null,
          "old_text": "indices.back()",
          "new_text": null,
          "old_line_content": "        index = indices.back();",
          "new_line_content": "      int64_t index = indices[i];",
          "content_same": false
        },
        {
          "line": 5737,
          "old_api": "tile_assignment_devices",
          "new_api": null,
          "old_text": "original.tile_assignment_devices(src_index)",
          "new_text": null,
          "old_line_content": "    *v = original.tile_assignment_devices(src_index);",
          "new_line_content": "      src_index += index;",
          "content_same": false
        },
        {
          "line": 1643,
          "old_api": "PadInternal",
          "new_api": null,
          "old_text": "PadInternal(shape, operand, padding_value, padding_config)",
          "new_text": null,
          "old_line_content": "    return PadInternal(shape, operand, padding_value, padding_config);",
          "new_line_content": "        Shape shape, ShapeInference::InferPadShape(",
          "content_same": false
        },
        {
          "line": 3691,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(\n        const Shape all_to_all_shape,\n        ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,\n                                           concat_dimension, split_count));\n    HloInstructionProto instr;\n    *instr.mutable_shape() = operand_shape->ToProto();\n    if (replica_groups.empty()) {\n      auto* group = instr.add_replica_groups();\n      for (int64_t i = 0; i < split_count; ++i) {\n        group->add_replica_ids(i);\n      }\n    } else {\n      for (const ReplicaGroup& group : replica_groups) {\n        *instr.add_replica_groups() = group;\n      }\n    }\n    instr.add_dimensions(split_dimension);\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n    TF_ASSIGN_OR_RETURN(\n        XlaOp all_to_all,\n        AddInstruction(std::move(instr), HloOpcode::kAllToAll, {operand}));\n    if (split_dimension == concat_dimension) {\n      return all_to_all;\n    }\n    DimensionVector sizes;\n    for (int64_t i = 0; i < operand_shape->rank(); ++i) {\n      if (i != split_dimension) {\n        sizes.push_back(operand_shape->dimensions(i));\n        continue;\n      }\n      sizes.push_back(split_count);\n      sizes.push_back(operand_shape->dimensions(i) / split_count);\n    }\n    all_to_all = Reshape(all_to_all, sizes);\n\n    std::vector<int64_t> permutation;\n    const auto rank = operand_shape->rank();\n    permutation.reserve(rank + 1);\n    for (int64_t i = 0; i < rank; ++i) {\n      int64_t dim_after_reshape = i >= split_dimension ? i + 1 : i;\n      if (i == concat_dimension) {\n        permutation.push_back(split_dimension);\n      }\n      permutation.push_back(dim_after_reshape);\n    }\n    all_to_all = Transpose(all_to_all, permutation);\n    return Reshape(all_to_all_shape, all_to_all);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    int64_t split_count, absl::Span<const ReplicaGroup> replica_groups,",
          "content_same": false
        },
        {
          "line": 5740,
          "old_api": "add_tile_assignment_dimensions",
          "new_api": null,
          "old_text": "manual.add_tile_assignment_dimensions(dim)",
          "new_text": null,
          "old_line_content": "    manual.add_tile_assignment_dimensions(dim);",
          "new_line_content": "  });",
          "content_same": false
        },
        {
          "line": 5743,
          "old_api": "add_tile_assignment_devices",
          "new_api": null,
          "old_text": "manual.add_tile_assignment_devices(device)",
          "new_text": null,
          "old_line_content": "    manual.add_tile_assignment_devices(device);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 1649,
          "old_api": "mutable_dimensions",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));\n    PaddingConfig padding_config = MakeNoPaddingConfig(shape->rank());\n    auto* dims = padding_config.mutable_dimensions(dimno);\n    dims->set_edge_padding_low(pad_lo);\n    dims->set_edge_padding_high(pad_hi);\n    return Pad(operand, padding_value, padding_config);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::PadInDim(XlaOp operand, XlaOp padding_value, int64_t dimno,",
          "content_same": false
        },
        {
          "line": 3698,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "operand_shape->ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = operand_shape->ToProto();",
          "new_line_content": "                                           concat_dimension, split_count));",
          "content_same": false
        },
        {
          "line": 3699,
          "old_api": "empty",
          "new_api": null,
          "old_text": "replica_groups.empty()",
          "new_text": null,
          "old_line_content": "    if (replica_groups.empty()) {",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 1652,
          "old_api": "mutable_dimensions",
          "new_api": null,
          "old_text": "padding_config.mutable_dimensions(dimno)",
          "new_text": null,
          "old_line_content": "    auto* dims = padding_config.mutable_dimensions(dimno);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 5746,
          "old_api": "add_last_tile_dims",
          "new_api": null,
          "old_text": "manual.add_last_tile_dims(OpSharding::REPLICATED)",
          "new_text": null,
          "old_line_content": "    manual.add_last_tile_dims(OpSharding::REPLICATED);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 5749,
          "old_api": "static_cast<OpSharding::Type>(type)",
          "new_api": null,
          "old_text": "static_cast<OpSharding::Type>(type)",
          "new_text": null,
          "old_line_content": "    manual.add_last_tile_dims(static_cast<OpSharding::Type>(type));",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 3706,
          "old_api": "add_replica_groups",
          "new_api": null,
          "old_text": "instr.add_replica_groups()",
          "new_text": null,
          "old_line_content": "        *instr.add_replica_groups() = group;",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 3709,
          "old_api": "add_dimensions",
          "new_api": null,
          "old_text": "instr.add_dimensions(split_dimension)",
          "new_text": null,
          "old_line_content": "    instr.add_dimensions(split_dimension);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 3710,
          "old_api": "has_value",
          "new_api": null,
          "old_text": "channel_id.has_value()",
          "new_text": null,
          "old_line_content": "    if (channel_id.has_value()) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1663,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                        const PaddingConfig& padding_config) {",
          "content_same": false
        },
        {
          "line": 1664,
          "old_api": "mutable_padding_config",
          "new_api": null,
          "old_text": "instr.mutable_padding_config()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_padding_config() = padding_config;",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5762,
          "old_api": "rank",
          "new_api": null,
          "old_text": "output_shape.rank()",
          "new_text": null,
          "old_line_content": "  const int64_t rank = output_shape.rank();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 5763,
          "old_api": "type",
          "new_api": null,
          "old_text": "manual_sharding.type()",
          "new_text": null,
          "old_line_content": "  if (manual_sharding.type() == OpSharding::OTHER) {",
          "new_line_content": "  Shape output_shape = input_shape;",
          "content_same": false
        },
        {
          "line": 1672,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(const Shape shape, ShapeInference::InferReshapeShape(\n                                               *operand_shape, dimensions,\n                                               new_sizes, inferred_dimension));\n    XlaOp transposed = IsIdentityPermutation(dimensions)\n                           ? operand\n                           : Transpose(operand, dimensions);\n    return ReshapeInternal(shape, transposed, inferred_dimension);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                          absl::Span<const int64_t> new_sizes,",
          "content_same": false
        },
        {
          "line": 3720,
          "old_api": "rank",
          "new_api": null,
          "old_text": "operand_shape->rank()",
          "new_text": null,
          "old_line_content": "    for (int64_t i = 0; i < operand_shape->rank(); ++i) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5769,
          "old_api": "tile_assignment_dimensions",
          "new_api": null,
          "old_text": "manual_sharding.tile_assignment_dimensions(i)",
          "new_text": null,
          "old_line_content": "          manual_sharding.tile_assignment_dimensions(i);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 5772,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "output_shape.dimensions(i)",
          "new_text": null,
          "old_line_content": "          CeilOfRatio(output_shape.dimensions(i), partitions_i);",
          "new_line_content": "      if (partitions_i == 1) continue;",
          "content_same": false
        },
        {
          "line": 1677,
          "old_api": "IsIdentityPermutation",
          "new_api": null,
          "old_text": "IsIdentityPermutation(dimensions)",
          "new_text": null,
          "old_line_content": "    XlaOp transposed = IsIdentityPermutation(dimensions)",
          "new_line_content": "                                               *operand_shape, dimensions,",
          "content_same": false
        },
        {
          "line": 3725,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "sizes.push_back(split_count)",
          "new_text": null,
          "old_line_content": "      sizes.push_back(split_count);",
          "new_line_content": "        continue;",
          "content_same": false
        },
        {
          "line": 3726,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "operand_shape->dimensions(i)",
          "new_text": null,
          "old_line_content": "      sizes.push_back(operand_shape->dimensions(i) / split_count);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 1680,
          "old_api": "ReshapeInternal",
          "new_api": null,
          "old_text": "ReshapeInternal(shape, transposed, inferred_dimension)",
          "new_text": null,
          "old_line_content": "    return ReshapeInternal(shape, transposed, inferred_dimension);",
          "new_line_content": "                           ? operand",
          "content_same": false
        },
        {
          "line": 5773,
          "old_api": "set_dimensions",
          "new_api": null,
          "old_text": "output_shape.set_dimensions(i, dim_size)",
          "new_text": null,
          "old_line_content": "      output_shape.set_dimensions(i, dim_size);",
          "new_line_content": "      const int64_t dim_size =",
          "content_same": false
        },
        {
          "line": 3731,
          "old_api": "rank",
          "new_api": null,
          "old_text": "operand_shape->rank()",
          "new_text": null,
          "old_line_content": "    const auto rank = operand_shape->rank();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3732,
          "old_api": "reserve",
          "new_api": null,
          "old_text": "permutation.reserve(rank + 1)",
          "new_text": null,
          "old_line_content": "    permutation.reserve(rank + 1);",
          "new_line_content": "    std::vector<int64_t> permutation;",
          "content_same": false
        },
        {
          "line": 5781,
          "old_api": "CustomCall",
          "new_api": null,
          "old_text": "CustomCall(\n        builder, /*call_target_name=*/\"Sharding\", {input}, input_shape,\n        /*opaque=*/\n        sharding_op_util::EncodeAttributes(unspecified_dims))",
          "new_text": null,
          "old_line_content": "    input_annotation = CustomCall(",
          "new_line_content": "    // Annotate the full-shape input with the sharding.",
          "content_same": false
        },
        {
          "line": 1686,
          "old_api": "begin",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));\n    std::vector<int64_t> dimensions(shape->dimensions_size());\n    std::iota(dimensions.begin(), dimensions.end(), 0);\n    return Reshape(operand, dimensions, new_sizes, inferred_dimension);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::Reshape(XlaOp operand, absl::Span<const int64_t> new_sizes,",
          "content_same": false
        },
        {
          "line": 3736,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "permutation.push_back(split_dimension)",
          "new_text": null,
          "old_line_content": "        permutation.push_back(split_dimension);",
          "new_line_content": "      int64_t dim_after_reshape = i >= split_dimension ? i + 1 : i;",
          "content_same": false
        },
        {
          "line": 1689,
          "old_api": "end",
          "new_api": null,
          "old_text": "dimensions.end()",
          "new_text": null,
          "old_line_content": "    std::iota(dimensions.begin(), dimensions.end(), 0);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 5784,
          "old_api": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "new_api": null,
          "old_text": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "new_text": null,
          "old_line_content": "        sharding_op_util::EncodeAttributes(unspecified_dims));",
          "new_line_content": "        builder, /*call_target_name=*/\"Sharding\", {input}, input_shape,",
          "content_same": false
        },
        {
          "line": 3741,
          "old_api": "Reshape",
          "new_api": null,
          "old_text": "Reshape(all_to_all_shape, all_to_all)",
          "new_text": null,
          "old_line_content": "    return Reshape(all_to_all_shape, all_to_all);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 5790,
          "old_api": "GetManualSharding",
          "new_api": null,
          "old_text": "GetManualSharding(manual_sharding, single_dim)",
          "new_text": null,
          "old_line_content": "    OpSharding manual = GetManualSharding(manual_sharding, single_dim);",
          "new_line_content": "    // Annotate the shard-shape output with manual sharding, so that the",
          "content_same": false
        },
        {
          "line": 1696,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    return ReshapeInternal(shape, operand, inferred_dimension);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::Reshape(const Shape& shape, XlaOp operand,",
          "content_same": false
        },
        {
          "line": 1697,
          "old_api": "ReshapeInternal",
          "new_api": null,
          "old_text": "ReshapeInternal(shape, operand, inferred_dimension)",
          "new_text": null,
          "old_line_content": "    return ReshapeInternal(shape, operand, inferred_dimension);",
          "new_line_content": "                          int64_t inferred_dimension) {",
          "content_same": false
        },
        {
          "line": 5796,
          "old_api": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "new_api": null,
          "old_text": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "new_text": null,
          "old_line_content": "                      sharding_op_util::EncodeAttributes(unspecified_dims));",
          "new_line_content": "                      {input_annotation}, output_shape,",
          "content_same": false
        },
        {
          "line": 3750,
          "old_api": "reserve",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(auto operand_shapes, this->GetOperandShapes(operands));\n    std::vector<const Shape*> operand_shape_ptrs;\n    operand_shape_ptrs.reserve(operand_shapes.size());\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(Shape shape, ShapeInference::InferAllToAllTupleShape(\n                                         operand_shape_ptrs));\n\n    if (layout) {\n      TF_RET_CHECK(shape.IsTuple() && !ShapeUtil::IsNestedTuple(shape));\n      for (int64_t i = 0; i < ShapeUtil::TupleElementCount(shape); ++i) {\n        const int64_t layout_minor_to_major_size =\n            layout->minor_to_major().size();\n        if (layout_minor_to_major_size != shape.tuple_shapes(i).rank()) {\n          return InvalidArgument(\n              \"Provided layout must be compatible with the operands' shape. \"\n              \"The layout is %s, but operand %d has shape %s.\",\n              layout->ToString(), i, shape.tuple_shapes(i).ToString());\n        }\n        *(shape.mutable_tuple_shapes(i)->mutable_layout()) = *layout;\n      }\n      instr.set_constrain_layout(true);\n    }\n    *instr.mutable_shape() = shape.ToProto();\n\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n\n    return AddInstruction(std::move(instr), HloOpcode::kAllToAll, operands);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 1705,
          "old_api": "reserve",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    std::vector<const Shape*> dim_size_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& dim_size_shapes,\n                        GetOperandShapes(dim_sizes));\n\n    absl::c_transform(dim_size_shapes, std::back_inserter(dim_size_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(const Shape shape,\n                        ShapeInference::InferDynamicReshapeShape(\n                            *operand_shape, dim_size_shape_ptrs,\n                            new_size_bounds, dims_are_dynamic));\n    TF_RETURN_IF_ERROR(first_error_);\n    std::vector<XlaOp> operands;\n    operands.reserve(1 + dim_sizes.size());\n    operands.push_back(operand);\n    for (const XlaOp& dim_size : dim_sizes) {\n      operands.push_back(dim_size);\n    }\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kDynamicReshape,\n                          operands);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                                 absl::Span<const int64_t> new_size_bounds,",
          "content_same": false
        },
        {
          "line": 3754,
          "old_api": "size",
          "new_api": null,
          "old_text": "operand_shapes.size()",
          "new_text": null,
          "old_line_content": "    operand_shape_ptrs.reserve(operand_shapes.size());",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(auto operand_shapes, this->GetOperandShapes(operands));",
          "content_same": false
        },
        {
          "line": 3755,
          "old_api": "std::back_inserter(operand_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(operand_shape_ptrs)",
          "new_text": null,
          "old_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "new_line_content": "    std::vector<const Shape*> operand_shape_ptrs;",
          "content_same": false
        },
        {
          "line": 1711,
          "old_api": "std::back_inserter(dim_size_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(dim_size_shape_ptrs)",
          "new_text": null,
          "old_line_content": "    absl::c_transform(dim_size_shapes, std::back_inserter(dim_size_shape_ptrs),",
          "new_line_content": "                        GetOperandShapes(dim_sizes));",
          "content_same": false
        },
        {
          "line": 3761,
          "old_api": "ShapeUtil::IsNestedTuple(shape)",
          "new_api": null,
          "old_text": "ShapeUtil::IsNestedTuple(shape)",
          "new_text": null,
          "old_line_content": "      TF_RET_CHECK(shape.IsTuple() && !ShapeUtil::IsNestedTuple(shape));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3762,
          "old_api": "ShapeUtil::TupleElementCount(shape)",
          "new_api": null,
          "old_text": "ShapeUtil::TupleElementCount(shape)",
          "new_text": null,
          "old_line_content": "      for (int64_t i = 0; i < ShapeUtil::TupleElementCount(shape); ++i) {",
          "new_line_content": "    if (layout) {",
          "content_same": false
        },
        {
          "line": 5810,
          "old_api": "GetManualSharding",
          "new_api": null,
          "old_text": "GetManualSharding(manual_sharding, single_dim)",
          "new_text": null,
          "old_line_content": "    OpSharding manual = GetManualSharding(manual_sharding, single_dim);",
          "new_line_content": "    // Annotate the shard-shape input with manual sharding, so that the",
          "content_same": false
        },
        {
          "line": 1717,
          "old_api": "TF_RETURN_IF_ERROR",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(first_error_)",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(first_error_);",
          "new_line_content": "                            *operand_shape, dim_size_shape_ptrs,",
          "content_same": false
        },
        {
          "line": 3765,
          "old_api": "tuple_shapes",
          "new_api": null,
          "old_text": "shape.tuple_shapes(i).rank()",
          "new_text": null,
          "old_line_content": "        if (layout_minor_to_major_size != shape.tuple_shapes(i).rank()) {",
          "new_line_content": "        const int64_t layout_minor_to_major_size =",
          "content_same": false
        },
        {
          "line": 1720,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "operands.push_back(operand)",
          "new_text": null,
          "old_line_content": "    operands.push_back(operand);",
          "new_line_content": "    std::vector<XlaOp> operands;",
          "content_same": false
        },
        {
          "line": 3769,
          "old_api": "tuple_shapes",
          "new_api": null,
          "old_text": "shape.tuple_shapes(i).ToString()",
          "new_text": null,
          "old_line_content": "              layout->ToString(), i, shape.tuple_shapes(i).ToString());",
          "new_line_content": "              \"Provided layout must be compatible with the operands' shape. \"",
          "content_same": false
        },
        {
          "line": 5820,
          "old_api": "CustomCall",
          "new_api": null,
          "old_text": "CustomCall(builder,\n                      /*call_target_name=*/\"SPMDShardToFullShape\",\n                      {input_annotation}, output_shape,\n                      sharding_op_util::EncodeAttributes(unspecified_dims))",
          "new_text": null,
          "old_line_content": "    return CustomCall(builder,",
          "new_line_content": "    // Annotate the full-shape output with the sharding.",
          "content_same": false
        },
        {
          "line": 1725,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1726,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kDynamicReshape,",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 5823,
          "old_api": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "new_api": null,
          "old_text": "sharding_op_util::EncodeAttributes(unspecified_dims)",
          "new_text": null,
          "old_line_content": "                      sharding_op_util::EncodeAttributes(unspecified_dims));",
          "new_line_content": "                      /*call_target_name=*/\"SPMDShardToFullShape\",",
          "content_same": false
        },
        {
          "line": 3778,
          "old_api": "add_replica_groups",
          "new_api": null,
          "old_text": "instr.add_replica_groups()",
          "new_text": null,
          "old_line_content": "      *instr.add_replica_groups() = group;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1733,
          "old_api": "size",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (dimensions.size() <= 1) {\n      // Not collapsing anything, trivially we can return the operand versus\n      // enqueueing a trivial reshape.\n      return operand;\n    }\n\n    // Out-of-order collapse is not supported.\n    // Checks that the collapsed dimensions are in order and consecutive.\n    for (absl::Span<const int64_t>::size_type i = 1; i < dimensions.size();\n         ++i) {\n      if (dimensions[i] - 1 != dimensions[i - 1]) {\n        return InvalidArgument(\n            \"Collapsed dimensions are not in consecutive order.\");\n      }\n    }\n\n    // Create a new sizes vector from the old shape, replacing the collapsed\n    // dimensions by the product of their sizes.\n    TF_ASSIGN_OR_RETURN(const Shape* original_shape, GetShapePtr(operand));\n\n    VLOG(3) << \"original shape: \" << ShapeUtil::HumanString(*original_shape);\n    VLOG(3) << \"dims to collapse: \" << absl::StrJoin(dimensions, \",\");\n\n    std::vector<int64_t> new_sizes;\n    for (int i = 0; i < original_shape->rank(); ++i) {\n      if (i <= dimensions.front() || i > dimensions.back()) {\n        new_sizes.push_back(original_shape->dimensions(i));\n      } else {\n        new_sizes.back() *= original_shape->dimensions(i);\n      }\n    }\n\n    VLOG(3) << \"new sizes: [\" << absl::StrJoin(new_sizes, \",\") << \"]\";\n\n    return Reshape(operand, new_sizes);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::Collapse(XlaOp operand,",
          "content_same": false
        },
        {
          "line": 1734,
          "old_api": "size",
          "new_api": null,
          "old_text": "dimensions.size()",
          "new_text": null,
          "old_line_content": "    if (dimensions.size() <= 1) {",
          "new_line_content": "                           absl::Span<const int64_t> dimensions) {",
          "content_same": false
        },
        {
          "line": 3781,
          "old_api": "handle",
          "new_api": null,
          "old_text": "channel_id->handle()",
          "new_text": null,
          "old_line_content": "      instr.set_channel_id(channel_id->handle());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3784,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kAllToAll, operands);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1742,
          "old_api": "size",
          "new_api": null,
          "old_text": "dimensions.size()",
          "new_text": null,
          "old_line_content": "    for (absl::Span<const int64_t>::size_type i = 1; i < dimensions.size();",
          "new_line_content": "    // Out-of-order collapse is not supported.",
          "content_same": false
        },
        {
          "line": 1745,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n            \"Collapsed dimensions are not in consecutive order.\")",
          "new_text": null,
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "         ++i) {",
          "content_same": false
        },
        {
          "line": 3793,
          "old_api": "status",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n\n    // The HloInstruction for Alltoall currently only handles the data\n    // communication: it accepts N already split parts and scatters them to N\n    // cores, and each core gathers the N received parts into a tuple as the\n    // output. So here we explicitly split the operand before the hlo alltoall,\n    // and concat the tuple elements.\n    //\n    // First, run shape inference to make sure the shapes are valid.\n    TF_RETURN_IF_ERROR(\n        ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,\n                                           concat_dimension, split_count)\n            .status());\n\n    // Split into N parts.\n    std::vector<XlaOp> slices;\n    slices.reserve(split_count);\n    const int64_t block_size =\n        operand_shape->dimensions(split_dimension) / split_count;\n    for (int i = 0; i < split_count; i++) {\n      slices.push_back(SliceInDim(operand, /*start_index=*/i * block_size,\n                                  /*limit_index=*/(i + 1) * block_size,\n                                  /*stride=*/1, /*dimno=*/split_dimension));\n    }\n\n    // Handle data communication.\n    XlaOp alltoall =\n        this->AllToAllTuple(slices, replica_groups, layout, channel_id);\n\n    // Concat the N received parts.\n    std::vector<XlaOp> received;\n    received.reserve(split_count);\n    for (int i = 0; i < split_count; i++) {\n      received.push_back(this->GetTupleElement(alltoall, i));\n    }\n    return this->ConcatInDim(received, concat_dimension);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const std::optional<Layout>& layout,",
          "content_same": false
        },
        {
          "line": 1754,
          "old_api": "ShapeUtil::HumanString(*original_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanString(*original_shape)",
          "new_text": null,
          "old_line_content": "    VLOG(3) << \"original shape: \" << ShapeUtil::HumanString(*original_shape);",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* original_shape, GetShapePtr(operand));",
          "content_same": false
        },
        {
          "line": 1755,
          "old_api": "absl::StrJoin(dimensions, \",\")",
          "new_api": null,
          "old_text": "absl::StrJoin(dimensions, \",\")",
          "new_text": null,
          "old_line_content": "    VLOG(3) << \"dims to collapse: \" << absl::StrJoin(dimensions, \",\");",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3803,
          "old_api": "status",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(\n        ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,\n                                           concat_dimension, split_count)\n            .status())",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(",
          "new_line_content": "    //",
          "content_same": false
        },
        {
          "line": 3804,
          "old_api": "status",
          "new_api": null,
          "old_text": "ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,\n                                           concat_dimension, split_count)\n            .status()",
          "new_text": null,
          "old_line_content": "        ShapeInference::InferAllToAllShape(*operand_shape, split_dimension,",
          "new_line_content": "    // First, run shape inference to make sure the shapes are valid.",
          "content_same": false
        },
        {
          "line": 1758,
          "old_api": "rank",
          "new_api": null,
          "old_text": "original_shape->rank()",
          "new_text": null,
          "old_line_content": "    for (int i = 0; i < original_shape->rank(); ++i) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1759,
          "old_api": "back",
          "new_api": null,
          "old_text": "dimensions.back()",
          "new_text": null,
          "old_line_content": "      if (i <= dimensions.front() || i > dimensions.back()) {",
          "new_line_content": "    std::vector<int64_t> new_sizes;",
          "content_same": false
        },
        {
          "line": 3810,
          "old_api": "reserve",
          "new_api": null,
          "old_text": "slices.reserve(split_count)",
          "new_text": null,
          "old_line_content": "    slices.reserve(split_count);",
          "new_line_content": "    // Split into N parts.",
          "content_same": false
        },
        {
          "line": 1766,
          "old_api": "absl::StrJoin(new_sizes, \",\")",
          "new_api": null,
          "old_text": "absl::StrJoin(new_sizes, \",\")",
          "new_text": null,
          "old_line_content": "    VLOG(3) << \"new sizes: [\" << absl::StrJoin(new_sizes, \",\") << \"]\";",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3821,
          "old_api": "AllToAllTuple",
          "new_api": null,
          "old_text": "this->AllToAllTuple(slices, replica_groups, layout, channel_id)",
          "new_text": null,
          "old_line_content": "        this->AllToAllTuple(slices, replica_groups, layout, channel_id);",
          "new_line_content": "    // Handle data communication.",
          "content_same": false
        },
        {
          "line": 1775,
          "old_api": "Parameter",
          "new_api": null,
          "old_text": "Parameter(&builder, 0, shape, \"p\")",
          "new_text": null,
          "old_line_content": "  XlaOp out = Parameter(&builder, 0, shape, \"p\");",
          "new_line_content": "static StatusOr<XlaComputation> PassthroughComputation(const Shape& shape) {",
          "content_same": false
        },
        {
          "line": 1776,
          "old_api": "Build",
          "new_api": null,
          "old_text": "builder.Build(out)",
          "new_text": null,
          "old_line_content": "  return builder.Build(out);",
          "new_line_content": "  XlaBuilder builder(\"dummy\");",
          "content_same": false
        },
        {
          "line": 3825,
          "old_api": "reserve",
          "new_api": null,
          "old_text": "received.reserve(split_count)",
          "new_text": null,
          "old_line_content": "    received.reserve(split_count);",
          "new_line_content": "    // Concat the N received parts.",
          "content_same": false
        },
        {
          "line": 1780,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* true_shape, GetShapePtr(on_true));\n    TF_ASSIGN_OR_RETURN(const Shape* false_shape, GetShapePtr(on_false));\n    TF_RET_CHECK(true_shape->IsTuple() == false_shape->IsTuple());\n    if (true_shape->IsTuple()) {\n      TF_ASSIGN_OR_RETURN(XlaComputation passthrough_true,\n                          PassthroughComputation(*true_shape));\n      TF_ASSIGN_OR_RETURN(XlaComputation passthrough_false,\n                          PassthroughComputation(*false_shape));\n      return Conditional(pred, on_true, passthrough_true, on_false,\n                         passthrough_false);\n    }\n    return TernaryOp(HloOpcode::kSelect, pred, on_true, on_false);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1783,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "false_shape->IsTuple()",
          "new_text": null,
          "old_line_content": "    TF_RET_CHECK(true_shape->IsTuple() == false_shape->IsTuple());",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* true_shape, GetShapePtr(on_true));",
          "content_same": false
        },
        {
          "line": 1784,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "true_shape->IsTuple()",
          "new_text": null,
          "old_line_content": "    if (true_shape->IsTuple()) {",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* false_shape, GetShapePtr(on_false));",
          "content_same": false
        },
        {
          "line": 3836,
          "old_api": "CollectiveBroadcastImpl",
          "new_api": null,
          "old_text": "CollectiveBroadcastImpl(operand, replica_groups, channel_id)",
          "new_text": null,
          "old_line_content": "  return CollectiveBroadcastImpl(operand, replica_groups, channel_id);",
          "new_line_content": "    XlaOp operand, absl::Span<const ReplicaGroup> replica_groups,",
          "content_same": false
        },
        {
          "line": 1789,
          "old_api": "Conditional",
          "new_api": null,
          "old_text": "Conditional(pred, on_true, passthrough_true, on_false,\n                         passthrough_false)",
          "new_text": null,
          "old_line_content": "      return Conditional(pred, on_true, passthrough_true, on_false,",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(XlaComputation passthrough_false,",
          "content_same": false
        },
        {
          "line": 1792,
          "old_api": "TernaryOp",
          "new_api": null,
          "old_text": "TernaryOp(HloOpcode::kSelect, pred, on_true, on_false)",
          "new_text": null,
          "old_line_content": "    return TernaryOp(HloOpcode::kSelect, pred, on_true, on_false);",
          "new_line_content": "                         passthrough_false);",
          "content_same": false
        },
        {
          "line": 3842,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> absl::StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferCollectiveBroadcastShape({operand_shape}));\n    *instr.mutable_shape() = shape.ToProto();\n    for (const ReplicaGroup& group : replica_groups) {\n      *instr.add_replica_groups() = group;\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n\n    return AddInstruction(std::move(instr), HloOpcode::kCollectiveBroadcast,\n                          {operand});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> absl::StatusOr<XlaOp> {",
          "new_line_content": "    XlaOp operand, absl::Span<const ReplicaGroup> replica_groups,",
          "content_same": false
        },
        {
          "line": 1797,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    std::vector<const Shape*> operand_shape_ptrs;\n    TF_ASSIGN_OR_RETURN(const auto& operand_shapes, GetOperandShapes(elements));\n    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),\n                      [](const Shape& shape) { return &shape; });\n    TF_ASSIGN_OR_RETURN(const Shape shape,\n                        ShapeInference::InferVariadicOpShape(\n                            HloOpcode::kTuple, operand_shape_ptrs));\n    return TupleInternal(shape, elements);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1800,
          "old_api": "std::back_inserter(operand_shape_ptrs)",
          "new_api": null,
          "old_text": "std::back_inserter(operand_shape_ptrs)",
          "new_text": null,
          "old_line_content": "    absl::c_transform(operand_shapes, std::back_inserter(operand_shape_ptrs),",
          "new_line_content": "    std::vector<const Shape*> operand_shape_ptrs;",
          "content_same": false
        },
        {
          "line": 3848,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "        Shape shape,",
          "content_same": false
        },
        {
          "line": 1805,
          "old_api": "TupleInternal",
          "new_api": null,
          "old_text": "TupleInternal(shape, elements)",
          "new_text": null,
          "old_line_content": "    return TupleInternal(shape, elements);",
          "new_line_content": "                        ShapeInference::InferVariadicOpShape(",
          "content_same": false
        },
        {
          "line": 3853,
          "old_api": "handle",
          "new_api": null,
          "old_text": "channel_id->handle()",
          "new_text": null,
          "old_line_content": "      instr.set_channel_id(channel_id->handle());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3856,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kCollectiveBroadcast,",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1812,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                          absl::Span<const XlaOp> elements) {",
          "content_same": false
        },
        {
          "line": 1813,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "  return AddInstruction(std::move(instr), HloOpcode::kTuple, elements);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 1817,
          "old_api": "IsTuple",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* tuple_shape, GetShapePtr(tuple_data));\n    if (!tuple_shape->IsTuple()) {\n      return InvalidArgument(\n          \"Operand to GetTupleElement() is not a tuple; got %s\",\n          ShapeUtil::HumanString(*tuple_shape));\n    }\n    if (index < 0 || index >= ShapeUtil::TupleElementCount(*tuple_shape)) {\n      return InvalidArgument(\n          \"GetTupleElement() index (%d) out of range for tuple shape %s\", index,\n          ShapeUtil::HumanString(*tuple_shape));\n    }\n    return GetTupleElementInternal(\n        ShapeUtil::GetTupleElementShape(*tuple_shape, index), tuple_data,\n        index);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3865,
          "old_api": "CollectivePermuteImpl",
          "new_api": null,
          "old_text": "CollectivePermuteImpl(operand, source_target_pairs, channel_id,\n                               /*async=*/false)",
          "new_text": null,
          "old_line_content": "  return CollectivePermuteImpl(operand, source_target_pairs, channel_id,",
          "new_line_content": "    const std::vector<std::pair<int64_t, int64_t>>& source_target_pairs,",
          "content_same": false
        },
        {
          "line": 1820,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Operand to GetTupleElement() is not a tuple; got %s\",\n          ShapeUtil::HumanString(*tuple_shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* tuple_shape, GetShapePtr(tuple_data));",
          "content_same": false
        },
        {
          "line": 1825,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"GetTupleElement() index (%d) out of range for tuple shape %s\", index,\n          ShapeUtil::HumanString(*tuple_shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3873,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    HloInstructionProto instr;\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferCollectivePermuteShape({operand_shape}));\n    *instr.mutable_shape() = shape.ToProto();\n\n    for (const auto& pair : source_target_pairs) {\n      auto* proto_pair = instr.add_source_target_pairs();\n      proto_pair->set_source(pair.first);\n      proto_pair->set_target(pair.second);\n    }\n    if (channel_id.has_value()) {\n      instr.set_channel_id(channel_id->handle());\n    }\n\n    return AddInstruction(std::move(instr),\n                          async ? HloOpcode::kCollectivePermuteStart\n                                : HloOpcode::kCollectivePermute,\n                          {operand});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const std::vector<std::pair<int64_t, int64_t>>& source_target_pairs,",
          "content_same": false
        },
        {
          "line": 1830,
          "old_api": "ShapeUtil::GetTupleElementShape(*tuple_shape, index)",
          "new_api": null,
          "old_text": "ShapeUtil::GetTupleElementShape(*tuple_shape, index)",
          "new_text": null,
          "old_line_content": "        ShapeUtil::GetTupleElementShape(*tuple_shape, index), tuple_data,",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3879,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "        Shape shape,",
          "content_same": false
        },
        {
          "line": 3882,
          "old_api": "add_source_target_pairs",
          "new_api": null,
          "old_text": "instr.add_source_target_pairs()",
          "new_text": null,
          "old_line_content": "      auto* proto_pair = instr.add_source_target_pairs();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 3883,
          "old_api": "set_source",
          "new_api": null,
          "old_text": "proto_pair->set_source(pair.first)",
          "new_text": null,
          "old_line_content": "      proto_pair->set_source(pair.first);",
          "new_line_content": "    for (const auto& pair : source_target_pairs) {",
          "content_same": false
        },
        {
          "line": 1839,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                                    int64_t index) {",
          "content_same": false
        },
        {
          "line": 1840,
          "old_api": "set_tuple_index",
          "new_api": null,
          "old_text": "instr.set_tuple_index(index)",
          "new_text": null,
          "old_line_content": "  instr.set_tuple_index(index);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 3887,
          "old_api": "handle",
          "new_api": null,
          "old_text": "channel_id->handle()",
          "new_text": null,
          "old_line_content": "      instr.set_channel_id(channel_id->handle());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3890,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr),",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1848,
          "old_api": "add_lhs_contracting_dimensions",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n\n    DotDimensionNumbers dimension_numbers;\n    dimension_numbers.add_lhs_contracting_dimensions(\n        lhs_shape->dimensions_size() == 1 ? 0 : 1);\n    dimension_numbers.add_rhs_contracting_dimensions(0);\n    return DotGeneral(lhs, rhs, dimension_numbers, precision_config,\n                      preferred_element_type);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                      const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 3898,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    HloInstructionProto instr;\n    *instr.mutable_shape() = ShapeUtil::MakeShape(U32, {}).ToProto();\n    return AddInstruction(std::move(instr), HloOpcode::kReplicaId, {});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1852,
          "old_api": "add_lhs_contracting_dimensions",
          "new_api": null,
          "old_text": "dimension_numbers.add_lhs_contracting_dimensions(\n        lhs_shape->dimensions_size() == 1 ? 0 : 1)",
          "new_text": null,
          "old_line_content": "    dimension_numbers.add_lhs_contracting_dimensions(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1853,
          "old_api": "dimensions_size",
          "new_api": null,
          "old_text": "lhs_shape->dimensions_size()",
          "new_text": null,
          "old_line_content": "        lhs_shape->dimensions_size() == 1 ? 0 : 1);",
          "new_line_content": "    DotDimensionNumbers dimension_numbers;",
          "content_same": false
        },
        {
          "line": 3901,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kReplicaId, {});",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 3911,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n\n    std::vector<std::pair<int64_t, int64_t>> padding_values =\n        MakePadding(operand_shape->dimensions(), window_dimensions,\n                    window_strides, padding);\n\n    TF_ASSIGN_OR_RETURN(auto window,\n                        ShapeInference::InferWindowFromDimensions(\n                            window_dimensions, window_strides, padding_values,\n                            /*lhs_dilation=*/{},\n                            /*rhs_dilation=*/{}));\n    PaddingType padding_type = PADDING_INVALID;\n    for (int64_t i = 0; i < operand_shape->rank(); ++i) {\n      if (operand_shape->is_dynamic_dimension(i) &&\n          !window_util::IsTrivialWindowDimension(window.dimensions(i)) &&\n          padding == Padding::kSame) {\n        // SAME padding can create dynamic padding sizes. The padding size\n        // need to be rewritten by dynamic padder using HloInstructions. We\n        // create a CustomCall to handle this.\n        padding_type = PADDING_SAME;\n      }\n    }\n    if (padding_type == PADDING_SAME) {\n      TF_ASSIGN_OR_RETURN(\n          HloInstructionProto instr,\n          SelectAndScatterInternal(operand, select, window_dimensions,\n                                   window_strides, padding_values, source,\n                                   init_value, scatter));\n      instr.set_custom_call_target(\"DynamicSelectAndScatterSamePadding\");\n      return AddInstruction(std::move(instr), HloOpcode::kCustomCall,\n                            {operand, source, init_value});\n    }\n    return SelectAndScatterWithGeneralPadding(\n        operand, select, window_dimensions, window_strides, padding_values,\n        source, init_value, scatter);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                                   XlaOp init_value,",
          "content_same": false
        },
        {
          "line": 1864,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n    TF_ASSIGN_OR_RETURN(\n        Shape shape,\n        ShapeInference::InferDotOpShape(\n            *lhs_shape, *rhs_shape, dimension_numbers, preferred_element_type));\n    return DotGeneralInternal(shape, lhs, rhs, dimension_numbers,\n                              precision_config);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 3915,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "operand_shape->dimensions()",
          "new_text": null,
          "old_line_content": "        MakePadding(operand_shape->dimensions(), window_dimensions,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1871,
          "old_api": "DotGeneralInternal",
          "new_api": null,
          "old_text": "DotGeneralInternal(shape, lhs, rhs, dimension_numbers,\n                              precision_config)",
          "new_text": null,
          "old_line_content": "    return DotGeneralInternal(shape, lhs, rhs, dimension_numbers,",
          "new_line_content": "        ShapeInference::InferDotOpShape(",
          "content_same": false
        },
        {
          "line": 3924,
          "old_api": "rank",
          "new_api": null,
          "old_text": "operand_shape->rank()",
          "new_text": null,
          "old_line_content": "    for (int64_t i = 0; i < operand_shape->rank(); ++i) {",
          "new_line_content": "                            /*rhs_dilation=*/{}));",
          "content_same": false
        },
        {
          "line": 3925,
          "old_api": "is_dynamic_dimension",
          "new_api": null,
          "old_text": "operand_shape->is_dynamic_dimension(i)",
          "new_text": null,
          "old_line_content": "      if (operand_shape->is_dynamic_dimension(i) &&",
          "new_line_content": "    PaddingType padding_type = PADDING_INVALID;",
          "content_same": false
        },
        {
          "line": 1881,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "    const PrecisionConfig* precision_config) {",
          "content_same": false
        },
        {
          "line": 1882,
          "old_api": "mutable_dot_dimension_numbers",
          "new_api": null,
          "old_text": "instr.mutable_dot_dimension_numbers()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_dot_dimension_numbers() = dimension_numbers;",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 3940,
          "old_api": "set_custom_call_target",
          "new_api": null,
          "old_text": "instr.set_custom_call_target(\"DynamicSelectAndScatterSamePadding\")",
          "new_text": null,
          "old_line_content": "      instr.set_custom_call_target(\"DynamicSelectAndScatterSamePadding\");",
          "new_line_content": "                                   window_strides, padding_values, source,",
          "content_same": false
        },
        {
          "line": 3941,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "      return AddInstruction(std::move(instr), HloOpcode::kCustomCall,",
          "new_line_content": "                                   init_value, scatter));",
          "content_same": false
        },
        {
          "line": 1895,
          "old_api": "insert",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferDotOpShape(\n                            *lhs_shape, *rhs_shape, dimension_numbers,\n                            preferred_element_type, sparsity));\n    std::vector<XlaOp> operands{lhs, rhs};\n    operands.insert(operands.end(), sparse_meta.begin(), sparse_meta.end());\n\n    HloInstructionProto instr;\n    *instr.mutable_shape() = shape.ToProto();\n    *instr.mutable_dot_dimension_numbers() = dimension_numbers;\n    if (precision_config != nullptr) {\n      *instr.mutable_precision_config() = *precision_config;\n    }\n    for (const SparsityDescriptor& descriptor : sparsity) {\n      *instr.add_dot_sparsity() = descriptor;\n    }\n    return AddInstruction(std::move(instr), HloOpcode::kDot, operands);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 3944,
          "old_api": "SelectAndScatterWithGeneralPadding",
          "new_api": null,
          "old_text": "SelectAndScatterWithGeneralPadding(\n        operand, select, window_dimensions, window_strides, padding_values,\n        source, init_value, scatter)",
          "new_text": null,
          "old_line_content": "    return SelectAndScatterWithGeneralPadding(",
          "new_line_content": "                            {operand, source, init_value});",
          "content_same": false
        },
        {
          "line": 1903,
          "old_api": "end",
          "new_api": null,
          "old_text": "sparse_meta.end()",
          "new_text": null,
          "old_line_content": "    operands.insert(operands.end(), sparse_meta.begin(), sparse_meta.end());",
          "new_line_content": "                            preferred_element_type, sparsity));",
          "content_same": false
        },
        {
          "line": 1906,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1907,
          "old_api": "mutable_dot_dimension_numbers",
          "new_api": null,
          "old_text": "instr.mutable_dot_dimension_numbers()",
          "new_text": null,
          "old_line_content": "    *instr.mutable_dot_dimension_numbers() = dimension_numbers;",
          "new_line_content": "    HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 1912,
          "old_api": "add_dot_sparsity",
          "new_api": null,
          "old_text": "instr.add_dot_sparsity()",
          "new_text": null,
          "old_line_content": "      *instr.add_dot_sparsity() = descriptor;",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 3965,
          "old_api": "mutable_window",
          "new_api": null,
          "old_text": "instr.mutable_window()",
          "new_text": null,
          "old_line_content": "  TF_ASSIGN_OR_RETURN(*instr.mutable_window(),",
          "new_line_content": "  TF_ASSIGN_OR_RETURN(const ProgramShape& scatter_shape,",
          "content_same": false
        },
        {
          "line": 3966,
          "old_api": "ShapeInference::InferWindowFromDimensions(\n                          window_dimensions, window_strides, padding,\n                          /*lhs_dilation=*/{}, /*rhs_dilation=*/{})",
          "new_api": null,
          "old_text": "ShapeInference::InferWindowFromDimensions(\n                          window_dimensions, window_strides, padding,\n                          /*lhs_dilation=*/{}, /*rhs_dilation=*/{})",
          "new_text": null,
          "old_line_content": "                      ShapeInference::InferWindowFromDimensions(",
          "new_line_content": "                      scatter.GetProgramShape());",
          "content_same": false
        },
        {
          "line": 1921,
          "old_api": "rank",
          "new_api": null,
          "old_text": "rhs_shape.rank()",
          "new_text": null,
          "old_line_content": "  if (lhs_shape.rank() != rhs_shape.rank()) {",
          "new_line_content": "    const Shape& lhs_shape, const Shape& rhs_shape,",
          "content_same": false
        },
        {
          "line": 1922,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n        \"Convolution arguments must have same number of \"\n        \"dimensions. Got: %s and %s\",\n        ShapeUtil::HumanString(lhs_shape), ShapeUtil::HumanString(rhs_shape))",
          "new_text": null,
          "old_line_content": "    return InvalidArgument(",
          "new_line_content": "    const ConvolutionDimensionNumbers& dimension_numbers) const {",
          "content_same": false
        },
        {
          "line": 1925,
          "old_api": "ShapeUtil::HumanString(rhs_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanString(rhs_shape)",
          "new_text": null,
          "old_line_content": "        ShapeUtil::HumanString(lhs_shape), ShapeUtil::HumanString(rhs_shape));",
          "new_line_content": "        \"Convolution arguments must have same number of \"",
          "content_same": false
        },
        {
          "line": 3973,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                          *operand_shape, select_shape, instr.window(),",
          "content_same": false
        },
        {
          "line": 3976,
          "old_api": "AddCalledComputation",
          "new_api": null,
          "old_text": "AddCalledComputation(scatter, &instr)",
          "new_text": null,
          "old_line_content": "  AddCalledComputation(scatter, &instr);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1932,
          "old_api": "ShapeUtil::HumanString(rhs_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanString(rhs_shape)",
          "new_text": null,
          "old_line_content": "        ShapeUtil::HumanString(lhs_shape), ShapeUtil::HumanString(rhs_shape));",
          "new_line_content": "        \"Convolution expects argument arrays with >= 3 dimensions. \"",
          "content_same": false
        },
        {
          "line": 1938,
          "old_api": "size",
          "new_api": null,
          "old_text": "numbers.size()",
          "new_text": null,
          "old_line_content": "    if (numbers.size() != num_spatial_dims) {",
          "new_line_content": "  const auto check_spatial_dimensions = [&](absl::string_view field_name,",
          "content_same": false
        },
        {
          "line": 1939,
          "old_api": "size",
          "new_api": null,
          "old_text": "InvalidArgument(\"Expected %d elements for %s, but got %d.\",\n                             num_spatial_dims, field_name, numbers.size())",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(\"Expected %d elements for %s, but got %d.\",",
          "new_line_content": "                                            absl::Span<const int64_t> numbers) {",
          "content_same": false
        },
        {
          "line": 3986,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(HloInstructionProto instr,\n                        SelectAndScatterInternal(\n                            operand, select, window_dimensions, window_strides,\n                            padding, source, init_value, scatter));\n\n    return AddInstruction(std::move(instr), HloOpcode::kSelectAndScatter,\n                          {operand, source, init_value});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    absl::Span<const std::pair<int64_t, int64_t>> padding, XlaOp source,",
          "content_same": false
        },
        {
          "line": 3992,
          "old_api": "std::move(instr)",
          "new_api": null,
          "old_text": "std::move(instr)",
          "new_text": null,
          "old_line_content": "    return AddInstruction(std::move(instr), HloOpcode::kSelectAndScatter,",
          "new_line_content": "                            padding, source, init_value, scatter));",
          "content_same": false
        },
        {
          "line": 1948,
          "old_api": "OkStatus",
          "new_api": null,
          "old_text": "OkStatus()",
          "new_text": null,
          "old_line_content": "    return OkStatus();",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 1951,
          "old_api": "input_spatial_dimensions",
          "new_api": null,
          "old_text": "check_spatial_dimensions(\"input_spatial_dimensions\",\n                               dimension_numbers.input_spatial_dimensions())",
          "new_text": null,
          "old_line_content": "      check_spatial_dimensions(\"input_spatial_dimensions\",",
          "new_line_content": "  };",
          "content_same": false
        },
        {
          "line": 3999,
          "old_api": "ReportErrorOrReturn",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    TF_ASSIGN_OR_RETURN(Shape shape,\n                        ShapeInference::InferReducePrecisionShape(\n                            *operand_shape, exponent_bits, mantissa_bits));\n    return ReducePrecisionInternal(shape, operand, exponent_bits,\n                                   mantissa_bits);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::ReducePrecision(XlaOp operand, const int exponent_bits,",
          "content_same": false
        },
        {
          "line": 4004,
          "old_api": "ReducePrecisionInternal",
          "new_api": null,
          "old_text": "ReducePrecisionInternal(shape, operand, exponent_bits,\n                                   mantissa_bits)",
          "new_text": null,
          "old_line_content": "    return ReducePrecisionInternal(shape, operand, exponent_bits,",
          "new_line_content": "                        ShapeInference::InferReducePrecisionShape(",
          "content_same": false
        },
        {
          "line": 4014,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "  *instr.mutable_shape() = shape.ToProto();",
          "new_line_content": "                                                    const int mantissa_bits) {",
          "content_same": false
        },
        {
          "line": 1967,
          "old_api": "size",
          "new_api": null,
          "old_text": "ConvWithGeneralDimensions(\n      lhs, rhs, window_strides, padding,\n      CreateDefaultConvDimensionNumbers(window_strides.size()),\n      feature_group_count, batch_group_count, precision_config,\n      preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return ConvWithGeneralDimensions(",
          "new_line_content": "                       const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 4015,
          "old_api": "set_exponent_bits",
          "new_api": null,
          "old_text": "instr.set_exponent_bits(exponent_bits)",
          "new_text": null,
          "old_line_content": "  instr.set_exponent_bits(exponent_bits);",
          "new_line_content": "  HloInstructionProto instr;",
          "content_same": false
        },
        {
          "line": 4022,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Send HLO takes two operands: a data operand and a token. Generate the\n    // token to pass into the send.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto token_instr;\n    *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    TF_ASSIGN_OR_RETURN(XlaOp token, AddInstruction(std::move(token_instr),\n                                                    HloOpcode::kAfterAll, {}));\n\n    return SendWithToken(operand, token, handle);\n  })",
          "new_text": null,
          "old_line_content": "  ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1980,
          "old_api": "size",
          "new_api": null,
          "old_text": "ConvGeneral(lhs, rhs, window_strides, padding,\n                     CreateDefaultConvDimensionNumbers(window_strides.size()),\n                     feature_group_count, batch_group_count, precision_config,\n                     preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return ConvGeneral(lhs, rhs, window_strides, padding,",
          "new_line_content": "    const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 1981,
          "old_api": "size",
          "new_api": null,
          "old_text": "window_strides.size()",
          "new_text": null,
          "old_line_content": "                     CreateDefaultConvDimensionNumbers(window_strides.size()),",
          "new_line_content": "    std::optional<PrimitiveType> preferred_element_type) {",
          "content_same": false
        },
        {
          "line": 4028,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": null,
          "old_line_content": "    *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "    // tokens.",
          "content_same": false
        },
        {
          "line": 4032,
          "old_api": "SendWithToken",
          "new_api": null,
          "old_text": "SendWithToken(operand, token, handle)",
          "new_text": null,
          "old_line_content": "    return SendWithToken(operand, token, handle);",
          "new_line_content": "                                                    HloOpcode::kAfterAll, {}));",
          "content_same": false
        },
        {
          "line": 4038,
          "old_api": "type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (handle.type() != ChannelHandle::DEVICE_TO_DEVICE) {\n      return InvalidArgument(\"Send must use a device-to-device channel\");\n    }\n\n    XlaOp send_op = internal::XlaBuilderFriend::BuildSend(this, operand, token,\n                                                          handle, false);\n    return internal::XlaBuilderFriend::BuildSendDone(this, send_op, handle,\n                                                     false);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::SendWithToken(XlaOp operand, XlaOp token,",
          "content_same": false
        },
        {
          "line": 4039,
          "old_api": "type",
          "new_api": null,
          "old_text": "handle.type()",
          "new_text": null,
          "old_line_content": "    if (handle.type() != ChannelHandle::DEVICE_TO_DEVICE) {",
          "new_line_content": "                                const ChannelHandle& handle) {",
          "content_same": false
        },
        {
          "line": 1992,
          "old_api": "input_spatial_dimensions_size",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(const Shape* lhs_shape, GetShapePtr(lhs));\n    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));\n\n    TF_RETURN_IF_ERROR(\n        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers));\n\n    std::vector<int64_t> base_area_dimensions(\n        dimension_numbers.input_spatial_dimensions_size());\n    for (std::vector<int64_t>::size_type i = 0; i < base_area_dimensions.size();\n         ++i) {\n      base_area_dimensions[i] =\n          lhs_shape->dimensions(dimension_numbers.input_spatial_dimensions(i));\n    }\n\n    std::vector<int64_t> window_dimensions(\n        dimension_numbers.kernel_spatial_dimensions_size());\n    for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();\n         ++i) {\n      window_dimensions[i] =\n          rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));\n    }\n\n    return ConvGeneral(lhs, rhs, window_strides,\n                       MakePadding(base_area_dimensions, window_dimensions,\n                                   window_strides, padding),\n                       dimension_numbers, feature_group_count,\n                       batch_group_count, precision_config,\n                       preferred_element_type);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 4043,
          "old_api": "internal::XlaBuilderFriend::BuildSend(this, operand, token,\n                                                          handle, false)",
          "new_api": null,
          "old_text": "internal::XlaBuilderFriend::BuildSend(this, operand, token,\n                                                          handle, false)",
          "new_text": null,
          "old_line_content": "    XlaOp send_op = internal::XlaBuilderFriend::BuildSend(this, operand, token,",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1996,
          "old_api": "TF_RETURN_IF_ERROR",
          "new_api": null,
          "old_text": "TF_RETURN_IF_ERROR(\n        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers))",
          "new_text": null,
          "old_line_content": "    TF_RETURN_IF_ERROR(",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(const Shape* rhs_shape, GetShapePtr(rhs));",
          "content_same": false
        },
        {
          "line": 1997,
          "old_api": "VerifyConvolution",
          "new_api": null,
          "old_text": "VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers)",
          "new_text": null,
          "old_line_content": "        VerifyConvolution(*lhs_shape, *rhs_shape, dimension_numbers));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2000,
          "old_api": "input_spatial_dimensions_size",
          "new_api": null,
          "old_text": "dimension_numbers.input_spatial_dimensions_size()",
          "new_text": null,
          "old_line_content": "        dimension_numbers.input_spatial_dimensions_size());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2001,
          "old_api": "size",
          "new_api": null,
          "old_text": "base_area_dimensions.size()",
          "new_text": null,
          "old_line_content": "    for (std::vector<int64_t>::size_type i = 0; i < base_area_dimensions.size();",
          "new_line_content": "    std::vector<int64_t> base_area_dimensions(",
          "content_same": false
        },
        {
          "line": 4051,
          "old_api": "mutable_shape",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Recv HLO takes a single token operand. Generate the token to pass into\n    // the Recv and RecvDone instructions.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto token_instr;\n    *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    TF_ASSIGN_OR_RETURN(XlaOp token, AddInstruction(std::move(token_instr),\n                                                    HloOpcode::kAfterAll, {}));\n\n    XlaOp recv = RecvWithToken(token, shape, handle);\n\n    // The RecvDone instruction produces a tuple of the data and a token\n    // type. Return XLA op containing the data.\n    // TODO(b/80000000): Remove this when clients have been updated to handle\n    // tokens.\n    HloInstructionProto recv_data;\n    *recv_data.mutable_shape() = shape.ToProto();\n    recv_data.set_tuple_index(0);\n    return AddInstruction(std::move(recv_data), HloOpcode::kGetTupleElement,\n                          {recv});\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2004,
          "old_api": "input_spatial_dimensions",
          "new_api": null,
          "old_text": "dimension_numbers.input_spatial_dimensions(i)",
          "new_text": null,
          "old_line_content": "          lhs_shape->dimensions(dimension_numbers.input_spatial_dimensions(i));",
          "new_line_content": "         ++i) {",
          "content_same": false
        },
        {
          "line": 2008,
          "old_api": "kernel_spatial_dimensions_size",
          "new_api": null,
          "old_text": "dimension_numbers.kernel_spatial_dimensions_size()",
          "new_text": null,
          "old_line_content": "        dimension_numbers.kernel_spatial_dimensions_size());",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 2009,
          "old_api": "size",
          "new_api": null,
          "old_text": "window_dimensions.size()",
          "new_text": null,
          "old_line_content": "    for (std::vector<int64_t>::size_type i = 0; i < window_dimensions.size();",
          "new_line_content": "    std::vector<int64_t> window_dimensions(",
          "content_same": false
        },
        {
          "line": 4057,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTokenShape().ToProto()",
          "new_text": null,
          "old_line_content": "    *token_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();",
          "new_line_content": "    // tokens.",
          "content_same": false
        },
        {
          "line": 2012,
          "old_api": "kernel_spatial_dimensions",
          "new_api": null,
          "old_text": "dimension_numbers.kernel_spatial_dimensions(i)",
          "new_text": null,
          "old_line_content": "          rhs_shape->dimensions(dimension_numbers.kernel_spatial_dimensions(i));",
          "new_line_content": "         ++i) {",
          "content_same": false
        },
        {
          "line": 4061,
          "old_api": "RecvWithToken",
          "new_api": null,
          "old_text": "RecvWithToken(token, shape, handle)",
          "new_text": null,
          "old_line_content": "    XlaOp recv = RecvWithToken(token, shape, handle);",
          "new_line_content": "                                                    HloOpcode::kAfterAll, {}));",
          "content_same": false
        },
        {
          "line": 2015,
          "old_api": "ConvGeneral",
          "new_api": null,
          "old_text": "ConvGeneral(lhs, rhs, window_strides,\n                       MakePadding(base_area_dimensions, window_dimensions,\n                                   window_strides, padding),\n                       dimension_numbers, feature_group_count,\n                       batch_group_count, precision_config,\n                       preferred_element_type)",
          "new_text": null,
          "old_line_content": "    return ConvGeneral(lhs, rhs, window_strides,",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 2016,
          "old_api": "MakePadding",
          "new_api": null,
          "old_text": "MakePadding(base_area_dimensions, window_dimensions,\n                                   window_strides, padding)",
          "new_text": null,
          "old_line_content": "                       MakePadding(base_area_dimensions, window_dimensions,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 4068,
          "old_api": "ToProto",
          "new_api": null,
          "old_text": "shape.ToProto()",
          "new_text": null,
          "old_line_content": "    *recv_data.mutable_shape() = shape.ToProto();",
          "new_line_content": "    // tokens.",
          "content_same": false
        },
        {
          "line": 4069,
          "old_api": "set_tuple_index",
          "new_api": null,
          "old_text": "recv_data.set_tuple_index(0)",
          "new_text": null,
          "old_line_content": "    recv_data.set_tuple_index(0);",
          "new_line_content": "    HloInstructionProto recv_data;",
          "content_same": false
        },
        {
          "line": 4077,
          "old_api": "type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (handle.type() != ChannelHandle::DEVICE_TO_DEVICE) {\n      return InvalidArgument(\"Recv must use a device-to-device channel\");\n    }\n\n    XlaOp recv_op = internal::XlaBuilderFriend::BuildRecv(this, token, shape,\n                                                          handle, false);\n    return internal::XlaBuilderFriend::BuildRecvDone(this, recv_op, shape,\n                                                     handle, false);\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "XlaOp XlaBuilder::RecvWithToken(XlaOp token, const Shape& shape,",
          "content_same": false
        },
        {
          "line": 4078,
          "old_api": "type",
          "new_api": null,
          "old_text": "handle.type()",
          "new_text": null,
          "old_line_content": "    if (handle.type() != ChannelHandle::DEVICE_TO_DEVICE) {",
          "new_line_content": "                                const ChannelHandle& handle) {",
          "content_same": false
        },
        {
          "line": 2031,
          "old_api": "ConvGeneralDilated",
          "new_api": null,
          "old_text": "ConvGeneralDilated(lhs, rhs, window_strides, padding, {}, {},\n                            dimension_numbers, feature_group_count,\n                            batch_group_count, precision_config,\n                            preferred_element_type)",
          "new_text": null,
          "old_line_content": "  return ConvGeneralDilated(lhs, rhs, window_strides, padding, {}, {},",
          "new_line_content": "    const PrecisionConfig* precision_config,",
          "content_same": false
        },
        {
          "line": 4082,
          "old_api": "internal::XlaBuilderFriend::BuildRecv(this, token, shape,\n                                                          handle, false)",
          "new_api": null,
          "old_text": "internal::XlaBuilderFriend::BuildRecv(this, token, shape,\n                                                          handle, false)",
          "new_text": null,
          "old_line_content": "    XlaOp recv_op = internal::XlaBuilderFriend::BuildRecv(this, token, shape,",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 4092,
          "old_api": "type",
          "new_api": null,
          "old_text": "ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    if (!LayoutUtil::HasLayout(shape_with_layout)) {\n      return InvalidArgument(\"Shape passed to SendToHost must have a layout\");\n    }\n    TF_ASSIGN_OR_RETURN(const Shape* operand_shape, GetShapePtr(operand));\n    if (!ShapeUtil::Compatible(*operand_shape, shape_with_layout)) {\n      return InvalidArgument(\n          \"SendToHost shape %s must be compatible with operand shape %s\",\n          ShapeUtil::HumanStringWithLayout(shape_with_layout),\n          ShapeUtil::HumanStringWithLayout(*operand_shape));\n    }\n    // TODO(b/111544877): Support tuple shapes.\n    if (!operand_shape->IsArray()) {\n      return InvalidArgument(\"SendToHost only supports array shapes, shape: %s\",\n                             ShapeUtil::HumanString(*operand_shape));\n    }\n\n    if (handle.type() != ChannelHandle::DEVICE_TO_HOST) {\n      return InvalidArgument(\"SendToHost must use a device-to-host channel\");\n    }\n\n    // Send instruction produces a tuple of {aliased operand, U32 context,\n    // token}.\n    HloInstructionProto send_instr;\n    *send_instr.mutable_shape() =\n        ShapeUtil::MakeTupleShape({shape_with_layout,\n                                   ShapeUtil::MakeShape(U32, {}),\n                                   ShapeUtil::MakeTokenShape()})\n            .ToProto();\n    send_instr.set_channel_id(handle.handle());\n    send_instr.set_is_host_transfer(true);\n    TF_ASSIGN_OR_RETURN(XlaOp send,\n                        AddInstruction(std::move(send_instr), HloOpcode::kSend,\n                                       {operand, token}));\n\n    HloInstructionProto send_done_instr;\n    *send_done_instr.mutable_shape() = ShapeUtil::MakeTokenShape().ToProto();\n    send_done_instr.set_channel_id(handle.handle());\n    send_done_instr.set_is_host_transfer(true);\n    TF_ASSIGN_OR_RETURN(XlaOp send_done,\n                        AddInstruction(std::move(send_done_instr),\n                                       HloOpcode::kSendDone, {send}));\n    return send_done;\n  })",
          "new_text": null,
          "old_line_content": "  return ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "                             const Shape& shape_with_layout,",
          "content_same": false
        },
        {
          "line": 4093,
          "old_api": "LayoutUtil::HasLayout(shape_with_layout)",
          "new_api": null,
          "old_text": "LayoutUtil::HasLayout(shape_with_layout)",
          "new_text": null,
          "old_line_content": "    if (!LayoutUtil::HasLayout(shape_with_layout)) {",
          "new_line_content": "                             const ChannelHandle& handle) {",
          "content_same": false
        }
      ]
    },
    "api_summary": {
      "total_replacements": 366,
      "total_additions": 906,
      "total_deletions": 905,
      "total_api_changes": 2177
    },
    "non_api_changes": {
      "has_non_api_changes": true,
      "evidence": {
        "total_diff_lines": 10,
        "api_related_lines": 2177,
        "non_api_lines": 7,
        "non_api_line_numbers": [
          1188,
          1189,
          1190,
          1191,
          1192,
          1193,
          1194
        ]
      }
    },
    "api_calls_before": 2405,
    "api_calls_after": 2406,
    "diff_info": {
      "added_lines": 10,
      "removed_lines": 8,
      "total_diff_lines": 30
    }
  }
}