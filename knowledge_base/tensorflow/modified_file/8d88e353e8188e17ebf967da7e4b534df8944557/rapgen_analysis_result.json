{
  "status": "completed",
  "commit_dir": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/8d88e353e8188e17ebf967da7e4b534df8944557",
  "file_info": {
    "before_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/8d88e353e8188e17ebf967da7e4b534df8944557/before.cc",
    "after_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/8d88e353e8188e17ebf967da7e4b534df8944557/after.cc",
    "diff_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/8d88e353e8188e17ebf967da7e4b534df8944557/diff.txt"
  },
  "semgrep_analysis": {
    "is_api_change": false,
    "api_changes": {
      "replacements": [
        {
          "line": 150,
          "old_api": "absl::ConsumePrefix(&consumable_arch, \"compute_\")",
          "new_api": "xla::gpu::PtxOptsFromConfig(config)",
          "old_text": "absl::ConsumePrefix(&consumable_arch, \"compute_\")",
          "new_text": "xla::gpu::PtxOptsFromConfig(config)",
          "old_line_content": "      if (absl::ConsumePrefix(&consumable_arch, \"compute_\")) {",
          "new_line_content": "    auto gpu_asm_opts = xla::gpu::PtxOptsFromConfig(config);",
          "content_same": false
        },
        {
          "line": 155,
          "old_api": "InternalError",
          "new_api": "absl::ConsumePrefix(&consumable_arch, \"compute_\")",
          "old_text": "InternalError(\n            \"Could not parse cuda architecture prefix (expected sm_ or \"\n            \"compute_)\")",
          "new_text": "absl::ConsumePrefix(&consumable_arch, \"compute_\")",
          "old_line_content": "        return InternalError(",
          "new_line_content": "      if (absl::ConsumePrefix(&consumable_arch, \"compute_\")) {",
          "content_same": false
        },
        {
          "line": 160,
          "old_api": "absl::SimpleAtoi(consumable_arch, &arch)",
          "new_api": "InternalError",
          "old_text": "absl::SimpleAtoi(consumable_arch, &arch)",
          "new_text": "InternalError(\n            \"Could not parse cuda architecture prefix (expected sm_ or \"\n            \"compute_)\")",
          "old_line_content": "      if (!absl::SimpleAtoi(consumable_arch, &arch)) {",
          "new_line_content": "        return InternalError(",
          "content_same": false
        },
        {
          "line": 192,
          "old_api": "std::move(ptx_bytes)",
          "new_api": "std::move(gpu_asm)",
          "old_text": "std::move(ptx_bytes)",
          "new_text": "std::move(gpu_asm)",
          "old_line_content": "            {absl::StrCat(\"compute_\", arch), std::move(ptx_bytes)});",
          "new_line_content": "      images.push_back({absl::StrCat(\"sm_\", arch), std::move(gpu_asm)});",
          "content_same": false
        },
        {
          "line": 214,
          "old_api": "ok",
          "new_api": "debug_options",
          "old_text": "tensorflow::Env::Default()->IsDirectory(libdevice_dir).ok()",
          "new_text": "tensorflow::CandidateCudaRoots(\n             hlo_module_config.debug_options().xla_gpu_cuda_data_dir())",
          "old_line_content": "      if (tensorflow::Env::Default()->IsDirectory(libdevice_dir).ok()) {",
          "new_line_content": "    for (const std::string& cuda_root : tensorflow::CandidateCudaRoots(",
          "content_same": false
        },
        {
          "line": 215,
          "old_api": "VLOG",
          "new_api": "debug_options",
          "old_text": "VLOG(2)",
          "new_text": "hlo_module_config.debug_options().xla_gpu_cuda_data_dir()",
          "old_line_content": "        VLOG(2) << \"Found libdevice dir \" << libdevice_dir;",
          "new_line_content": "             hlo_module_config.debug_options().xla_gpu_cuda_data_dir())) {",
          "content_same": false
        },
        {
          "line": 219,
          "old_api": "InternalError",
          "new_api": "ok",
          "old_text": "InternalError(\n        \"Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice\")",
          "new_text": "tensorflow::Env::Default()->IsDirectory(libdevice_dir).ok()",
          "old_line_content": "    return InternalError(",
          "new_line_content": "      if (tensorflow::Env::Default()->IsDirectory(libdevice_dir).ok()) {",
          "content_same": false
        }
      ],
      "additions": [
        {
          "line": 133,
          "old_api": null,
          "new_api": "set_xla_gpu_dump_llvmir",
          "old_text": null,
          "new_text": "options.set_xla_gpu_dump_llvmir(print_llvmir_)",
          "old_line_content": "    // Make sure we use full precision division operations.",
          "new_line_content": "    options.set_xla_gpu_dump_llvmir(print_llvmir_);",
          "content_same": false
        },
        {
          "line": 138,
          "old_api": null,
          "new_api": "mutable_xla_backend_extra_options",
          "old_text": null,
          "new_text": "options.mutable_xla_backend_extra_options()",
          "old_line_content": "    auto enable_fusion = [](llvm::TargetMachine* target) {",
          "new_line_content": "    (*options.mutable_xla_backend_extra_options())[\"-simplifycfg-sink-common\"] =",
          "content_same": false
        },
        {
          "line": 141,
          "old_api": null,
          "new_api": "set_debug_options",
          "old_text": null,
          "new_text": "config.set_debug_options(options)",
          "old_line_content": "",
          "new_line_content": "    config.set_debug_options(options);",
          "content_same": false
        },
        {
          "line": 157,
          "old_api": null,
          "new_api": "absl::ConsumePrefix(&consumable_arch, \"sm_\")",
          "old_text": null,
          "new_text": "absl::ConsumePrefix(&consumable_arch, \"sm_\")",
          "old_line_content": "            \"compute_)\");",
          "new_line_content": "      } else if (absl::ConsumePrefix(&consumable_arch, \"sm_\")) {",
          "content_same": false
        },
        {
          "line": 165,
          "old_api": null,
          "new_api": "absl::SimpleAtoi(consumable_arch, &arch)",
          "old_text": null,
          "new_text": "absl::SimpleAtoi(consumable_arch, &arch)",
          "old_line_content": "      int cc_minor = arch % 10;",
          "new_line_content": "      if (!absl::SimpleAtoi(consumable_arch, &arch)) {",
          "content_same": false
        },
        {
          "line": 166,
          "old_api": null,
          "new_api": "InternalError",
          "old_text": null,
          "new_text": "InternalError(\"Could not parse cuda architecture number\")",
          "old_line_content": "      // Module may be changed by CompileToPtx.",
          "new_line_content": "        return InternalError(\"Could not parse cuda architecture number\");",
          "content_same": false
        },
        {
          "line": 172,
          "old_api": null,
          "new_api": "llvm::CloneModule(*llvmModule)",
          "old_text": null,
          "new_text": "llvm::CloneModule(*llvmModule)",
          "old_line_content": "              tensorflow::se::CudaComputeCapability{cc_major, cc_minor}, config,",
          "new_line_content": "      auto llvm_module_copy = llvm::CloneModule(*llvmModule);",
          "content_same": false
        },
        {
          "line": 181,
          "old_api": null,
          "new_api": "llvm::dbgs()",
          "old_text": null,
          "new_text": "llvm::dbgs()",
          "old_line_content": "",
          "new_line_content": "        llvm::dbgs() << \"Generated PTX code for module '\"",
          "content_same": false
        },
        {
          "line": 182,
          "old_api": null,
          "new_api": "getName",
          "old_text": null,
          "new_text": "gpu_module.getName()",
          "old_line_content": "      TF_ASSIGN_OR_RETURN(std::vector<uint8_t> gpu_asm,",
          "new_line_content": "                     << gpu_module.getName() << \"' on architecture sm_\" << arch",
          "content_same": false
        },
        {
          "line": 184,
          "old_api": null,
          "new_api": "llvm::dbgs()",
          "old_text": null,
          "new_text": "llvm::dbgs()",
          "old_line_content": "                              cc_major, cc_minor, ptx.c_str(), gpu_asm_opts));",
          "new_line_content": "        llvm::dbgs() << ptx << \"\\n\";",
          "content_same": false
        },
        {
          "line": 195,
          "old_api": null,
          "new_api": "std::back_inserter(ptx_bytes)",
          "old_text": null,
          "new_text": "std::back_inserter(ptx_bytes)",
          "old_line_content": "",
          "new_line_content": "        std::copy(ptx.begin(), ptx.end(), std::back_inserter(ptx_bytes));",
          "content_same": false
        },
        {
          "line": 196,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "images.push_back(\n            {absl::StrCat(\"compute_\", arch), std::move(ptx_bytes)})",
          "old_line_content": "    // TODO(b/169870789): Revisit the use of fatbins.",
          "new_line_content": "        images.push_back(",
          "content_same": false
        },
        {
          "line": 197,
          "old_api": null,
          "new_api": "std::move(ptx_bytes)",
          "old_text": null,
          "new_text": "std::move(ptx_bytes)",
          "old_line_content": "    // Bundle cubin and PTX images into a single fatbin.",
          "new_line_content": "            {absl::StrCat(\"compute_\", arch), std::move(ptx_bytes)});",
          "content_same": false
        },
        {
          "line": 203,
          "old_api": null,
          "new_api": "tensorflow::se::BundleGpuAsm(images, gpu_asm_opts)",
          "old_text": null,
          "new_text": "tensorflow::se::BundleGpuAsm(images, gpu_asm_opts)",
          "old_line_content": "        \" Did you specify either --config=rocm or --config=cuda ?\");",
          "new_line_content": "    return tensorflow::se::BundleGpuAsm(images, gpu_asm_opts);",
          "content_same": false
        },
        {
          "line": 206,
          "old_api": null,
          "new_api": "InternalError",
          "old_text": null,
          "new_text": "InternalError(\n        \"Neither TENSORFLOW_USE_ROCM nor GOOGLE_CUDA are defined.\"\n        \" Did you specify either --config=rocm or --config=cuda ?\")",
          "old_line_content": " private:",
          "new_line_content": "    return InternalError(",
          "content_same": false
        },
        {
          "line": 217,
          "old_api": null,
          "new_api": "tensorflow::io::JoinPath(cuda_root, \"nvvm\", \"libdevice\")",
          "old_text": null,
          "new_text": "tensorflow::io::JoinPath(cuda_root, \"nvvm\", \"libdevice\")",
          "old_line_content": "      }",
          "new_line_content": "          tensorflow::io::JoinPath(cuda_root, \"nvvm\", \"libdevice\");",
          "content_same": false
        },
        {
          "line": 218,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(2)",
          "old_line_content": "    }",
          "new_line_content": "      VLOG(2) << \"Looking for libdevice at \" << libdevice_dir;",
          "content_same": false
        },
        {
          "line": 220,
          "old_api": null,
          "new_api": "VLOG",
          "old_text": null,
          "new_text": "VLOG(2)",
          "old_line_content": "        \"Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice\");",
          "new_line_content": "        VLOG(2) << \"Found libdevice dir \" << libdevice_dir;",
          "content_same": false
        },
        {
          "line": 224,
          "old_api": null,
          "new_api": "InternalError",
          "old_text": null,
          "new_text": "InternalError(\n        \"Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice\")",
          "old_line_content": "",
          "new_line_content": "    return InternalError(",
          "content_same": false
        },
        {
          "line": 235,
          "old_api": null,
          "new_api": "std::make_unique<GpuKernelToBlobPass>(\n      blob_annotation, architectures, print_ptx, print_llvmir, enable_ftz)",
          "old_text": null,
          "new_text": "std::make_unique<GpuKernelToBlobPass>(\n      blob_annotation, architectures, print_ptx, print_llvmir, enable_ftz)",
          "old_line_content": "}  // namespace kernel_gen",
          "new_line_content": "  return std::make_unique<GpuKernelToBlobPass>(",
          "content_same": false
        }
      ],
      "deletions": [
        {
          "line": 134,
          "old_api": "set_xla_gpu_dump_llvmir",
          "new_api": null,
          "old_text": "options.set_xla_gpu_dump_llvmir(print_llvmir_)",
          "new_text": null,
          "old_line_content": "    options.set_xla_gpu_dump_llvmir(print_llvmir_);",
          "new_line_content": "    // Make sure we use full precision division operations.",
          "content_same": false
        },
        {
          "line": 136,
          "old_api": "set_debug_options",
          "new_api": null,
          "old_text": "config.set_debug_options(options)",
          "new_text": null,
          "old_line_content": "    config.set_debug_options(options);",
          "new_line_content": "    // Disable tail sinking as it interferes with load/store vectorization. If",
          "content_same": false
        },
        {
          "line": 145,
          "old_api": "xla::gpu::PtxOptsFromConfig(config)",
          "new_api": null,
          "old_text": "xla::gpu::PtxOptsFromConfig(config)",
          "new_text": null,
          "old_line_content": "    auto gpu_asm_opts = xla::gpu::PtxOptsFromConfig(config);",
          "new_line_content": "    };",
          "content_same": false
        },
        {
          "line": 152,
          "old_api": "absl::ConsumePrefix(&consumable_arch, \"sm_\")",
          "new_api": null,
          "old_text": "absl::ConsumePrefix(&consumable_arch, \"sm_\")",
          "new_text": null,
          "old_line_content": "      } else if (absl::ConsumePrefix(&consumable_arch, \"sm_\")) {",
          "new_line_content": "      // Parse CUDA architecture.",
          "content_same": false
        },
        {
          "line": 161,
          "old_api": "InternalError",
          "new_api": null,
          "old_text": "InternalError(\"Could not parse cuda architecture number\")",
          "new_text": null,
          "old_line_content": "        return InternalError(\"Could not parse cuda architecture number\");",
          "new_line_content": "            \"Could not parse cuda architecture prefix (expected sm_ or \"",
          "content_same": false
        },
        {
          "line": 167,
          "old_api": "llvm::CloneModule(*llvmModule)",
          "new_api": null,
          "old_text": "llvm::CloneModule(*llvmModule)",
          "new_text": null,
          "old_line_content": "      auto llvm_module_copy = llvm::CloneModule(*llvmModule);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 176,
          "old_api": "llvm::dbgs()",
          "new_api": null,
          "old_text": "llvm::dbgs()",
          "new_text": null,
          "old_line_content": "        llvm::dbgs() << \"Generated PTX code for module '\"",
          "new_line_content": "              llvm_module_copy.get(),",
          "content_same": false
        },
        {
          "line": 177,
          "old_api": "getName",
          "new_api": null,
          "old_text": "gpu_module.getName()",
          "new_text": null,
          "old_line_content": "                     << gpu_module.getName() << \"' on architecture sm_\" << arch",
          "new_line_content": "              tensorflow::se::CudaComputeCapability{cc_major, cc_minor}, config,",
          "content_same": false
        },
        {
          "line": 179,
          "old_api": "llvm::dbgs()",
          "new_api": null,
          "old_text": "llvm::dbgs()",
          "new_text": null,
          "old_line_content": "        llvm::dbgs() << ptx << \"\\n\";",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 187,
          "old_api": "std::move(gpu_asm)",
          "new_api": null,
          "old_text": "std::move(gpu_asm)",
          "new_text": null,
          "old_line_content": "      images.push_back({absl::StrCat(\"sm_\", arch), std::move(gpu_asm)});",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(std::vector<uint8_t> gpu_asm,",
          "content_same": false
        },
        {
          "line": 190,
          "old_api": "std::back_inserter(ptx_bytes)",
          "new_api": null,
          "old_text": "std::back_inserter(ptx_bytes)",
          "new_text": null,
          "old_line_content": "        std::copy(ptx.begin(), ptx.end(), std::back_inserter(ptx_bytes));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 191,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "images.push_back(\n            {absl::StrCat(\"compute_\", arch), std::move(ptx_bytes)})",
          "new_text": null,
          "old_line_content": "        images.push_back(",
          "new_line_content": "      // Collect cubin (and ptx image if requested).",
          "content_same": false
        },
        {
          "line": 198,
          "old_api": "tensorflow::se::BundleGpuAsm(images, gpu_asm_opts)",
          "new_api": null,
          "old_text": "tensorflow::se::BundleGpuAsm(images, gpu_asm_opts)",
          "new_text": null,
          "old_line_content": "    return tensorflow::se::BundleGpuAsm(images, gpu_asm_opts);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 201,
          "old_api": "InternalError",
          "new_api": null,
          "old_text": "InternalError(\n        \"Neither TENSORFLOW_USE_ROCM nor GOOGLE_CUDA are defined.\"\n        \" Did you specify either --config=rocm or --config=cuda ?\")",
          "new_text": null,
          "old_line_content": "    return InternalError(",
          "new_line_content": "    // TODO(b/169870789): Revisit the use of fatbins.",
          "content_same": false
        },
        {
          "line": 209,
          "old_api": "debug_options",
          "new_api": null,
          "old_text": "tensorflow::CandidateCudaRoots(\n             hlo_module_config.debug_options().xla_gpu_cuda_data_dir())",
          "new_text": null,
          "old_line_content": "    for (const std::string& cuda_root : tensorflow::CandidateCudaRoots(",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 210,
          "old_api": "debug_options",
          "new_api": null,
          "old_text": "hlo_module_config.debug_options().xla_gpu_cuda_data_dir()",
          "new_text": null,
          "old_line_content": "             hlo_module_config.debug_options().xla_gpu_cuda_data_dir())) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 212,
          "old_api": "tensorflow::io::JoinPath(cuda_root, \"nvvm\", \"libdevice\")",
          "new_api": null,
          "old_text": "tensorflow::io::JoinPath(cuda_root, \"nvvm\", \"libdevice\")",
          "new_text": null,
          "old_line_content": "          tensorflow::io::JoinPath(cuda_root, \"nvvm\", \"libdevice\");",
          "new_line_content": "  xla::StatusOr<std::string> GetLibdeviceDir(",
          "content_same": false
        },
        {
          "line": 213,
          "old_api": "VLOG",
          "new_api": null,
          "old_text": "VLOG(2)",
          "new_text": null,
          "old_line_content": "      VLOG(2) << \"Looking for libdevice at \" << libdevice_dir;",
          "new_line_content": "      const xla::HloModuleConfig& hlo_module_config) {",
          "content_same": false
        },
        {
          "line": 230,
          "old_api": "std::make_unique<GpuKernelToBlobPass>(\n      blob_annotation, architectures, print_ptx, print_llvmir, enable_ftz)",
          "new_api": null,
          "old_text": "std::make_unique<GpuKernelToBlobPass>(\n      blob_annotation, architectures, print_ptx, print_llvmir, enable_ftz)",
          "new_text": null,
          "old_line_content": "  return std::make_unique<GpuKernelToBlobPass>(",
          "new_line_content": "}  // namespace",
          "content_same": false
        }
      ]
    },
    "api_summary": {
      "total_replacements": 7,
      "total_additions": 20,
      "total_deletions": 19,
      "total_api_changes": 46
    },
    "non_api_changes": {
      "has_non_api_changes": true,
      "evidence": {
        "total_diff_lines": 7,
        "api_related_lines": 46,
        "non_api_lines": 3,
        "non_api_line_numbers": [
          137,
          139,
          140
        ]
      }
    },
    "api_calls_before": 77,
    "api_calls_after": 78,
    "diff_info": {
      "added_lines": 6,
      "removed_lines": 1,
      "total_diff_lines": 21
    }
  }
}