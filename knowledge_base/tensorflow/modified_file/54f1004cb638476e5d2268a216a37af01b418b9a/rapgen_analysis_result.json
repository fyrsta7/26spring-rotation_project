{
  "status": "completed",
  "commit_dir": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/54f1004cb638476e5d2268a216a37af01b418b9a",
  "file_info": {
    "before_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/54f1004cb638476e5d2268a216a37af01b418b9a/before.h",
    "after_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/54f1004cb638476e5d2268a216a37af01b418b9a/after.h",
    "diff_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/54f1004cb638476e5d2268a216a37af01b418b9a/diff.txt"
  },
  "semgrep_analysis": {
    "is_api_change": false,
    "api_changes": {
      "replacements": [
        {
          "line": 276,
          "old_api": "ok",
          "new_api": "allocate_temp",
          "old_text": "allocation_status.ok()",
          "new_text": "context_->allocate_temp(\n        DT_UINT8, TensorShape({byte_size}), &temporary_memory)",
          "old_line_content": "    if (!allocation_status.ok()) {",
          "new_line_content": "    Status allocation_status(context_->allocate_temp(",
          "content_same": false
        },
        {
          "line": 277,
          "old_api": "se::port::StatusOr<DeviceMemoryBytes>(\n          DeviceMemoryBytes::MakeFromByteSize(nullptr, 0))",
          "new_api": "TensorShape",
          "old_text": "se::port::StatusOr<DeviceMemoryBytes>(\n          DeviceMemoryBytes::MakeFromByteSize(nullptr, 0))",
          "new_text": "TensorShape({byte_size})",
          "old_line_content": "      return se::port::StatusOr<DeviceMemoryBytes>(",
          "new_line_content": "        DT_UINT8, TensorShape({byte_size}), &temporary_memory));",
          "content_same": false
        },
        {
          "line": 278,
          "old_api": "DeviceMemoryBytes::MakeFromByteSize(nullptr, 0)",
          "new_api": "ok",
          "old_text": "DeviceMemoryBytes::MakeFromByteSize(nullptr, 0)",
          "new_text": "allocation_status.ok()",
          "old_line_content": "          DeviceMemoryBytes::MakeFromByteSize(nullptr, 0));",
          "new_line_content": "    if (!allocation_status.ok()) {",
          "content_same": false
        },
        {
          "line": 284,
          "old_api": "data",
          "new_api": "push_back",
          "old_text": "DeviceMemoryBytes::MakeFromByteSize(\n            temporary_memory.flat<uint8>().data(),\n            temporary_memory.flat<uint8>().size())",
          "new_text": "allocated_tensors_.push_back(temporary_memory)",
          "old_line_content": "        DeviceMemoryBytes::MakeFromByteSize(",
          "new_line_content": "    allocated_tensors_.push_back(temporary_memory);",
          "content_same": false
        },
        {
          "line": 286,
          "old_api": "size",
          "new_api": "data",
          "old_text": "temporary_memory.flat<uint8>().size()",
          "new_text": "DeviceMemoryBytes::MakeFromByteSize(\n            temporary_memory.flat<uint8>().data(),\n            temporary_memory.flat<uint8>().size())",
          "old_line_content": "            temporary_memory.flat<uint8>().size()));",
          "new_line_content": "        DeviceMemoryBytes::MakeFromByteSize(",
          "content_same": false
        },
        {
          "line": 308,
          "old_api": "output_batch_size",
          "new_api": "dim_size",
          "old_text": "bcast.output_batch_size()",
          "new_text": "in_x.dim_size(adj_x ? 1 : 2)",
          "old_line_content": "    const int64 batch_size = bcast.output_batch_size();",
          "new_line_content": "    const uint64 k = in_x.dim_size(adj_x ? 1 : 2);",
          "content_same": false
        },
        {
          "line": 324,
          "old_api": "reserve",
          "new_api": "x_batch_size",
          "old_text": "c_device_memory.reserve(batch_size)",
          "new_text": "bcast.x_batch_size()",
          "old_line_content": "    c_device_memory.reserve(batch_size);",
          "new_line_content": "    a_device_memory.reserve(bcast.x_batch_size());",
          "content_same": false
        },
        {
          "line": 325,
          "old_api": "reserve",
          "new_api": "y_batch_size",
          "old_text": "a_ptrs.reserve(batch_size)",
          "new_text": "bcast.y_batch_size()",
          "old_line_content": "    a_ptrs.reserve(batch_size);",
          "new_line_content": "    b_device_memory.reserve(bcast.y_batch_size());",
          "content_same": false
        },
        {
          "line": 328,
          "old_api": "data",
          "new_api": "reserve",
          "old_text": "in_x.template flat<Scalar>().data()",
          "new_text": "b_ptrs.reserve(batch_size)",
          "old_line_content": "    auto* a_base_ptr = in_x.template flat<Scalar>().data();",
          "new_line_content": "    b_ptrs.reserve(batch_size);",
          "content_same": false
        },
        {
          "line": 329,
          "old_api": "data",
          "new_api": "reserve",
          "old_text": "in_y.template flat<Scalar>().data()",
          "new_text": "c_ptrs.reserve(batch_size)",
          "old_line_content": "    auto* b_base_ptr = in_y.template flat<Scalar>().data();",
          "new_line_content": "    c_ptrs.reserve(batch_size);",
          "content_same": false
        },
        {
          "line": 332,
          "old_api": "IsBroadcastingRequired",
          "new_api": "data",
          "old_text": "bcast.IsBroadcastingRequired()",
          "new_text": "out->template flat<Scalar>().data()",
          "old_line_content": "    if (!bcast.IsBroadcastingRequired()) {",
          "new_line_content": "    auto* c_base_ptr = out->template flat<Scalar>().data();",
          "content_same": false
        },
        {
          "line": 334,
          "old_api": "AsDeviceMemory",
          "new_api": "IsBroadcastingRequired",
          "old_text": "AsDeviceMemory(a_base_ptr + i * m * k)",
          "new_text": "bcast.IsBroadcastingRequired()",
          "old_line_content": "        a_device_memory.push_back(AsDeviceMemory(a_base_ptr + i * m * k));",
          "new_line_content": "    if (!bcast.IsBroadcastingRequired()) {",
          "content_same": false
        },
        {
          "line": 337,
          "old_api": "back",
          "new_api": "AsDeviceMemory",
          "old_text": "a_device_memory.back()",
          "new_text": "AsDeviceMemory(b_base_ptr + i * k * n)",
          "old_line_content": "        a_ptrs.push_back(&a_device_memory.back());",
          "new_line_content": "        b_device_memory.push_back(AsDeviceMemory(b_base_ptr + i * k * n));",
          "content_same": false
        },
        {
          "line": 338,
          "old_api": "back",
          "new_api": "AsDeviceMemory",
          "old_text": "b_device_memory.back()",
          "new_text": "AsDeviceMemory(c_base_ptr + i * m * n)",
          "old_line_content": "        b_ptrs.push_back(&b_device_memory.back());",
          "new_line_content": "        c_device_memory.push_back(AsDeviceMemory(c_base_ptr + i * m * n));",
          "content_same": false
        },
        {
          "line": 344,
          "old_api": "x_batch_size",
          "new_api": "x_batch_indices",
          "old_text": "bcast.x_batch_size()",
          "new_text": "bcast.x_batch_indices()",
          "old_line_content": "      for (int64 i = 0; i < bcast.x_batch_size(); ++i) {",
          "new_line_content": "      const std::vector<int64>& a_batch_indices = bcast.x_batch_indices();",
          "content_same": false
        },
        {
          "line": 345,
          "old_api": "AsDeviceMemory",
          "new_api": "y_batch_indices",
          "old_text": "AsDeviceMemory(a_base_ptr + i * m * k)",
          "new_text": "bcast.y_batch_indices()",
          "old_line_content": "        a_device_memory.push_back(AsDeviceMemory(a_base_ptr + i * m * k));",
          "new_line_content": "      const std::vector<int64>& b_batch_indices = bcast.y_batch_indices();",
          "content_same": false
        },
        {
          "line": 347,
          "old_api": "y_batch_size",
          "new_api": "AsDeviceMemory",
          "old_text": "bcast.y_batch_size()",
          "new_text": "AsDeviceMemory(a_base_ptr + i * m * k)",
          "old_line_content": "      for (int64 i = 0; i < bcast.y_batch_size(); ++i) {",
          "new_line_content": "        a_device_memory.push_back(AsDeviceMemory(a_base_ptr + i * m * k));",
          "content_same": false
        },
        {
          "line": 353,
          "old_api": "push_back",
          "new_api": "AsDeviceMemory",
          "old_text": "b_ptrs.push_back(&b_device_memory[b_batch_indices[i]])",
          "new_text": "AsDeviceMemory(c_base_ptr + i * m * n)",
          "old_line_content": "        b_ptrs.push_back(&b_device_memory[b_batch_indices[i]]);",
          "new_line_content": "        c_device_memory.push_back(AsDeviceMemory(c_base_ptr + i * m * n));",
          "content_same": false
        },
        {
          "line": 354,
          "old_api": "back",
          "new_api": "push_back",
          "old_text": "c_device_memory.back()",
          "new_text": "a_ptrs.push_back(&a_device_memory[a_batch_indices[i]])",
          "old_line_content": "        c_ptrs.push_back(&c_device_memory.back());",
          "new_line_content": "        a_ptrs.push_back(&a_device_memory[a_batch_indices[i]]);",
          "content_same": false
        },
        {
          "line": 383,
          "old_api": "static_cast<Coefficient>(1.0)",
          "new_api": "ok",
          "old_text": "static_cast<Coefficient>(1.0)",
          "new_text": "stream\n                ->ThenBlasGemv(gemv_trans_a, adj_x ? m : k, adj_x ? k : m,\n                               static_cast<Coefficient>(1.0), *(a_ptrs[0]),\n                               adj_x ? m : k, *(b_ptrs[0]), 1,\n                               static_cast<Coefficient>(0.0), c_ptrs[0], 1)\n                .ok()",
          "old_line_content": "                               static_cast<Coefficient>(1.0), *(a_ptrs[0]),",
          "new_line_content": "            stream",
          "content_same": false
        },
        {
          "line": 385,
          "old_api": "static_cast<Coefficient>(0.0)",
          "new_api": "static_cast<Coefficient>(1.0)",
          "old_text": "static_cast<Coefficient>(0.0)",
          "new_text": "static_cast<Coefficient>(1.0)",
          "old_line_content": "                               static_cast<Coefficient>(0.0), c_ptrs[0], 1)",
          "new_line_content": "                               static_cast<Coefficient>(1.0), *(a_ptrs[0]),",
          "content_same": false
        },
        {
          "line": 397,
          "old_api": "static_cast<Coefficient>(1.0)",
          "new_api": "ok",
          "old_text": "static_cast<Coefficient>(1.0)",
          "new_text": "stream\n                ->ThenBlasGemm(blas_transpose_b, blas_transpose_a, n, m, k,\n                               static_cast<Coefficient>(1.0), *(b_ptrs[0]),\n                               adj_y ? k : n, *(a_ptrs[0]), adj_x ? m : k,\n                               static_cast<Coefficient>(0.0), c_ptrs[0], n)\n                .ok()",
          "old_line_content": "                               static_cast<Coefficient>(1.0), *(b_ptrs[0]),",
          "new_line_content": "            stream",
          "content_same": false
        },
        {
          "line": 399,
          "old_api": "static_cast<Coefficient>(0.0)",
          "new_api": "static_cast<Coefficient>(1.0)",
          "old_text": "static_cast<Coefficient>(0.0)",
          "new_text": "static_cast<Coefficient>(1.0)",
          "old_line_content": "                               static_cast<Coefficient>(0.0), c_ptrs[0], n)",
          "new_line_content": "                               static_cast<Coefficient>(1.0), *(b_ptrs[0]),",
          "content_same": false
        },
        {
          "line": 444,
          "old_api": "output_batch_size",
          "new_api": "dim_size",
          "old_text": "bcast.output_batch_size()",
          "new_text": "in_x.dim_size(adj_x ? 1 : 2)",
          "old_line_content": "    const uint64 batch_size = bcast.output_batch_size();",
          "new_line_content": "    const uint64 k = in_x.dim_size(adj_x ? 1 : 2);",
          "content_same": false
        },
        {
          "line": 460,
          "old_api": "reserve",
          "new_api": "x_batch_size",
          "old_text": "c_device_memory.reserve(batch_size)",
          "new_text": "bcast.x_batch_size()",
          "old_line_content": "    c_device_memory.reserve(batch_size);",
          "new_line_content": "    a_device_memory.reserve(bcast.x_batch_size());",
          "content_same": false
        },
        {
          "line": 461,
          "old_api": "reserve",
          "new_api": "y_batch_size",
          "old_text": "a_ptrs.reserve(batch_size)",
          "new_text": "bcast.y_batch_size()",
          "old_line_content": "    a_ptrs.reserve(batch_size);",
          "new_line_content": "    b_device_memory.reserve(bcast.y_batch_size());",
          "content_same": false
        },
        {
          "line": 464,
          "old_api": "data",
          "new_api": "reserve",
          "old_text": "in_x.template flat<Scalar>().data()",
          "new_text": "b_ptrs.reserve(batch_size)",
          "old_line_content": "    auto* a_base_ptr = in_x.template flat<Scalar>().data();",
          "new_line_content": "    b_ptrs.reserve(batch_size);",
          "content_same": false
        },
        {
          "line": 465,
          "old_api": "data",
          "new_api": "reserve",
          "old_text": "in_y.template flat<Scalar>().data()",
          "new_text": "c_ptrs.reserve(batch_size)",
          "old_line_content": "    auto* b_base_ptr = in_y.template flat<Scalar>().data();",
          "new_line_content": "    c_ptrs.reserve(batch_size);",
          "content_same": false
        },
        {
          "line": 468,
          "old_api": "IsBroadcastingRequired",
          "new_api": "data",
          "old_text": "bcast.IsBroadcastingRequired()",
          "new_text": "out->template flat<Scalar>().data()",
          "old_line_content": "    if (!bcast.IsBroadcastingRequired()) {",
          "new_line_content": "    auto* c_base_ptr = out->template flat<Scalar>().data();",
          "content_same": false
        },
        {
          "line": 470,
          "old_api": "AsDeviceMemory",
          "new_api": "IsBroadcastingRequired",
          "old_text": "AsDeviceMemory(a_base_ptr + i * m * k)",
          "new_text": "bcast.IsBroadcastingRequired()",
          "old_line_content": "        a_device_memory.push_back(AsDeviceMemory(a_base_ptr + i * m * k));",
          "new_line_content": "    if (!bcast.IsBroadcastingRequired()) {",
          "content_same": false
        },
        {
          "line": 473,
          "old_api": "back",
          "new_api": "AsDeviceMemory",
          "old_text": "a_device_memory.back()",
          "new_text": "AsDeviceMemory(b_base_ptr + i * k * n)",
          "old_line_content": "        a_ptrs.push_back(&a_device_memory.back());",
          "new_line_content": "        b_device_memory.push_back(AsDeviceMemory(b_base_ptr + i * k * n));",
          "content_same": false
        },
        {
          "line": 474,
          "old_api": "back",
          "new_api": "AsDeviceMemory",
          "old_text": "b_device_memory.back()",
          "new_text": "AsDeviceMemory(c_base_ptr + i * m * n)",
          "old_line_content": "        b_ptrs.push_back(&b_device_memory.back());",
          "new_line_content": "        c_device_memory.push_back(AsDeviceMemory(c_base_ptr + i * m * n));",
          "content_same": false
        },
        {
          "line": 480,
          "old_api": "x_batch_size",
          "new_api": "x_batch_indices",
          "old_text": "bcast.x_batch_size()",
          "new_text": "bcast.x_batch_indices()",
          "old_line_content": "      for (int64 i = 0; i < bcast.x_batch_size(); ++i) {",
          "new_line_content": "      const std::vector<int64>& a_batch_indices = bcast.x_batch_indices();",
          "content_same": false
        },
        {
          "line": 481,
          "old_api": "AsDeviceMemory",
          "new_api": "y_batch_indices",
          "old_text": "AsDeviceMemory(a_base_ptr + i * m * k)",
          "new_text": "bcast.y_batch_indices()",
          "old_line_content": "        a_device_memory.push_back(AsDeviceMemory(a_base_ptr + i * m * k));",
          "new_line_content": "      const std::vector<int64>& b_batch_indices = bcast.y_batch_indices();",
          "content_same": false
        },
        {
          "line": 483,
          "old_api": "y_batch_size",
          "new_api": "AsDeviceMemory",
          "old_text": "bcast.y_batch_size()",
          "new_text": "AsDeviceMemory(a_base_ptr + i * m * k)",
          "old_line_content": "      for (int64 i = 0; i < bcast.y_batch_size(); ++i) {",
          "new_line_content": "        a_device_memory.push_back(AsDeviceMemory(a_base_ptr + i * m * k));",
          "content_same": false
        },
        {
          "line": 489,
          "old_api": "push_back",
          "new_api": "AsDeviceMemory",
          "old_text": "b_ptrs.push_back(&b_device_memory[b_batch_indices[i]])",
          "new_text": "AsDeviceMemory(c_base_ptr + i * m * n)",
          "old_line_content": "        b_ptrs.push_back(&b_device_memory[b_batch_indices[i]]);",
          "new_line_content": "        c_device_memory.push_back(AsDeviceMemory(c_base_ptr + i * m * n));",
          "content_same": false
        },
        {
          "line": 490,
          "old_api": "back",
          "new_api": "push_back",
          "old_text": "c_device_memory.back()",
          "new_text": "a_ptrs.push_back(&a_device_memory[a_batch_indices[i]])",
          "old_line_content": "        c_ptrs.push_back(&c_device_memory.back());",
          "new_line_content": "        a_ptrs.push_back(&a_device_memory[a_batch_indices[i]]);",
          "content_same": false
        },
        {
          "line": 509,
          "old_api": "static_cast<Coefficient>(1.0)",
          "new_api": "ok",
          "old_text": "static_cast<Coefficient>(1.0)",
          "new_text": "stream\n              ->ThenBlasGemm(blas_transpose_b, blas_transpose_a, n, m, k,\n                             static_cast<Coefficient>(1.0), *(b_ptrs[0]),\n                             adj_y ? k : n, *(a_ptrs[0]), adj_x ? m : k,\n                             static_cast<Coefficient>(0.0), c_ptrs[0], n)\n              .ok()",
          "old_line_content": "                             static_cast<Coefficient>(1.0), *(b_ptrs[0]),",
          "new_line_content": "          stream",
          "content_same": false
        },
        {
          "line": 511,
          "old_api": "static_cast<Coefficient>(0.0)",
          "new_api": "static_cast<Coefficient>(1.0)",
          "old_text": "static_cast<Coefficient>(0.0)",
          "new_text": "static_cast<Coefficient>(1.0)",
          "old_line_content": "                             static_cast<Coefficient>(0.0), c_ptrs[0], n)",
          "new_line_content": "                             static_cast<Coefficient>(1.0), *(b_ptrs[0]),",
          "content_same": false
        },
        {
          "line": 550,
          "old_api": "out->tensor<Scalar, 3>()",
          "new_api": "in_x.tensor<Scalar, 3>()",
          "old_text": "out->tensor<Scalar, 3>()",
          "new_text": "in_x.tensor<Scalar, 3>()",
          "old_line_content": "    auto Tz = out->tensor<Scalar, 3>();",
          "new_line_content": "    auto Tx = in_x.tensor<Scalar, 3>();",
          "content_same": false
        },
        {
          "line": 552,
          "old_api": "ContractionDims",
          "new_api": "out->tensor<Scalar, 3>()",
          "old_text": "ContractionDims(adj_x, adj_y)",
          "new_text": "out->tensor<Scalar, 3>()",
          "old_line_content": "    contract_pairs[0] = ContractionDims(adj_x, adj_y);",
          "new_line_content": "    auto Tz = out->tensor<Scalar, 3>();",
          "content_same": false
        },
        {
          "line": 555,
          "old_api": "IsBroadcastingRequired",
          "new_api": "eigen_sycl_device",
          "old_text": "bcast.IsBroadcastingRequired()",
          "new_text": "context->eigen_sycl_device()",
          "old_line_content": "    const bool should_bcast = bcast.IsBroadcastingRequired();",
          "new_line_content": "    auto d = context->eigen_sycl_device();",
          "content_same": false
        },
        {
          "line": 557,
          "old_api": "y_batch_indices",
          "new_api": "IsBroadcastingRequired",
          "old_text": "bcast.y_batch_indices()",
          "new_text": "bcast.IsBroadcastingRequired()",
          "old_line_content": "    const auto& y_batch_indices = bcast.y_batch_indices();",
          "new_line_content": "    const bool should_bcast = bcast.IsBroadcastingRequired();",
          "content_same": false
        },
        {
          "line": 564,
          "old_api": "Tz.template chip<0>(i)",
          "new_api": "Tx.template chip<0>(x_batch_index)",
          "old_text": "Tz.template chip<0>(i)",
          "new_text": "Tx.template chip<0>(x_batch_index)",
          "old_line_content": "      auto z = Tz.template chip<0>(i);",
          "new_line_content": "      auto x = Tx.template chip<0>(x_batch_index);",
          "content_same": false
        },
        {
          "line": 565,
          "old_api": "contract",
          "new_api": "Ty.template chip<0>(y_batch_index)",
          "old_text": "x.contract(y, contract_pairs)",
          "new_text": "Ty.template chip<0>(y_batch_index)",
          "old_line_content": "      z.device(d) = x.contract(y, contract_pairs);",
          "new_line_content": "      auto y = Ty.template chip<0>(y_batch_index);",
          "content_same": false
        },
        {
          "line": 588,
          "old_api": "GetAttr",
          "new_api": "explicit",
          "old_text": "context->GetAttr(\"adj_x\", &adj_x_)",
          "new_text": "explicit",
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"adj_x\", &adj_x_));",
          "new_line_content": "  explicit BaseBatchMatMulOp(OpKernelConstruction* context)",
          "content_same": false
        },
        {
          "line": 598,
          "old_api": "ValidateInputTensors",
          "new_api": "input",
          "old_text": "ValidateInputTensors(ctx, in0, in1)",
          "new_text": "ctx->input(1)",
          "old_line_content": "    ValidateInputTensors(ctx, in0, in1);",
          "new_line_content": "    const Tensor& in1 = ctx->input(1);",
          "content_same": false
        },
        {
          "line": 600,
          "old_api": "shape",
          "new_api": "ValidateInputTensors",
          "old_text": "in1.shape().dim_sizes()",
          "new_text": "ValidateInputTensors(ctx, in0, in1)",
          "old_line_content": "    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());",
          "new_line_content": "    ValidateInputTensors(ctx, in0, in1);",
          "content_same": false
        },
        {
          "line": 602,
          "old_api": "IsValid",
          "new_api": "shape",
          "old_text": "bcast.IsValid()",
          "new_text": "in1.shape().dim_sizes()",
          "old_line_content": "        ctx, bcast.IsValid(),",
          "new_line_content": "    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());",
          "content_same": false
        },
        {
          "line": 603,
          "old_api": "shape",
          "new_api": "IsValid",
          "old_text": "errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString())",
          "new_text": "OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()))",
          "old_line_content": "        errors::InvalidArgument(",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 607,
          "old_api": "output_batch_shape",
          "new_api": "shape",
          "old_text": "bcast.output_batch_shape()",
          "new_text": "in1.shape().DebugString()",
          "old_line_content": "    TensorShape out_shape = bcast.output_batch_shape();",
          "new_line_content": "            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()));",
          "content_same": false
        },
        {
          "line": 609,
          "old_api": "dims",
          "new_api": "output_batch_shape",
          "old_text": "in0.dims()",
          "new_text": "bcast.output_batch_shape()",
          "old_line_content": "    auto d0 = in0.dim_size(in0.dims() - 2);",
          "new_line_content": "    TensorShape out_shape = bcast.output_batch_shape();",
          "content_same": false
        },
        {
          "line": 610,
          "old_api": "dims",
          "new_api": "output_batch_size",
          "old_text": "in0.dims()",
          "new_text": "bcast.output_batch_size()",
          "old_line_content": "    auto d1 = in0.dim_size(in0.dims() - 1);",
          "new_line_content": "    auto batch_size = bcast.output_batch_size();",
          "content_same": false
        },
        {
          "line": 612,
          "old_api": "CopyFrom",
          "new_api": "dims",
          "old_text": "OP_REQUIRES(\n        ctx,\n        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),\n        errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString()))",
          "new_text": "in0.dims()",
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "    auto d1 = in0.dim_size(in0.dims() - 1);",
          "content_same": false
        },
        {
          "line": 614,
          "old_api": "x_batch_size",
          "new_api": "CopyFrom",
          "old_text": "bcast.x_batch_size()",
          "new_text": "OP_REQUIRES(\n        ctx,\n        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),\n        errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString()))",
          "old_line_content": "        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 616,
          "old_api": "shape",
          "new_api": "x_batch_size",
          "old_text": "in0.shape().DebugString()",
          "new_text": "bcast.x_batch_size()",
          "old_line_content": "                         in0.shape().DebugString()));",
          "new_line_content": "        in0_reshaped.CopyFrom(in0, TensorShape({bcast.x_batch_size(), d0, d1})),",
          "content_same": false
        },
        {
          "line": 617,
          "old_api": "dims",
          "new_api": "shape",
          "old_text": "in1.dims()",
          "new_text": "errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString())",
          "old_line_content": "    auto d2 = in1.dim_size(in1.dims() - 2);",
          "new_line_content": "        errors::Internal(\"Failed to reshape In[0] from \",",
          "content_same": false
        },
        {
          "line": 618,
          "old_api": "dims",
          "new_api": "shape",
          "old_text": "in1.dims()",
          "new_text": "in0.shape().DebugString()",
          "old_line_content": "    auto d3 = in1.dim_size(in1.dims() - 1);",
          "new_line_content": "                         in0.shape().DebugString()));",
          "content_same": false
        },
        {
          "line": 620,
          "old_api": "CopyFrom",
          "new_api": "dims",
          "old_text": "OP_REQUIRES(\n        ctx,\n        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),\n        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()))",
          "new_text": "in1.dims()",
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "    auto d3 = in1.dim_size(in1.dims() - 1);",
          "content_same": false
        },
        {
          "line": 622,
          "old_api": "y_batch_size",
          "new_api": "CopyFrom",
          "old_text": "bcast.y_batch_size()",
          "new_text": "OP_REQUIRES(\n        ctx,\n        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),\n        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()))",
          "old_line_content": "        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 624,
          "old_api": "shape",
          "new_api": "y_batch_size",
          "old_text": "in1.shape().DebugString()",
          "new_text": "bcast.y_batch_size()",
          "old_line_content": "                         in1.shape().DebugString()));",
          "new_line_content": "        in1_reshaped.CopyFrom(in1, TensorShape({bcast.y_batch_size(), d2, d3})),",
          "content_same": false
        },
        {
          "line": 625,
          "old_api": "std::swap(d0, d1)",
          "new_api": "shape",
          "old_text": "std::swap(d0, d1)",
          "new_text": "errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString())",
          "old_line_content": "    if (adj_x_) std::swap(d0, d1);",
          "new_line_content": "        errors::Internal(\"Failed to reshape In[1] from \",",
          "content_same": false
        },
        {
          "line": 626,
          "old_api": "std::swap(d2, d3)",
          "new_api": "shape",
          "old_text": "std::swap(d2, d3)",
          "new_text": "in1.shape().DebugString()",
          "old_line_content": "    if (adj_y_) std::swap(d2, d3);",
          "new_line_content": "                         in1.shape().DebugString()));",
          "content_same": false
        },
        {
          "line": 627,
          "old_api": "shape",
          "new_api": "std::swap(d0, d1)",
          "old_text": "OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", adj_x_, \" \", adj_y_))",
          "new_text": "std::swap(d0, d1)",
          "old_line_content": "    OP_REQUIRES(ctx, d1 == d2,",
          "new_line_content": "    if (adj_x_) std::swap(d0, d1);",
          "content_same": false
        },
        {
          "line": 628,
          "old_api": "shape",
          "new_api": "std::swap(d2, d3)",
          "old_text": "errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", adj_x_, \" \", adj_y_)",
          "new_text": "std::swap(d2, d3)",
          "old_line_content": "                errors::InvalidArgument(",
          "new_line_content": "    if (adj_y_) std::swap(d2, d3);",
          "content_same": false
        },
        {
          "line": 632,
          "old_api": "AddDim",
          "new_api": "shape",
          "old_text": "out_shape.AddDim(d0)",
          "new_text": "in1.shape().DebugString()",
          "old_line_content": "    out_shape.AddDim(d0);",
          "new_line_content": "                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),",
          "content_same": false
        },
        {
          "line": 635,
          "old_api": "allocate_output",
          "new_api": "AddDim",
          "old_text": "ctx->allocate_output(0, out_shape, &out)",
          "new_text": "out_shape.AddDim(d3)",
          "old_line_content": "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));",
          "new_line_content": "    out_shape.AddDim(d3);",
          "content_same": false
        },
        {
          "line": 641,
          "old_api": "out->flat<Scalar>()",
          "new_api": "NumElements",
          "old_text": "out->flat<Scalar>()",
          "new_text": "in1.NumElements()",
          "old_line_content": "      f(ctx->eigen_device<Device>(), out->flat<Scalar>());",
          "new_line_content": "    if (in0.NumElements() == 0 || in1.NumElements() == 0) {",
          "content_same": false
        },
        {
          "line": 647,
          "old_api": "DebugString",
          "new_api": "CopyFrom",
          "old_text": "errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString())",
          "new_text": "OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d0, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()))",
          "old_line_content": "                errors::Internal(\"Failed to reshape output from \",",
          "new_line_content": "    OP_REQUIRES(ctx,",
          "content_same": false
        },
        {
          "line": 648,
          "old_api": "DebugString",
          "new_api": "TensorShape",
          "old_text": "out->shape().DebugString()",
          "new_text": "TensorShape({batch_size, d0, d3})",
          "old_line_content": "                                 out->shape().DebugString()));",
          "new_line_content": "                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d0, d3})),",
          "content_same": false
        },
        {
          "line": 649,
          "old_api": "LaunchBatchMatMul<Device, Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adj_x_, adj_y_, bcast, &out_reshaped)",
          "new_api": "DebugString",
          "old_text": "LaunchBatchMatMul<Device, Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adj_x_, adj_y_, bcast, &out_reshaped)",
          "new_text": "errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString())",
          "old_line_content": "    LaunchBatchMatMul<Device, Scalar>::Launch(",
          "new_line_content": "                errors::Internal(\"Failed to reshape output from \",",
          "content_same": false
        },
        {
          "line": 678,
          "old_api": "shape",
          "new_api": "dims",
          "old_text": "in0.shape().DebugString()",
          "new_text": "in1.dims()",
          "old_line_content": "                                        in0.shape().DebugString(), \" vs. \",",
          "new_line_content": "    OP_REQUIRES(ctx, in0.dims() == in1.dims(),",
          "content_same": false
        },
        {
          "line": 680,
          "old_api": "dims",
          "new_api": "shape",
          "old_text": "in0.dims()",
          "new_text": "in0.shape().DebugString()",
          "old_line_content": "    const int ndims = in0.dims();",
          "new_line_content": "                                        in0.shape().DebugString(), \" vs. \",",
          "content_same": false
        },
        {
          "line": 681,
          "old_api": "OP_REQUIRES",
          "new_api": "shape",
          "old_text": "OP_REQUIRES(\n        ctx, ndims >= 2,\n        errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims))",
          "new_text": "in1.shape().DebugString()",
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "                                        in1.shape().DebugString()));",
          "content_same": false
        },
        {
          "line": 683,
          "old_api": "errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims)",
          "new_api": "OP_REQUIRES",
          "old_text": "errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims)",
          "new_text": "OP_REQUIRES(\n        ctx, ndims >= 2,\n        errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims))",
          "old_line_content": "        errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims));",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 685,
          "old_api": "dim_size",
          "new_api": "errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims)",
          "old_text": "in1.dim_size(i)",
          "new_text": "errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims)",
          "old_line_content": "      OP_REQUIRES(ctx, in0.dim_size(i) == in1.dim_size(i),",
          "new_line_content": "        errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims));",
          "content_same": false
        },
        {
          "line": 688,
          "old_api": "shape",
          "new_api": "dim",
          "old_text": "in0.shape().DebugString()",
          "new_text": "errors::InvalidArgument(\n                      \"In[0].dim(\", i, \") and In[1].dim(\", i,\n                      \") must be the same: \", in0.shape().DebugString(), \" vs \",\n                      in1.shape().DebugString())",
          "old_line_content": "                      \") must be the same: \", in0.shape().DebugString(), \" vs \",",
          "new_line_content": "                  errors::InvalidArgument(",
          "content_same": false
        }
      ],
      "additions": [
        {
          "line": 513,
          "old_api": null,
          "new_api": "static_cast<Coefficient>(0.0)",
          "old_text": null,
          "new_text": "static_cast<Coefficient>(0.0)",
          "old_line_content": "      if (!blas_launch_status) {",
          "new_line_content": "                             static_cast<Coefficient>(0.0), c_ptrs[0], n)",
          "content_same": false
        },
        {
          "line": 517,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_x.shape().DebugString()",
          "old_line_content": "            \", k=\", k));",
          "new_line_content": "            \"Blas xGEMM launch failed : a.shape=\", in_x.shape().DebugString(),",
          "content_same": false
        },
        {
          "line": 518,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_y.shape().DebugString()",
          "old_line_content": "      }",
          "new_line_content": "            \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,",
          "content_same": false
        },
        {
          "line": 524,
          "old_api": null,
          "new_api": "ok",
          "old_text": null,
          "new_text": "stream\n              ->ThenBlasGemmBatchedWithScratch(\n                  blas_transpose_b, blas_transpose_a, n, m, k,\n                  static_cast<Coefficient>(1.0), b_ptrs, adj_y ? k : n, a_ptrs,\n                  adj_x ? m : k, static_cast<Coefficient>(0.0), c_ptrs, n,\n                  batch_size, &scratch_allocator)\n              .ok()",
          "old_line_content": "                  blas_transpose_b, blas_transpose_a, n, m, k,",
          "new_line_content": "          stream",
          "content_same": false
        },
        {
          "line": 527,
          "old_api": null,
          "new_api": "static_cast<Coefficient>(1.0)",
          "old_text": null,
          "new_text": "static_cast<Coefficient>(1.0)",
          "old_line_content": "                  batch_size, &scratch_allocator)",
          "new_line_content": "                  static_cast<Coefficient>(1.0), b_ptrs, adj_y ? k : n, a_ptrs,",
          "content_same": false
        },
        {
          "line": 528,
          "old_api": null,
          "new_api": "static_cast<Coefficient>(0.0)",
          "old_text": null,
          "new_text": "static_cast<Coefficient>(0.0)",
          "old_line_content": "              .ok();",
          "new_line_content": "                  adj_x ? m : k, static_cast<Coefficient>(0.0), c_ptrs, n,",
          "content_same": false
        },
        {
          "line": 534,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_x.shape().DebugString()",
          "old_line_content": "            \", k=\", k, \", batch_size=\", batch_size));",
          "new_line_content": "            in_x.shape().DebugString(),",
          "content_same": false
        },
        {
          "line": 535,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_y.shape().DebugString()",
          "old_line_content": "      }",
          "new_line_content": "            \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,",
          "content_same": false
        },
        {
          "line": 551,
          "old_api": null,
          "new_api": "in_y.tensor<Scalar, 3>()",
          "old_text": null,
          "new_text": "in_y.tensor<Scalar, 3>()",
          "old_line_content": "    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract_pairs;",
          "new_line_content": "    auto Ty = in_y.tensor<Scalar, 3>();",
          "content_same": false
        },
        {
          "line": 554,
          "old_api": null,
          "new_api": "ContractionDims",
          "old_text": null,
          "new_text": "ContractionDims(adj_x, adj_y)",
          "old_line_content": "",
          "new_line_content": "    contract_pairs[0] = ContractionDims(adj_x, adj_y);",
          "content_same": false
        },
        {
          "line": 558,
          "old_api": null,
          "new_api": "x_batch_indices",
          "old_text": null,
          "new_text": "bcast.x_batch_indices()",
          "old_line_content": "    for (int64 i = start; i < limit; ++i) {",
          "new_line_content": "    const auto& x_batch_indices = bcast.x_batch_indices();",
          "content_same": false
        },
        {
          "line": 559,
          "old_api": null,
          "new_api": "y_batch_indices",
          "old_text": null,
          "new_text": "bcast.y_batch_indices()",
          "old_line_content": "      const int64 x_batch_index = should_bcast ? x_batch_indices[i] : i;",
          "new_line_content": "    const auto& y_batch_indices = bcast.y_batch_indices();",
          "content_same": false
        },
        {
          "line": 566,
          "old_api": null,
          "new_api": "Tz.template chip<0>(i)",
          "old_text": null,
          "new_text": "Tz.template chip<0>(i)",
          "old_line_content": "    }",
          "new_line_content": "      auto z = Tz.template chip<0>(i);",
          "content_same": false
        },
        {
          "line": 567,
          "old_api": null,
          "new_api": "contract",
          "old_text": null,
          "new_text": "x.contract(y, contract_pairs)",
          "old_line_content": "  }",
          "new_line_content": "      z.device(d) = x.contract(y, contract_pairs);",
          "content_same": false
        },
        {
          "line": 578,
          "old_api": null,
          "new_api": "output_batch_size",
          "old_text": null,
          "new_text": "bcast.output_batch_size()",
          "old_line_content": "                                          bcast, out, 0, batch_size);",
          "new_line_content": "    const int64 batch_size = bcast.output_batch_size();",
          "content_same": false
        },
        {
          "line": 579,
          "old_api": null,
          "new_api": "ParallelMatMulKernelSYCL<Scalar>::Run(context, in_x, in_y, adj_x, adj_y,\n                                          bcast, out, 0, batch_size)",
          "old_text": null,
          "new_text": "ParallelMatMulKernelSYCL<Scalar>::Run(context, in_x, in_y, adj_x, adj_y,\n                                          bcast, out, 0, batch_size)",
          "old_line_content": "  }",
          "new_line_content": "    ParallelMatMulKernelSYCL<Scalar>::Run(context, in_x, in_y, adj_x, adj_y,",
          "content_same": false
        },
        {
          "line": 590,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"adj_x\", &adj_x_)",
          "old_line_content": "  }",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"adj_x\", &adj_x_));",
          "content_same": false
        },
        {
          "line": 591,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"adj_y\", &adj_y_)",
          "old_line_content": "",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"adj_y\", &adj_y_));",
          "content_same": false
        },
        {
          "line": 597,
          "old_api": null,
          "new_api": "input",
          "old_text": null,
          "new_text": "ctx->input(0)",
          "old_line_content": "",
          "new_line_content": "    const Tensor& in0 = ctx->input(0);",
          "content_same": false
        },
        {
          "line": 604,
          "old_api": null,
          "new_api": "IsValid",
          "old_text": null,
          "new_text": "bcast.IsValid()",
          "old_line_content": "            \"In[0] and In[1] must have compatible batch dimensions: \",",
          "new_line_content": "        ctx, bcast.IsValid(),",
          "content_same": false
        },
        {
          "line": 611,
          "old_api": null,
          "new_api": "dims",
          "old_text": null,
          "new_text": "in0.dims()",
          "old_line_content": "    Tensor in0_reshaped;",
          "new_line_content": "    auto d0 = in0.dim_size(in0.dims() - 2);",
          "content_same": false
        },
        {
          "line": 619,
          "old_api": null,
          "new_api": "dims",
          "old_text": null,
          "new_text": "in1.dims()",
          "old_line_content": "    Tensor in1_reshaped;",
          "new_line_content": "    auto d2 = in1.dim_size(in1.dims() - 2);",
          "content_same": false
        },
        {
          "line": 629,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", adj_x_, \" \", adj_y_))",
          "old_line_content": "                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",",
          "new_line_content": "    OP_REQUIRES(ctx, d1 == d2,",
          "content_same": false
        },
        {
          "line": 634,
          "old_api": null,
          "new_api": "AddDim",
          "old_text": null,
          "new_text": "out_shape.AddDim(d0)",
          "old_line_content": "    Tensor* out = nullptr;",
          "new_line_content": "    out_shape.AddDim(d0);",
          "content_same": false
        },
        {
          "line": 637,
          "old_api": null,
          "new_api": "allocate_output",
          "old_text": null,
          "new_text": "ctx->allocate_output(0, out_shape, &out)",
          "old_line_content": "      return;",
          "new_line_content": "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));",
          "content_same": false
        },
        {
          "line": 638,
          "old_api": null,
          "new_api": "NumElements",
          "old_text": null,
          "new_text": "out->NumElements()",
          "old_line_content": "    }",
          "new_line_content": "    if (out->NumElements() == 0) {",
          "content_same": false
        },
        {
          "line": 643,
          "old_api": null,
          "new_api": "out->flat<Scalar>()",
          "old_text": null,
          "new_text": "out->flat<Scalar>()",
          "old_line_content": "    }",
          "new_line_content": "      f(ctx->eigen_device<Device>(), out->flat<Scalar>());",
          "content_same": false
        },
        {
          "line": 650,
          "old_api": null,
          "new_api": "DebugString",
          "old_text": null,
          "new_text": "out->shape().DebugString()",
          "old_line_content": "        ctx, in0_reshaped, in1_reshaped, adj_x_, adj_y_, bcast, &out_reshaped);",
          "new_line_content": "                                 out->shape().DebugString()));",
          "content_same": false
        },
        {
          "line": 651,
          "old_api": null,
          "new_api": "LaunchBatchMatMul<Device, Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adj_x_, adj_y_, bcast, &out_reshaped)",
          "old_text": null,
          "new_text": "LaunchBatchMatMul<Device, Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adj_x_, adj_y_, bcast, &out_reshaped)",
          "old_line_content": "  }",
          "new_line_content": "    LaunchBatchMatMul<Device, Scalar>::Launch(",
          "content_same": false
        },
        {
          "line": 668,
          "old_api": null,
          "new_api": "explicit",
          "old_text": null,
          "new_text": "explicit",
          "old_line_content": "",
          "new_line_content": "  explicit BatchMatMulOp(OpKernelConstruction* context)",
          "content_same": false
        },
        {
          "line": 682,
          "old_api": null,
          "new_api": "dims",
          "old_text": null,
          "new_text": "in0.dims()",
          "old_line_content": "        ctx, ndims >= 2,",
          "new_line_content": "    const int ndims = in0.dims();",
          "content_same": false
        },
        {
          "line": 687,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "in1.dim_size(i)",
          "old_line_content": "                      \"In[0].dim(\", i, \") and In[1].dim(\", i,",
          "new_line_content": "      OP_REQUIRES(ctx, in0.dim_size(i) == in1.dim_size(i),",
          "content_same": false
        },
        {
          "line": 690,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in0.shape().DebugString()",
          "old_line_content": "    }",
          "new_line_content": "                      \") must be the same: \", in0.shape().DebugString(), \" vs \",",
          "content_same": false
        },
        {
          "line": 691,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in1.shape().DebugString()",
          "old_line_content": "  }",
          "new_line_content": "                      in1.shape().DebugString()));",
          "content_same": false
        },
        {
          "line": 700,
          "old_api": null,
          "new_api": "explicit",
          "old_text": null,
          "new_text": "explicit",
          "old_line_content": "",
          "new_line_content": "  explicit BatchMatMulV2Op(OpKernelConstruction* context)",
          "content_same": false
        },
        {
          "line": 714,
          "old_api": null,
          "new_api": "dims",
          "old_text": null,
          "new_text": "in1.dims()",
          "old_line_content": "  }",
          "new_line_content": "        ctx, in1.dims() >= 2,",
          "content_same": false
        },
        {
          "line": 715,
          "old_api": null,
          "new_api": "dims",
          "old_text": null,
          "new_text": "in1.dims()",
          "old_line_content": "};",
          "new_line_content": "        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1.dims()));",
          "content_same": false
        },
        {
          "line": 223,
          "old_api": null,
          "new_api": "device",
          "old_text": null,
          "new_text": "context->device()->tensorflow_cpu_worker_threads()",
          "old_line_content": "        (batch_size == 1 || cost_per_unit > kMaxCostOuterParallelism)) {",
          "new_line_content": "    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());",
          "content_same": false
        },
        {
          "line": 229,
          "old_api": null,
          "new_api": "ParallelMatMulKernel::Run(context, in_x, in_y, adj_x, adj_y, bcast, out,\n                                0, batch_size)",
          "old_text": null,
          "new_text": "ParallelMatMulKernel::Run(context, in_x, in_y, adj_x, adj_y, bcast, out,\n                                0, batch_size)",
          "old_line_content": "      conjugate_result = adj_x;",
          "new_line_content": "      ParallelMatMulKernel::Run(context, in_x, in_y, adj_x, adj_y, bcast, out,",
          "content_same": false
        },
        {
          "line": 235,
          "old_api": null,
          "new_api": "Shard",
          "old_text": null,
          "new_text": "Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n            cost_per_unit,\n            [&in_x, &in_y, adj_x, adj_y, &bcast, out](int start, int limit) {\n              SequentialMatMulKernel<Scalar>::Run(in_x, in_y, adj_x, adj_y,\n                                                  bcast, out, start, limit);\n            })",
          "old_line_content": "            [&in_x, &in_y, adj_x, adj_y, &bcast, out](int start, int limit) {",
          "new_line_content": "      Shard(worker_threads.num_threads, worker_threads.workers, batch_size,",
          "content_same": false
        },
        {
          "line": 238,
          "old_api": null,
          "new_api": "SequentialMatMulKernel<Scalar>::Run(in_x, in_y, adj_x, adj_y,\n                                                  bcast, out, start, limit)",
          "old_text": null,
          "new_text": "SequentialMatMulKernel<Scalar>::Run(in_x, in_y, adj_x, adj_y,\n                                                  bcast, out, start, limit)",
          "old_line_content": "            });",
          "new_line_content": "              SequentialMatMulKernel<Scalar>::Run(in_x, in_y, adj_x, adj_y,",
          "content_same": false
        },
        {
          "line": 248,
          "old_api": null,
          "new_api": "ParallelMatMulKernel::Conjugate(context, out)",
          "old_text": null,
          "new_text": "ParallelMatMulKernel::Conjugate(context, out)",
          "old_line_content": "  }",
          "new_line_content": "      ParallelMatMulKernel::Conjugate(context, out);",
          "content_same": false
        },
        {
          "line": 279,
          "old_api": null,
          "new_api": "se::port::StatusOr<DeviceMemoryBytes>(\n          DeviceMemoryBytes::MakeFromByteSize(nullptr, 0))",
          "old_text": null,
          "new_text": "se::port::StatusOr<DeviceMemoryBytes>(\n          DeviceMemoryBytes::MakeFromByteSize(nullptr, 0))",
          "old_line_content": "    }",
          "new_line_content": "      return se::port::StatusOr<DeviceMemoryBytes>(",
          "content_same": false
        },
        {
          "line": 280,
          "old_api": null,
          "new_api": "DeviceMemoryBytes::MakeFromByteSize(nullptr, 0)",
          "old_text": null,
          "new_text": "DeviceMemoryBytes::MakeFromByteSize(nullptr, 0)",
          "old_line_content": "    // Hold the reference of the allocated tensors until the end of the",
          "new_line_content": "          DeviceMemoryBytes::MakeFromByteSize(nullptr, 0));",
          "content_same": false
        },
        {
          "line": 287,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "temporary_memory.flat<uint8>().data()",
          "old_line_content": "  }",
          "new_line_content": "            temporary_memory.flat<uint8>().data(),",
          "content_same": false
        },
        {
          "line": 288,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "temporary_memory.flat<uint8>().size()",
          "old_line_content": "",
          "new_line_content": "            temporary_memory.flat<uint8>().size()));",
          "content_same": false
        },
        {
          "line": 302,
          "old_api": null,
          "new_api": "constexpr",
          "old_text": null,
          "new_text": "constexpr",
          "old_line_content": "                                  : se::blas::Transpose::kTranspose;",
          "new_line_content": "    constexpr se::blas::Transpose kTranspose =",
          "content_same": false
        },
        {
          "line": 309,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "in_y.dim_size(adj_y ? 1 : 2)",
          "old_line_content": "    auto blas_transpose_a = trans[adj_x];",
          "new_line_content": "    const uint64 n = in_y.dim_size(adj_y ? 1 : 2);",
          "content_same": false
        },
        {
          "line": 310,
          "old_api": null,
          "new_api": "output_batch_size",
          "old_text": null,
          "new_text": "bcast.output_batch_size()",
          "old_line_content": "    auto blas_transpose_b = trans[adj_y];",
          "new_line_content": "    const int64 batch_size = bcast.output_batch_size();",
          "content_same": false
        },
        {
          "line": 314,
          "old_api": null,
          "new_api": "op_device_context",
          "old_text": null,
          "new_text": "context->op_device_context()->stream()",
          "old_line_content": "",
          "new_line_content": "    auto* stream = context->op_device_context()->stream();",
          "content_same": false
        },
        {
          "line": 315,
          "old_api": null,
          "new_api": "errors::Internal(\"No GPU stream available.\")",
          "old_text": null,
          "new_text": "errors::Internal(\"No GPU stream available.\")",
          "old_line_content": "    typedef se::DeviceMemory<Scalar> DeviceMemoryType;",
          "new_line_content": "    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));",
          "content_same": false
        },
        {
          "line": 331,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "in_y.template flat<Scalar>().data()",
          "old_line_content": "",
          "new_line_content": "    auto* b_base_ptr = in_y.template flat<Scalar>().data();",
          "content_same": false
        },
        {
          "line": 340,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "b_device_memory.back()",
          "old_line_content": "      }",
          "new_line_content": "        b_ptrs.push_back(&b_device_memory.back());",
          "content_same": false
        },
        {
          "line": 341,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "c_device_memory.back()",
          "old_line_content": "    } else {",
          "new_line_content": "        c_ptrs.push_back(&c_device_memory.back());",
          "content_same": false
        },
        {
          "line": 346,
          "old_api": null,
          "new_api": "x_batch_size",
          "old_text": null,
          "new_text": "bcast.x_batch_size()",
          "old_line_content": "      }",
          "new_line_content": "      for (int64 i = 0; i < bcast.x_batch_size(); ++i) {",
          "content_same": false
        },
        {
          "line": 349,
          "old_api": null,
          "new_api": "y_batch_size",
          "old_text": null,
          "new_text": "bcast.y_batch_size()",
          "old_line_content": "      }",
          "new_line_content": "      for (int64 i = 0; i < bcast.y_batch_size(); ++i) {",
          "content_same": false
        },
        {
          "line": 350,
          "old_api": null,
          "new_api": "AsDeviceMemory",
          "old_text": null,
          "new_text": "AsDeviceMemory(b_base_ptr + i * k * n)",
          "old_line_content": "      for (int64 i = 0; i < batch_size; ++i) {",
          "new_line_content": "        b_device_memory.push_back(AsDeviceMemory(b_base_ptr + i * k * n));",
          "content_same": false
        },
        {
          "line": 355,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "b_ptrs.push_back(&b_device_memory[b_batch_indices[i]])",
          "old_line_content": "      }",
          "new_line_content": "        b_ptrs.push_back(&b_device_memory[b_batch_indices[i]]);",
          "content_same": false
        },
        {
          "line": 356,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "c_device_memory.back()",
          "old_line_content": "    }",
          "new_line_content": "        c_ptrs.push_back(&c_device_memory.back());",
          "content_same": false
        },
        {
          "line": 387,
          "old_api": null,
          "new_api": "static_cast<Coefficient>(0.0)",
          "old_text": null,
          "new_text": "static_cast<Coefficient>(0.0)",
          "old_line_content": "        if (!blas_launch_status) {",
          "new_line_content": "                               static_cast<Coefficient>(0.0), c_ptrs[0], 1)",
          "content_same": false
        },
        {
          "line": 391,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_x.shape().DebugString()",
          "old_line_content": "              \", k=\", k));",
          "new_line_content": "              \"Blas xGEMV launch failed : a.shape=\", in_x.shape().DebugString(),",
          "content_same": false
        },
        {
          "line": 392,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_y.shape().DebugString()",
          "old_line_content": "        }",
          "new_line_content": "              \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,",
          "content_same": false
        },
        {
          "line": 401,
          "old_api": null,
          "new_api": "static_cast<Coefficient>(0.0)",
          "old_text": null,
          "new_text": "static_cast<Coefficient>(0.0)",
          "old_line_content": "        if (!blas_launch_status) {",
          "new_line_content": "                               static_cast<Coefficient>(0.0), c_ptrs[0], n)",
          "content_same": false
        },
        {
          "line": 405,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_x.shape().DebugString()",
          "old_line_content": "              \", k=\", k));",
          "new_line_content": "              \"Blas xGEMM launch failed : a.shape=\", in_x.shape().DebugString(),",
          "content_same": false
        },
        {
          "line": 406,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_y.shape().DebugString()",
          "old_line_content": "        }",
          "new_line_content": "              \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,",
          "content_same": false
        },
        {
          "line": 413,
          "old_api": null,
          "new_api": "ok",
          "old_text": null,
          "new_text": "stream\n              ->ThenBlasGemmBatchedWithScratch(\n                  blas_transpose_b, blas_transpose_a, n, m, k,\n                  static_cast<Coefficient>(1.0), b_ptrs, adj_y ? k : n, a_ptrs,\n                  adj_x ? m : k, static_cast<Coefficient>(0.0), c_ptrs, n,\n                  batch_size, &scratch_allocator)\n              .ok()",
          "old_line_content": "                  blas_transpose_b, blas_transpose_a, n, m, k,",
          "new_line_content": "          stream",
          "content_same": false
        },
        {
          "line": 416,
          "old_api": null,
          "new_api": "static_cast<Coefficient>(1.0)",
          "old_text": null,
          "new_text": "static_cast<Coefficient>(1.0)",
          "old_line_content": "                  batch_size, &scratch_allocator)",
          "new_line_content": "                  static_cast<Coefficient>(1.0), b_ptrs, adj_y ? k : n, a_ptrs,",
          "content_same": false
        },
        {
          "line": 417,
          "old_api": null,
          "new_api": "static_cast<Coefficient>(0.0)",
          "old_text": null,
          "new_text": "static_cast<Coefficient>(0.0)",
          "old_line_content": "              .ok();",
          "new_line_content": "                  adj_x ? m : k, static_cast<Coefficient>(0.0), c_ptrs, n,",
          "content_same": false
        },
        {
          "line": 423,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_x.shape().DebugString()",
          "old_line_content": "            \", k=\", k, \", batch_size=\", batch_size));",
          "new_line_content": "            in_x.shape().DebugString(),",
          "content_same": false
        },
        {
          "line": 424,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "in_y.shape().DebugString()",
          "old_line_content": "      }",
          "new_line_content": "            \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,",
          "content_same": false
        },
        {
          "line": 437,
          "old_api": null,
          "new_api": "constexpr",
          "old_text": null,
          "new_text": "constexpr",
          "old_line_content": "            ? perftools::gputools::blas::Transpose::kConjugateTranspose",
          "new_line_content": "    constexpr perftools::gputools::blas::Transpose kTranspose =",
          "content_same": false
        },
        {
          "line": 445,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "in_y.dim_size(adj_y ? 1 : 2)",
          "old_line_content": "    auto blas_transpose_a = trans[adj_x];",
          "new_line_content": "    const uint64 n = in_y.dim_size(adj_y ? 1 : 2);",
          "content_same": false
        },
        {
          "line": 446,
          "old_api": null,
          "new_api": "output_batch_size",
          "old_text": null,
          "new_text": "bcast.output_batch_size()",
          "old_line_content": "    auto blas_transpose_b = trans[adj_y];",
          "new_line_content": "    const uint64 batch_size = bcast.output_batch_size();",
          "content_same": false
        },
        {
          "line": 450,
          "old_api": null,
          "new_api": "op_device_context",
          "old_text": null,
          "new_text": "context->op_device_context()->stream()",
          "old_line_content": "",
          "new_line_content": "    auto* stream = context->op_device_context()->stream();",
          "content_same": false
        },
        {
          "line": 451,
          "old_api": null,
          "new_api": "errors::Internal(\"No GPU stream available.\")",
          "old_text": null,
          "new_text": "errors::Internal(\"No GPU stream available.\")",
          "old_line_content": "    typedef perftools::gputools::DeviceMemory<Scalar> DeviceMemoryType;",
          "new_line_content": "    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));",
          "content_same": false
        },
        {
          "line": 467,
          "old_api": null,
          "new_api": "data",
          "old_text": null,
          "new_text": "in_y.template flat<Scalar>().data()",
          "old_line_content": "",
          "new_line_content": "    auto* b_base_ptr = in_y.template flat<Scalar>().data();",
          "content_same": false
        },
        {
          "line": 476,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "b_device_memory.back()",
          "old_line_content": "      }",
          "new_line_content": "        b_ptrs.push_back(&b_device_memory.back());",
          "content_same": false
        },
        {
          "line": 477,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "c_device_memory.back()",
          "old_line_content": "    } else {",
          "new_line_content": "        c_ptrs.push_back(&c_device_memory.back());",
          "content_same": false
        },
        {
          "line": 482,
          "old_api": null,
          "new_api": "x_batch_size",
          "old_text": null,
          "new_text": "bcast.x_batch_size()",
          "old_line_content": "      }",
          "new_line_content": "      for (int64 i = 0; i < bcast.x_batch_size(); ++i) {",
          "content_same": false
        },
        {
          "line": 485,
          "old_api": null,
          "new_api": "y_batch_size",
          "old_text": null,
          "new_text": "bcast.y_batch_size()",
          "old_line_content": "      }",
          "new_line_content": "      for (int64 i = 0; i < bcast.y_batch_size(); ++i) {",
          "content_same": false
        },
        {
          "line": 486,
          "old_api": null,
          "new_api": "AsDeviceMemory",
          "old_text": null,
          "new_text": "AsDeviceMemory(b_base_ptr + i * k * n)",
          "old_line_content": "      for (int64 i = 0; i < batch_size; ++i) {",
          "new_line_content": "        b_device_memory.push_back(AsDeviceMemory(b_base_ptr + i * k * n));",
          "content_same": false
        },
        {
          "line": 491,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "b_ptrs.push_back(&b_device_memory[b_batch_indices[i]])",
          "old_line_content": "      }",
          "new_line_content": "        b_ptrs.push_back(&b_device_memory[b_batch_indices[i]]);",
          "content_same": false
        },
        {
          "line": 492,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "c_device_memory.back()",
          "old_line_content": "    }",
          "new_line_content": "        c_ptrs.push_back(&c_device_memory.back());",
          "content_same": false
        }
      ],
      "deletions": [
        {
          "line": 514,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::Internal(\n            \"Blas xGEMM launch failed : a.shape=\", in_x.shape().DebugString(),\n            \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,\n            \", k=\", k)",
          "new_text": null,
          "old_line_content": "        context->SetStatus(errors::Internal(",
          "new_line_content": "              .ok();",
          "content_same": false
        },
        {
          "line": 515,
          "old_api": "shape",
          "new_api": null,
          "old_text": "in_x.shape().DebugString()",
          "new_text": null,
          "old_line_content": "            \"Blas xGEMM launch failed : a.shape=\", in_x.shape().DebugString(),",
          "new_line_content": "      if (!blas_launch_status) {",
          "content_same": false
        },
        {
          "line": 522,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n              ->ThenBlasGemmBatchedWithScratch(\n                  blas_transpose_b, blas_transpose_a, n, m, k,\n                  static_cast<Coefficient>(1.0), b_ptrs, adj_y ? k : n, a_ptrs,\n                  adj_x ? m : k, static_cast<Coefficient>(0.0), c_ptrs, n,\n                  batch_size, &scratch_allocator)\n              .ok()",
          "new_text": null,
          "old_line_content": "          stream",
          "new_line_content": "      BlasScratchAllocator scratch_allocator(context);",
          "content_same": false
        },
        {
          "line": 525,
          "old_api": "static_cast<Coefficient>(1.0)",
          "new_api": null,
          "old_text": "static_cast<Coefficient>(1.0)",
          "new_text": null,
          "old_line_content": "                  static_cast<Coefficient>(1.0), b_ptrs, adj_y ? k : n, a_ptrs,",
          "new_line_content": "              ->ThenBlasGemmBatchedWithScratch(",
          "content_same": false
        },
        {
          "line": 526,
          "old_api": "static_cast<Coefficient>(0.0)",
          "new_api": null,
          "old_text": "static_cast<Coefficient>(0.0)",
          "new_text": null,
          "old_line_content": "                  adj_x ? m : k, static_cast<Coefficient>(0.0), c_ptrs, n,",
          "new_line_content": "                  blas_transpose_b, blas_transpose_a, n, m, k,",
          "content_same": false
        },
        {
          "line": 530,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::Internal(\n            \"Blas xGEMMBatched launch failed : a.shape=\",\n            in_x.shape().DebugString(),\n            \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,\n            \", k=\", k, \", batch_size=\", batch_size)",
          "new_text": null,
          "old_line_content": "        context->SetStatus(errors::Internal(",
          "new_line_content": "              .ok();",
          "content_same": false
        },
        {
          "line": 533,
          "old_api": "shape",
          "new_api": null,
          "old_text": "in_y.shape().DebugString()",
          "new_text": null,
          "old_line_content": "            \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,",
          "new_line_content": "            \"Blas xGEMMBatched launch failed : a.shape=\",",
          "content_same": false
        },
        {
          "line": 548,
          "old_api": "in_x.tensor<Scalar, 3>()",
          "new_api": null,
          "old_text": "in_x.tensor<Scalar, 3>()",
          "new_text": null,
          "old_line_content": "    auto Tx = in_x.tensor<Scalar, 3>();",
          "new_line_content": "                  const Tensor& in_y, bool adj_x, bool adj_y,",
          "content_same": false
        },
        {
          "line": 549,
          "old_api": "in_y.tensor<Scalar, 3>()",
          "new_api": null,
          "old_text": "in_y.tensor<Scalar, 3>()",
          "new_text": null,
          "old_line_content": "    auto Ty = in_y.tensor<Scalar, 3>();",
          "new_line_content": "                  const MatMulBCast& bcast, Tensor* out, int start, int limit) {",
          "content_same": false
        },
        {
          "line": 553,
          "old_api": "eigen_sycl_device",
          "new_api": null,
          "old_text": "context->eigen_sycl_device()",
          "new_text": null,
          "old_line_content": "    auto d = context->eigen_sycl_device();",
          "new_line_content": "    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract_pairs;",
          "content_same": false
        },
        {
          "line": 556,
          "old_api": "x_batch_indices",
          "new_api": null,
          "old_text": "bcast.x_batch_indices()",
          "new_text": null,
          "old_line_content": "    const auto& x_batch_indices = bcast.x_batch_indices();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 562,
          "old_api": "Tx.template chip<0>(x_batch_index)",
          "new_api": null,
          "old_text": "Tx.template chip<0>(x_batch_index)",
          "new_text": null,
          "old_line_content": "      auto x = Tx.template chip<0>(x_batch_index);",
          "new_line_content": "      const int64 y_batch_index = should_bcast ? y_batch_indices[i] : i;",
          "content_same": false
        },
        {
          "line": 563,
          "old_api": "Ty.template chip<0>(y_batch_index)",
          "new_api": null,
          "old_text": "Ty.template chip<0>(y_batch_index)",
          "new_text": null,
          "old_line_content": "      auto y = Ty.template chip<0>(y_batch_index);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 576,
          "old_api": "output_batch_size",
          "new_api": null,
          "old_text": "bcast.output_batch_size()",
          "new_text": null,
          "old_line_content": "    const int64 batch_size = bcast.output_batch_size();",
          "new_line_content": "                     const MatMulBCast& bcast, Tensor* out) {",
          "content_same": false
        },
        {
          "line": 577,
          "old_api": "ParallelMatMulKernelSYCL<Scalar>::Run(context, in_x, in_y, adj_x, adj_y,\n                                          bcast, out, 0, batch_size)",
          "new_api": null,
          "old_text": "ParallelMatMulKernelSYCL<Scalar>::Run(context, in_x, in_y, adj_x, adj_y,\n                                          bcast, out, 0, batch_size)",
          "new_text": null,
          "old_line_content": "    ParallelMatMulKernelSYCL<Scalar>::Run(context, in_x, in_y, adj_x, adj_y,",
          "new_line_content": "    // Number of matrix multiplies i.e. size of the batch.",
          "content_same": false
        },
        {
          "line": 586,
          "old_api": "explicit",
          "new_api": null,
          "old_text": "explicit",
          "new_text": null,
          "old_line_content": "  explicit BaseBatchMatMulOp(OpKernelConstruction* context)",
          "new_line_content": "class BaseBatchMatMulOp : public OpKernel {",
          "content_same": false
        },
        {
          "line": 589,
          "old_api": "GetAttr",
          "new_api": null,
          "old_text": "context->GetAttr(\"adj_y\", &adj_y_)",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"adj_y\", &adj_y_));",
          "new_line_content": "      : OpKernel(context) {",
          "content_same": false
        },
        {
          "line": 595,
          "old_api": "input",
          "new_api": null,
          "old_text": "ctx->input(0)",
          "new_text": null,
          "old_line_content": "    const Tensor& in0 = ctx->input(0);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 596,
          "old_api": "input",
          "new_api": null,
          "old_text": "ctx->input(1)",
          "new_text": null,
          "old_line_content": "    const Tensor& in1 = ctx->input(1);",
          "new_line_content": "  void Compute(OpKernelContext* ctx) override {",
          "content_same": false
        },
        {
          "line": 601,
          "old_api": "IsValid",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(\n            \"In[0] and In[1] must have compatible batch dimensions: \",\n            in0.shape().DebugString(), \" vs. \", in1.shape().DebugString()))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 608,
          "old_api": "output_batch_size",
          "new_api": null,
          "old_text": "bcast.output_batch_size()",
          "new_text": null,
          "old_line_content": "    auto batch_size = bcast.output_batch_size();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 615,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::Internal(\"Failed to reshape In[0] from \",\n                         in0.shape().DebugString())",
          "new_text": null,
          "old_line_content": "        errors::Internal(\"Failed to reshape In[0] from \",",
          "new_line_content": "        ctx,",
          "content_same": false
        },
        {
          "line": 623,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString())",
          "new_text": null,
          "old_line_content": "        errors::Internal(\"Failed to reshape In[1] from \",",
          "new_line_content": "        ctx,",
          "content_same": false
        },
        {
          "line": 633,
          "old_api": "AddDim",
          "new_api": null,
          "old_text": "out_shape.AddDim(d3)",
          "new_text": null,
          "old_line_content": "    out_shape.AddDim(d3);",
          "new_line_content": "                    \" \", adj_x_, \" \", adj_y_));",
          "content_same": false
        },
        {
          "line": 636,
          "old_api": "NumElements",
          "new_api": null,
          "old_text": "out->NumElements()",
          "new_text": null,
          "old_line_content": "    if (out->NumElements() == 0) {",
          "new_line_content": "    Tensor* out = nullptr;",
          "content_same": false
        },
        {
          "line": 639,
          "old_api": "NumElements",
          "new_api": null,
          "old_text": "in1.NumElements()",
          "new_text": null,
          "old_line_content": "    if (in0.NumElements() == 0 || in1.NumElements() == 0) {",
          "new_line_content": "      return;",
          "content_same": false
        },
        {
          "line": 645,
          "old_api": "CopyFrom",
          "new_api": null,
          "old_text": "OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d0, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(ctx,",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 646,
          "old_api": "TensorShape",
          "new_api": null,
          "old_text": "TensorShape({batch_size, d0, d3})",
          "new_text": null,
          "old_line_content": "                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d0, d3})),",
          "new_line_content": "    Tensor out_reshaped;",
          "content_same": false
        },
        {
          "line": 666,
          "old_api": "explicit",
          "new_api": null,
          "old_text": "explicit",
          "new_text": null,
          "old_line_content": "  explicit BatchMatMulOp(OpKernelConstruction* context)",
          "new_line_content": "class BatchMatMulOp : public BaseBatchMatMulOp<Device, Scalar> {",
          "content_same": false
        },
        {
          "line": 676,
          "old_api": "dims",
          "new_api": null,
          "old_text": "in1.dims()",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(ctx, in0.dims() == in1.dims(),",
          "new_line_content": "    // Disallow broadcasting support. Ensure that all batch dimensions of the",
          "content_same": false
        },
        {
          "line": 677,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"In[0] and In[1] has different ndims: \",\n                                        in0.shape().DebugString(), \" vs. \",\n                                        in1.shape().DebugString())",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(\"In[0] and In[1] has different ndims: \",",
          "new_line_content": "    // input tensors match.",
          "content_same": false
        },
        {
          "line": 686,
          "old_api": "dim",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\n                      \"In[0].dim(\", i, \") and In[1].dim(\", i,\n                      \") must be the same: \", in0.shape().DebugString(), \" vs \",\n                      in1.shape().DebugString())",
          "new_text": null,
          "old_line_content": "                  errors::InvalidArgument(",
          "new_line_content": "    for (int i = 0; i < ndims - 2; ++i) {",
          "content_same": false
        },
        {
          "line": 689,
          "old_api": "shape",
          "new_api": null,
          "old_text": "in1.shape().DebugString()",
          "new_text": null,
          "old_line_content": "                      in1.shape().DebugString()));",
          "new_line_content": "                      \"In[0].dim(\", i, \") and In[1].dim(\", i,",
          "content_same": false
        },
        {
          "line": 698,
          "old_api": "explicit",
          "new_api": null,
          "old_text": "explicit",
          "new_text": null,
          "old_line_content": "  explicit BatchMatMulV2Op(OpKernelConstruction* context)",
          "new_line_content": "class BatchMatMulV2Op : public BaseBatchMatMulOp<Device, Scalar> {",
          "content_same": false
        },
        {
          "line": 708,
          "old_api": "dims",
          "new_api": null,
          "old_text": "OP_REQUIRES(\n        ctx, in0.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()))",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "    // Enable broadcasting support. Validity of broadcasting is checked in",
          "content_same": false
        },
        {
          "line": 709,
          "old_api": "dims",
          "new_api": null,
          "old_text": "in0.dims()",
          "new_text": null,
          "old_line_content": "        ctx, in0.dims() >= 2,",
          "new_line_content": "    // BaseBatchMatMulOp.",
          "content_same": false
        },
        {
          "line": 221,
          "old_api": "device",
          "new_api": null,
          "old_text": "context->device()->tensorflow_cpu_worker_threads()",
          "new_text": null,
          "old_line_content": "    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());",
          "new_line_content": "    // Jan 21, 2020.",
          "content_same": false
        },
        {
          "line": 227,
          "old_api": "ParallelMatMulKernel::Run(context, in_x, in_y, adj_x, adj_y, bcast, out,\n                                0, batch_size)",
          "new_api": null,
          "old_text": "ParallelMatMulKernel::Run(context, in_x, in_y, adj_x, adj_y, bcast, out,\n                                0, batch_size)",
          "new_text": null,
          "old_line_content": "      ParallelMatMulKernel::Run(context, in_x, in_y, adj_x, adj_y, bcast, out,",
          "new_line_content": "      // For large matrix products it is counter-productive to parallelize",
          "content_same": false
        },
        {
          "line": 233,
          "old_api": "Shard",
          "new_api": null,
          "old_text": "Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n            cost_per_unit,\n            [&in_x, &in_y, adj_x, adj_y, &bcast, out](int start, int limit) {\n              SequentialMatMulKernel<Scalar>::Run(in_x, in_y, adj_x, adj_y,\n                                                  bcast, out, start, limit);\n            })",
          "new_text": null,
          "old_line_content": "      Shard(worker_threads.num_threads, worker_threads.workers, batch_size,",
          "new_line_content": "      // Parallelize over outer dims. For small matrices and large batches, it",
          "content_same": false
        },
        {
          "line": 236,
          "old_api": "SequentialMatMulKernel<Scalar>::Run(in_x, in_y, adj_x, adj_y,\n                                                  bcast, out, start, limit)",
          "new_api": null,
          "old_text": "SequentialMatMulKernel<Scalar>::Run(in_x, in_y, adj_x, adj_y,\n                                                  bcast, out, start, limit)",
          "new_text": null,
          "old_line_content": "              SequentialMatMulKernel<Scalar>::Run(in_x, in_y, adj_x, adj_y,",
          "new_line_content": "            cost_per_unit,",
          "content_same": false
        },
        {
          "line": 246,
          "old_api": "ParallelMatMulKernel::Conjugate(context, out)",
          "new_api": null,
          "old_text": "ParallelMatMulKernel::Conjugate(context, out)",
          "new_text": null,
          "old_line_content": "      ParallelMatMulKernel::Conjugate(context, out);",
          "new_line_content": "      // above, we need to conjugate the final output. This is a",
          "content_same": false
        },
        {
          "line": 274,
          "old_api": "allocate_temp",
          "new_api": null,
          "old_text": "context_->allocate_temp(\n        DT_UINT8, TensorShape({byte_size}), &temporary_memory)",
          "new_text": null,
          "old_line_content": "    Status allocation_status(context_->allocate_temp(",
          "new_line_content": "    Tensor temporary_memory;",
          "content_same": false
        },
        {
          "line": 275,
          "old_api": "TensorShape",
          "new_api": null,
          "old_text": "TensorShape({byte_size})",
          "new_text": null,
          "old_line_content": "        DT_UINT8, TensorShape({byte_size}), &temporary_memory));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 282,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "allocated_tensors_.push_back(temporary_memory)",
          "new_text": null,
          "old_line_content": "    allocated_tensors_.push_back(temporary_memory);",
          "new_line_content": "    // Hold the reference of the allocated tensors until the end of the",
          "content_same": false
        },
        {
          "line": 283,
          "old_api": "data",
          "new_api": null,
          "old_text": "se::port::StatusOr<DeviceMemoryBytes>(\n        DeviceMemoryBytes::MakeFromByteSize(\n            temporary_memory.flat<uint8>().data(),\n            temporary_memory.flat<uint8>().size()))",
          "new_text": null,
          "old_line_content": "    return se::port::StatusOr<DeviceMemoryBytes>(",
          "new_line_content": "    // allocator.",
          "content_same": false
        },
        {
          "line": 300,
          "old_api": "constexpr",
          "new_api": null,
          "old_text": "constexpr",
          "new_text": null,
          "old_line_content": "    constexpr se::blas::Transpose kTranspose =",
          "new_line_content": "                     const Tensor& in_y, bool adj_x, bool adj_y,",
          "content_same": false
        },
        {
          "line": 305,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "in_x.dim_size(adj_x ? 2 : 1)",
          "new_text": null,
          "old_line_content": "    const uint64 m = in_x.dim_size(adj_x ? 2 : 1);",
          "new_line_content": "    se::blas::Transpose trans[] = {se::blas::Transpose::kNoTranspose,",
          "content_same": false
        },
        {
          "line": 306,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "in_x.dim_size(adj_x ? 1 : 2)",
          "new_text": null,
          "old_line_content": "    const uint64 k = in_x.dim_size(adj_x ? 1 : 2);",
          "new_line_content": "                                   kTranspose};",
          "content_same": false
        },
        {
          "line": 312,
          "old_api": "op_device_context",
          "new_api": null,
          "old_text": "context->op_device_context()->stream()",
          "new_text": null,
          "old_line_content": "    auto* stream = context->op_device_context()->stream();",
          "new_line_content": "    auto blas_transpose_b = trans[adj_y];",
          "content_same": false
        },
        {
          "line": 313,
          "old_api": "errors::Internal(\"No GPU stream available.\")",
          "new_api": null,
          "old_text": "errors::Internal(\"No GPU stream available.\")",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 322,
          "old_api": "x_batch_size",
          "new_api": null,
          "old_text": "bcast.x_batch_size()",
          "new_text": null,
          "old_line_content": "    a_device_memory.reserve(bcast.x_batch_size());",
          "new_line_content": "    std::vector<DeviceMemoryType*> b_ptrs;",
          "content_same": false
        },
        {
          "line": 323,
          "old_api": "y_batch_size",
          "new_api": null,
          "old_text": "bcast.y_batch_size()",
          "new_text": null,
          "old_line_content": "    b_device_memory.reserve(bcast.y_batch_size());",
          "new_line_content": "    std::vector<DeviceMemoryType*> c_ptrs;",
          "content_same": false
        },
        {
          "line": 335,
          "old_api": "AsDeviceMemory",
          "new_api": null,
          "old_text": "AsDeviceMemory(b_base_ptr + i * k * n)",
          "new_text": null,
          "old_line_content": "        b_device_memory.push_back(AsDeviceMemory(b_base_ptr + i * k * n));",
          "new_line_content": "      for (int64 i = 0; i < batch_size; ++i) {",
          "content_same": false
        },
        {
          "line": 342,
          "old_api": "x_batch_indices",
          "new_api": null,
          "old_text": "bcast.x_batch_indices()",
          "new_text": null,
          "old_line_content": "      const std::vector<int64>& a_batch_indices = bcast.x_batch_indices();",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 343,
          "old_api": "y_batch_indices",
          "new_api": null,
          "old_text": "bcast.y_batch_indices()",
          "new_text": null,
          "old_line_content": "      const std::vector<int64>& b_batch_indices = bcast.y_batch_indices();",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 348,
          "old_api": "AsDeviceMemory",
          "new_api": null,
          "old_text": "AsDeviceMemory(b_base_ptr + i * k * n)",
          "new_text": null,
          "old_line_content": "        b_device_memory.push_back(AsDeviceMemory(b_base_ptr + i * k * n));",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 351,
          "old_api": "AsDeviceMemory",
          "new_api": null,
          "old_text": "AsDeviceMemory(c_base_ptr + i * m * n)",
          "new_text": null,
          "old_line_content": "        c_device_memory.push_back(AsDeviceMemory(c_base_ptr + i * m * n));",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 352,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "a_ptrs.push_back(&a_device_memory[a_batch_indices[i]])",
          "new_text": null,
          "old_line_content": "        a_ptrs.push_back(&a_device_memory[a_batch_indices[i]]);",
          "new_line_content": "      for (int64 i = 0; i < batch_size; ++i) {",
          "content_same": false
        },
        {
          "line": 381,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n                ->ThenBlasGemv(gemv_trans_a, adj_x ? m : k, adj_x ? k : m,\n                               static_cast<Coefficient>(1.0), *(a_ptrs[0]),\n                               adj_x ? m : k, *(b_ptrs[0]), 1,\n                               static_cast<Coefficient>(0.0), c_ptrs[0], 1)\n                .ok()",
          "new_text": null,
          "old_line_content": "            stream",
          "new_line_content": "                                : se::blas::Transpose::kTranspose;",
          "content_same": false
        },
        {
          "line": 388,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::Internal(\n              \"Blas xGEMV launch failed : a.shape=\", in_x.shape().DebugString(),\n              \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,\n              \", k=\", k)",
          "new_text": null,
          "old_line_content": "          context->SetStatus(errors::Internal(",
          "new_line_content": "                .ok();",
          "content_same": false
        },
        {
          "line": 389,
          "old_api": "shape",
          "new_api": null,
          "old_text": "in_x.shape().DebugString()",
          "new_text": null,
          "old_line_content": "              \"Blas xGEMV launch failed : a.shape=\", in_x.shape().DebugString(),",
          "new_line_content": "        if (!blas_launch_status) {",
          "content_same": false
        },
        {
          "line": 395,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n                ->ThenBlasGemm(blas_transpose_b, blas_transpose_a, n, m, k,\n                               static_cast<Coefficient>(1.0), *(b_ptrs[0]),\n                               adj_y ? k : n, *(a_ptrs[0]), adj_x ? m : k,\n                               static_cast<Coefficient>(0.0), c_ptrs[0], n)\n                .ok()",
          "new_text": null,
          "old_line_content": "            stream",
          "new_line_content": "      } else {",
          "content_same": false
        },
        {
          "line": 402,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::Internal(\n              \"Blas xGEMM launch failed : a.shape=\", in_x.shape().DebugString(),\n              \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,\n              \", k=\", k)",
          "new_text": null,
          "old_line_content": "          context->SetStatus(errors::Internal(",
          "new_line_content": "                .ok();",
          "content_same": false
        },
        {
          "line": 403,
          "old_api": "shape",
          "new_api": null,
          "old_text": "in_x.shape().DebugString()",
          "new_text": null,
          "old_line_content": "              \"Blas xGEMM launch failed : a.shape=\", in_x.shape().DebugString(),",
          "new_line_content": "        if (!blas_launch_status) {",
          "content_same": false
        },
        {
          "line": 411,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n              ->ThenBlasGemmBatchedWithScratch(\n                  blas_transpose_b, blas_transpose_a, n, m, k,\n                  static_cast<Coefficient>(1.0), b_ptrs, adj_y ? k : n, a_ptrs,\n                  adj_x ? m : k, static_cast<Coefficient>(0.0), c_ptrs, n,\n                  batch_size, &scratch_allocator)\n              .ok()",
          "new_text": null,
          "old_line_content": "          stream",
          "new_line_content": "      BlasScratchAllocator scratch_allocator(context);",
          "content_same": false
        },
        {
          "line": 414,
          "old_api": "static_cast<Coefficient>(1.0)",
          "new_api": null,
          "old_text": "static_cast<Coefficient>(1.0)",
          "new_text": null,
          "old_line_content": "                  static_cast<Coefficient>(1.0), b_ptrs, adj_y ? k : n, a_ptrs,",
          "new_line_content": "              ->ThenBlasGemmBatchedWithScratch(",
          "content_same": false
        },
        {
          "line": 415,
          "old_api": "static_cast<Coefficient>(0.0)",
          "new_api": null,
          "old_text": "static_cast<Coefficient>(0.0)",
          "new_text": null,
          "old_line_content": "                  adj_x ? m : k, static_cast<Coefficient>(0.0), c_ptrs, n,",
          "new_line_content": "                  blas_transpose_b, blas_transpose_a, n, m, k,",
          "content_same": false
        },
        {
          "line": 419,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::Internal(\n            \"Blas xGEMMBatched launch failed : a.shape=\",\n            in_x.shape().DebugString(),\n            \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,\n            \", k=\", k, \", batch_size=\", batch_size)",
          "new_text": null,
          "old_line_content": "        context->SetStatus(errors::Internal(",
          "new_line_content": "              .ok();",
          "content_same": false
        },
        {
          "line": 422,
          "old_api": "shape",
          "new_api": null,
          "old_text": "in_y.shape().DebugString()",
          "new_text": null,
          "old_line_content": "            \", b.shape=\", in_y.shape().DebugString(), \", m=\", m, \", n=\", n,",
          "new_line_content": "            \"Blas xGEMMBatched launch failed : a.shape=\",",
          "content_same": false
        },
        {
          "line": 435,
          "old_api": "constexpr",
          "new_api": null,
          "old_text": "constexpr",
          "new_text": null,
          "old_line_content": "    constexpr perftools::gputools::blas::Transpose kTranspose =",
          "new_line_content": "                     const MatMulBCast& bcast, Tensor* out) {",
          "content_same": false
        },
        {
          "line": 441,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "in_x.dim_size(adj_x ? 2 : 1)",
          "new_text": null,
          "old_line_content": "    const uint64 m = in_x.dim_size(adj_x ? 2 : 1);",
          "new_line_content": "    perftools::gputools::blas::Transpose trans[] = {",
          "content_same": false
        },
        {
          "line": 442,
          "old_api": "dim_size",
          "new_api": null,
          "old_text": "in_x.dim_size(adj_x ? 1 : 2)",
          "new_text": null,
          "old_line_content": "    const uint64 k = in_x.dim_size(adj_x ? 1 : 2);",
          "new_line_content": "        perftools::gputools::blas::Transpose::kNoTranspose, kTranspose};",
          "content_same": false
        },
        {
          "line": 448,
          "old_api": "op_device_context",
          "new_api": null,
          "old_text": "context->op_device_context()->stream()",
          "new_text": null,
          "old_line_content": "    auto* stream = context->op_device_context()->stream();",
          "new_line_content": "    auto blas_transpose_b = trans[adj_y];",
          "content_same": false
        },
        {
          "line": 449,
          "old_api": "errors::Internal(\"No GPU stream available.\")",
          "new_api": null,
          "old_text": "errors::Internal(\"No GPU stream available.\")",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 458,
          "old_api": "x_batch_size",
          "new_api": null,
          "old_text": "bcast.x_batch_size()",
          "new_text": null,
          "old_line_content": "    a_device_memory.reserve(bcast.x_batch_size());",
          "new_line_content": "    std::vector<DeviceMemoryType*> b_ptrs;",
          "content_same": false
        },
        {
          "line": 459,
          "old_api": "y_batch_size",
          "new_api": null,
          "old_text": "bcast.y_batch_size()",
          "new_text": null,
          "old_line_content": "    b_device_memory.reserve(bcast.y_batch_size());",
          "new_line_content": "    std::vector<DeviceMemoryType*> c_ptrs;",
          "content_same": false
        },
        {
          "line": 471,
          "old_api": "AsDeviceMemory",
          "new_api": null,
          "old_text": "AsDeviceMemory(b_base_ptr + i * k * n)",
          "new_text": null,
          "old_line_content": "        b_device_memory.push_back(AsDeviceMemory(b_base_ptr + i * k * n));",
          "new_line_content": "      for (int64 i = 0; i < batch_size; ++i) {",
          "content_same": false
        },
        {
          "line": 478,
          "old_api": "x_batch_indices",
          "new_api": null,
          "old_text": "bcast.x_batch_indices()",
          "new_text": null,
          "old_line_content": "      const std::vector<int64>& a_batch_indices = bcast.x_batch_indices();",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 479,
          "old_api": "y_batch_indices",
          "new_api": null,
          "old_text": "bcast.y_batch_indices()",
          "new_text": null,
          "old_line_content": "      const std::vector<int64>& b_batch_indices = bcast.y_batch_indices();",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 484,
          "old_api": "AsDeviceMemory",
          "new_api": null,
          "old_text": "AsDeviceMemory(b_base_ptr + i * k * n)",
          "new_text": null,
          "old_line_content": "        b_device_memory.push_back(AsDeviceMemory(b_base_ptr + i * k * n));",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 487,
          "old_api": "AsDeviceMemory",
          "new_api": null,
          "old_text": "AsDeviceMemory(c_base_ptr + i * m * n)",
          "new_text": null,
          "old_line_content": "        c_device_memory.push_back(AsDeviceMemory(c_base_ptr + i * m * n));",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 488,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "a_ptrs.push_back(&a_device_memory[a_batch_indices[i]])",
          "new_text": null,
          "old_line_content": "        a_ptrs.push_back(&a_device_memory[a_batch_indices[i]]);",
          "new_line_content": "      for (int64 i = 0; i < batch_size; ++i) {",
          "content_same": false
        },
        {
          "line": 507,
          "old_api": "ok",
          "new_api": null,
          "old_text": "stream\n              ->ThenBlasGemm(blas_transpose_b, blas_transpose_a, n, m, k,\n                             static_cast<Coefficient>(1.0), *(b_ptrs[0]),\n                             adj_y ? k : n, *(a_ptrs[0]), adj_x ? m : k,\n                             static_cast<Coefficient>(0.0), c_ptrs[0], n)\n              .ok()",
          "new_text": null,
          "old_line_content": "          stream",
          "new_line_content": "      // TODO(benbarsdell): Use fp16 Gemv if it becomes supported by CUBLAS",
          "content_same": false
        }
      ]
    },
    "api_summary": {
      "total_replacements": 77,
      "total_additions": 83,
      "total_deletions": 83,
      "total_api_changes": 243
    },
    "non_api_changes": {
      "has_non_api_changes": true,
      "evidence": {
        "total_diff_lines": 3,
        "api_related_lines": 243,
        "non_api_lines": 2,
        "non_api_line_numbers": [
          220,
          222
        ]
      }
    },
    "api_calls_before": 352,
    "api_calls_after": 352,
    "diff_info": {
      "added_lines": 3,
      "removed_lines": 1,
      "total_diff_lines": 16
    }
  }
}