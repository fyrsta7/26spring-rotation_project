{
  "status": "completed",
  "commit_dir": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/3c910992caeceb7a36f4993793aef073fb61ea22",
  "file_info": {
    "before_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/3c910992caeceb7a36f4993793aef073fb61ea22/before.cc",
    "after_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/3c910992caeceb7a36f4993793aef073fb61ea22/after.cc",
    "diff_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/3c910992caeceb7a36f4993793aef073fb61ea22/diff.txt"
  },
  "semgrep_analysis": {
    "is_api_change": false,
    "api_changes": {
      "replacements": [
        {
          "line": 367,
          "old_api": "shape",
          "new_api": "get",
          "old_text": "OP_REQUIRES_OK(context,\n                     errors::Internal(\"BlasLt Matmul launch failed : a.shape=\",\n                                      a.shape().DebugString(),\n                                      \", b.shape=\", b.shape().DebugString(),\n                                      \", m=\", m, \", n=\", n, \", k=\", k))",
          "new_text": "algorithms[0].get()",
          "old_line_content": "      OP_REQUIRES_OK(context,",
          "new_line_content": "      algorithm = algorithms[0].get();",
          "content_same": false
        },
        {
          "line": 409,
          "old_api": "input",
          "new_api": "InitializeFusedComputation",
          "old_text": "ctx->input(1)",
          "new_text": "InitializeFusedComputation(\n                                context, \"MatMul\", patterns,\n                                &fused_computation_, &fused_computation_args_)",
          "old_line_content": "    const Tensor& b = ctx->input(1);",
          "new_line_content": "    OP_REQUIRES_OK(context, InitializeFusedComputation(",
          "content_same": false
        },
        {
          "line": 412,
          "old_api": "dims",
          "new_api": "MatmulAutotuneEnable",
          "old_text": "b.dims()",
          "new_text": "MatmulAutotuneEnable()",
          "old_line_content": "    OP_REQUIRES(ctx, a.dims() == b.dims(),",
          "new_line_content": "    use_autotune_ = MatmulAutotuneEnable();",
          "content_same": false
        },
        {
          "line": 416,
          "old_api": "shape",
          "new_api": "input",
          "old_text": "OP_REQUIRES(\n        ctx, TensorShapeUtils::IsMatrix(a.shape()),\n        errors::InvalidArgument(\"In[0] is not a matrix. Instead it has shape \",\n                                a.shape().DebugString()))",
          "new_text": "ctx->input(0)",
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "    const Tensor& a = ctx->input(0);",
          "content_same": false
        },
        {
          "line": 417,
          "old_api": "shape",
          "new_api": "input",
          "old_text": "a.shape()",
          "new_text": "ctx->input(1)",
          "old_line_content": "        ctx, TensorShapeUtils::IsMatrix(a.shape()),",
          "new_line_content": "    const Tensor& b = ctx->input(1);",
          "content_same": false
        },
        {
          "line": 420,
          "old_api": "shape",
          "new_api": "dims",
          "old_text": "OP_REQUIRES(\n        ctx, TensorShapeUtils::IsMatrix(b.shape()),\n        errors::InvalidArgument(\"In[1] is not a matrix. Instead it has shape \",\n                                b.shape().DebugString()))",
          "new_text": "b.dims()",
          "old_line_content": "    OP_REQUIRES(",
          "new_line_content": "    OP_REQUIRES(ctx, a.dims() == b.dims(),",
          "content_same": false
        },
        {
          "line": 428,
          "old_api": "dim_size",
          "new_api": "shape",
          "old_text": "OP_REQUIRES(\n        ctx, a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),\n        errors::InvalidArgument(\n            \"Matrix size-incompatible: In[0]: \", a.shape().DebugString(),\n            \", In[1]: \", b.shape().DebugString()))",
          "new_text": "OP_REQUIRES(\n        ctx, TensorShapeUtils::IsMatrix(b.shape()),\n        errors::InvalidArgument(\"In[1] is not a matrix. Instead it has shape \",\n                                b.shape().DebugString()))",
          "old_line_content": null,
          "new_line_content": "    OP_REQUIRES(",
          "content_same": true
        },
        {
          "line": 429,
          "old_api": "dim_size",
          "new_api": "shape",
          "old_text": "b.dim_size(dim_pair[0].second)",
          "new_text": "b.shape()",
          "old_line_content": "        ctx, a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),",
          "new_line_content": "        ctx, TensorShapeUtils::IsMatrix(b.shape()),",
          "content_same": false
        },
        {
          "line": 438,
          "old_api": "allocate_output",
          "new_api": "shape",
          "old_text": "ctx->allocate_output(0, out_shape, &out)",
          "new_text": "errors::InvalidArgument(\n            \"Matrix size-incompatible: In[0]: \", a.shape().DebugString(),\n            \", In[1]: \", b.shape().DebugString())",
          "old_line_content": "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));",
          "new_line_content": "        errors::InvalidArgument(",
          "content_same": false
        },
        {
          "line": 440,
          "old_api": "NumElements",
          "new_api": "shape",
          "old_text": "out->NumElements()",
          "new_text": "b.shape().DebugString()",
          "old_line_content": "    if (out->NumElements() == 0) {",
          "new_line_content": "            \", In[1]: \", b.shape().DebugString()));",
          "content_same": false
        },
        {
          "line": 446,
          "old_api": "NumElements",
          "new_api": "allocate_output",
          "old_text": "b.NumElements()",
          "new_text": "ctx->allocate_output(0, out_shape, &out)",
          "old_line_content": "    if (a.NumElements() == 0 && b.NumElements() == 0) {",
          "new_line_content": "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));",
          "content_same": false
        }
      ],
      "additions": [
        {
          "line": 389,
          "old_api": null,
          "new_api": "explicit",
          "old_text": null,
          "new_text": "explicit",
          "old_line_content": "      patterns = {",
          "new_line_content": "  explicit FusedMatMulOp(OpKernelConstruction* context) : OpKernel(context) {",
          "content_same": false
        },
        {
          "line": 390,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"transpose_a\", &transpose_a_)",
          "old_line_content": "          {FCT::kBiasAdd, {\"BiasAdd\"}},",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"transpose_a\", &transpose_a_));",
          "content_same": false
        },
        {
          "line": 391,
          "old_api": null,
          "new_api": "GetAttr",
          "old_text": null,
          "new_text": "context->GetAttr(\"transpose_b\", &transpose_b_)",
          "old_line_content": "          {FCT::kBiasAddWithRelu, {\"BiasAdd\", \"Relu\"}},",
          "new_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"transpose_b\", &transpose_b_));",
          "content_same": false
        },
        {
          "line": 424,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "OP_REQUIRES(\n        ctx, TensorShapeUtils::IsMatrix(a.shape()),\n        errors::InvalidArgument(\"In[0] is not a matrix. Instead it has shape \",\n                                a.shape().DebugString()))",
          "old_line_content": "    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;",
          "new_line_content": "    OP_REQUIRES(",
          "content_same": false
        },
        {
          "line": 425,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "a.shape()",
          "old_line_content": "    dim_pair[0].first = transpose_a_ ? 0 : 1;",
          "new_line_content": "        ctx, TensorShapeUtils::IsMatrix(a.shape()),",
          "content_same": false
        },
        {
          "line": 426,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "errors::InvalidArgument(\"In[0] is not a matrix. Instead it has shape \",\n                                a.shape().DebugString())",
          "old_line_content": "    dim_pair[0].second = transpose_b_ ? 1 : 0;",
          "new_line_content": "        errors::InvalidArgument(\"In[0] is not a matrix. Instead it has shape \",",
          "content_same": false
        },
        {
          "line": 427,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "a.shape().DebugString()",
          "old_line_content": "",
          "new_line_content": "                                a.shape().DebugString()));",
          "content_same": false
        },
        {
          "line": 437,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "b.dim_size(dim_pair[0].second)",
          "old_line_content": "    Tensor* out = nullptr;",
          "new_line_content": "        ctx, a.dim_size(dim_pair[0].first) == b.dim_size(dim_pair[0].second),",
          "content_same": false
        },
        {
          "line": 439,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "a.shape().DebugString()",
          "old_line_content": "",
          "new_line_content": "            \"Matrix size-incompatible: In[0]: \", a.shape().DebugString(),",
          "content_same": false
        },
        {
          "line": 444,
          "old_api": null,
          "new_api": "dim_size",
          "old_text": null,
          "new_text": "b.dim_size(b_dim_remaining)",
          "old_line_content": "    }",
          "new_line_content": "        {a.dim_size(a_dim_remaining), b.dim_size(b_dim_remaining)});",
          "content_same": false
        },
        {
          "line": 448,
          "old_api": null,
          "new_api": "NumElements",
          "old_text": null,
          "new_text": "out->NumElements()",
          "old_line_content": "      // output shape is [x, y] where x and y are non-zero, so we fill",
          "new_line_content": "    if (out->NumElements() == 0) {",
          "content_same": false
        },
        {
          "line": 454,
          "old_api": null,
          "new_api": "NumElements",
          "old_text": null,
          "new_text": "b.NumElements()",
          "old_line_content": "",
          "new_line_content": "    if (a.NumElements() == 0 && b.NumElements() == 0) {",
          "content_same": false
        },
        {
          "line": 459,
          "old_api": null,
          "new_api": "out->flat<T>()",
          "old_text": null,
          "new_text": "out->flat<T>()",
          "old_line_content": "",
          "new_line_content": "      f(ctx->eigen_device<Device>(), out->flat<T>());",
          "content_same": false
        },
        {
          "line": 463,
          "old_api": null,
          "new_api": "LaunchFusedMatMulOp<Device, T>()",
          "old_text": null,
          "new_text": "LaunchFusedMatMulOp<Device, T>()",
          "old_line_content": "  bool use_autotune_;",
          "new_line_content": "    auto launch = LaunchFusedMatMulOp<Device, T>();",
          "content_same": false
        },
        {
          "line": 464,
          "old_api": null,
          "new_api": "launch",
          "old_text": null,
          "new_text": "launch(ctx, a, b, dim_pair, fused_computation_, fused_computation_args_,\n           out, use_autotune_)",
          "old_line_content": "",
          "new_line_content": "    launch(ctx, a, b, dim_pair, fused_computation_, fused_computation_args_,",
          "content_same": false
        },
        {
          "line": 343,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "algorithms.size()",
          "old_line_content": "",
          "new_line_content": "    OP_REQUIRES(context, algorithms.size() > 0,",
          "content_same": false
        },
        {
          "line": 344,
          "old_api": null,
          "new_api": "errors::InvalidArgument(\"No matmul algorithm returned!\")",
          "old_text": null,
          "new_text": "errors::InvalidArgument(\"No matmul algorithm returned!\")",
          "old_line_content": "    T alpha(1.0);",
          "new_line_content": "                errors::InvalidArgument(\"No matmul algorithm returned!\"));",
          "content_same": false
        },
        {
          "line": 352,
          "old_api": null,
          "new_api": "get",
          "old_text": null,
          "new_text": "stream\n          ->ThenBlasLtMatmul(plan.get(), alpha, b_ptr, a_ptr, beta, &c_ptr,\n                             scratch_allocator, algorithm, bias_ptr,\n                             profile_result)\n          .ok()",
          "old_line_content": "                             scratch_allocator, algorithm, bias_ptr,",
          "new_line_content": "      return stream",
          "content_same": false
        },
        {
          "line": 353,
          "old_api": null,
          "new_api": "get",
          "old_text": null,
          "new_text": "plan.get()",
          "old_line_content": "                             profile_result)",
          "new_line_content": "          ->ThenBlasLtMatmul(plan.get(), alpha, b_ptr, a_ptr, beta, &c_ptr,",
          "content_same": false
        },
        {
          "line": 485,
          "old_api": null,
          "new_api": "TF_CALL_float",
          "old_text": null,
          "new_text": "TF_CALL_float(REGISTER_FUSED_CPU_MATMUL)",
          "old_line_content": "  REGISTER_KERNEL_BUILDER(                                            \\",
          "new_line_content": "TF_CALL_float(REGISTER_FUSED_CPU_MATMUL);",
          "content_same": false
        },
        {
          "line": 362,
          "old_api": null,
          "new_api": "AutotuneMatmul",
          "old_text": null,
          "new_text": "AutotuneMatmul(algorithms, matmul_params, context, launch_func)",
          "old_line_content": "    BlasScratchAllocator scratch_allocator(context);",
          "new_line_content": "          AutotuneMatmul(algorithms, matmul_params, context, launch_func);",
          "content_same": false
        },
        {
          "line": 364,
          "old_api": null,
          "new_api": "algorithm",
          "old_text": null,
          "new_text": "algorithm_config.algorithm()",
          "old_line_content": "    bool cublaslt_launch_ok =",
          "new_line_content": "      se::blas::AlgorithmType algorithm_idx = algorithm_config.algorithm();",
          "content_same": false
        },
        {
          "line": 497,
          "old_api": null,
          "new_api": "TF_CALL_float",
          "old_text": null,
          "new_text": "TF_CALL_float(REGISTER_FUSED_GPU_MATMUL)",
          "old_line_content": "#endif  // TENSORFLOW_CORE_KERNELS_MATMUL_OP_FUSED_H_",
          "new_line_content": "TF_CALL_float(REGISTER_FUSED_GPU_MATMUL);",
          "content_same": false
        },
        {
          "line": 498,
          "old_api": null,
          "new_api": "TF_CALL_half",
          "old_text": null,
          "new_text": "TF_CALL_half(REGISTER_FUSED_GPU_MATMUL)",
          "old_line_content": "",
          "new_line_content": "TF_CALL_half(REGISTER_FUSED_GPU_MATMUL);",
          "content_same": false
        },
        {
          "line": 373,
          "old_api": null,
          "new_api": "launch_func",
          "old_text": null,
          "new_text": "launch_func(&scratch_allocator, algorithm, nullptr)",
          "old_line_content": "  }",
          "new_line_content": "        launch_func(&scratch_allocator, algorithm, nullptr);",
          "content_same": false
        },
        {
          "line": 375,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "OP_REQUIRES_OK(context,\n                     errors::Internal(\"BlasLt Matmul launch failed : a.shape=\",\n                                      a.shape().DebugString(),\n                                      \", b.shape=\", b.shape().DebugString(),\n                                      \", m=\", m, \", n=\", n, \", k=\", k))",
          "old_line_content": "",
          "new_line_content": "      OP_REQUIRES_OK(context,",
          "content_same": false
        },
        {
          "line": 376,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "errors::Internal(\"BlasLt Matmul launch failed : a.shape=\",\n                                      a.shape().DebugString(),\n                                      \", b.shape=\", b.shape().DebugString(),\n                                      \", m=\", m, \", n=\", n, \", k=\", k)",
          "old_line_content": "#endif  // GOOGLE_CUDA",
          "new_line_content": "                     errors::Internal(\"BlasLt Matmul launch failed : a.shape=\",",
          "content_same": false
        },
        {
          "line": 377,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "a.shape().DebugString()",
          "old_line_content": "",
          "new_line_content": "                                      a.shape().DebugString(),",
          "content_same": false
        },
        {
          "line": 378,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "b.shape().DebugString()",
          "old_line_content": "template <typename Device, typename T>",
          "new_line_content": "                                      \", b.shape=\", b.shape().DebugString(),",
          "content_same": false
        }
      ],
      "deletions": [
        {
          "line": 401,
          "old_api": "InitializeFusedComputation",
          "new_api": null,
          "old_text": "InitializeFusedComputation(\n                                context, \"MatMul\", patterns,\n                                &fused_computation_, &fused_computation_args_)",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(context, InitializeFusedComputation(",
          "new_line_content": "          {FCT::kBiasAddWithElu, {\"BiasAdd\", \"Elu\"}},",
          "content_same": false
        },
        {
          "line": 404,
          "old_api": "MatmulAutotuneEnable",
          "new_api": null,
          "old_text": "MatmulAutotuneEnable()",
          "new_text": null,
          "old_line_content": "    use_autotune_ = MatmulAutotuneEnable();",
          "new_line_content": "    } else if (std::is_same<Device, GPUDevice>::value) {",
          "content_same": false
        },
        {
          "line": 408,
          "old_api": "input",
          "new_api": null,
          "old_text": "ctx->input(0)",
          "new_text": null,
          "old_line_content": "    const Tensor& a = ctx->input(0);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 413,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"In[0] and In[1] has different ndims: \",\n                                        a.shape().DebugString(), \" vs. \",\n                                        b.shape().DebugString())",
          "new_text": null,
          "old_line_content": "                errors::InvalidArgument(\"In[0] and In[1] has different ndims: \",",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 414,
          "old_api": "shape",
          "new_api": null,
          "old_text": "a.shape().DebugString()",
          "new_text": null,
          "old_line_content": "                                        a.shape().DebugString(), \" vs. \",",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 415,
          "old_api": "shape",
          "new_api": null,
          "old_text": "b.shape().DebugString()",
          "new_text": null,
          "old_line_content": "                                        b.shape().DebugString()));",
          "new_line_content": "  void Compute(OpKernelContext* ctx) override {",
          "content_same": false
        },
        {
          "line": 418,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::InvalidArgument(\"In[0] is not a matrix. Instead it has shape \",\n                                a.shape().DebugString())",
          "new_text": null,
          "old_line_content": "        errors::InvalidArgument(\"In[0] is not a matrix. Instead it has shape \",",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 419,
          "old_api": "shape",
          "new_api": null,
          "old_text": "a.shape().DebugString()",
          "new_text": null,
          "old_line_content": "                                a.shape().DebugString()));",
          "new_line_content": "    // Check that the dimensions of the two matrices are valid.",
          "content_same": false
        },
        {
          "line": 432,
          "old_api": "shape",
          "new_api": null,
          "old_text": "b.shape().DebugString()",
          "new_text": null,
          "old_line_content": "            \", In[1]: \", b.shape().DebugString()));",
          "new_line_content": "    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;",
          "content_same": false
        },
        {
          "line": 451,
          "old_api": "out->flat<T>()",
          "new_api": null,
          "old_text": "out->flat<T>()",
          "new_text": null,
          "old_line_content": "      f(ctx->eigen_device<Device>(), out->flat<T>());",
          "new_line_content": "      return;",
          "content_same": false
        },
        {
          "line": 455,
          "old_api": "LaunchFusedMatMulOp<Device, T>()",
          "new_api": null,
          "old_text": "LaunchFusedMatMulOp<Device, T>()",
          "new_text": null,
          "old_line_content": "    auto launch = LaunchFusedMatMulOp<Device, T>();",
          "new_line_content": "      // If a has shape [x, 0] and b has shape [0, y], the",
          "content_same": false
        },
        {
          "line": 456,
          "old_api": "launch",
          "new_api": null,
          "old_text": "launch(ctx, a, b, dim_pair, fused_computation_, fused_computation_args_,\n           out, use_autotune_)",
          "new_text": null,
          "old_line_content": "    launch(ctx, a, b, dim_pair, fused_computation_, fused_computation_args_,",
          "new_line_content": "      // output shape is [x, y] where x and y are non-zero, so we fill",
          "content_same": false
        },
        {
          "line": 477,
          "old_api": "TF_CALL_float",
          "new_api": null,
          "old_text": "TF_CALL_float(REGISTER_FUSED_CPU_MATMUL)",
          "new_text": null,
          "old_line_content": "TF_CALL_float(REGISTER_FUSED_CPU_MATMUL);",
          "new_line_content": "};",
          "content_same": false
        },
        {
          "line": 350,
          "old_api": "get",
          "new_api": null,
          "old_text": "stream\n          ->ThenBlasLtMatmul(plan.get(), alpha, b_ptr, a_ptr, beta, &c_ptr,\n                             scratch_allocator, algorithm, bias_ptr,\n                             profile_result)\n          .ok()",
          "new_text": null,
          "old_line_content": "      return stream",
          "new_line_content": "                           se::blas::IBlasLtMatmulAlgorithm* algorithm,",
          "content_same": false
        },
        {
          "line": 351,
          "old_api": "get",
          "new_api": null,
          "old_text": "plan.get()",
          "new_text": null,
          "old_line_content": "          ->ThenBlasLtMatmul(plan.get(), alpha, b_ptr, a_ptr, beta, &c_ptr,",
          "new_line_content": "                           se::blas::ProfileResult* profile_result) -> bool {",
          "content_same": false
        },
        {
          "line": 358,
          "old_api": "AutotuneMatmul",
          "new_api": null,
          "old_text": "AutotuneMatmul(algorithms, matmul_params, context, launch_func)",
          "new_text": null,
          "old_line_content": "        AutotuneMatmul(algorithms, matmul_params, context, launch_func);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 360,
          "old_api": "algorithm",
          "new_api": null,
          "old_text": "algorithm_config.algorithm()",
          "new_text": null,
          "old_line_content": "    se::blas::AlgorithmType algorithm_idx = algorithm_config.algorithm();",
          "new_line_content": "    if (use_autotune) {",
          "content_same": false
        },
        {
          "line": 489,
          "old_api": "TF_CALL_float",
          "new_api": null,
          "old_text": "TF_CALL_float(REGISTER_FUSED_GPU_MATMUL)",
          "new_text": null,
          "old_line_content": "TF_CALL_float(REGISTER_FUSED_GPU_MATMUL);",
          "new_line_content": "#if GOOGLE_CUDA",
          "content_same": false
        },
        {
          "line": 490,
          "old_api": "TF_CALL_half",
          "new_api": null,
          "old_text": "TF_CALL_half(REGISTER_FUSED_GPU_MATMUL)",
          "new_text": null,
          "old_line_content": "TF_CALL_half(REGISTER_FUSED_GPU_MATMUL);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 368,
          "old_api": "shape",
          "new_api": null,
          "old_text": "errors::Internal(\"BlasLt Matmul launch failed : a.shape=\",\n                                      a.shape().DebugString(),\n                                      \", b.shape=\", b.shape().DebugString(),\n                                      \", m=\", m, \", n=\", n, \", k=\", k)",
          "new_text": null,
          "old_line_content": "                     errors::Internal(\"BlasLt Matmul launch failed : a.shape=\",",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 369,
          "old_api": "shape",
          "new_api": null,
          "old_text": "a.shape().DebugString()",
          "new_text": null,
          "old_line_content": "                                      a.shape().DebugString(),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 370,
          "old_api": "shape",
          "new_api": null,
          "old_text": "b.shape().DebugString()",
          "new_text": null,
          "old_line_content": "                                      \", b.shape=\", b.shape().DebugString(),",
          "new_line_content": "    BlasScratchAllocator scratch_allocator(context);",
          "content_same": false
        },
        {
          "line": 381,
          "old_api": "explicit",
          "new_api": null,
          "old_text": "explicit",
          "new_text": null,
          "old_line_content": "  explicit FusedMatMulOp(OpKernelConstruction* context) : OpKernel(context) {",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 382,
          "old_api": "GetAttr",
          "new_api": null,
          "old_text": "context->GetAttr(\"transpose_a\", &transpose_a_)",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"transpose_a\", &transpose_a_));",
          "new_line_content": "};",
          "content_same": false
        },
        {
          "line": 383,
          "old_api": "GetAttr",
          "new_api": null,
          "old_text": "context->GetAttr(\"transpose_b\", &transpose_b_)",
          "new_text": null,
          "old_line_content": "    OP_REQUIRES_OK(context, context->GetAttr(\"transpose_b\", &transpose_b_));",
          "new_line_content": "",
          "content_same": false
        }
      ]
    },
    "api_summary": {
      "total_replacements": 11,
      "total_additions": 29,
      "total_deletions": 25,
      "total_api_changes": 65
    },
    "non_api_changes": {
      "has_non_api_changes": true,
      "evidence": {
        "total_diff_lines": 15,
        "api_related_lines": 65,
        "non_api_lines": 6,
        "non_api_line_numbers": [
          357,
          359,
          361,
          363,
          365,
          366
        ]
      }
    },
    "api_calls_before": 163,
    "api_calls_after": 167,
    "diff_info": {
      "added_lines": 13,
      "removed_lines": 5,
      "total_diff_lines": 41
    }
  }
}