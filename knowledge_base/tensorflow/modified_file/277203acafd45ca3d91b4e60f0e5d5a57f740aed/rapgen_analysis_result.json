{
  "status": "completed",
  "commit_dir": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/277203acafd45ca3d91b4e60f0e5d5a57f740aed",
  "file_info": {
    "before_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/277203acafd45ca3d91b4e60f0e5d5a57f740aed/before.cc",
    "after_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/277203acafd45ca3d91b4e60f0e5d5a57f740aed/after.cc",
    "diff_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/tensorflow/modified_file/277203acafd45ca3d91b4e60f0e5d5a57f740aed/diff.txt"
  },
  "semgrep_analysis": {
    "is_api_change": false,
    "api_changes": {
      "replacements": [
        {
          "line": 81,
          "old_api": "ShapeUtil::GetDimension(shape, i)",
          "new_api": "add_start_index_map",
          "old_text": "ShapeUtil::GetDimension(shape, i)",
          "new_text": "dim_numbers.add_start_index_map(ndims - 1)",
          "old_line_content": "        slice_sizes[i] = ShapeUtil::GetDimension(shape, i);",
          "new_line_content": "      dim_numbers.add_start_index_map(ndims - 1);",
          "content_same": false
        },
        {
          "line": 100,
          "old_api": "mutable_dimensions",
          "new_api": "MakeNoPaddingConfig",
          "old_text": "config.mutable_dimensions(ndims - 2)->set_edge_padding_high(padding)",
          "new_text": "MakeNoPaddingConfig(2)",
          "old_line_content": "      config.mutable_dimensions(ndims - 2)->set_edge_padding_high(padding);",
          "new_line_content": "      config = MakeNoPaddingConfig(2);",
          "content_same": false
        },
        {
          "line": 109,
          "old_api": "Broadcast",
          "new_api": "dimensions",
          "old_text": "Broadcast(eye, batch_dims)",
          "new_text": "blocks_shape.dimensions()",
          "old_line_content": "      eye = Broadcast(eye, batch_dims);",
          "new_line_content": "      auto shape_dims = blocks_shape.dimensions();",
          "content_same": false
        },
        {
          "line": 110,
          "old_api": "ConcatInDim",
          "new_api": "std::vector<int64_t>(ndims)",
          "old_text": "ConcatInDim(builder, {last_blocks, eye}, ndims - 1)",
          "new_text": "std::vector<int64_t>(ndims)",
          "old_line_content": "      last_blocks = ConcatInDim(builder, {last_blocks, eye}, ndims - 1);",
          "new_line_content": "      auto last_blocks_dims = std::vector<int64_t>(ndims);",
          "content_same": false
        },
        {
          "line": 118,
          "old_api": "end",
          "new_api": "ConcatInDim",
          "old_text": "last_blocks_dims.end()",
          "new_text": "ConcatInDim(builder, {diag_blocks, last_blocks}, ndims - 2)",
          "old_line_content": "      last_blocks_dims.insert(last_blocks_dims.end() - 2, 1);",
          "new_line_content": "            ConcatInDim(builder, {diag_blocks, last_blocks}, ndims - 2);",
          "content_same": false
        },
        {
          "line": 146,
          "old_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_api": "rank",
          "old_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_text": "b_shape.rank()",
          "old_line_content": "    int64_t n = ShapeUtil::GetDimension(a_shape, -1);",
          "new_line_content": "    int bdims = b_shape.rank();",
          "content_same": false
        },
        {
          "line": 175,
          "old_api": "SliceInMinorDims",
          "new_api": "std::min((j + 1) * block_size, n)",
          "old_text": "SliceInMinorDims(inv_diag_blocks, {j, 0, 0},\n                                                   {j + 1, block, block})",
          "new_text": "std::min((j + 1) * block_size, n)",
          "old_line_content": "          MaybeConjugate(Collapse(SliceInMinorDims(inv_diag_blocks, {j, 0, 0},",
          "new_line_content": "      int64_t k = std::min((j + 1) * block_size, n);",
          "content_same": false
        },
        {
          "line": 206,
          "old_api": "std::swap(start[0], start[1])",
          "new_api": "BatchDot",
          "old_text": "std::swap(start[0], start[1])",
          "new_text": "BatchDot(a_row, transpose_a, x, false, precision)",
          "old_line_content": "          std::swap(start[0], start[1]);",
          "new_line_content": "          remainder = b_row - BatchDot(a_row, transpose_a, x, false, precision);",
          "content_same": false
        },
        {
          "line": 247,
          "old_api": "builder",
          "new_api": "IPow",
          "old_text": "diag_blocks.builder()",
          "new_text": "IPow(block_size, 2)",
          "old_line_content": "  XlaBuilder* builder = diag_blocks.builder();",
          "new_line_content": "    int64_t num_blocks = ShapeUtil::ElementsIn(shape) / IPow(block_size, 2);",
          "content_same": false
        },
        {
          "line": 248,
          "old_api": "element_type",
          "new_api": "Reshape",
          "old_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Input is a batch of square lower triangular square matrices. Its shape is\n    // (..., size, size). We resize this to (num_blocks, size, size).\n    TF_ASSIGN_OR_RETURN(Shape shape, builder->GetShape(diag_blocks));\n    int64_t block_size = ShapeUtil::GetDimension(shape, -1);\n    int64_t num_blocks = ShapeUtil::ElementsIn(shape) / IPow(block_size, 2);\n    diag_blocks = Reshape(diag_blocks, {num_blocks, block_size, block_size});\n\n    // The input must be triangular because we rely on that when doing\n    // multiplications later on\n    diag_blocks = Triangle(diag_blocks, /*lower=*/lower_triangular);\n\n    // Rescale blocks to be unit triangular, but avoid dividing by\n    // zero (which can happen if the last block was padded) otherwise it will\n    // introduce nans which will propagate\n    auto diags = GetMatrixDiagonal(diag_blocks);\n    auto ones = FullLike(diags, 1);\n    diags = Select(Eq(diags, Zero(builder, shape.element_type())), ones, diags);\n    auto scaled_diag_blocks = Div(diag_blocks, diags, {0, 2});\n\n    // We can now use the fact that for an upper triangular matrix\n    // [[L11, 0], [L21, L22]], given the inverses L11' and L22', we have\n    // L22' = -L22' * L21 * L11'. In our case, L21 is a vector and our blocks\n    // have been rescaled to be unit triangular, so L22 = L22' = 1.\n\n    // Initialize the output matrix with -1s on the diagonal. We use -1 instead\n    // of 1 because we cannot do matrix-vector multiplies with variable shapes\n    // inside of a loop, or do irregularly shaped in-place updates. Hence,\n    // L21 <- -L22 * L21 * L11 cannot be done naively. Instead, we update the\n    // entire row i.e. we calculate\n    // [L21 L22 0] <- -[L21 L22 0] @ diag_blocks([L11', -I, -I])\n    // which means [L21 L22 0] <- [-L21 * L11', L22, 0].\n    auto identity =\n        IdentityMatrix(builder, shape.element_type(), block_size, block_size);\n    auto neg_identity = -identity;\n\n    // The first or last  diagonal element should be set to 1 instead of -1\n    // though, since we never update it\n    auto pos_one = Reshape(One(builder, shape.element_type()), {1, 1});\n    auto start_index =\n        ConstantR0<int>(builder, lower_triangular ? 0 : block_size - 1);\n    auto output_block =\n        DynamicUpdateSlice(neg_identity, pos_one,\n                           /*start_indices=*/{start_index, start_index});\n\n    // Broadcast diag([1, -1, -1, ...]) to every block\n    XlaOp output = Broadcast(output_block,\n                             /*broadcast_sizes=*/{num_blocks});\n\n    // Now we construct a loop that performs matrix-vector multiplications\n    // inverting the blocks one row at a time\n    std::vector<Shape> tuple_shapes = {\n        // The loop iteration counter is a scalar, incremented each iteration.\n        ShapeUtil::MakeShape(S32, {}),\n        // The output has the shape of A, with one row updated each iteration.\n        ShapeUtil::MakeShape(shape.element_type(),\n                             {num_blocks, block_size, block_size}),\n        // The input is a loop invariant.\n        ShapeUtil::MakeShape(shape.element_type(),\n                             {num_blocks, block_size, block_size})};\n    Shape tuple_shape = ShapeUtil::MakeTupleShape(tuple_shapes);\n\n    auto init_i = One(builder, S32);\n    auto init = Tuple(builder, {init_i, output, scaled_diag_blocks});\n\n    // Construct the loop condition function.\n    std::unique_ptr<XlaBuilder> condb =\n        builder->CreateSubBuilder(\"InvertDiagCond\");\n    {\n      auto i = GetTupleElement(\n          Parameter(condb.get(), 0, tuple_shape, \"InvertDiagCondTuple\"), 0);\n      Lt(i, ConstantR0<int32_t>(condb.get(), block_size));\n    }\n    TF_ASSIGN_OR_RETURN(auto cond, condb->Build());\n\n    // Construct the loop body function.\n    std::unique_ptr<XlaBuilder> bodyb =\n        builder->CreateSubBuilder(\"InvertDiagBody\");\n    {\n      auto input_tuple =\n          Parameter(bodyb.get(), 0, tuple_shape, \"InvertDiagBodyTuple\");\n\n      auto i = GetTupleElement(input_tuple, 0);\n      auto body_out = GetTupleElement(input_tuple, 1);\n      auto body_input = GetTupleElement(input_tuple, 2);\n\n      auto zero = ConstantR0<int32_t>(bodyb.get(), 0);\n      auto j = lower_triangular ? i : ScalarLike(i, block_size - 1) - i;\n      auto input_row =\n          DynamicSlice(body_input, {zero, j, zero},\n                       /*slice_sizes=*/{num_blocks, 1, block_size});\n\n      // We want -L21 L11^{-1}\n      DotDimensionNumbers dnums;\n      dnums.add_lhs_batch_dimensions(0);\n      dnums.add_rhs_batch_dimensions(0);\n      dnums.add_lhs_contracting_dimensions(2);\n      dnums.add_rhs_contracting_dimensions(1);\n      PrecisionConfig precision_proto;\n      precision_proto.add_operand_precision(precision);\n      precision_proto.add_operand_precision(precision);\n      auto update = -DotGeneral(input_row, body_out, dnums, &precision_proto);\n\n      body_out = DynamicUpdateSlice(body_out, update, {zero, j, zero});\n\n      auto next_i = i + ScalarLike(i, 1);\n      Tuple(bodyb.get(), {next_i, body_out, body_input});\n    }\n    TF_ASSIGN_OR_RETURN(auto body, bodyb->Build());\n\n    // Construct the While loop and return the result,\n    // return while_loop(cond_fun, body_fun, init)[1]\n    auto invert_while = While(cond, body, init);\n    auto inv_diag_blocks = GetTupleElement(invert_while, 1);\n    // Undo the scaling\n    inv_diag_blocks = Div(inv_diag_blocks, diags,\n                          /*broadcast_dimensions=*/{0, 1});\n\n    // Reshape back to original batch major dimensions\n    return Reshape(inv_diag_blocks, shape.dimensions());\n  })",
          "new_text": "Reshape(diag_blocks, {num_blocks, block_size, block_size})",
          "old_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    diag_blocks = Reshape(diag_blocks, {num_blocks, block_size, block_size});",
          "content_same": false
        },
        {
          "line": 252,
          "old_api": "ShapeUtil::GetDimension(shape, -1)",
          "new_api": "Triangle",
          "old_text": "ShapeUtil::GetDimension(shape, -1)",
          "new_text": "Triangle(diag_blocks, /*lower=*/lower_triangular)",
          "old_line_content": "    int64_t block_size = ShapeUtil::GetDimension(shape, -1);",
          "new_line_content": "    diag_blocks = Triangle(diag_blocks, /*lower=*/lower_triangular);",
          "content_same": false
        },
        {
          "line": 258,
          "old_api": "Triangle",
          "new_api": "FullLike",
          "old_text": "Triangle(diag_blocks, /*lower=*/lower_triangular)",
          "new_text": "FullLike(diags, 1)",
          "old_line_content": "    diag_blocks = Triangle(diag_blocks, /*lower=*/lower_triangular);",
          "new_line_content": "    auto ones = FullLike(diags, 1);",
          "content_same": false
        },
        {
          "line": 288,
          "old_api": "ConstantR0<int>(builder, lower_triangular ? 0 : block_size - 1)",
          "new_api": "Broadcast",
          "old_text": "ConstantR0<int>(builder, lower_triangular ? 0 : block_size - 1)",
          "new_text": "Broadcast(output_block,\n                             /*broadcast_sizes=*/{num_blocks})",
          "old_line_content": "        ConstantR0<int>(builder, lower_triangular ? 0 : block_size - 1);",
          "new_line_content": "    XlaOp output = Broadcast(output_block,",
          "content_same": false
        },
        {
          "line": 311,
          "old_api": "Tuple",
          "new_api": "get",
          "old_text": "Tuple(builder, {init_i, output, scaled_diag_blocks})",
          "new_text": "GetTupleElement(\n          Parameter(condb.get(), 0, tuple_shape, \"InvertDiagCondTuple\"), 0)",
          "old_line_content": "    auto init = Tuple(builder, {init_i, output, scaled_diag_blocks});",
          "new_line_content": "      auto i = GetTupleElement(",
          "content_same": false
        },
        {
          "line": 319,
          "old_api": "get",
          "new_api": "CreateSubBuilder",
          "old_text": "condb.get()",
          "new_text": "builder->CreateSubBuilder(\"InvertDiagBody\")",
          "old_line_content": "      Lt(i, ConstantR0<int32_t>(condb.get(), block_size));",
          "new_line_content": "        builder->CreateSubBuilder(\"InvertDiagBody\");",
          "content_same": false
        },
        {
          "line": 325,
          "old_api": "CreateSubBuilder",
          "new_api": "GetTupleElement",
          "old_text": "builder->CreateSubBuilder(\"InvertDiagBody\")",
          "new_text": "GetTupleElement(input_tuple, 1)",
          "old_line_content": "        builder->CreateSubBuilder(\"InvertDiagBody\");",
          "new_line_content": "      auto body_out = GetTupleElement(input_tuple, 1);",
          "content_same": false
        },
        {
          "line": 331,
          "old_api": "GetTupleElement",
          "new_api": "DynamicSlice",
          "old_text": "GetTupleElement(input_tuple, 1)",
          "new_text": "DynamicSlice(body_input, {zero, j, zero},\n                       /*slice_sizes=*/{num_blocks, 1, block_size})",
          "old_line_content": "      auto body_out = GetTupleElement(input_tuple, 1);",
          "new_line_content": "          DynamicSlice(body_input, {zero, j, zero},",
          "content_same": false
        },
        {
          "line": 337,
          "old_api": "DynamicSlice",
          "new_api": "add_rhs_batch_dimensions",
          "old_text": "DynamicSlice(body_input, {zero, j, zero},\n                       /*slice_sizes=*/{num_blocks, 1, block_size})",
          "new_text": "dnums.add_rhs_batch_dimensions(0)",
          "old_line_content": "          DynamicSlice(body_input, {zero, j, zero},",
          "new_line_content": "      dnums.add_rhs_batch_dimensions(0);",
          "content_same": false
        },
        {
          "line": 342,
          "old_api": "add_lhs_batch_dimensions",
          "new_api": "add_operand_precision",
          "old_text": "dnums.add_lhs_batch_dimensions(0)",
          "new_text": "precision_proto.add_operand_precision(precision)",
          "old_line_content": "      dnums.add_lhs_batch_dimensions(0);",
          "new_line_content": "      precision_proto.add_operand_precision(precision);",
          "content_same": false
        },
        {
          "line": 343,
          "old_api": "add_rhs_batch_dimensions",
          "new_api": "DotGeneral",
          "old_text": "dnums.add_rhs_batch_dimensions(0)",
          "new_text": "DotGeneral(input_row, body_out, dnums, &precision_proto)",
          "old_line_content": "      dnums.add_rhs_batch_dimensions(0);",
          "new_line_content": "      auto update = -DotGeneral(input_row, body_out, dnums, &precision_proto);",
          "content_same": false
        },
        {
          "line": 345,
          "old_api": "add_rhs_contracting_dimensions",
          "new_api": "DynamicUpdateSlice",
          "old_text": "dnums.add_rhs_contracting_dimensions(1)",
          "new_text": "DynamicUpdateSlice(body_out, update, {zero, j, zero})",
          "old_line_content": "      dnums.add_rhs_contracting_dimensions(1);",
          "new_line_content": "      body_out = DynamicUpdateSlice(body_out, update, {zero, j, zero});",
          "content_same": false
        },
        {
          "line": 347,
          "old_api": "add_operand_precision",
          "new_api": "ScalarLike",
          "old_text": "precision_proto.add_operand_precision(precision)",
          "new_text": "ScalarLike(i, 1)",
          "old_line_content": "      precision_proto.add_operand_precision(precision);",
          "new_line_content": "      auto next_i = i + ScalarLike(i, 1);",
          "content_same": false
        },
        {
          "line": 348,
          "old_api": "add_operand_precision",
          "new_api": "get",
          "old_text": "precision_proto.add_operand_precision(precision)",
          "new_text": "bodyb.get()",
          "old_line_content": "      precision_proto.add_operand_precision(precision);",
          "new_line_content": "      Tuple(bodyb.get(), {next_i, body_out, body_input});",
          "content_same": false
        },
        {
          "line": 354,
          "old_api": "get",
          "new_api": "While",
          "old_text": "bodyb.get()",
          "new_text": "While(cond, body, init)",
          "old_line_content": "      Tuple(bodyb.get(), {next_i, body_out, body_input});",
          "new_line_content": "    auto invert_while = While(cond, body, init);",
          "content_same": false
        },
        {
          "line": 361,
          "old_api": "GetTupleElement",
          "new_api": "dimensions",
          "old_text": "GetTupleElement(invert_while, 1)",
          "new_text": "shape.dimensions()",
          "old_line_content": "    auto inv_diag_blocks = GetTupleElement(invert_while, 1);",
          "new_line_content": "    return Reshape(inv_diag_blocks, shape.dimensions());",
          "content_same": false
        },
        {
          "line": 379,
          "old_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_api": "ZerosLike",
          "old_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_text": "ZerosLike(a)",
          "old_line_content": "    int64_t k = ShapeUtil::GetDimension(a_shape, -1);",
          "new_line_content": "      a = lower ? Select(TriangleMask(a, -1), a, ZerosLike(a))",
          "content_same": false
        },
        {
          "line": 385,
          "old_api": "ZerosLike",
          "new_api": "Triangle",
          "old_text": "ZerosLike(a)",
          "new_text": "Triangle(a, lower)",
          "old_line_content": "      a = lower ? Select(TriangleMask(a, -1), a, ZerosLike(a))",
          "new_line_content": "      a = Triangle(a, lower);",
          "content_same": false
        },
        {
          "line": 396,
          "old_api": "DiagonalBlocks",
          "new_api": "SolveWithInvertedDiagonalBlocks",
          "old_text": "DiagonalBlocks(a, block_size)",
          "new_text": "SolveWithInvertedDiagonalBlocks(a, b, inv_diag_blocks, left_side,\n                                           lower, transpose_a, conjugate_a,\n                                           precision)",
          "old_line_content": "    auto diag_blocks = DiagonalBlocks(a, block_size);",
          "new_line_content": "    return SolveWithInvertedDiagonalBlocks(a, b, inv_diag_blocks, left_side,",
          "content_same": false
        },
        {
          "line": 419,
          "old_api": "builder",
          "new_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_text": "a.builder()",
          "new_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_line_content": "  XlaBuilder* builder = a.builder();",
          "new_line_content": "    const int64_t a_size = ShapeUtil::GetDimension(a_shape, -1);",
          "content_same": false
        },
        {
          "line": 420,
          "old_api": "ReportErrorOrReturn",
          "new_api": "MaybeConjugate",
          "old_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));\n    TF_ASSIGN_OR_RETURN(Shape b_shape, builder->GetShape(b));\n    int64_t m = ShapeUtil::GetDimension(b_shape, -2);\n    int64_t n = ShapeUtil::GetDimension(b_shape, -1);\n    const int64_t a_size = ShapeUtil::GetDimension(a_shape, -1);\n    a = MaybeConjugate(a, conjugate_a);\n    bool backwards = transpose_a ^ lower ^ !left_side;\n    for (int64_t i = 0; i < a_size; ++i) {\n      int64_t j = backwards ? i : (a_size - i - 1);\n      std::vector<int64_t> b_row_start, b_row_end;\n      if (left_side) {\n        b_row_start = {j, 0};\n        b_row_end = {j + 1, n};\n      } else {\n        b_row_start = {0, j};\n        b_row_end = {m, j + 1};\n      }\n      auto b_row = SliceInMinorDims(b, b_row_start, b_row_end);\n\n      std::vector<int64_t> a_start = {j, backwards ? 0 : (j + 1)};\n      std::vector<int64_t> a_end = {j + 1, backwards ? j : a_size};\n      if (transpose_a ^ !left_side) {\n        std::swap(a_start[0], a_start[1]);\n        std::swap(a_end[0], a_end[1]);\n      }\n      auto a_chunk = SliceInMinorDims(a, a_start, a_end);\n      if (left_side) {\n        bool which = transpose_a ^ lower;\n        auto b_chunk =\n            SliceInMinorDims(b, {which ? 0 : (j + 1), 0}, {which ? j : m, n});\n        b_row = b_row - BatchDot(a_chunk, /*transpose_x=*/transpose_a, b_chunk,\n                                 /*transpose_y=*/false, precision);\n      } else {\n        bool which = transpose_a ^ !lower;\n        auto b_chunk =\n            SliceInMinorDims(b, {0, which ? 0 : (j + 1)}, {m, which ? j : n});\n        b_row = b_row - BatchDot(b_chunk, /*transpose_x=*/false, a_chunk,\n                                 /*transpose_y=*/transpose_a, precision);\n      }\n      if (!unit_diagonal) {\n        auto a_diag = SliceInMinorDims(a, {j, j}, {j + 1, j + 1});\n        b_row = b_row / a_diag;\n      }\n\n      b = UpdateSliceInMinorDims(b, b_row, b_row_start);\n    }\n\n    return b;\n  })",
          "new_text": "MaybeConjugate(a, conjugate_a)",
          "old_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    a = MaybeConjugate(a, conjugate_a);",
          "content_same": false
        },
        {
          "line": 438,
          "old_api": "SliceInMinorDims",
          "new_api": "std::swap(a_end[0], a_end[1])",
          "old_text": "SliceInMinorDims(b, b_row_start, b_row_end)",
          "new_text": "std::swap(a_end[0], a_end[1])",
          "old_line_content": "      auto b_row = SliceInMinorDims(b, b_row_start, b_row_end);",
          "new_line_content": "        std::swap(a_end[0], a_end[1]);",
          "content_same": false
        },
        {
          "line": 444,
          "old_api": "std::swap(a_end[0], a_end[1])",
          "new_api": "SliceInMinorDims",
          "old_text": "std::swap(a_end[0], a_end[1])",
          "new_text": "SliceInMinorDims(b, {which ? 0 : (j + 1), 0}, {which ? j : m, n})",
          "old_line_content": "        std::swap(a_end[0], a_end[1]);",
          "new_line_content": "            SliceInMinorDims(b, {which ? 0 : (j + 1), 0}, {which ? j : m, n});",
          "content_same": false
        },
        {
          "line": 496,
          "old_api": "dimensions",
          "new_api": "ShapeUtil::HumanString(b_shape)",
          "old_text": "a_shape.dimensions(i)",
          "new_text": "ShapeUtil::HumanString(b_shape)",
          "old_line_content": "      int64_t a_size = a_shape.dimensions(i);",
          "new_line_content": "            ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));",
          "content_same": false
        },
        {
          "line": 502,
          "old_api": "ShapeUtil::HumanString(b_shape)",
          "new_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_text": "ShapeUtil::HumanString(b_shape)",
          "new_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_line_content": "            ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));",
          "new_line_content": "    if (ShapeUtil::GetDimension(a_shape, -1) !=",
          "content_same": false
        },
        {
          "line": 504,
          "old_api": "push_back",
          "new_api": "InvalidArgument",
          "old_text": "batch_dimensions.push_back(a_size)",
          "new_text": "InvalidArgument(\n          \"The 'a' argument to TriangularSolve must be a batched square matrix;\"\n          \" shape was: %s\",\n          ShapeUtil::HumanString(a_shape))",
          "old_line_content": "      batch_dimensions.push_back(a_size);",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 509,
          "old_api": "ShapeUtil::GetDimension(a_shape, -2)",
          "new_api": "ShapeUtil::GetDimension(b_shape, -2)",
          "old_text": "ShapeUtil::GetDimension(a_shape, -2)",
          "new_text": "ShapeUtil::GetDimension(b_shape, -2)",
          "old_line_content": "        ShapeUtil::GetDimension(a_shape, -2)) {",
          "new_line_content": "    const int64_t m = ShapeUtil::GetDimension(b_shape, -2);",
          "content_same": false
        },
        {
          "line": 510,
          "old_api": "InvalidArgument",
          "new_api": "ShapeUtil::GetDimension(b_shape, -1)",
          "old_text": "InvalidArgument(\n          \"The 'a' argument to TriangularSolve must be a batched square matrix;\"\n          \" shape was: %s\",\n          ShapeUtil::HumanString(a_shape))",
          "new_text": "ShapeUtil::GetDimension(b_shape, -1)",
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    const int64_t n = ShapeUtil::GetDimension(b_shape, -1);",
          "content_same": false
        },
        {
          "line": 515,
          "old_api": "ShapeUtil::GetDimension(b_shape, -2)",
          "new_api": "ShapeUtil::HumanString(b_shape)",
          "old_text": "ShapeUtil::GetDimension(b_shape, -2)",
          "new_text": "ShapeUtil::HumanString(b_shape)",
          "old_line_content": "    const int64_t m = ShapeUtil::GetDimension(b_shape, -2);",
          "new_line_content": "          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));",
          "content_same": false
        },
        {
          "line": 518,
          "old_api": "InvalidArgument",
          "new_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_text": "InvalidArgument(\n          \"Arguments to TriangularSolve have incompatible matrix shapes %s and \"\n          \"%s\",\n          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape))",
          "new_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    int64_t a_size = ShapeUtil::GetDimension(a_shape, -1);",
          "content_same": false
        },
        {
          "line": 558,
          "old_api": "opcode",
          "new_api": "triangular_solve_options",
          "old_text": "instruction->opcode()",
          "new_text": "instruction->triangular_solve_options()",
          "old_line_content": "  return instruction->opcode() == HloOpcode::kTriangularSolve;",
          "new_line_content": "      instruction->triangular_solve_options();",
          "content_same": false
        },
        {
          "line": 564,
          "old_api": "triangular_solve_options",
          "new_api": "lower",
          "old_text": "instruction->triangular_solve_options()",
          "new_text": "options.lower()",
          "old_line_content": "      instruction->triangular_solve_options();",
          "new_line_content": "      options.lower() ? \"lower\" : \"upper\",",
          "content_same": false
        },
        {
          "line": 565,
          "old_api": "ToString",
          "new_api": "transpose_a",
          "old_text": "absl::StrFormat(\n      \"xla.triangular_solve_%s_%s_%s_%s_%s_%s\",\n      instruction->operand(0)->shape().ToString(),\n      instruction->operand(1)->shape().ToString(),\n      options.left_side() ? \"left\" : \"right\",\n      options.lower() ? \"lower\" : \"upper\",\n      TriangularSolveOptions_Transpose_Name(options.transpose_a()),\n      options.unit_diagonal() ? \"unit\" : \"nonunit\")",
          "new_text": "options.transpose_a()",
          "old_line_content": "  const std::string name = absl::StrFormat(",
          "new_line_content": "      TriangularSolveOptions_Transpose_Name(options.transpose_a()),",
          "content_same": false
        },
        {
          "line": 568,
          "old_api": "ToString",
          "new_api": "parent",
          "old_text": "instruction->operand(1)->shape().ToString()",
          "new_text": "instruction->parent()->parent()",
          "old_line_content": "      instruction->operand(1)->shape().ToString(),",
          "new_line_content": "  HloModule* module = instruction->parent()->parent();",
          "content_same": false
        },
        {
          "line": 571,
          "old_api": "transpose_a",
          "new_api": "emplace",
          "old_text": "options.transpose_a()",
          "new_text": "computation_cache_.emplace(name, nullptr)",
          "old_line_content": "      TriangularSolveOptions_Transpose_Name(options.transpose_a()),",
          "new_line_content": "      computation_cache_.emplace(name, nullptr).first->second;",
          "content_same": false
        },
        {
          "line": 590,
          "old_api": "operand",
          "new_api": "lower",
          "old_text": "struction->operand(0)->shape(), ",
          "new_text": "tions.lower(),\n",
          "old_line_content": "    XlaOp a = Parameter(&builder, 0, instruction->operand(0)->shape(), \"a\");",
          "new_line_content": "    BuildTriangularSolve(a, b, options.left_side(), options.lower(),",
          "content_same": false
        },
        {
          "line": 591,
          "old_api": "operand",
          "new_api": "unit_diagonal",
          "old_text": "struction->operand(1)->shape(), ",
          "new_text": "tions.unit_diagonal(),\n",
          "old_line_content": "    XlaOp b = Parameter(&builder, 1, instruction->operand(1)->shape(), \"b\");",
          "new_line_content": "                         transpose_a, conjugate_a, options.unit_diagonal(),",
          "content_same": false
        }
      ],
      "additions": [
        {
          "line": 512,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Arguments to TriangularSolve have incompatible matrix shapes %s and \"\n          \"%s\",\n          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape))",
          "old_line_content": "          \" shape was: %s\",",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 520,
          "old_api": null,
          "new_api": "ShapeUtil::IsZeroElementArray(b_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::IsZeroElementArray(b_shape)",
          "old_line_content": "          \"%s\",",
          "new_line_content": "    if (ShapeUtil::IsZeroElementArray(b_shape)) {",
          "content_same": false
        },
        {
          "line": 528,
          "old_api": null,
          "new_api": "MaybeConjugate",
          "old_text": null,
          "new_text": "MaybeConjugate(a, conjugate_a)",
          "old_line_content": "      // elements, any such array will do.",
          "new_line_content": "      return unit_diagonal ? b : Div(b, MaybeConjugate(a, conjugate_a));",
          "content_same": false
        },
        {
          "line": 533,
          "old_api": null,
          "new_api": "UseDirectSolves",
          "old_text": null,
          "new_text": "UseDirectSolves()",
          "old_line_content": "    if (a_size == 1) {",
          "new_line_content": "    if (UseDirectSolves() && batch > block_size_ / 16 &&",
          "content_same": false
        },
        {
          "line": 535,
          "old_api": null,
          "new_api": "SolveDirectly",
          "old_text": null,
          "new_text": "SolveDirectly(a, b, left_side, lower, transpose_a, conjugate_a,\n                           unit_diagonal, precision)",
          "old_line_content": "    }",
          "new_line_content": "      return SolveDirectly(a, b, left_side, lower, transpose_a, conjugate_a,",
          "content_same": false
        },
        {
          "line": 538,
          "old_api": null,
          "new_api": "SolveByInvertingDiagonalBlocks",
          "old_text": null,
          "new_text": "SolveByInvertingDiagonalBlocks(a, b, left_side, lower, transpose_a,\n                                            conjugate_a, unit_diagonal,\n                                            precision)",
          "old_line_content": "    // dimension and the matrix is very small.",
          "new_line_content": "      return SolveByInvertingDiagonalBlocks(a, b, left_side, lower, transpose_a,",
          "content_same": false
        },
        {
          "line": 547,
          "old_api": null,
          "new_api": "CHECK_GE",
          "old_text": null,
          "new_text": "CHECK_GE(block_size_, 1)",
          "old_line_content": "    }",
          "new_line_content": "  CHECK_GE(block_size_, 1);",
          "content_same": false
        },
        {
          "line": 552,
          "old_api": null,
          "new_api": "opcode",
          "old_text": null,
          "new_text": "instruction->opcode()",
          "old_line_content": "    : block_size_(block_size) {",
          "new_line_content": "  return instruction->opcode() == HloOpcode::kTriangularSolve;",
          "content_same": false
        },
        {
          "line": 559,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "absl::StrFormat(\n      \"xla.triangular_solve_%s_%s_%s_%s_%s_%s\",\n      instruction->operand(0)->shape().ToString(),\n      instruction->operand(1)->shape().ToString(),\n      options.left_side() ? \"left\" : \"right\",\n      options.lower() ? \"lower\" : \"upper\",\n      TriangularSolveOptions_Transpose_Name(options.transpose_a()),\n      options.unit_diagonal() ? \"unit\" : \"nonunit\")",
          "old_line_content": "}",
          "new_line_content": "  const std::string name = absl::StrFormat(",
          "content_same": false
        },
        {
          "line": 561,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "instruction->operand(0)->shape().ToString()",
          "old_line_content": "StatusOr<HloInstruction*> TriangularSolveExpander::ExpandInstruction(",
          "new_line_content": "      instruction->operand(0)->shape().ToString(),",
          "content_same": false
        },
        {
          "line": 562,
          "old_api": null,
          "new_api": "ToString",
          "old_text": null,
          "new_text": "instruction->operand(1)->shape().ToString()",
          "old_line_content": "    HloInstruction* instruction) {",
          "new_line_content": "      instruction->operand(1)->shape().ToString(),",
          "content_same": false
        },
        {
          "line": 563,
          "old_api": null,
          "new_api": "left_side",
          "old_text": null,
          "new_text": "options.left_side()",
          "old_line_content": "  const TriangularSolveOptions& options =",
          "new_line_content": "      options.left_side() ? \"left\" : \"right\",",
          "content_same": false
        },
        {
          "line": 566,
          "old_api": null,
          "new_api": "unit_diagonal",
          "old_text": null,
          "new_text": "options.unit_diagonal()",
          "old_line_content": "      \"xla.triangular_solve_%s_%s_%s_%s_%s_%s\",",
          "new_line_content": "      options.unit_diagonal() ? \"unit\" : \"nonunit\");",
          "content_same": false
        },
        {
          "line": 584,
          "old_api": null,
          "new_api": "operand",
          "old_text": null,
          "new_text": "struction->operand(0)->shape(), ",
          "old_line_content": "    // are much more ergonomic than the internal ones. As it turns out,",
          "new_line_content": "    XlaOp a = Parameter(&builder, 0, instruction->operand(0)->shape(), \"a\");",
          "content_same": false
        },
        {
          "line": 585,
          "old_api": null,
          "new_api": "operand",
          "old_text": null,
          "new_text": "struction->operand(1)->shape(), ",
          "old_line_content": "    // XlaBuilder isn't really a client APIâ€”what it does is build a",
          "new_line_content": "    XlaOp b = Parameter(&builder, 1, instruction->operand(1)->shape(), \"b\");",
          "content_same": false
        },
        {
          "line": 74,
          "old_api": null,
          "new_api": "add_offset_dims",
          "old_text": null,
          "new_text": "dim_numbers.add_offset_dims(i)",
          "old_line_content": "",
          "new_line_content": "        dim_numbers.add_offset_dims(i);",
          "content_same": false
        },
        {
          "line": 75,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(shape, i)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(shape, i)",
          "old_line_content": "      // Gather the diagonal blocks",
          "new_line_content": "        slice_sizes[i] = ShapeUtil::GetDimension(shape, i);",
          "content_same": false
        },
        {
          "line": 587,
          "old_api": null,
          "new_api": "transpose_a",
          "old_text": null,
          "new_text": "tions.transpose_a() !",
          "old_line_content": "    // into our HloModule. Ideally we would avoid the protocol buffer step;",
          "new_line_content": "        options.transpose_a() != TriangularSolveOptions::NO_TRANSPOSE;",
          "content_same": false
        },
        {
          "line": 588,
          "old_api": null,
          "new_api": "transpose_a",
          "old_text": null,
          "new_text": "tions.transpose_a() =",
          "old_line_content": "    // that is left as an exercise for future work.",
          "new_line_content": "    bool conjugate_a = options.transpose_a() == TriangularSolveOptions::ADJOINT;",
          "content_same": false
        },
        {
          "line": 78,
          "old_api": null,
          "new_api": "add_offset_dims",
          "old_text": null,
          "new_text": "dim_numbers.add_offset_dims(ndims - 1)",
          "old_line_content": "      for (int i = 0; i < ndims - 2; ++i) {",
          "new_line_content": "      dim_numbers.add_offset_dims(ndims - 1);",
          "content_same": false
        },
        {
          "line": 82,
          "old_api": null,
          "new_api": "set_index_vector_dim",
          "old_text": null,
          "new_text": "dim_numbers.set_index_vector_dim(1)",
          "old_line_content": "      }",
          "new_line_content": "      dim_numbers.set_index_vector_dim(1);",
          "content_same": false
        },
        {
          "line": 83,
          "old_api": null,
          "new_api": "Gather",
          "old_text": null,
          "new_text": "Gather(a, start_indices, dim_numbers, slice_sizes)",
          "old_line_content": "      slice_sizes[ndims - 2] = slice_sizes[ndims - 1] = block_size;",
          "new_line_content": "      diag_blocks = Gather(a, start_indices, dim_numbers, slice_sizes);",
          "content_same": false
        },
        {
          "line": 91,
          "old_api": null,
          "new_api": "SliceInMinorDims",
          "old_text": null,
          "new_text": "SliceInMinorDims(a, {n - n % block_size, n - n % block_size}, {n, n})",
          "old_line_content": "",
          "new_line_content": "          SliceInMinorDims(a, {n - n % block_size, n - n % block_size}, {n, n});",
          "content_same": false
        },
        {
          "line": 92,
          "old_api": null,
          "new_api": "MakeNoPaddingConfig",
          "old_text": null,
          "new_text": "MakeNoPaddingConfig(ndims)",
          "old_line_content": "    // The last block might be smaller than the block size,",
          "new_line_content": "      PaddingConfig config = MakeNoPaddingConfig(ndims);",
          "content_same": false
        },
        {
          "line": 603,
          "old_api": null,
          "new_api": "entry_computation",
          "old_text": null,
          "new_text": "w_module->entry_computation(), ",
          "old_line_content": "                        xla_computation.GetProgramShape());",
          "new_line_content": "        module->DeepCloneComputation(new_module->entry_computation(), &context);",
          "content_same": false
        },
        {
          "line": 94,
          "old_api": null,
          "new_api": "mutable_dimensions",
          "old_text": null,
          "new_text": "config.mutable_dimensions(ndims - 2)->set_edge_padding_high(padding)",
          "old_line_content": "    if (n % block_size != 0) {",
          "new_line_content": "      config.mutable_dimensions(ndims - 2)->set_edge_padding_high(padding);",
          "content_same": false
        },
        {
          "line": 606,
          "old_api": null,
          "new_api": "shape",
          "old_text": null,
          "new_text": "oInstruction::CreateCall(\n      instruction->shape(), instruction->operands(), computation));",
          "old_line_content": "                                             xla_computation.proto(), config));",
          "new_line_content": "  return instruction->parent()->AddInstruction(HloInstruction::CreateCall(",
          "content_same": false
        },
        {
          "line": 96,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape.element_type()",
          "old_line_content": "      auto last_blocks =",
          "new_line_content": "          Pad(last_blocks, Zero(builder, shape.element_type()), config);",
          "content_same": false
        },
        {
          "line": 607,
          "old_api": null,
          "new_api": "operands",
          "old_text": null,
          "new_text": "struction->operands(), ",
          "old_line_content": "    HloCloneContext context(module);",
          "new_line_content": "      instruction->shape(), instruction->operands(), computation));",
          "content_same": false
        },
        {
          "line": 99,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape.element_type()",
          "old_line_content": "      int64_t padding = block_size - n % block_size;",
          "new_line_content": "          IdentityMatrix(builder, shape.element_type(), padding, padding);",
          "content_same": false
        },
        {
          "line": 101,
          "old_api": null,
          "new_api": "mutable_dimensions",
          "old_text": null,
          "new_text": "config.mutable_dimensions(0)->set_edge_padding_low(n % block_size)",
          "old_line_content": "      last_blocks =",
          "new_line_content": "      config.mutable_dimensions(0)->set_edge_padding_low(n % block_size);",
          "content_same": false
        },
        {
          "line": 103,
          "old_api": null,
          "new_api": "Broadcast",
          "old_text": null,
          "new_text": "Broadcast(eye, batch_dims)",
          "old_line_content": "",
          "new_line_content": "      eye = Broadcast(eye, batch_dims);",
          "content_same": false
        },
        {
          "line": 104,
          "old_api": null,
          "new_api": "ConcatInDim",
          "old_text": null,
          "new_text": "ConcatInDim(builder, {last_blocks, eye}, ndims - 1)",
          "old_line_content": "      auto eye =",
          "new_line_content": "      last_blocks = ConcatInDim(builder, {last_blocks, eye}, ndims - 1);",
          "content_same": false
        },
        {
          "line": 111,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "last_blocks_dims.begin()",
          "old_line_content": "",
          "new_line_content": "      std::copy(shape_dims.begin(), shape_dims.end(), last_blocks_dims.begin());",
          "content_same": false
        },
        {
          "line": 112,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "last_blocks_dims.end()",
          "old_line_content": "      // Add a singleton dimension",
          "new_line_content": "      last_blocks_dims.insert(last_blocks_dims.end() - 2, 1);",
          "content_same": false
        },
        {
          "line": 113,
          "old_api": null,
          "new_api": "Reshape",
          "old_text": null,
          "new_text": "Reshape(last_blocks, last_blocks_dims)",
          "old_line_content": "      // i.e. [..., block_size, block_size] -> [..., 1, block_size, block_size]",
          "new_line_content": "      last_blocks = Reshape(last_blocks, last_blocks_dims);",
          "content_same": false
        },
        {
          "line": 132,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "a.builder()",
          "old_line_content": "}",
          "new_line_content": "  XlaBuilder* builder = a.builder();",
          "content_same": false
        },
        {
          "line": 133,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(Shape blocks_shape, builder->GetShape(inv_diag_blocks));\n    TF_ASSIGN_OR_RETURN(Shape b_shape, builder->GetShape(b));\n    int64_t block_size = ShapeUtil::GetDimension(blocks_shape, -1);\n\n    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));\n    int64_t ndims = a_shape.rank();\n    int64_t n = ShapeUtil::GetDimension(a_shape, -1);\n    int64_t num_blocks = n / block_size + (n % block_size != 0);\n    int64_t m_dim = (left_side) ? -1 : -2;\n    int64_t m = ShapeUtil::GetDimension(b_shape, m_dim);\n\n    std::vector<XlaOp> update_ops;\n    int bdims = b_shape.rank();\n    int64_t block_dim = (left_side) ? bdims - 2 : bdims - 1;\n\n    // Initialize the solution\n    XlaOp x;\n\n    // This loop is unrolled for performance reasons, but it could be expressed\n    // rolled as well since the matrices are of the same size each iteration\n    for (int i = 0; i < num_blocks; i++) {\n      // High-level intuition: We have B[i] = L[i] @ X. Since L is upper\n      // triangular this means B[i] = L[i, :i + 1] @ X[:i + 1]. We can split\n      // this into two parts: B[i] = L[i, :i] @ X[:i] + L[i, i] @ X[i] which\n      // can be solved for X[i] as X[i] = inv(L[i, i]) @ B[i] - L[i, :i] @ X[:i]\n\n      // Decide whether we go from first block to last or vice versa\n      bool backward = left_side ^ lower ^ transpose_a;\n      auto j = backward ? num_blocks - 1 - i : i;\n\n      // Get the size of the inverse blocks (the last one might be smaller)\n      int64_t block = (n % block_size != 0 && j + 1 == num_blocks)\n                          ? n % block_size\n                          : block_size;\n      auto inv_block =\n          MaybeConjugate(Collapse(SliceInMinorDims(inv_diag_blocks, {j, 0, 0},\n                                                   {j + 1, block, block}),\n                                  /*dimensions=*/{ndims - 2, ndims - 1}),\n                         conjugate_a);\n\n      // Get the corresponding row of B\n      int64_t k = std::min((j + 1) * block_size, n);\n      std::vector<int64_t> start = {j * block_size, 0};\n      std::vector<int64_t> end = {k, m};\n      if (!left_side) {\n        std::swap(start[0], start[1]);\n        std::swap(end[0], end[1]);\n      }\n      auto b_row = SliceInMinorDims(b, start, end);\n\n      XlaOp remainder;\n      if (i == 0) {\n        remainder = b_row;\n      } else {\n        // This matrix multiply get rid of a lot of multiplying with zero\n        // (namely, X[i * block_size:] = 0), L[i, :i] @ X[:i]\n        if (backward) {\n          start = {j * block_size,\n                   std::max(int64_t{0}, (num_blocks - i) * block_size)};\n          end = {k, n};\n        } else {\n          start = {j * block_size, 0};\n          end = {k, std::min(i * block_size, n)};\n        }\n\n        if (!left_side ^ transpose_a) {\n          std::swap(start[0], start[1]);\n          std::swap(end[0], end[1]);\n        }\n        auto a_row =\n            MaybeConjugate(SliceInMinorDims(a, start, end), conjugate_a);\n        if (left_side) {\n          remainder = b_row - BatchDot(a_row, transpose_a, x, false, precision);\n        } else {\n          remainder = b_row - BatchDot(x, false, a_row, transpose_a, precision);\n        }\n      }\n\n      XlaOp x_update;\n      if (left_side) {\n        x_update =\n            BatchDot(inv_block, transpose_a, remainder, false, precision);\n      } else {\n        x_update =\n            BatchDot(remainder, false, inv_block, transpose_a, precision);\n      }\n\n      if (i == 0) {\n        x = x_update;\n      } else {\n        if (backward) {\n          x = ConcatInDim(builder, {x_update, x}, block_dim);\n        } else {\n          x = ConcatInDim(builder, {x, x_update}, block_dim);\n        }\n      }\n    }\n\n    return x;\n  })",
          "old_line_content": "",
          "new_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 136,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(blocks_shape, -1)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(blocks_shape, -1)",
          "old_line_content": "                                      bool transpose_a, bool conjugate_a,",
          "new_line_content": "    int64_t block_size = ShapeUtil::GetDimension(blocks_shape, -1);",
          "content_same": false
        },
        {
          "line": 140,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape blocks_shape, builder->GetShape(inv_diag_blocks));",
          "new_line_content": "    int64_t n = ShapeUtil::GetDimension(a_shape, -1);",
          "content_same": false
        },
        {
          "line": 143,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(b_shape, m_dim)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(b_shape, m_dim)",
          "old_line_content": "",
          "new_line_content": "    int64_t m = ShapeUtil::GetDimension(b_shape, m_dim);",
          "content_same": false
        },
        {
          "line": 169,
          "old_api": null,
          "new_api": "SliceInMinorDims",
          "old_text": null,
          "new_text": "SliceInMinorDims(inv_diag_blocks, {j, 0, 0},\n                                                   {j + 1, block, block})",
          "old_line_content": "",
          "new_line_content": "          MaybeConjugate(Collapse(SliceInMinorDims(inv_diag_blocks, {j, 0, 0},",
          "content_same": false
        },
        {
          "line": 179,
          "old_api": null,
          "new_api": "std::swap(start[0], start[1])",
          "old_text": null,
          "new_text": "std::swap(start[0], start[1])",
          "old_line_content": "",
          "new_line_content": "        std::swap(start[0], start[1]);",
          "content_same": false
        },
        {
          "line": 180,
          "old_api": null,
          "new_api": "std::swap(end[0], end[1])",
          "old_text": null,
          "new_text": "std::swap(end[0], end[1])",
          "old_line_content": "      // Get the corresponding row of B",
          "new_line_content": "        std::swap(end[0], end[1]);",
          "content_same": false
        },
        {
          "line": 182,
          "old_api": null,
          "new_api": "SliceInMinorDims",
          "old_text": null,
          "new_text": "SliceInMinorDims(b, start, end)",
          "old_line_content": "      std::vector<int64_t> start = {j * block_size, 0};",
          "new_line_content": "      auto b_row = SliceInMinorDims(b, start, end);",
          "content_same": false
        },
        {
          "line": 192,
          "old_api": null,
          "new_api": "std::max(int64_t{0}, (num_blocks - i) * block_size)",
          "old_text": null,
          "new_text": "std::max(int64_t{0}, (num_blocks - i) * block_size)",
          "old_line_content": "        remainder = b_row;",
          "new_line_content": "                   std::max(int64_t{0}, (num_blocks - i) * block_size)};",
          "content_same": false
        },
        {
          "line": 196,
          "old_api": null,
          "new_api": "std::min(i * block_size, n)",
          "old_text": null,
          "new_text": "std::min(i * block_size, n)",
          "old_line_content": "        if (backward) {",
          "new_line_content": "          end = {k, std::min(i * block_size, n)};",
          "content_same": false
        },
        {
          "line": 200,
          "old_api": null,
          "new_api": "std::swap(start[0], start[1])",
          "old_text": null,
          "new_text": "std::swap(start[0], start[1])",
          "old_line_content": "        } else {",
          "new_line_content": "          std::swap(start[0], start[1]);",
          "content_same": false
        },
        {
          "line": 201,
          "old_api": null,
          "new_api": "std::swap(end[0], end[1])",
          "old_text": null,
          "new_text": "std::swap(end[0], end[1])",
          "old_line_content": "          start = {j * block_size, 0};",
          "new_line_content": "          std::swap(end[0], end[1]);",
          "content_same": false
        },
        {
          "line": 204,
          "old_api": null,
          "new_api": "SliceInMinorDims",
          "old_text": null,
          "new_text": "SliceInMinorDims(a, start, end)",
          "old_line_content": "",
          "new_line_content": "            MaybeConjugate(SliceInMinorDims(a, start, end), conjugate_a);",
          "content_same": false
        },
        {
          "line": 208,
          "old_api": null,
          "new_api": "BatchDot",
          "old_text": null,
          "new_text": "BatchDot(x, false, a_row, transpose_a, precision)",
          "old_line_content": "        }",
          "new_line_content": "          remainder = b_row - BatchDot(x, false, a_row, transpose_a, precision);",
          "content_same": false
        },
        {
          "line": 215,
          "old_api": null,
          "new_api": "BatchDot",
          "old_text": null,
          "new_text": "BatchDot(inv_block, transpose_a, remainder, false, precision)",
          "old_line_content": "        }",
          "new_line_content": "            BatchDot(inv_block, transpose_a, remainder, false, precision);",
          "content_same": false
        },
        {
          "line": 218,
          "old_api": null,
          "new_api": "BatchDot",
          "old_text": null,
          "new_text": "BatchDot(remainder, false, inv_block, transpose_a, precision)",
          "old_line_content": "      XlaOp x_update;",
          "new_line_content": "            BatchDot(remainder, false, inv_block, transpose_a, precision);",
          "content_same": false
        },
        {
          "line": 225,
          "old_api": null,
          "new_api": "ConcatInDim",
          "old_text": null,
          "new_text": "ConcatInDim(builder, {x_update, x}, block_dim)",
          "old_line_content": "      }",
          "new_line_content": "          x = ConcatInDim(builder, {x_update, x}, block_dim);",
          "content_same": false
        },
        {
          "line": 227,
          "old_api": null,
          "new_api": "ConcatInDim",
          "old_text": null,
          "new_text": "ConcatInDim(builder, {x, x_update}, block_dim)",
          "old_line_content": "      if (i == 0) {",
          "new_line_content": "          x = ConcatInDim(builder, {x, x_update}, block_dim);",
          "content_same": false
        },
        {
          "line": 241,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "diag_blocks.builder()",
          "old_line_content": "",
          "new_line_content": "  XlaBuilder* builder = diag_blocks.builder();",
          "content_same": false
        },
        {
          "line": 242,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    // Input is a batch of square lower triangular square matrices. Its shape is\n    // (..., size, size). We resize this to (num_blocks, size, size).\n    TF_ASSIGN_OR_RETURN(Shape shape, builder->GetShape(diag_blocks));\n    int64_t block_size = ShapeUtil::GetDimension(shape, -1);\n    int64_t num_blocks = ShapeUtil::ElementsIn(shape) / IPow(block_size, 2);\n    diag_blocks = Reshape(diag_blocks, {num_blocks, block_size, block_size});\n\n    // The input must be triangular because we rely on that when doing\n    // multiplications later on\n    diag_blocks = Triangle(diag_blocks, /*lower=*/lower_triangular);\n\n    // Rescale blocks to be unit triangular, but avoid dividing by\n    // zero (which can happen if the last block was padded) otherwise it will\n    // introduce nans which will propagate\n    auto diags = GetMatrixDiagonal(diag_blocks);\n    auto ones = FullLike(diags, 1);\n    diags = Select(Eq(diags, Zero(builder, shape.element_type())), ones, diags);\n    auto scaled_diag_blocks = Div(diag_blocks, diags, {0, 2});\n\n    // We can now use the fact that for an upper triangular matrix\n    // [[L11, 0], [L21, L22]], given the inverses L11' and L22', we have\n    // L22' = -L22' * L21 * L11'. In our case, L21 is a vector and our blocks\n    // have been rescaled to be unit triangular, so L22 = L22' = 1.\n\n    // Initialize the output matrix with -1s on the diagonal. We use -1 instead\n    // of 1 because we cannot do matrix-vector multiplies with variable shapes\n    // inside of a loop, or do irregularly shaped in-place updates. Hence,\n    // L21 <- -L22 * L21 * L11 cannot be done naively. Instead, we update the\n    // entire row i.e. we calculate\n    // [L21 L22 0] <- -[L21 L22 0] @ diag_blocks([L11', -I, -I])\n    // which means [L21 L22 0] <- [-L21 * L11', L22, 0].\n    auto identity =\n        IdentityMatrix(builder, shape.element_type(), block_size, block_size);\n    auto neg_identity = -identity;\n\n    // The first or last  diagonal element should be set to 1 instead of -1\n    // though, since we never update it\n    auto pos_one = Reshape(One(builder, shape.element_type()), {1, 1});\n    auto start_index =\n        ConstantR0<int>(builder, lower_triangular ? 0 : block_size - 1);\n    auto output_block =\n        DynamicUpdateSlice(neg_identity, pos_one,\n                           /*start_indices=*/{start_index, start_index});\n\n    // Broadcast diag([1, -1, -1, ...]) to every block\n    XlaOp output = Broadcast(output_block,\n                             /*broadcast_sizes=*/{num_blocks});\n\n    // Now we construct a loop that performs matrix-vector multiplications\n    // inverting the blocks one row at a time\n    std::vector<Shape> tuple_shapes = {\n        // The loop iteration counter is a scalar, incremented each iteration.\n        ShapeUtil::MakeShape(S32, {}),\n        // The output has the shape of A, with one row updated each iteration.\n        ShapeUtil::MakeShape(shape.element_type(),\n                             {num_blocks, block_size, block_size}),\n        // The input is a loop invariant.\n        ShapeUtil::MakeShape(shape.element_type(),\n                             {num_blocks, block_size, block_size})};\n    Shape tuple_shape = ShapeUtil::MakeTupleShape(tuple_shapes);\n\n    auto init_i = One(builder, S32);\n    auto init = Tuple(builder, {init_i, output, scaled_diag_blocks});\n\n    // Construct the loop condition function.\n    std::unique_ptr<XlaBuilder> condb =\n        builder->CreateSubBuilder(\"InvertDiagCond\");\n    {\n      auto i = GetTupleElement(\n          Parameter(condb.get(), 0, tuple_shape, \"InvertDiagCondTuple\"), 0);\n      Lt(i, ConstantR0<int32_t>(condb.get(), block_size));\n    }\n    TF_ASSIGN_OR_RETURN(auto cond, condb->Build());\n\n    // Construct the loop body function.\n    std::unique_ptr<XlaBuilder> bodyb =\n        builder->CreateSubBuilder(\"InvertDiagBody\");\n    {\n      auto input_tuple =\n          Parameter(bodyb.get(), 0, tuple_shape, \"InvertDiagBodyTuple\");\n\n      auto i = GetTupleElement(input_tuple, 0);\n      auto body_out = GetTupleElement(input_tuple, 1);\n      auto body_input = GetTupleElement(input_tuple, 2);\n\n      auto zero = ConstantR0<int32_t>(bodyb.get(), 0);\n      auto j = lower_triangular ? i : ScalarLike(i, block_size - 1) - i;\n      auto input_row =\n          DynamicSlice(body_input, {zero, j, zero},\n                       /*slice_sizes=*/{num_blocks, 1, block_size});\n\n      // We want -L21 L11^{-1}\n      DotDimensionNumbers dnums;\n      dnums.add_lhs_batch_dimensions(0);\n      dnums.add_rhs_batch_dimensions(0);\n      dnums.add_lhs_contracting_dimensions(2);\n      dnums.add_rhs_contracting_dimensions(1);\n      PrecisionConfig precision_proto;\n      precision_proto.add_operand_precision(precision);\n      precision_proto.add_operand_precision(precision);\n      auto update = -DotGeneral(input_row, body_out, dnums, &precision_proto);\n\n      body_out = DynamicUpdateSlice(body_out, update, {zero, j, zero});\n\n      auto next_i = i + ScalarLike(i, 1);\n      Tuple(bodyb.get(), {next_i, body_out, body_input});\n    }\n    TF_ASSIGN_OR_RETURN(auto body, bodyb->Build());\n\n    // Construct the While loop and return the result,\n    // return while_loop(cond_fun, body_fun, init)[1]\n    auto invert_while = While(cond, body, init);\n    auto inv_diag_blocks = GetTupleElement(invert_while, 1);\n    // Undo the scaling\n    inv_diag_blocks = Div(inv_diag_blocks, diags,\n                          /*broadcast_dimensions=*/{0, 1});\n\n    // Reshape back to original batch major dimensions\n    return Reshape(inv_diag_blocks, shape.dimensions());\n  })",
          "old_line_content": "}  // namespace",
          "new_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 246,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(shape, -1)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(shape, -1)",
          "old_line_content": "    PrecisionConfig::Precision precision) {",
          "new_line_content": "    int64_t block_size = ShapeUtil::GetDimension(shape, -1);",
          "content_same": false
        },
        {
          "line": 257,
          "old_api": null,
          "new_api": "GetMatrixDiagonal",
          "old_text": null,
          "new_text": "GetMatrixDiagonal(diag_blocks)",
          "old_line_content": "    // multiplications later on",
          "new_line_content": "    auto diags = GetMatrixDiagonal(diag_blocks);",
          "content_same": false
        },
        {
          "line": 259,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape.element_type()",
          "old_line_content": "",
          "new_line_content": "    diags = Select(Eq(diags, Zero(builder, shape.element_type())), ones, diags);",
          "content_same": false
        },
        {
          "line": 260,
          "old_api": null,
          "new_api": "Div",
          "old_text": null,
          "new_text": "Div(diag_blocks, diags, {0, 2})",
          "old_line_content": "    // Rescale blocks to be unit triangular, but avoid dividing by",
          "new_line_content": "    auto scaled_diag_blocks = Div(diag_blocks, diags, {0, 2});",
          "content_same": false
        },
        {
          "line": 275,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape.element_type()",
          "old_line_content": "    // inside of a loop, or do irregularly shaped in-place updates. Hence,",
          "new_line_content": "        IdentityMatrix(builder, shape.element_type(), block_size, block_size);",
          "content_same": false
        },
        {
          "line": 280,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape.element_type()",
          "old_line_content": "    auto identity =",
          "new_line_content": "    auto pos_one = Reshape(One(builder, shape.element_type()), {1, 1});",
          "content_same": false
        },
        {
          "line": 282,
          "old_api": null,
          "new_api": "ConstantR0<int>(builder, lower_triangular ? 0 : block_size - 1)",
          "old_text": null,
          "new_text": "ConstantR0<int>(builder, lower_triangular ? 0 : block_size - 1)",
          "old_line_content": "    auto neg_identity = -identity;",
          "new_line_content": "        ConstantR0<int>(builder, lower_triangular ? 0 : block_size - 1);",
          "content_same": false
        },
        {
          "line": 284,
          "old_api": null,
          "new_api": "DynamicUpdateSlice",
          "old_text": null,
          "new_text": "DynamicUpdateSlice(neg_identity, pos_one,\n                           /*start_indices=*/{start_index, start_index})",
          "old_line_content": "    // The first or last  diagonal element should be set to 1 instead of -1",
          "new_line_content": "        DynamicUpdateSlice(neg_identity, pos_one,",
          "content_same": false
        },
        {
          "line": 295,
          "old_api": null,
          "new_api": "ShapeUtil::MakeShape(S32, {})",
          "old_text": null,
          "new_text": "ShapeUtil::MakeShape(S32, {})",
          "old_line_content": "                             /*broadcast_sizes=*/{num_blocks});",
          "new_line_content": "        ShapeUtil::MakeShape(S32, {}),",
          "content_same": false
        },
        {
          "line": 297,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape.element_type()",
          "old_line_content": "    // Now we construct a loop that performs matrix-vector multiplications",
          "new_line_content": "        ShapeUtil::MakeShape(shape.element_type(),",
          "content_same": false
        },
        {
          "line": 300,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "shape.element_type()",
          "old_line_content": "        // The loop iteration counter is a scalar, incremented each iteration.",
          "new_line_content": "        ShapeUtil::MakeShape(shape.element_type(),",
          "content_same": false
        },
        {
          "line": 302,
          "old_api": null,
          "new_api": "ShapeUtil::MakeTupleShape(tuple_shapes)",
          "old_text": null,
          "new_text": "ShapeUtil::MakeTupleShape(tuple_shapes)",
          "old_line_content": "        // The output has the shape of A, with one row updated each iteration.",
          "new_line_content": "    Shape tuple_shape = ShapeUtil::MakeTupleShape(tuple_shapes);",
          "content_same": false
        },
        {
          "line": 304,
          "old_api": null,
          "new_api": "One",
          "old_text": null,
          "new_text": "One(builder, S32)",
          "old_line_content": "                             {num_blocks, block_size, block_size}),",
          "new_line_content": "    auto init_i = One(builder, S32);",
          "content_same": false
        },
        {
          "line": 305,
          "old_api": null,
          "new_api": "Tuple",
          "old_text": null,
          "new_text": "Tuple(builder, {init_i, output, scaled_diag_blocks})",
          "old_line_content": "        // The input is a loop invariant.",
          "new_line_content": "    auto init = Tuple(builder, {init_i, output, scaled_diag_blocks});",
          "content_same": false
        },
        {
          "line": 309,
          "old_api": null,
          "new_api": "CreateSubBuilder",
          "old_text": null,
          "new_text": "builder->CreateSubBuilder(\"InvertDiagCond\")",
          "old_line_content": "",
          "new_line_content": "        builder->CreateSubBuilder(\"InvertDiagCond\");",
          "content_same": false
        },
        {
          "line": 312,
          "old_api": null,
          "new_api": "get",
          "old_text": null,
          "new_text": "condb.get()",
          "old_line_content": "",
          "new_line_content": "          Parameter(condb.get(), 0, tuple_shape, \"InvertDiagCondTuple\"), 0);",
          "content_same": false
        },
        {
          "line": 313,
          "old_api": null,
          "new_api": "get",
          "old_text": null,
          "new_text": "condb.get()",
          "old_line_content": "    // Construct the loop condition function.",
          "new_line_content": "      Lt(i, ConstantR0<int32_t>(condb.get(), block_size));",
          "content_same": false
        },
        {
          "line": 322,
          "old_api": null,
          "new_api": "get",
          "old_text": null,
          "new_text": "bodyb.get()",
          "old_line_content": "",
          "new_line_content": "          Parameter(bodyb.get(), 0, tuple_shape, \"InvertDiagBodyTuple\");",
          "content_same": false
        },
        {
          "line": 324,
          "old_api": null,
          "new_api": "GetTupleElement",
          "old_text": null,
          "new_text": "GetTupleElement(input_tuple, 0)",
          "old_line_content": "    std::unique_ptr<XlaBuilder> bodyb =",
          "new_line_content": "      auto i = GetTupleElement(input_tuple, 0);",
          "content_same": false
        },
        {
          "line": 326,
          "old_api": null,
          "new_api": "GetTupleElement",
          "old_text": null,
          "new_text": "GetTupleElement(input_tuple, 2)",
          "old_line_content": "    {",
          "new_line_content": "      auto body_input = GetTupleElement(input_tuple, 2);",
          "content_same": false
        },
        {
          "line": 329,
          "old_api": null,
          "new_api": "ScalarLike",
          "old_text": null,
          "new_text": "ScalarLike(i, block_size - 1)",
          "old_line_content": "",
          "new_line_content": "      auto j = lower_triangular ? i : ScalarLike(i, block_size - 1) - i;",
          "content_same": false
        },
        {
          "line": 336,
          "old_api": null,
          "new_api": "add_lhs_batch_dimensions",
          "old_text": null,
          "new_text": "dnums.add_lhs_batch_dimensions(0)",
          "old_line_content": "      auto input_row =",
          "new_line_content": "      dnums.add_lhs_batch_dimensions(0);",
          "content_same": false
        },
        {
          "line": 338,
          "old_api": null,
          "new_api": "add_lhs_contracting_dimensions",
          "old_text": null,
          "new_text": "dnums.add_lhs_contracting_dimensions(2)",
          "old_line_content": "                       /*slice_sizes=*/{num_blocks, 1, block_size});",
          "new_line_content": "      dnums.add_lhs_contracting_dimensions(2);",
          "content_same": false
        },
        {
          "line": 339,
          "old_api": null,
          "new_api": "add_rhs_contracting_dimensions",
          "old_text": null,
          "new_text": "dnums.add_rhs_contracting_dimensions(1)",
          "old_line_content": "",
          "new_line_content": "      dnums.add_rhs_contracting_dimensions(1);",
          "content_same": false
        },
        {
          "line": 341,
          "old_api": null,
          "new_api": "add_operand_precision",
          "old_text": null,
          "new_text": "precision_proto.add_operand_precision(precision)",
          "old_line_content": "      DotDimensionNumbers dnums;",
          "new_line_content": "      precision_proto.add_operand_precision(precision);",
          "content_same": false
        },
        {
          "line": 355,
          "old_api": null,
          "new_api": "GetTupleElement",
          "old_text": null,
          "new_text": "GetTupleElement(invert_while, 1)",
          "old_line_content": "    }",
          "new_line_content": "    auto inv_diag_blocks = GetTupleElement(invert_while, 1);",
          "content_same": false
        },
        {
          "line": 357,
          "old_api": null,
          "new_api": "Div",
          "old_text": null,
          "new_text": "Div(inv_diag_blocks, diags,\n                          /*broadcast_dimensions=*/{0, 1})",
          "old_line_content": "",
          "new_line_content": "    inv_diag_blocks = Div(inv_diag_blocks, diags,",
          "content_same": false
        },
        {
          "line": 369,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "a.builder()",
          "old_line_content": "}",
          "new_line_content": "  XlaBuilder* builder = a.builder();",
          "content_same": false
        },
        {
          "line": 370,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));\n    const int64_t ndims = a_shape.rank();\n    int64_t k = ShapeUtil::GetDimension(a_shape, -1);\n\n    // TODO(phawkins): consider pushing triangle masking into\n    // InvertDiagonalBlocks.\n    if (unit_diagonal) {\n      // Mask everything but the subdiagonal/superdiagonal elements.\n      a = lower ? Select(TriangleMask(a, -1), a, ZerosLike(a))\n                : Select(TriangleMask(a, 0), ZerosLike(a), a);\n      a = xla::Add(a, IdentityMatrix(builder, a_shape.element_type(), k, k),\n                   /*broadcast_dimensions=*/{ndims - 2, ndims - 1});\n    } else {\n      // Mask off the ignored elements of the triangular matrix a.\n      a = Triangle(a, lower);\n    }\n\n    // We find the diagonal blocks of the coefficient matrix\n    int64_t block_size = std::min(block_size_, k);\n    auto diag_blocks = DiagonalBlocks(a, block_size);\n\n    // We invert these blocks in parallel using batched matrix-vector products\n    auto inv_diag_blocks = InvertDiagonalBlocks(diag_blocks, lower, precision);\n\n    // We now find the solution using GEMMs\n    return SolveWithInvertedDiagonalBlocks(a, b, inv_diag_blocks, left_side,\n                                           lower, transpose_a, conjugate_a,\n                                           precision);\n  })",
          "old_line_content": "",
          "new_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 372,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "a_shape.rank()",
          "old_line_content": "    XlaOp a, XlaOp b, bool left_side, bool lower, bool transpose_a,",
          "new_line_content": "    const int64_t ndims = a_shape.rank();",
          "content_same": false
        },
        {
          "line": 373,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_line_content": "    bool conjugate_a, bool unit_diagonal,",
          "new_line_content": "    int64_t k = ShapeUtil::GetDimension(a_shape, -1);",
          "content_same": false
        },
        {
          "line": 380,
          "old_api": null,
          "new_api": "ZerosLike",
          "old_text": null,
          "new_text": "ZerosLike(a)",
          "old_line_content": "",
          "new_line_content": "                : Select(TriangleMask(a, 0), ZerosLike(a), a);",
          "content_same": false
        },
        {
          "line": 381,
          "old_api": null,
          "new_api": "element_type",
          "old_text": null,
          "new_text": "a_shape.element_type()",
          "old_line_content": "    // TODO(phawkins): consider pushing triangle masking into",
          "new_line_content": "      a = xla::Add(a, IdentityMatrix(builder, a_shape.element_type(), k, k),",
          "content_same": false
        },
        {
          "line": 389,
          "old_api": null,
          "new_api": "std::min(block_size_, k)",
          "old_text": null,
          "new_text": "std::min(block_size_, k)",
          "old_line_content": "    } else {",
          "new_line_content": "    int64_t block_size = std::min(block_size_, k);",
          "content_same": false
        },
        {
          "line": 390,
          "old_api": null,
          "new_api": "DiagonalBlocks",
          "old_text": null,
          "new_text": "DiagonalBlocks(a, block_size)",
          "old_line_content": "      // Mask off the ignored elements of the triangular matrix a.",
          "new_line_content": "    auto diag_blocks = DiagonalBlocks(a, block_size);",
          "content_same": false
        },
        {
          "line": 393,
          "old_api": null,
          "new_api": "InvertDiagonalBlocks",
          "old_text": null,
          "new_text": "InvertDiagonalBlocks(diag_blocks, lower, precision)",
          "old_line_content": "",
          "new_line_content": "    auto inv_diag_blocks = InvertDiagonalBlocks(diag_blocks, lower, precision);",
          "content_same": false
        },
        {
          "line": 413,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "a.builder()",
          "old_line_content": "//     b[j, :] = (b[j, :] - np.dot(a[j, :j], b[:j, :])) / a[j, j]",
          "new_line_content": "  XlaBuilder* builder = a.builder();",
          "content_same": false
        },
        {
          "line": 414,
          "old_api": null,
          "new_api": "ReportErrorOrReturn",
          "old_text": null,
          "new_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));\n    TF_ASSIGN_OR_RETURN(Shape b_shape, builder->GetShape(b));\n    int64_t m = ShapeUtil::GetDimension(b_shape, -2);\n    int64_t n = ShapeUtil::GetDimension(b_shape, -1);\n    const int64_t a_size = ShapeUtil::GetDimension(a_shape, -1);\n    a = MaybeConjugate(a, conjugate_a);\n    bool backwards = transpose_a ^ lower ^ !left_side;\n    for (int64_t i = 0; i < a_size; ++i) {\n      int64_t j = backwards ? i : (a_size - i - 1);\n      std::vector<int64_t> b_row_start, b_row_end;\n      if (left_side) {\n        b_row_start = {j, 0};\n        b_row_end = {j + 1, n};\n      } else {\n        b_row_start = {0, j};\n        b_row_end = {m, j + 1};\n      }\n      auto b_row = SliceInMinorDims(b, b_row_start, b_row_end);\n\n      std::vector<int64_t> a_start = {j, backwards ? 0 : (j + 1)};\n      std::vector<int64_t> a_end = {j + 1, backwards ? j : a_size};\n      if (transpose_a ^ !left_side) {\n        std::swap(a_start[0], a_start[1]);\n        std::swap(a_end[0], a_end[1]);\n      }\n      auto a_chunk = SliceInMinorDims(a, a_start, a_end);\n      if (left_side) {\n        bool which = transpose_a ^ lower;\n        auto b_chunk =\n            SliceInMinorDims(b, {which ? 0 : (j + 1), 0}, {which ? j : m, n});\n        b_row = b_row - BatchDot(a_chunk, /*transpose_x=*/transpose_a, b_chunk,\n                                 /*transpose_y=*/false, precision);\n      } else {\n        bool which = transpose_a ^ !lower;\n        auto b_chunk =\n            SliceInMinorDims(b, {0, which ? 0 : (j + 1)}, {m, which ? j : n});\n        b_row = b_row - BatchDot(b_chunk, /*transpose_x=*/false, a_chunk,\n                                 /*transpose_y=*/transpose_a, precision);\n      }\n      if (!unit_diagonal) {\n        auto a_diag = SliceInMinorDims(a, {j, j}, {j + 1, j + 1});\n        b_row = b_row / a_diag;\n      }\n\n      b = UpdateSliceInMinorDims(b, b_row, b_row_start);\n    }\n\n    return b;\n  })",
          "old_line_content": "//   return b",
          "new_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 417,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(b_shape, -2)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(b_shape, -2)",
          "old_line_content": "    bool conjugate_a, bool unit_diagonal,",
          "new_line_content": "    int64_t m = ShapeUtil::GetDimension(b_shape, -2);",
          "content_same": false
        },
        {
          "line": 418,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(b_shape, -1)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(b_shape, -1)",
          "old_line_content": "    PrecisionConfig::Precision precision) {",
          "new_line_content": "    int64_t n = ShapeUtil::GetDimension(b_shape, -1);",
          "content_same": false
        },
        {
          "line": 432,
          "old_api": null,
          "new_api": "SliceInMinorDims",
          "old_text": null,
          "new_text": "SliceInMinorDims(b, b_row_start, b_row_end)",
          "old_line_content": "        b_row_start = {j, 0};",
          "new_line_content": "      auto b_row = SliceInMinorDims(b, b_row_start, b_row_end);",
          "content_same": false
        },
        {
          "line": 437,
          "old_api": null,
          "new_api": "std::swap(a_start[0], a_start[1])",
          "old_text": null,
          "new_text": "std::swap(a_start[0], a_start[1])",
          "old_line_content": "      }",
          "new_line_content": "        std::swap(a_start[0], a_start[1]);",
          "content_same": false
        },
        {
          "line": 440,
          "old_api": null,
          "new_api": "SliceInMinorDims",
          "old_text": null,
          "new_text": "SliceInMinorDims(a, a_start, a_end)",
          "old_line_content": "      std::vector<int64_t> a_start = {j, backwards ? 0 : (j + 1)};",
          "new_line_content": "      auto a_chunk = SliceInMinorDims(a, a_start, a_end);",
          "content_same": false
        },
        {
          "line": 445,
          "old_api": null,
          "new_api": "BatchDot",
          "old_text": null,
          "new_text": "BatchDot(a_chunk, /*transpose_x=*/transpose_a, b_chunk,\n                                 /*transpose_y=*/false, precision)",
          "old_line_content": "      }",
          "new_line_content": "        b_row = b_row - BatchDot(a_chunk, /*transpose_x=*/transpose_a, b_chunk,",
          "content_same": false
        },
        {
          "line": 455,
          "old_api": null,
          "new_api": "SliceInMinorDims",
          "old_text": null,
          "new_text": "SliceInMinorDims(a, {j, j}, {j + 1, j + 1})",
          "old_line_content": "        auto b_chunk =",
          "new_line_content": "        auto a_diag = SliceInMinorDims(a, {j, j}, {j + 1, j + 1});",
          "content_same": false
        },
        {
          "line": 459,
          "old_api": null,
          "new_api": "UpdateSliceInMinorDims",
          "old_text": null,
          "new_text": "UpdateSliceInMinorDims(b, b_row, b_row_start)",
          "old_line_content": "      }",
          "new_line_content": "      b = UpdateSliceInMinorDims(b, b_row, b_row_start);",
          "content_same": false
        },
        {
          "line": 470,
          "old_api": null,
          "new_api": "builder",
          "old_text": null,
          "new_text": "a.builder()",
          "old_line_content": "}",
          "new_line_content": "  XlaBuilder* builder = a.builder();",
          "content_same": false
        },
        {
          "line": 471,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));\n    TF_ASSIGN_OR_RETURN(Shape b_shape, builder->GetShape(b));\n    if (a_shape.rank() != b_shape.rank()) {\n      return InvalidArgument(\n          \"Arguments to TriangularSolve have shapes with different ranks: \"\n          \"%s vs. %s\",\n          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));\n    }\n    const int64_t ndims = a_shape.rank();\n    if (ndims < 2) {\n      return InvalidArgument(\n          \"Arguments to TriangularSolve was rank %d but must have rank >= 2.\",\n          ndims);\n    }\n    // The batch dimensions must be equal.\n    std::vector<int64_t> batch_dimensions;\n    int64_t batch = 1;\n    for (int i = 0; i < ndims - 2; ++i) {\n      int64_t a_size = a_shape.dimensions(i);\n      int64_t b_size = b_shape.dimensions(i);\n      if (a_size != b_size) {\n        return InvalidArgument(\n            \"Batch dimensions of arguments to TriangularSolve must be equal; \"\n            \"shapes were %s and %s.\",\n            ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));\n      }\n      batch_dimensions.push_back(a_size);\n      batch *= a_size;\n    }\n\n    if (ShapeUtil::GetDimension(a_shape, -1) !=\n        ShapeUtil::GetDimension(a_shape, -2)) {\n      return InvalidArgument(\n          \"The 'a' argument to TriangularSolve must be a batched square matrix;\"\n          \" shape was: %s\",\n          ShapeUtil::HumanString(a_shape));\n    }\n    const int64_t m = ShapeUtil::GetDimension(b_shape, -2);\n    const int64_t n = ShapeUtil::GetDimension(b_shape, -1);\n    if ((left_side ? m : n) != ShapeUtil::GetDimension(a_shape, -1)) {\n      return InvalidArgument(\n          \"Arguments to TriangularSolve have incompatible matrix shapes %s and \"\n          \"%s\",\n          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));\n    }\n\n    int64_t a_size = ShapeUtil::GetDimension(a_shape, -1);\n\n    if (ShapeUtil::IsZeroElementArray(b_shape)) {\n      // The output has the same shape as 'b', and since the output has zero\n      // elements, any such array will do.\n      return b;\n    }\n\n    // Degenerate case: 1x1 matrices.\n    if (a_size == 1) {\n      return unit_diagonal ? b : Div(b, MaybeConjugate(a, conjugate_a));\n    }\n\n    // Prefer the direct implementation whenever there is a nontrivial batch\n    // dimension and the matrix is very small.\n    if (UseDirectSolves() && batch > block_size_ / 16 &&\n        a_size < block_size_ / 4) {\n      return SolveDirectly(a, b, left_side, lower, transpose_a, conjugate_a,\n                           unit_diagonal, precision);\n    } else {\n      return SolveByInvertingDiagonalBlocks(a, b, left_side, lower, transpose_a,\n                                            conjugate_a, unit_diagonal,\n                                            precision);\n    }\n  })",
          "old_line_content": "",
          "new_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "content_same": false
        },
        {
          "line": 474,
          "old_api": null,
          "new_api": "rank",
          "old_text": null,
          "new_text": "b_shape.rank()",
          "old_line_content": "    bool conjugate_a, bool unit_diagonal, int64_t block_size,",
          "new_line_content": "    if (a_shape.rank() != b_shape.rank()) {",
          "content_same": false
        },
        {
          "line": 475,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Arguments to TriangularSolve have shapes with different ranks: \"\n          \"%s vs. %s\",\n          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape))",
          "old_line_content": "    PrecisionConfig::Precision precision) {",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 478,
          "old_api": null,
          "new_api": "ShapeUtil::HumanString(b_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanString(b_shape)",
          "old_line_content": "    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));",
          "new_line_content": "          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));",
          "content_same": false
        },
        {
          "line": 482,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n          \"Arguments to TriangularSolve was rank %d but must have rank >= 2.\",\n          ndims)",
          "old_line_content": "          \"Arguments to TriangularSolve have shapes with different ranks: \"",
          "new_line_content": "      return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 490,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "a_shape.dimensions(i)",
          "old_line_content": "          ndims);",
          "new_line_content": "      int64_t a_size = a_shape.dimensions(i);",
          "content_same": false
        },
        {
          "line": 491,
          "old_api": null,
          "new_api": "dimensions",
          "old_text": null,
          "new_text": "b_shape.dimensions(i)",
          "old_line_content": "    }",
          "new_line_content": "      int64_t b_size = b_shape.dimensions(i);",
          "content_same": false
        },
        {
          "line": 493,
          "old_api": null,
          "new_api": "InvalidArgument",
          "old_text": null,
          "new_text": "InvalidArgument(\n            \"Batch dimensions of arguments to TriangularSolve must be equal; \"\n            \"shapes were %s and %s.\",\n            ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape))",
          "old_line_content": "    std::vector<int64_t> batch_dimensions;",
          "new_line_content": "        return InvalidArgument(",
          "content_same": false
        },
        {
          "line": 498,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "batch_dimensions.push_back(a_size)",
          "old_line_content": "      if (a_size != b_size) {",
          "new_line_content": "      batch_dimensions.push_back(a_size);",
          "content_same": false
        },
        {
          "line": 503,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(a_shape, -2)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(a_shape, -2)",
          "old_line_content": "      }",
          "new_line_content": "        ShapeUtil::GetDimension(a_shape, -2)) {",
          "content_same": false
        },
        {
          "line": 507,
          "old_api": null,
          "new_api": "ShapeUtil::HumanString(a_shape)",
          "old_text": null,
          "new_text": "ShapeUtil::HumanString(a_shape)",
          "old_line_content": "",
          "new_line_content": "          ShapeUtil::HumanString(a_shape));",
          "content_same": false
        },
        {
          "line": 511,
          "old_api": null,
          "new_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_text": null,
          "new_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "old_line_content": "          \"The 'a' argument to TriangularSolve must be a batched square matrix;\"",
          "new_line_content": "    if ((left_side ? m : n) != ShapeUtil::GetDimension(a_shape, -1)) {",
          "content_same": false
        }
      ],
      "deletions": [
        {
          "line": 513,
          "old_api": "ShapeUtil::HumanString(a_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanString(a_shape)",
          "new_text": null,
          "old_line_content": "          ShapeUtil::HumanString(a_shape));",
          "new_line_content": "          \"Arguments to TriangularSolve have incompatible matrix shapes %s and \"",
          "content_same": false
        },
        {
          "line": 516,
          "old_api": "ShapeUtil::GetDimension(b_shape, -1)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(b_shape, -1)",
          "new_text": null,
          "old_line_content": "    const int64_t n = ShapeUtil::GetDimension(b_shape, -1);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 517,
          "old_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_text": null,
          "old_line_content": "    if ((left_side ? m : n) != ShapeUtil::GetDimension(a_shape, -1)) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 521,
          "old_api": "ShapeUtil::HumanString(b_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanString(b_shape)",
          "new_text": null,
          "old_line_content": "          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));",
          "new_line_content": "      // The output has the same shape as 'b', and since the output has zero",
          "content_same": false
        },
        {
          "line": 524,
          "old_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_text": null,
          "old_line_content": "    int64_t a_size = ShapeUtil::GetDimension(a_shape, -1);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 526,
          "old_api": "ShapeUtil::IsZeroElementArray(b_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::IsZeroElementArray(b_shape)",
          "new_text": null,
          "old_line_content": "    if (ShapeUtil::IsZeroElementArray(b_shape)) {",
          "new_line_content": "    // Degenerate case: 1x1 matrices.",
          "content_same": false
        },
        {
          "line": 534,
          "old_api": "MaybeConjugate",
          "new_api": null,
          "old_text": "MaybeConjugate(a, conjugate_a)",
          "new_text": null,
          "old_line_content": "      return unit_diagonal ? b : Div(b, MaybeConjugate(a, conjugate_a));",
          "new_line_content": "        a_size < block_size_ / 4) {",
          "content_same": false
        },
        {
          "line": 539,
          "old_api": "UseDirectSolves",
          "new_api": null,
          "old_text": "UseDirectSolves()",
          "new_text": null,
          "old_line_content": "    if (UseDirectSolves() && batch > block_size_ / 16 &&",
          "new_line_content": "                                            conjugate_a, unit_diagonal,",
          "content_same": false
        },
        {
          "line": 541,
          "old_api": "SolveDirectly",
          "new_api": null,
          "old_text": "SolveDirectly(a, b, left_side, lower, transpose_a, conjugate_a,\n                           unit_diagonal, precision)",
          "new_text": null,
          "old_line_content": "      return SolveDirectly(a, b, left_side, lower, transpose_a, conjugate_a,",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 544,
          "old_api": "SolveByInvertingDiagonalBlocks",
          "new_api": null,
          "old_text": "SolveByInvertingDiagonalBlocks(a, b, left_side, lower, transpose_a,\n                                            conjugate_a, unit_diagonal,\n                                            precision)",
          "new_text": null,
          "old_line_content": "      return SolveByInvertingDiagonalBlocks(a, b, left_side, lower, transpose_a,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 553,
          "old_api": "CHECK_GE",
          "new_api": null,
          "old_text": "CHECK_GE(block_size_, 1)",
          "new_text": null,
          "old_line_content": "  CHECK_GE(block_size_, 1);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 567,
          "old_api": "ToString",
          "new_api": null,
          "old_text": "instruction->operand(0)->shape().ToString()",
          "new_text": null,
          "old_line_content": "      instruction->operand(0)->shape().ToString(),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 569,
          "old_api": "left_side",
          "new_api": null,
          "old_text": "options.left_side()",
          "new_text": null,
          "old_line_content": "      options.left_side() ? \"left\" : \"right\",",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 570,
          "old_api": "lower",
          "new_api": null,
          "old_text": "options.lower()",
          "new_text": null,
          "old_line_content": "      options.lower() ? \"lower\" : \"upper\",",
          "new_line_content": "  HloComputation*& computation =",
          "content_same": false
        },
        {
          "line": 572,
          "old_api": "unit_diagonal",
          "new_api": null,
          "old_text": "options.unit_diagonal()",
          "new_text": null,
          "old_line_content": "      options.unit_diagonal() ? \"unit\" : \"nonunit\");",
          "new_line_content": "  if (!computation) {",
          "content_same": false
        },
        {
          "line": 574,
          "old_api": "parent",
          "new_api": null,
          "old_text": "instruction->parent()->parent()",
          "new_text": null,
          "old_line_content": "  HloModule* module = instruction->parent()->parent();",
          "new_line_content": "    //",
          "content_same": false
        },
        {
          "line": 577,
          "old_api": "emplace",
          "new_api": null,
          "old_text": "computation_cache_.emplace(name, nullptr)",
          "new_text": null,
          "old_line_content": "      computation_cache_.emplace(name, nullptr).first->second;",
          "new_line_content": "    // the external APIs for building complicated computations (XlaBuilder)",
          "content_same": false
        },
        {
          "line": 71,
          "old_api": "MakeEdgePaddingConfig",
          "new_api": null,
          "old_text": "MakeEdgePaddingConfig({{0, 0}, {ndims - 2, 0}})",
          "new_text": null,
          "old_line_content": "          MakeEdgePaddingConfig({{0, 0}, {ndims - 2, 0}});",
          "new_line_content": "      std::vector<int64_t> slice_sizes(ndims);",
          "content_same": false
        },
        {
          "line": 73,
          "old_api": "ConstantR0<int32_t>(builder, 0)",
          "new_api": null,
          "old_text": "ConstantR0<int32_t>(builder, 0)",
          "new_text": null,
          "old_line_content": "          Pad(start_indices, ConstantR0<int32_t>(builder, 0), padding_config);",
          "new_line_content": "      for (int i = 0; i < ndims - 2; ++i) {",
          "content_same": false
        },
        {
          "line": 593,
          "old_api": "transpose_a",
          "new_api": null,
          "old_text": "tions.transpose_a() !",
          "new_text": null,
          "old_line_content": "        options.transpose_a() != TriangularSolveOptions::NO_TRANSPOSE;",
          "new_line_content": "                         /*precision=*/PrecisionConfig::HIGHEST);",
          "content_same": false
        },
        {
          "line": 594,
          "old_api": "transpose_a",
          "new_api": null,
          "old_text": "tions.transpose_a() =",
          "new_text": null,
          "old_line_content": "    bool conjugate_a = options.transpose_a() == TriangularSolveOptions::ADJOINT;",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(XlaComputation xla_computation, builder.Build());",
          "content_same": false
        },
        {
          "line": 84,
          "old_api": "add_offset_dims",
          "new_api": null,
          "old_text": "dim_numbers.add_offset_dims(ndims - 1)",
          "new_text": null,
          "old_line_content": "      dim_numbers.add_offset_dims(ndims - 1);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 85,
          "old_api": "add_offset_dims",
          "new_api": null,
          "old_text": "dim_numbers.add_offset_dims(ndims)",
          "new_text": null,
          "old_line_content": "      dim_numbers.add_offset_dims(ndims);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 86,
          "old_api": "add_start_index_map",
          "new_api": null,
          "old_text": "dim_numbers.add_start_index_map(ndims - 2)",
          "new_text": null,
          "old_line_content": "      dim_numbers.add_start_index_map(ndims - 2);",
          "new_line_content": "    // The last block might be smaller than the block size,",
          "content_same": false
        },
        {
          "line": 87,
          "old_api": "add_start_index_map",
          "new_api": null,
          "old_text": "dim_numbers.add_start_index_map(ndims - 1)",
          "new_text": null,
          "old_line_content": "      dim_numbers.add_start_index_map(ndims - 1);",
          "new_line_content": "    // so we will need to pad it",
          "content_same": false
        },
        {
          "line": 88,
          "old_api": "set_index_vector_dim",
          "new_api": null,
          "old_text": "dim_numbers.set_index_vector_dim(1)",
          "new_text": null,
          "old_line_content": "      dim_numbers.set_index_vector_dim(1);",
          "new_line_content": "    if (n % block_size != 0) {",
          "content_same": false
        },
        {
          "line": 89,
          "old_api": "Gather",
          "new_api": null,
          "old_text": "Gather(a, start_indices, dim_numbers, slice_sizes)",
          "new_text": null,
          "old_line_content": "      diag_blocks = Gather(a, start_indices, dim_numbers, slice_sizes);",
          "new_line_content": "      // Pad with identity matrix.",
          "content_same": false
        },
        {
          "line": 596,
          "old_api": "lower",
          "new_api": null,
          "old_text": "tions.lower(),\n",
          "new_text": null,
          "old_line_content": "    BuildTriangularSolve(a, b, options.left_side(), options.lower(),",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(ProgramShape program_shape,",
          "content_same": false
        },
        {
          "line": 597,
          "old_api": "unit_diagonal",
          "new_api": null,
          "old_text": "tions.unit_diagonal(),\n",
          "new_text": null,
          "old_line_content": "                         transpose_a, conjugate_a, options.unit_diagonal(),",
          "new_line_content": "                        xla_computation.GetProgramShape());",
          "content_same": false
        },
        {
          "line": 97,
          "old_api": "SliceInMinorDims",
          "new_api": null,
          "old_text": "SliceInMinorDims(a, {n - n % block_size, n - n % block_size}, {n, n})",
          "new_text": null,
          "old_line_content": "          SliceInMinorDims(a, {n - n % block_size, n - n % block_size}, {n, n});",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 98,
          "old_api": "MakeNoPaddingConfig",
          "new_api": null,
          "old_text": "MakeNoPaddingConfig(ndims)",
          "new_text": null,
          "old_line_content": "      PaddingConfig config = MakeNoPaddingConfig(ndims);",
          "new_line_content": "      auto eye =",
          "content_same": false
        },
        {
          "line": 609,
          "old_api": "entry_computation",
          "new_api": null,
          "old_text": "w_module->entry_computation(), ",
          "new_text": null,
          "old_line_content": "        module->DeepCloneComputation(new_module->entry_computation(), &context);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 612,
          "old_api": "shape",
          "new_api": null,
          "old_text": "oInstruction::CreateCall(\n      instruction->shape(), instruction->operands(), computation));",
          "new_text": null,
          "old_line_content": "  return instruction->parent()->AddInstruction(HloInstruction::CreateCall(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 613,
          "old_api": "operands",
          "new_api": null,
          "old_text": "struction->operands(), ",
          "new_text": null,
          "old_line_content": "      instruction->shape(), instruction->operands(), computation));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 105,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape.element_type()",
          "new_text": null,
          "old_line_content": "          IdentityMatrix(builder, shape.element_type(), padding, padding);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 106,
          "old_api": "MakeNoPaddingConfig",
          "new_api": null,
          "old_text": "MakeNoPaddingConfig(2)",
          "new_text": null,
          "old_line_content": "      config = MakeNoPaddingConfig(2);",
          "new_line_content": "      // Add a singleton dimension",
          "content_same": false
        },
        {
          "line": 107,
          "old_api": "mutable_dimensions",
          "new_api": null,
          "old_text": "config.mutable_dimensions(0)->set_edge_padding_low(n % block_size)",
          "new_text": null,
          "old_line_content": "      config.mutable_dimensions(0)->set_edge_padding_low(n % block_size);",
          "new_line_content": "      // i.e. [..., block_size, block_size] -> [..., 1, block_size, block_size]",
          "content_same": false
        },
        {
          "line": 108,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape.element_type()",
          "new_text": null,
          "old_line_content": "      eye = Pad(eye, Zero(builder, shape.element_type()), config);",
          "new_line_content": "      TF_ASSIGN_OR_RETURN(Shape blocks_shape, builder->GetShape(last_blocks));",
          "content_same": false
        },
        {
          "line": 115,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "blocks_shape.dimensions()",
          "new_text": null,
          "old_line_content": "      auto shape_dims = blocks_shape.dimensions();",
          "new_line_content": "      // Concatenate with the other blocks if necessary",
          "content_same": false
        },
        {
          "line": 116,
          "old_api": "std::vector<int64_t>(ndims)",
          "new_api": null,
          "old_text": "std::vector<int64_t>(ndims)",
          "new_text": null,
          "old_line_content": "      auto last_blocks_dims = std::vector<int64_t>(ndims);",
          "new_line_content": "      if (n > block_size) {",
          "content_same": false
        },
        {
          "line": 117,
          "old_api": "begin",
          "new_api": null,
          "old_text": "last_blocks_dims.begin()",
          "new_text": null,
          "old_line_content": "      std::copy(shape_dims.begin(), shape_dims.end(), last_blocks_dims.begin());",
          "new_line_content": "        diag_blocks =",
          "content_same": false
        },
        {
          "line": 119,
          "old_api": "Reshape",
          "new_api": null,
          "old_text": "Reshape(last_blocks, last_blocks_dims)",
          "new_text": null,
          "old_line_content": "      last_blocks = Reshape(last_blocks, last_blocks_dims);",
          "new_line_content": "      } else {",
          "content_same": false
        },
        {
          "line": 124,
          "old_api": "ConcatInDim",
          "new_api": null,
          "old_text": "ConcatInDim(builder, {diag_blocks, last_blocks}, ndims - 2)",
          "new_text": null,
          "old_line_content": "            ConcatInDim(builder, {diag_blocks, last_blocks}, ndims - 2);",
          "new_line_content": "    return diag_blocks;",
          "content_same": false
        },
        {
          "line": 138,
          "old_api": "builder",
          "new_api": null,
          "old_text": "a.builder()",
          "new_text": null,
          "old_line_content": "  XlaBuilder* builder = a.builder();",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));",
          "content_same": false
        },
        {
          "line": 142,
          "old_api": "ShapeUtil::GetDimension(blocks_shape, -1)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(blocks_shape, -1)",
          "new_text": null,
          "old_line_content": "    int64_t block_size = ShapeUtil::GetDimension(blocks_shape, -1);",
          "new_line_content": "    int64_t m_dim = (left_side) ? -1 : -2;",
          "content_same": false
        },
        {
          "line": 145,
          "old_api": "rank",
          "new_api": null,
          "old_text": "a_shape.rank()",
          "new_text": null,
          "old_line_content": "    int64_t ndims = a_shape.rank();",
          "new_line_content": "    std::vector<XlaOp> update_ops;",
          "content_same": false
        },
        {
          "line": 149,
          "old_api": "ShapeUtil::GetDimension(b_shape, m_dim)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(b_shape, m_dim)",
          "new_text": null,
          "old_line_content": "    int64_t m = ShapeUtil::GetDimension(b_shape, m_dim);",
          "new_line_content": "    // Initialize the solution",
          "content_same": false
        },
        {
          "line": 152,
          "old_api": "rank",
          "new_api": null,
          "old_text": "b_shape.rank()",
          "new_text": null,
          "old_line_content": "    int bdims = b_shape.rank();",
          "new_line_content": "    // This loop is unrolled for performance reasons, but it could be expressed",
          "content_same": false
        },
        {
          "line": 181,
          "old_api": "std::min((j + 1) * block_size, n)",
          "new_api": null,
          "old_text": "std::min((j + 1) * block_size, n)",
          "new_text": null,
          "old_line_content": "      int64_t k = std::min((j + 1) * block_size, n);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 185,
          "old_api": "std::swap(start[0], start[1])",
          "new_api": null,
          "old_text": "std::swap(start[0], start[1])",
          "new_text": null,
          "old_line_content": "        std::swap(start[0], start[1]);",
          "new_line_content": "      if (i == 0) {",
          "content_same": false
        },
        {
          "line": 186,
          "old_api": "std::swap(end[0], end[1])",
          "new_api": null,
          "old_text": "std::swap(end[0], end[1])",
          "new_text": null,
          "old_line_content": "        std::swap(end[0], end[1]);",
          "new_line_content": "        remainder = b_row;",
          "content_same": false
        },
        {
          "line": 188,
          "old_api": "SliceInMinorDims",
          "new_api": null,
          "old_text": "SliceInMinorDims(b, start, end)",
          "new_text": null,
          "old_line_content": "      auto b_row = SliceInMinorDims(b, start, end);",
          "new_line_content": "        // This matrix multiply get rid of a lot of multiplying with zero",
          "content_same": false
        },
        {
          "line": 198,
          "old_api": "std::max(int64_t{0}, (num_blocks - i) * block_size)",
          "new_api": null,
          "old_text": "std::max(int64_t{0}, (num_blocks - i) * block_size)",
          "new_text": null,
          "old_line_content": "                   std::max(int64_t{0}, (num_blocks - i) * block_size)};",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 202,
          "old_api": "std::min(i * block_size, n)",
          "new_api": null,
          "old_text": "std::min(i * block_size, n)",
          "new_text": null,
          "old_line_content": "          end = {k, std::min(i * block_size, n)};",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 207,
          "old_api": "std::swap(end[0], end[1])",
          "new_api": null,
          "old_text": "std::swap(end[0], end[1])",
          "new_text": null,
          "old_line_content": "          std::swap(end[0], end[1]);",
          "new_line_content": "        } else {",
          "content_same": false
        },
        {
          "line": 210,
          "old_api": "SliceInMinorDims",
          "new_api": null,
          "old_text": "SliceInMinorDims(a, start, end)",
          "new_text": null,
          "old_line_content": "            MaybeConjugate(SliceInMinorDims(a, start, end), conjugate_a);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 212,
          "old_api": "BatchDot",
          "new_api": null,
          "old_text": "BatchDot(a_row, transpose_a, x, false, precision)",
          "new_text": null,
          "old_line_content": "          remainder = b_row - BatchDot(a_row, transpose_a, x, false, precision);",
          "new_line_content": "      XlaOp x_update;",
          "content_same": false
        },
        {
          "line": 214,
          "old_api": "BatchDot",
          "new_api": null,
          "old_text": "BatchDot(x, false, a_row, transpose_a, precision)",
          "new_text": null,
          "old_line_content": "          remainder = b_row - BatchDot(x, false, a_row, transpose_a, precision);",
          "new_line_content": "        x_update =",
          "content_same": false
        },
        {
          "line": 221,
          "old_api": "BatchDot",
          "new_api": null,
          "old_text": "BatchDot(inv_block, transpose_a, remainder, false, precision)",
          "new_text": null,
          "old_line_content": "            BatchDot(inv_block, transpose_a, remainder, false, precision);",
          "new_line_content": "      if (i == 0) {",
          "content_same": false
        },
        {
          "line": 224,
          "old_api": "BatchDot",
          "new_api": null,
          "old_text": "BatchDot(remainder, false, inv_block, transpose_a, precision)",
          "new_text": null,
          "old_line_content": "            BatchDot(remainder, false, inv_block, transpose_a, precision);",
          "new_line_content": "        if (backward) {",
          "content_same": false
        },
        {
          "line": 231,
          "old_api": "ConcatInDim",
          "new_api": null,
          "old_text": "ConcatInDim(builder, {x_update, x}, block_dim)",
          "new_text": null,
          "old_line_content": "          x = ConcatInDim(builder, {x_update, x}, block_dim);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 233,
          "old_api": "ConcatInDim",
          "new_api": null,
          "old_text": "ConcatInDim(builder, {x, x_update}, block_dim)",
          "new_text": null,
          "old_line_content": "          x = ConcatInDim(builder, {x, x_update}, block_dim);",
          "new_line_content": "  });",
          "content_same": false
        },
        {
          "line": 253,
          "old_api": "IPow",
          "new_api": null,
          "old_text": "IPow(block_size, 2)",
          "new_text": null,
          "old_line_content": "    int64_t num_blocks = ShapeUtil::ElementsIn(shape) / IPow(block_size, 2);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 254,
          "old_api": "Reshape",
          "new_api": null,
          "old_text": "Reshape(diag_blocks, {num_blocks, block_size, block_size})",
          "new_text": null,
          "old_line_content": "    diag_blocks = Reshape(diag_blocks, {num_blocks, block_size, block_size});",
          "new_line_content": "    // Rescale blocks to be unit triangular, but avoid dividing by",
          "content_same": false
        },
        {
          "line": 263,
          "old_api": "GetMatrixDiagonal",
          "new_api": null,
          "old_text": "GetMatrixDiagonal(diag_blocks)",
          "new_text": null,
          "old_line_content": "    auto diags = GetMatrixDiagonal(diag_blocks);",
          "new_line_content": "    // [[L11, 0], [L21, L22]], given the inverses L11' and L22', we have",
          "content_same": false
        },
        {
          "line": 264,
          "old_api": "FullLike",
          "new_api": null,
          "old_text": "FullLike(diags, 1)",
          "new_text": null,
          "old_line_content": "    auto ones = FullLike(diags, 1);",
          "new_line_content": "    // L22' = -L22' * L21 * L11'. In our case, L21 is a vector and our blocks",
          "content_same": false
        },
        {
          "line": 265,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape.element_type()",
          "new_text": null,
          "old_line_content": "    diags = Select(Eq(diags, Zero(builder, shape.element_type())), ones, diags);",
          "new_line_content": "    // have been rescaled to be unit triangular, so L22 = L22' = 1.",
          "content_same": false
        },
        {
          "line": 266,
          "old_api": "Div",
          "new_api": null,
          "old_text": "Div(diag_blocks, diags, {0, 2})",
          "new_text": null,
          "old_line_content": "    auto scaled_diag_blocks = Div(diag_blocks, diags, {0, 2});",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 281,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape.element_type()",
          "new_text": null,
          "old_line_content": "        IdentityMatrix(builder, shape.element_type(), block_size, block_size);",
          "new_line_content": "    auto start_index =",
          "content_same": false
        },
        {
          "line": 286,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape.element_type()",
          "new_text": null,
          "old_line_content": "    auto pos_one = Reshape(One(builder, shape.element_type()), {1, 1});",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 290,
          "old_api": "DynamicUpdateSlice",
          "new_api": null,
          "old_text": "DynamicUpdateSlice(neg_identity, pos_one,\n                           /*start_indices=*/{start_index, start_index})",
          "new_text": null,
          "old_line_content": "        DynamicUpdateSlice(neg_identity, pos_one,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 294,
          "old_api": "Broadcast",
          "new_api": null,
          "old_text": "Broadcast(output_block,\n                             /*broadcast_sizes=*/{num_blocks})",
          "new_text": null,
          "old_line_content": "    XlaOp output = Broadcast(output_block,",
          "new_line_content": "        // The loop iteration counter is a scalar, incremented each iteration.",
          "content_same": false
        },
        {
          "line": 301,
          "old_api": "ShapeUtil::MakeShape(S32, {})",
          "new_api": null,
          "old_text": "ShapeUtil::MakeShape(S32, {})",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeShape(S32, {}),",
          "new_line_content": "                             {num_blocks, block_size, block_size})};",
          "content_same": false
        },
        {
          "line": 303,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape.element_type()",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeShape(shape.element_type(),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 306,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "shape.element_type()",
          "new_text": null,
          "old_line_content": "        ShapeUtil::MakeShape(shape.element_type(),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 308,
          "old_api": "ShapeUtil::MakeTupleShape(tuple_shapes)",
          "new_api": null,
          "old_text": "ShapeUtil::MakeTupleShape(tuple_shapes)",
          "new_text": null,
          "old_line_content": "    Shape tuple_shape = ShapeUtil::MakeTupleShape(tuple_shapes);",
          "new_line_content": "    std::unique_ptr<XlaBuilder> condb =",
          "content_same": false
        },
        {
          "line": 310,
          "old_api": "One",
          "new_api": null,
          "old_text": "One(builder, S32)",
          "new_text": null,
          "old_line_content": "    auto init_i = One(builder, S32);",
          "new_line_content": "    {",
          "content_same": false
        },
        {
          "line": 315,
          "old_api": "CreateSubBuilder",
          "new_api": null,
          "old_text": "builder->CreateSubBuilder(\"InvertDiagCond\")",
          "new_text": null,
          "old_line_content": "        builder->CreateSubBuilder(\"InvertDiagCond\");",
          "new_line_content": "    TF_ASSIGN_OR_RETURN(auto cond, condb->Build());",
          "content_same": false
        },
        {
          "line": 317,
          "old_api": "get",
          "new_api": null,
          "old_text": "GetTupleElement(\n          Parameter(condb.get(), 0, tuple_shape, \"InvertDiagCondTuple\"), 0)",
          "new_text": null,
          "old_line_content": "      auto i = GetTupleElement(",
          "new_line_content": "    // Construct the loop body function.",
          "content_same": false
        },
        {
          "line": 318,
          "old_api": "get",
          "new_api": null,
          "old_text": "condb.get()",
          "new_text": null,
          "old_line_content": "          Parameter(condb.get(), 0, tuple_shape, \"InvertDiagCondTuple\"), 0);",
          "new_line_content": "    std::unique_ptr<XlaBuilder> bodyb =",
          "content_same": false
        },
        {
          "line": 330,
          "old_api": "GetTupleElement",
          "new_api": null,
          "old_text": "GetTupleElement(input_tuple, 0)",
          "new_text": null,
          "old_line_content": "      auto i = GetTupleElement(input_tuple, 0);",
          "new_line_content": "      auto input_row =",
          "content_same": false
        },
        {
          "line": 332,
          "old_api": "GetTupleElement",
          "new_api": null,
          "old_text": "GetTupleElement(input_tuple, 2)",
          "new_text": null,
          "old_line_content": "      auto body_input = GetTupleElement(input_tuple, 2);",
          "new_line_content": "                       /*slice_sizes=*/{num_blocks, 1, block_size});",
          "content_same": false
        },
        {
          "line": 334,
          "old_api": "get",
          "new_api": null,
          "old_text": "bodyb.get()",
          "new_text": null,
          "old_line_content": "      auto zero = ConstantR0<int32_t>(bodyb.get(), 0);",
          "new_line_content": "      // We want -L21 L11^{-1}",
          "content_same": false
        },
        {
          "line": 335,
          "old_api": "ScalarLike",
          "new_api": null,
          "old_text": "ScalarLike(i, block_size - 1)",
          "new_text": null,
          "old_line_content": "      auto j = lower_triangular ? i : ScalarLike(i, block_size - 1) - i;",
          "new_line_content": "      DotDimensionNumbers dnums;",
          "content_same": false
        },
        {
          "line": 344,
          "old_api": "add_lhs_contracting_dimensions",
          "new_api": null,
          "old_text": "dnums.add_lhs_contracting_dimensions(2)",
          "new_text": null,
          "old_line_content": "      dnums.add_lhs_contracting_dimensions(2);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 349,
          "old_api": "DotGeneral",
          "new_api": null,
          "old_text": "DotGeneral(input_row, body_out, dnums, &precision_proto)",
          "new_text": null,
          "old_line_content": "      auto update = -DotGeneral(input_row, body_out, dnums, &precision_proto);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 351,
          "old_api": "DynamicUpdateSlice",
          "new_api": null,
          "old_text": "DynamicUpdateSlice(body_out, update, {zero, j, zero})",
          "new_text": null,
          "old_line_content": "      body_out = DynamicUpdateSlice(body_out, update, {zero, j, zero});",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 353,
          "old_api": "ScalarLike",
          "new_api": null,
          "old_text": "ScalarLike(i, 1)",
          "new_text": null,
          "old_line_content": "      auto next_i = i + ScalarLike(i, 1);",
          "new_line_content": "    // return while_loop(cond_fun, body_fun, init)[1]",
          "content_same": false
        },
        {
          "line": 360,
          "old_api": "While",
          "new_api": null,
          "old_text": "While(cond, body, init)",
          "new_text": null,
          "old_line_content": "    auto invert_while = While(cond, body, init);",
          "new_line_content": "    // Reshape back to original batch major dimensions",
          "content_same": false
        },
        {
          "line": 363,
          "old_api": "Div",
          "new_api": null,
          "old_text": "Div(inv_diag_blocks, diags,\n                          /*broadcast_dimensions=*/{0, 1})",
          "new_text": null,
          "old_line_content": "    inv_diag_blocks = Div(inv_diag_blocks, diags,",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 367,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "shape.dimensions()",
          "new_text": null,
          "old_line_content": "    return Reshape(inv_diag_blocks, shape.dimensions());",
          "new_line_content": "    bool conjugate_a, bool unit_diagonal,",
          "content_same": false
        },
        {
          "line": 375,
          "old_api": "builder",
          "new_api": null,
          "old_text": "a.builder()",
          "new_text": null,
          "old_line_content": "  XlaBuilder* builder = a.builder();",
          "new_line_content": "    // TODO(phawkins): consider pushing triangle masking into",
          "content_same": false
        },
        {
          "line": 376,
          "old_api": "rank",
          "new_api": null,
          "old_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));\n    const int64_t ndims = a_shape.rank();\n    int64_t k = ShapeUtil::GetDimension(a_shape, -1);\n\n    // TODO(phawkins): consider pushing triangle masking into\n    // InvertDiagonalBlocks.\n    if (unit_diagonal) {\n      // Mask everything but the subdiagonal/superdiagonal elements.\n      a = lower ? Select(TriangleMask(a, -1), a, ZerosLike(a))\n                : Select(TriangleMask(a, 0), ZerosLike(a), a);\n      a = xla::Add(a, IdentityMatrix(builder, a_shape.element_type(), k, k),\n                   /*broadcast_dimensions=*/{ndims - 2, ndims - 1});\n    } else {\n      // Mask off the ignored elements of the triangular matrix a.\n      a = Triangle(a, lower);\n    }\n\n    // We find the diagonal blocks of the coefficient matrix\n    int64_t block_size = std::min(block_size_, k);\n    auto diag_blocks = DiagonalBlocks(a, block_size);\n\n    // We invert these blocks in parallel using batched matrix-vector products\n    auto inv_diag_blocks = InvertDiagonalBlocks(diag_blocks, lower, precision);\n\n    // We now find the solution using GEMMs\n    return SolveWithInvertedDiagonalBlocks(a, b, inv_diag_blocks, left_side,\n                                           lower, transpose_a, conjugate_a,\n                                           precision);\n  })",
          "new_text": null,
          "old_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "    // InvertDiagonalBlocks.",
          "content_same": false
        },
        {
          "line": 378,
          "old_api": "rank",
          "new_api": null,
          "old_text": "a_shape.rank()",
          "new_text": null,
          "old_line_content": "    const int64_t ndims = a_shape.rank();",
          "new_line_content": "      // Mask everything but the subdiagonal/superdiagonal elements.",
          "content_same": false
        },
        {
          "line": 386,
          "old_api": "ZerosLike",
          "new_api": null,
          "old_text": "ZerosLike(a)",
          "new_text": null,
          "old_line_content": "                : Select(TriangleMask(a, 0), ZerosLike(a), a);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 387,
          "old_api": "element_type",
          "new_api": null,
          "old_text": "a_shape.element_type()",
          "new_text": null,
          "old_line_content": "      a = xla::Add(a, IdentityMatrix(builder, a_shape.element_type(), k, k),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 391,
          "old_api": "Triangle",
          "new_api": null,
          "old_text": "Triangle(a, lower)",
          "new_text": null,
          "old_line_content": "      a = Triangle(a, lower);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 395,
          "old_api": "std::min(block_size_, k)",
          "new_api": null,
          "old_text": "std::min(block_size_, k)",
          "new_text": null,
          "old_line_content": "    int64_t block_size = std::min(block_size_, k);",
          "new_line_content": "    // We now find the solution using GEMMs",
          "content_same": false
        },
        {
          "line": 399,
          "old_api": "InvertDiagonalBlocks",
          "new_api": null,
          "old_text": "InvertDiagonalBlocks(diag_blocks, lower, precision)",
          "new_text": null,
          "old_line_content": "    auto inv_diag_blocks = InvertDiagonalBlocks(diag_blocks, lower, precision);",
          "new_line_content": "  });",
          "content_same": false
        },
        {
          "line": 402,
          "old_api": "SolveWithInvertedDiagonalBlocks",
          "new_api": null,
          "old_text": "SolveWithInvertedDiagonalBlocks(a, b, inv_diag_blocks, left_side,\n                                           lower, transpose_a, conjugate_a,\n                                           precision)",
          "new_text": null,
          "old_line_content": "    return SolveWithInvertedDiagonalBlocks(a, b, inv_diag_blocks, left_side,",
          "new_line_content": "// def trsm_left_lower_leftlooking(a, b):",
          "content_same": false
        },
        {
          "line": 423,
          "old_api": "ShapeUtil::GetDimension(b_shape, -2)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(b_shape, -2)",
          "new_text": null,
          "old_line_content": "    int64_t m = ShapeUtil::GetDimension(b_shape, -2);",
          "new_line_content": "      int64_t j = backwards ? i : (a_size - i - 1);",
          "content_same": false
        },
        {
          "line": 424,
          "old_api": "ShapeUtil::GetDimension(b_shape, -1)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(b_shape, -1)",
          "new_text": null,
          "old_line_content": "    int64_t n = ShapeUtil::GetDimension(b_shape, -1);",
          "new_line_content": "      std::vector<int64_t> b_row_start, b_row_end;",
          "content_same": false
        },
        {
          "line": 425,
          "old_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_text": null,
          "old_line_content": "    const int64_t a_size = ShapeUtil::GetDimension(a_shape, -1);",
          "new_line_content": "      if (left_side) {",
          "content_same": false
        },
        {
          "line": 426,
          "old_api": "MaybeConjugate",
          "new_api": null,
          "old_text": "MaybeConjugate(a, conjugate_a)",
          "new_text": null,
          "old_line_content": "    a = MaybeConjugate(a, conjugate_a);",
          "new_line_content": "        b_row_start = {j, 0};",
          "content_same": false
        },
        {
          "line": 443,
          "old_api": "std::swap(a_start[0], a_start[1])",
          "new_api": null,
          "old_text": "std::swap(a_start[0], a_start[1])",
          "new_text": null,
          "old_line_content": "        std::swap(a_start[0], a_start[1]);",
          "new_line_content": "        auto b_chunk =",
          "content_same": false
        },
        {
          "line": 446,
          "old_api": "SliceInMinorDims",
          "new_api": null,
          "old_text": "SliceInMinorDims(a, a_start, a_end)",
          "new_text": null,
          "old_line_content": "      auto a_chunk = SliceInMinorDims(a, a_start, a_end);",
          "new_line_content": "                                 /*transpose_y=*/false, precision);",
          "content_same": false
        },
        {
          "line": 456,
          "old_api": "SliceInMinorDims",
          "new_api": null,
          "old_text": "SliceInMinorDims(b, {0, which ? 0 : (j + 1)}, {m, which ? j : n})",
          "new_text": null,
          "old_line_content": "            SliceInMinorDims(b, {0, which ? 0 : (j + 1)}, {m, which ? j : n});",
          "new_line_content": "        b_row = b_row / a_diag;",
          "content_same": false
        },
        {
          "line": 457,
          "old_api": "BatchDot",
          "new_api": null,
          "old_text": "BatchDot(b_chunk, /*transpose_x=*/false, a_chunk,\n                                 /*transpose_y=*/transpose_a, precision)",
          "new_text": null,
          "old_line_content": "        b_row = b_row - BatchDot(b_chunk, /*transpose_x=*/false, a_chunk,",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 461,
          "old_api": "SliceInMinorDims",
          "new_api": null,
          "old_text": "SliceInMinorDims(a, {j, j}, {j + 1, j + 1})",
          "new_text": null,
          "old_line_content": "        auto a_diag = SliceInMinorDims(a, {j, j}, {j + 1, j + 1});",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 465,
          "old_api": "UpdateSliceInMinorDims",
          "new_api": null,
          "old_text": "UpdateSliceInMinorDims(b, b_row, b_row_start)",
          "new_text": null,
          "old_line_content": "      b = UpdateSliceInMinorDims(b, b_row, b_row_start);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 476,
          "old_api": "builder",
          "new_api": null,
          "old_text": "a.builder()",
          "new_text": null,
          "old_line_content": "  XlaBuilder* builder = a.builder();",
          "new_line_content": "          \"Arguments to TriangularSolve have shapes with different ranks: \"",
          "content_same": false
        },
        {
          "line": 477,
          "old_api": "rank",
          "new_api": null,
          "old_text": "builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {\n    TF_ASSIGN_OR_RETURN(Shape a_shape, builder->GetShape(a));\n    TF_ASSIGN_OR_RETURN(Shape b_shape, builder->GetShape(b));\n    if (a_shape.rank() != b_shape.rank()) {\n      return InvalidArgument(\n          \"Arguments to TriangularSolve have shapes with different ranks: \"\n          \"%s vs. %s\",\n          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));\n    }\n    const int64_t ndims = a_shape.rank();\n    if (ndims < 2) {\n      return InvalidArgument(\n          \"Arguments to TriangularSolve was rank %d but must have rank >= 2.\",\n          ndims);\n    }\n    // The batch dimensions must be equal.\n    std::vector<int64_t> batch_dimensions;\n    int64_t batch = 1;\n    for (int i = 0; i < ndims - 2; ++i) {\n      int64_t a_size = a_shape.dimensions(i);\n      int64_t b_size = b_shape.dimensions(i);\n      if (a_size != b_size) {\n        return InvalidArgument(\n            \"Batch dimensions of arguments to TriangularSolve must be equal; \"\n            \"shapes were %s and %s.\",\n            ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));\n      }\n      batch_dimensions.push_back(a_size);\n      batch *= a_size;\n    }\n\n    if (ShapeUtil::GetDimension(a_shape, -1) !=\n        ShapeUtil::GetDimension(a_shape, -2)) {\n      return InvalidArgument(\n          \"The 'a' argument to TriangularSolve must be a batched square matrix;\"\n          \" shape was: %s\",\n          ShapeUtil::HumanString(a_shape));\n    }\n    const int64_t m = ShapeUtil::GetDimension(b_shape, -2);\n    const int64_t n = ShapeUtil::GetDimension(b_shape, -1);\n    if ((left_side ? m : n) != ShapeUtil::GetDimension(a_shape, -1)) {\n      return InvalidArgument(\n          \"Arguments to TriangularSolve have incompatible matrix shapes %s and \"\n          \"%s\",\n          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));\n    }\n\n    int64_t a_size = ShapeUtil::GetDimension(a_shape, -1);\n\n    if (ShapeUtil::IsZeroElementArray(b_shape)) {\n      // The output has the same shape as 'b', and since the output has zero\n      // elements, any such array will do.\n      return b;\n    }\n\n    // Degenerate case: 1x1 matrices.\n    if (a_size == 1) {\n      return unit_diagonal ? b : Div(b, MaybeConjugate(a, conjugate_a));\n    }\n\n    // Prefer the direct implementation whenever there is a nontrivial batch\n    // dimension and the matrix is very small.\n    if (UseDirectSolves() && batch > block_size_ / 16 &&\n        a_size < block_size_ / 4) {\n      return SolveDirectly(a, b, left_side, lower, transpose_a, conjugate_a,\n                           unit_diagonal, precision);\n    } else {\n      return SolveByInvertingDiagonalBlocks(a, b, left_side, lower, transpose_a,\n                                            conjugate_a, unit_diagonal,\n                                            precision);\n    }\n  })",
          "new_text": null,
          "old_line_content": "  return builder->ReportErrorOrReturn([&]() -> StatusOr<XlaOp> {",
          "new_line_content": "          \"%s vs. %s\",",
          "content_same": false
        },
        {
          "line": 481,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Arguments to TriangularSolve have shapes with different ranks: \"\n          \"%s vs. %s\",\n          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape))",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    if (ndims < 2) {",
          "content_same": false
        },
        {
          "line": 484,
          "old_api": "ShapeUtil::HumanString(b_shape)",
          "new_api": null,
          "old_text": "ShapeUtil::HumanString(b_shape)",
          "new_text": null,
          "old_line_content": "          ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape));",
          "new_line_content": "          ndims);",
          "content_same": false
        },
        {
          "line": 486,
          "old_api": "rank",
          "new_api": null,
          "old_text": "a_shape.rank()",
          "new_text": null,
          "old_line_content": "    const int64_t ndims = a_shape.rank();",
          "new_line_content": "    // The batch dimensions must be equal.",
          "content_same": false
        },
        {
          "line": 488,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n          \"Arguments to TriangularSolve was rank %d but must have rank >= 2.\",\n          ndims)",
          "new_text": null,
          "old_line_content": "      return InvalidArgument(",
          "new_line_content": "    int64_t batch = 1;",
          "content_same": false
        },
        {
          "line": 497,
          "old_api": "dimensions",
          "new_api": null,
          "old_text": "b_shape.dimensions(i)",
          "new_text": null,
          "old_line_content": "      int64_t b_size = b_shape.dimensions(i);",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 499,
          "old_api": "InvalidArgument",
          "new_api": null,
          "old_text": "InvalidArgument(\n            \"Batch dimensions of arguments to TriangularSolve must be equal; \"\n            \"shapes were %s and %s.\",\n            ShapeUtil::HumanString(a_shape), ShapeUtil::HumanString(b_shape))",
          "new_text": null,
          "old_line_content": "        return InvalidArgument(",
          "new_line_content": "      batch *= a_size;",
          "content_same": false
        },
        {
          "line": 508,
          "old_api": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_api": null,
          "old_text": "ShapeUtil::GetDimension(a_shape, -1)",
          "new_text": null,
          "old_line_content": "    if (ShapeUtil::GetDimension(a_shape, -1) !=",
          "new_line_content": "    }",
          "content_same": false
        }
      ]
    },
    "api_summary": {
      "total_replacements": 46,
      "total_additions": 116,
      "total_deletions": 119,
      "total_api_changes": 281
    },
    "non_api_changes": {
      "has_non_api_changes": true,
      "evidence": {
        "total_diff_lines": 6,
        "api_related_lines": 281,
        "non_api_lines": 3,
        "non_api_line_numbers": [
          72,
          80,
          70
        ]
      }
    },
    "api_calls_before": 252,
    "api_calls_after": 248,
    "diff_info": {
      "added_lines": 0,
      "removed_lines": 6,
      "total_diff_lines": 23
    }
  }
}