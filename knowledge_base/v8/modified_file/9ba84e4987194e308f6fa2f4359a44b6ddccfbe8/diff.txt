diff --git a/src/compiler/backend/register-allocator.cc b/src/compiler/backend/register-allocator.cc
index 785c75148e2..4469f670aa4 100644
--- a/src/compiler/backend/register-allocator.cc
+++ b/src/compiler/backend/register-allocator.cc
@@ -887,10 +887,13 @@ void TopLevelLiveRange::AddUsePosition(UsePosition* use_pos, Zone* zone,
                                        bool trace_alloc) {
   TRACE_COND(trace_alloc, "Add to live range %d use position %d\n", vreg(),
              use_pos->pos().value());
-  UsePositionVector::const_iterator insert_it = std::upper_bound(
-      positions_.begin(), positions_.end(), use_pos, UsePosition::Ordering());
-  // Since we `ProcessInstructions` in reverse, `positions_` are mostly
-  // inserted at the front, hence grow towards that direction exclusively.
+  // Since we `ProcessInstructions` in reverse, the `use_pos` is almost always
+  // inserted at the front of `positions_`, hence (i) use linear instead of
+  // binary search and (ii) grow towards the `kFront` exclusively on `insert`.
+  UsePositionVector::iterator insert_it = std::find_if(
+      positions_.begin(), positions_.end(), [=](const UsePosition* pos) {
+        return UsePosition::Ordering()(use_pos, pos);
+      });
   positions_.insert<kFront>(zone, insert_it, use_pos);
 
   positions_span_ = base::VectorOf(positions_);
