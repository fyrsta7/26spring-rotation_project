{
  "status": "completed",
  "commit_dir": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/carbon-lang/modified_file/0e2b6c7f1acd55ffb5350144f4319be4721169f8",
  "file_info": {
    "before_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/carbon-lang/modified_file/0e2b6c7f1acd55ffb5350144f4319be4721169f8/before.cpp",
    "after_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/carbon-lang/modified_file/0e2b6c7f1acd55ffb5350144f4319be4721169f8/after.cpp",
    "diff_file": "/home/zyw/llm_on_code/llm_on_code_optimization/knowledge_base/carbon-lang/modified_file/0e2b6c7f1acd55ffb5350144f4319be4721169f8/diff.txt"
  },
  "semgrep_analysis": {
    "is_api_change": false,
    "api_changes": {
      "replacements": [
        {
          "line": 308,
          "old_api": "front",
          "new_api": "size",
          "old_text": "source_text.front()",
          "new_text": "source_text.size()",
          "old_line_content": "    CARBON_DCHECK(source_text.front() == '\\n');",
          "new_line_content": "    ssize_t size = source_text.size();",
          "content_same": false
        },
        {
          "line": 320,
          "old_api": "size",
          "new_api": "drop_front",
          "old_text": "source_text.size()",
          "new_text": "source_text.drop_front()",
          "old_line_content": "    if (source_text.size() > 1 && source_text[1] == '/') {",
          "new_line_content": "    source_text = source_text.drop_front();",
          "content_same": false
        },
        {
          "line": 321,
          "old_api": "LexComment",
          "new_api": "HandleNewline",
          "old_text": "LexComment(source_text)",
          "new_text": "HandleNewline()",
          "old_line_content": "      LexComment(source_text);",
          "new_line_content": "    HandleNewline();",
          "content_same": false
        },
        {
          "line": 331,
          "old_api": "startswith",
          "new_api": "LexComment",
          "old_text": "source_text.startswith(\"//\")",
          "new_text": "LexComment(source_text)",
          "old_line_content": "    CARBON_DCHECK(source_text.startswith(\"//\"));",
          "new_line_content": "      LexComment(source_text);",
          "content_same": false
        },
        {
          "line": 345,
          "old_api": "begin",
          "new_api": "CARBON_DIAGNOSTIC",
          "old_text": "source_text.begin()",
          "new_text": "CARBON_DIAGNOSTIC(TrailingComment, Error,\n                        \"Trailing comments are not permitted.\")",
          "old_line_content": "      emitter_.Emit(source_text.begin() + 2,",
          "new_line_content": "      CARBON_DIAGNOSTIC(TrailingComment, Error,",
          "content_same": false
        },
        {
          "line": 353,
          "old_api": "empty",
          "new_api": "CARBON_DIAGNOSTIC",
          "old_text": "source_text.empty()",
          "new_text": "CARBON_DIAGNOSTIC(NoWhitespaceAfterCommentIntroducer, Error,\n                        \"Whitespace is required after '//'.\")",
          "old_line_content": "    if (source_text.empty()) {",
          "new_line_content": "      CARBON_DIAGNOSTIC(NoWhitespaceAfterCommentIntroducer, Error,",
          "content_same": false
        },
        {
          "line": 370,
          "old_api": "size",
          "new_api": "LexVerticalWhitespace",
          "old_text": "literal->text().size()",
          "new_text": "LexVerticalWhitespace(source_text)",
          "old_line_content": "    int token_size = literal->text().size();",
          "new_line_content": "    LexVerticalWhitespace(source_text);",
          "content_same": false
        },
        {
          "line": 380,
          "old_api": "ComputeValue",
          "new_api": "size",
          "old_text": "literal->ComputeValue(emitter_)",
          "new_text": "literal->text().size()",
          "old_line_content": "        literal->ComputeValue(emitter_),",
          "new_line_content": "    int token_size = literal->text().size();",
          "content_same": false
        },
        {
          "line": 382,
          "old_api": "AddToken",
          "new_api": "drop_front",
          "old_text": "buffer_->AddToken({.kind = TokenKind::IntegerLiteral,\n                                          .token_line = current_line_,\n                                          .column = int_column})",
          "new_text": "source_text.drop_front(token_size)",
          "old_line_content": "          auto token = buffer_->AddToken({.kind = TokenKind::IntegerLiteral,",
          "new_line_content": "    source_text = source_text.drop_front(token_size);",
          "content_same": false
        },
        {
          "line": 395,
          "old_api": "size",
          "new_api": "GetTokenInfo",
          "old_text": "buffer_->literal_int_storage_.size()",
          "new_text": "buffer_->GetTokenInfo(token)",
          "old_line_content": "              buffer_->literal_int_storage_.size();",
          "new_line_content": "          buffer_->GetTokenInfo(token).literal_index =",
          "content_same": false
        },
        {
          "line": 396,
          "old_api": "std::move(value.mantissa)",
          "new_api": "size",
          "old_text": "std::move(value.mantissa)",
          "new_text": "buffer_->literal_int_storage_.size()",
          "old_line_content": "          buffer_->literal_int_storage_.push_back(std::move(value.mantissa));",
          "new_line_content": "              buffer_->literal_int_storage_.size();",
          "content_same": false
        },
        {
          "line": 397,
          "old_api": "std::move(value.exponent)",
          "new_api": "std::move(value.value)",
          "old_text": "std::move(value.exponent)",
          "new_text": "std::move(value.value)",
          "old_line_content": "          buffer_->literal_int_storage_.push_back(std::move(value.exponent));",
          "new_line_content": "          buffer_->literal_int_storage_.push_back(std::move(value.value));",
          "content_same": false
        },
        {
          "line": 470,
          "old_api": "CARBON_DCHECK",
          "new_api": "AddToken",
          "old_text": "CARBON_DCHECK(kind != TokenKind::Error)",
          "new_text": "buffer_->AddToken({.kind = TokenKind::Error,\n                                .token_line = string_line,\n                                .column = string_column,\n                                .error_length = literal_size})",
          "old_line_content": "    CARBON_DCHECK(kind != TokenKind::Error);",
          "new_line_content": "      return buffer_->AddToken({.kind = TokenKind::Error,",
          "content_same": false
        },
        {
          "line": 482,
          "old_api": "AddToken",
          "new_api": "fixed_spelling",
          "old_text": "buffer_->AddToken(\n        {.kind = kind, .token_line = current_line_, .column = current_column_})",
          "new_text": "kind.fixed_spelling().front()",
          "old_line_content": "    Token token = buffer_->AddToken(",
          "new_line_content": "    CARBON_DCHECK(source_text.front() == kind.fixed_spelling().front())",
          "content_same": false
        },
        {
          "line": 492,
          "old_api": "push_back",
          "new_api": "AddToken",
          "old_text": "open_groups_.push_back(token)",
          "new_text": "buffer_->AddToken(\n        {.kind = kind, .token_line = current_line_, .column = current_column_})",
          "old_line_content": "    open_groups_.push_back(token);",
          "new_line_content": "    Token token = buffer_->AddToken(",
          "content_same": false
        },
        {
          "line": 502,
          "old_api": "begin",
          "new_api": "push_back",
          "old_text": "source_text.begin()",
          "new_text": "open_groups_.push_back(token)",
          "old_line_content": "      emitter_.Emit(source_text.begin(), UnmatchedClosing);",
          "new_line_content": "    open_groups_.push_back(token);",
          "content_same": false
        },
        {
          "line": 513,
          "old_api": "empty",
          "new_api": "AddToken",
          "old_text": "open_groups_.empty()",
          "new_text": "buffer_->AddToken({.kind = TokenKind::Error,\n                                       .token_line = current_line_,\n                                       .column = current_column_,\n                                       .error_length = 1})",
          "old_line_content": "    if (LLVM_UNLIKELY(open_groups_.empty())) {",
          "new_line_content": "      Token token = buffer_->AddToken({.kind = TokenKind::Error,",
          "content_same": false
        },
        {
          "line": 527,
          "old_api": "GetTokenInfo",
          "new_api": "back",
          "old_text": "buffer_->GetTokenInfo(opening_token)",
          "new_text": "open_groups_.back()",
          "old_line_content": "      CARBON_DCHECK(buffer_->GetTokenInfo(opening_token).kind ==",
          "new_line_content": "    Token opening_token = open_groups_.back();",
          "content_same": false
        },
        {
          "line": 530,
          "old_api": "pop_back",
          "new_api": "opening_symbol",
          "old_text": "open_groups_.pop_back()",
          "new_text": "kind.opening_symbol()",
          "old_line_content": "    open_groups_.pop_back();",
          "new_line_content": "                      kind.opening_symbol())) {",
          "content_same": false
        },
        {
          "line": 533,
          "old_api": "LexOneCharSymbolToken",
          "new_api": "empty",
          "old_text": "LexOneCharSymbolToken(source_text, kind)",
          "new_text": "open_groups_.empty()",
          "old_line_content": "    Token token = LexOneCharSymbolToken(source_text, kind);",
          "new_line_content": "      if (open_groups_.empty()) {",
          "content_same": false
        },
        {
          "line": 538,
          "old_api": "GetTokenInfo",
          "new_api": "opening_symbol",
          "old_text": "buffer_->GetTokenInfo(token)",
          "new_text": "kind.opening_symbol()",
          "old_line_content": "    buffer_->GetTokenInfo(token).opening_token = opening_token;",
          "new_line_content": "                    kind.opening_symbol());",
          "content_same": false
        },
        {
          "line": 563,
          "old_api": "AddToken",
          "new_api": "Default",
          "old_text": "buffer_->AddToken(\n        {.kind = kind, .token_line = current_line_, .column = current_column_})",
          "new_text": "                         .Default(TokenKind::Error)",
          "old_line_content": "    Token token = buffer_->AddToken(",
          "new_line_content": "                         .Default(TokenKind::Error);",
          "content_same": false
        },
        {
          "line": 565,
          "old_api": "fixed_spelling",
          "new_api": "LexError",
          "old_text": "kind.fixed_spelling().size()",
          "new_text": "LexError(source_text)",
          "old_line_content": "    current_column_ += kind.fixed_spelling().size();",
          "new_line_content": "      return LexError(source_text);",
          "content_same": false
        },
        {
          "line": 576,
          "old_api": "LexResult::NoMatch()",
          "new_api": "fixed_spelling",
          "old_text": "LexResult::NoMatch()",
          "new_text": "kind.fixed_spelling().size()",
          "old_line_content": "      return LexResult::NoMatch();",
          "new_line_content": "    source_text = source_text.drop_front(kind.fixed_spelling().size());",
          "content_same": false
        },
        {
          "line": 584,
          "old_api": "front",
          "new_api": "size",
          "old_text": "word.front()",
          "new_text": "word.size()",
          "old_line_content": "    switch (word.front()) {",
          "new_line_content": "    if (word.size() < 2) {",
          "content_same": false
        },
        {
          "line": 608,
          "old_api": "LexResult::NoMatch()",
          "new_api": "substr",
          "old_text": "LexResult::NoMatch()",
          "new_text": "word.substr(1)",
          "old_line_content": "      return LexResult::NoMatch();",
          "new_line_content": "    llvm::StringRef suffix = word.substr(1);",
          "content_same": false
        },
        {
          "line": 621,
          "old_api": "gnu::noinline",
          "new_api": "AddToken",
          "old_text": "gnu::noinline",
          "new_text": "buffer_->AddToken(\n        {.kind = *kind, .token_line = current_line_, .column = column})",
          "old_line_content": "  [[gnu::noinline]] auto CloseInvalidOpenGroups(TokenKind kind) -> void {",
          "new_line_content": "    auto token = buffer_->AddToken(",
          "content_same": false
        },
        {
          "line": 623,
          "old_api": "empty",
          "new_api": "GetTokenInfo",
          "old_text": "open_groups_.empty()",
          "new_text": "buffer_->GetTokenInfo(token)",
          "old_line_content": "    CARBON_CHECK(!open_groups_.empty());",
          "new_line_content": "    buffer_->GetTokenInfo(token).literal_index =",
          "content_same": false
        },
        {
          "line": 632,
          "old_api": "pop_back",
          "new_api": "is_closing_symbol",
          "old_text": "open_groups_.pop_back()",
          "new_text": "kind.is_closing_symbol()",
          "old_line_content": "      open_groups_.pop_back();",
          "new_line_content": "    CARBON_CHECK(kind.is_closing_symbol() || kind == TokenKind::Error);",
          "content_same": false
        },
        {
          "line": 633,
          "old_api": "CARBON_DIAGNOSTIC",
          "new_api": "empty",
          "old_text": "CARBON_DIAGNOSTIC(\n          MismatchedClosing, Error,\n          \"Closing symbol does not match most recent opening symbol.\")",
          "new_text": "open_groups_.empty()",
          "old_line_content": "      CARBON_DIAGNOSTIC(",
          "new_line_content": "    CARBON_CHECK(!open_groups_.empty());",
          "content_same": false
        },
        {
          "line": 636,
          "old_api": "Emit",
          "new_api": "back",
          "old_text": "token_emitter_.Emit(opening_token, MismatchedClosing)",
          "new_text": "open_groups_.back()",
          "old_line_content": "      token_emitter_.Emit(opening_token, MismatchedClosing);",
          "new_line_content": "      Token opening_token = open_groups_.back();",
          "content_same": false
        },
        {
          "line": 638,
          "old_api": "empty",
          "new_api": "closing_symbol",
          "old_text": "buffer_->tokens().empty()",
          "new_text": "opening_kind.closing_symbol()",
          "old_line_content": "      CARBON_CHECK(!buffer_->tokens().empty())",
          "new_line_content": "      if (kind == opening_kind.closing_symbol()) {",
          "content_same": false
        },
        {
          "line": 646,
          "old_api": "HasTrailingWhitespace",
          "new_api": "Emit",
          "old_text": "buffer_->HasTrailingWhitespace(prev_token)",
          "new_text": "token_emitter_.Emit(opening_token, MismatchedClosing)",
          "old_line_content": "           .has_trailing_space = buffer_->HasTrailingWhitespace(prev_token),",
          "new_line_content": "      token_emitter_.Emit(opening_token, MismatchedClosing);",
          "content_same": false
        },
        {
          "line": 650,
          "old_api": "GetTokenInfo",
          "new_api": "end",
          "old_text": "buffer_->GetTokenInfo(opening_token)",
          "new_text": "buffer_->tokens().end()",
          "old_line_content": "      TokenInfo& opening_token_info = buffer_->GetTokenInfo(opening_token);",
          "new_line_content": "      Token prev_token = buffer_->tokens().end()[-1];",
          "content_same": false
        },
        {
          "line": 654,
          "old_api": "empty",
          "new_api": "closing_symbol",
          "old_text": "open_groups_.empty()",
          "new_text": "buffer_->AddToken(\n          {.kind = opening_kind.closing_symbol(),\n           .has_trailing_space = buffer_->HasTrailingWhitespace(prev_token),\n           .is_recovery = true,\n           .token_line = current_line_,\n           .column = current_column_})",
          "old_line_content": "    } while (!open_groups_.empty());",
          "new_line_content": "      Token closing_token = buffer_->AddToken(",
          "content_same": false
        },
        {
          "line": 661,
          "old_api": "push_back",
          "new_api": "GetTokenInfo",
          "old_text": "buffer_->identifier_infos_.push_back({text})",
          "new_text": "buffer_->GetTokenInfo(closing_token)",
          "old_line_content": "      buffer_->identifier_infos_.push_back({text});",
          "new_line_content": "      TokenInfo& closing_token_info = buffer_->GetTokenInfo(closing_token);",
          "content_same": false
        },
        {
          "line": 669,
          "old_api": "LexError",
          "new_api": "size",
          "old_text": "LexError(source_text)",
          "new_text": "buffer_->identifier_infos_.size()",
          "old_line_content": "      return LexError(source_text);",
          "new_line_content": "        {text, Identifier(buffer_->identifier_infos_.size())});",
          "content_same": false
        },
        {
          "line": 671,
          "old_api": "front",
          "new_api": "push_back",
          "old_text": "source_text.front()",
          "new_text": "buffer_->identifier_infos_.push_back({text})",
          "old_line_content": "    CARBON_CHECK(IsAlpha(source_text.front()) || source_text.front() == '_');",
          "new_line_content": "      buffer_->identifier_infos_.push_back({text});",
          "content_same": false
        },
        {
          "line": 679,
          "old_api": "ScanForIdentifierPrefix",
          "new_api": "LexError",
          "old_text": "ScanForIdentifierPrefix(source_text)",
          "new_text": "LexError(source_text)",
          "old_line_content": "    llvm::StringRef identifier_text = ScanForIdentifierPrefix(source_text);",
          "new_line_content": "      return LexError(source_text);",
          "content_same": false
        },
        {
          "line": 693,
          "old_api": "llvm::StringSwitch<TokenKind>(identifier_text)",
          "new_api": "size",
          "old_text": "llvm::StringSwitch<TokenKind>(identifier_text)",
          "new_text": "identifier_text.size()",
          "old_line_content": "    TokenKind kind = llvm::StringSwitch<TokenKind>(identifier_text)",
          "new_line_content": "    current_column_ += identifier_text.size();",
          "content_same": false
        },
        {
          "line": 698,
          "old_api": "AddToken",
          "new_api": "LexWordAsTypeLiteralToken",
          "old_text": "buffer_->AddToken({.kind = kind,\n                                .token_line = current_line_,\n                                .column = identifier_column})",
          "new_text": "LexWordAsTypeLiteralToken(identifier_text, identifier_column)",
          "old_line_content": "      return buffer_->AddToken({.kind = kind,",
          "new_line_content": "            LexWordAsTypeLiteralToken(identifier_text, identifier_column)) {",
          "content_same": false
        },
        {
          "line": 738,
          "old_api": "size",
          "new_api": "empty",
          "old_text": "error_text.size()",
          "new_text": "error_text.empty()",
          "old_line_content": "         .error_length = static_cast<int32_t>(error_text.size())});",
          "new_line_content": "    if (error_text.empty()) {",
          "content_same": false
        },
        {
          "line": 741,
          "old_api": "begin",
          "new_api": "take_front",
          "old_text": "error_text.begin()",
          "new_text": "source_text.take_front(1)",
          "old_line_content": "    emitter_.Emit(error_text.begin(), UnrecognizedCharacters);",
          "new_line_content": "      error_text = source_text.take_front(1);",
          "content_same": false
        },
        {
          "line": 769,
          "old_api": "GetLineInfo",
          "new_api": "empty",
          "old_text": "buffer_->GetLineInfo(current_line_)",
          "new_text": "source_text.empty()",
          "old_line_content": "      current_line_info_ = &buffer_->GetLineInfo(current_line_);",
          "new_line_content": "    CARBON_DCHECK(source_text.empty());",
          "content_same": false
        },
        {
          "line": 777,
          "old_api": "NoteWhitespace",
          "new_api": "GetLineNumber",
          "old_text": "NoteWhitespace()",
          "new_text": "buffer_->GetLineNumber(current_line_)",
          "old_line_content": "    NoteWhitespace();",
          "new_line_content": "    if (current_column_ == 0 && buffer_->GetLineNumber(current_line_) != 1) {",
          "content_same": false
        },
        {
          "line": 812,
          "old_api": "front",
          "new_api": "empty",
          "old_text": "source_text.front()",
          "new_text": "source_text.empty()",
          "old_line_content": "        source_text.front())](lexer, source_text);",
          "new_line_content": "    if (LLVM_UNLIKELY(source_text.empty())) {",
          "content_same": false
        },
        {
          "line": 1030,
          "old_api": "GetTokenInfo",
          "new_api": "Dispatch",
          "old_text": "GetTokenInfo(token)",
          "new_text": "lexer.Dispatch(source_text)",
          "old_line_content": "  return GetTokenInfo(token).kind;",
          "new_line_content": "  lexer.Dispatch(source_text);",
          "content_same": false
        },
        {
          "line": 1048,
          "old_api": "empty",
          "new_api": "GetLine",
          "old_text": "fixed_spelling.empty()",
          "new_text": "GetLine(token)",
          "old_line_content": "  if (!fixed_spelling.empty()) {",
          "new_line_content": "  return GetLineNumber(GetLine(token));",
          "content_same": false
        },
        {
          "line": 1076,
          "old_api": "substr",
          "new_api": "CARBON_CHECK",
          "old_text": "source_->text().substr(token_start)",
          "new_text": "CARBON_CHECK(relexed_token)",
          "old_line_content": "        StringLiteral::Lex(source_->text().substr(token_start));",
          "new_line_content": "    CARBON_CHECK(relexed_token) << \"Could not reform numeric literal token.\";",
          "content_same": false
        },
        {
          "line": 1077,
          "old_api": "CARBON_CHECK",
          "new_api": "text",
          "old_text": "CARBON_CHECK(relexed_token)",
          "new_text": "relexed_token->text()",
          "old_line_content": "    CARBON_CHECK(relexed_token) << \"Could not reform string literal token.\";",
          "new_line_content": "    return relexed_token->text();",
          "content_same": false
        },
        {
          "line": 1083,
          "old_api": "is_sized_type_literal",
          "new_api": "GetLineInfo",
          "old_text": "token_info.kind.is_sized_type_literal()",
          "new_text": "GetLineInfo(token_info.token_line)",
          "old_line_content": "  if (token_info.kind.is_sized_type_literal()) {",
          "new_line_content": "    const auto& line_info = GetLineInfo(token_info.token_line);",
          "content_same": false
        },
        {
          "line": 1087,
          "old_api": "substr",
          "new_api": "CARBON_CHECK",
          "old_text": "source_->text().substr(token_start + 1).take_while(IsDecimalDigit)",
          "new_text": "CARBON_CHECK(relexed_token)",
          "old_line_content": "        source_->text().substr(token_start + 1).take_while(IsDecimalDigit);",
          "new_line_content": "    CARBON_CHECK(relexed_token) << \"Could not reform string literal token.\";",
          "content_same": false
        },
        {
          "line": 1088,
          "old_api": "size",
          "new_api": "text",
          "old_text": "suffix.size()",
          "new_text": "relexed_token->text()",
          "old_line_content": "    return llvm::StringRef(suffix.data() - 1, suffix.size() + 1);",
          "new_line_content": "    return relexed_token->text();",
          "content_same": false
        },
        {
          "line": 1093,
          "old_api": "llvm::StringRef()",
          "new_api": "is_sized_type_literal",
          "old_text": "llvm::StringRef()",
          "new_text": "token_info.kind.is_sized_type_literal()",
          "old_line_content": "    return llvm::StringRef();",
          "new_line_content": "  if (token_info.kind.is_sized_type_literal()) {",
          "content_same": false
        },
        {
          "line": 1097,
          "old_api": "GetIdentifierText",
          "new_api": "substr",
          "old_text": "GetIdentifierText(token_info.id)",
          "new_text": "source_->text().substr(token_start + 1).take_while(IsDecimalDigit)",
          "old_line_content": "  return GetIdentifierText(token_info.id);",
          "new_line_content": "        source_->text().substr(token_start + 1).take_while(IsDecimalDigit);",
          "content_same": false
        },
        {
          "line": 1132,
          "old_api": "CARBON_CHECK",
          "new_api": "text",
          "old_text": "CARBON_CHECK(token_info.kind == TokenKind::StringLiteral)",
          "new_text": "source_->text()",
          "old_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::StringLiteral) << token_info.kind;",
          "new_line_content": "  char second_char = source_->text()[token_start + 1];",
          "content_same": false
        },
        {
          "line": 1188,
          "old_api": "GetLineInfo",
          "new_api": "size",
          "old_text": "GetLineInfo(line)",
          "new_text": "line_infos_.size()",
          "old_line_content": "  return GetLineInfo(line).indent + 1;",
          "new_line_content": "  CARBON_DCHECK(static_cast<size_t>(next.index) < line_infos_.size());",
          "content_same": false
        },
        {
          "line": 1198,
          "old_api": "std::max(widths.kind, kind)",
          "new_api": "GetLineInfo",
          "old_text": "std::max(widths.kind, kind)",
          "new_text": "GetLineInfo(line)",
          "old_line_content": "  kind = std::max(widths.kind, kind);",
          "new_line_content": "  return GetLineInfo(line).indent + 1;",
          "content_same": false
        },
        {
          "line": 1210,
          "old_api": "CARBON_CHECK",
          "new_api": "std::max(widths.line, line)",
          "old_text": "CARBON_CHECK(number >= 0)",
          "new_text": "std::max(widths.line, line)",
          "old_line_content": "  CARBON_CHECK(number >= 0) << \"Negative numbers are not supported.\";",
          "new_line_content": "  line = std::max(widths.line, line);",
          "content_same": false
        },
        {
          "line": 1220,
          "old_api": "size",
          "new_api": "CARBON_CHECK",
          "old_text": "token_infos_.size()",
          "new_text": "CARBON_CHECK(number >= 0)",
          "old_line_content": "  widths.index = ComputeDecimalPrintedWidth(token_infos_.size());",
          "new_line_content": "  CARBON_CHECK(number >= 0) << \"Negative numbers are not supported.\";",
          "content_same": false
        },
        {
          "line": 1225,
          "old_api": "GetLine",
          "new_api": "std::log10(number)",
          "old_text": "GetLine(token)",
          "new_text": "std::log10(number)",
          "old_line_content": "      ComputeDecimalPrintedWidth(GetIndentColumnNumber(GetLine(token)));",
          "new_line_content": "  return static_cast<int>(std::log10(number)) + 1;",
          "content_same": false
        },
        {
          "line": 1230,
          "old_api": "end",
          "new_api": "size",
          "old_text": "tokens().end()",
          "new_text": "token_infos_.size()",
          "old_line_content": "  if (tokens().begin() == tokens().end()) {",
          "new_line_content": "  widths.index = ComputeDecimalPrintedWidth(token_infos_.size());",
          "content_same": false
        },
        {
          "line": 1240,
          "old_api": "GetTokenPrintWidths",
          "new_api": "end",
          "old_text": "GetTokenPrintWidths(token)",
          "new_text": "tokens().end()",
          "old_line_content": "    widths.Widen(GetTokenPrintWidths(token));",
          "new_line_content": "  if (tokens().begin() == tokens().end()) {",
          "content_same": false
        },
        {
          "line": 1244,
          "old_api": "PrintToken",
          "new_api": "filename",
          "old_text": "PrintToken(output_stream, token, widths)",
          "new_text": "source_->filename()",
          "old_line_content": "    PrintToken(output_stream, token, widths);",
          "new_line_content": "  output_stream << \"- filename: \" << source_->filename() << \"\\n\"",
          "content_same": false
        },
        {
          "line": 1269,
          "old_api": "name",
          "new_api": "GetTokenInfo",
          "old_text": "token_info.kind.name()",
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "      llvm::right_justify(llvm::formatv(\"'{0}'\", token_info.kind.name()).str(),",
          "new_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "content_same": false
        },
        {
          "line": 1279,
          "old_api": "GetIdentifier",
          "new_api": "name",
          "old_text": "GetIdentifier(token)",
          "new_text": "token_info.kind.name()",
          "old_line_content": "      output_stream << \", identifier: \" << GetIdentifier(token).index;",
          "new_line_content": "      llvm::right_justify(llvm::formatv(\"'{0}'\", token_info.kind.name()).str(),",
          "content_same": false
        },
        {
          "line": 1283,
          "old_api": "print",
          "new_api": "GetIndentColumnNumber",
          "old_text": "GetIntegerLiteral(token).print(output_stream, /*isSigned=*/false)",
          "new_text": "GetIndentColumnNumber(token_info.token_line)",
          "old_line_content": "      GetIntegerLiteral(token).print(output_stream, /*isSigned=*/false);",
          "new_line_content": "      llvm::format_decimal(GetIndentColumnNumber(token_info.token_line),",
          "content_same": false
        },
        {
          "line": 1293,
          "old_api": "is_opening_symbol",
          "new_api": "print",
          "old_text": "token_info.kind.is_opening_symbol()",
          "new_text": "GetIntegerLiteral(token).print(output_stream, /*isSigned=*/false)",
          "old_line_content": "      if (token_info.kind.is_opening_symbol()) {",
          "new_line_content": "      GetIntegerLiteral(token).print(output_stream, /*isSigned=*/false);",
          "content_same": false
        },
        {
          "line": 1346,
          "old_api": "text",
          "new_api": "expected_parse_tree_size",
          "old_text": "buffer_->source_->text()",
          "new_text": "info.kind.expected_parse_tree_size()",
          "old_line_content": "  CARBON_CHECK(StringRefContainsPointer(buffer_->source_->text(), loc))",
          "new_line_content": "  expected_parse_tree_size_ += info.kind.expected_parse_tree_size();",
          "content_same": false
        },
        {
          "line": 1368,
          "old_api": "substr",
          "new_api": "begin",
          "old_text": "buffer_->source_->text().substr(line_it->start, line_it->length)",
          "new_text": "buffer_->line_infos_.begin()",
          "old_line_content": "      buffer_->source_->text().substr(line_it->start, line_it->length);",
          "new_line_content": "  CARBON_CHECK(line_it != buffer_->line_infos_.begin())",
          "content_same": false
        },
        {
          "line": 1379,
          "old_api": "take_front",
          "new_api": "static_cast<int32_t>(llvm::StringRef::npos)",
          "old_text": "line.take_front(end_newline_pos)",
          "new_text": "static_cast<int32_t>(llvm::StringRef::npos)",
          "old_line_content": "      line = line.take_front(end_newline_pos);",
          "new_line_content": "  if (line_it->length == static_cast<int32_t>(llvm::StringRef::npos)) {",
          "content_same": false
        }
      ],
      "additions": [
        {
          "line": 512,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "source_text.begin()",
          "old_line_content": "    // If we have no open groups, this is an error.",
          "new_line_content": "      emitter_.Emit(source_text.begin(), UnmatchedClosing);",
          "content_same": false
        },
        {
          "line": 1029,
          "old_api": null,
          "new_api": "text",
          "old_text": null,
          "new_text": "source.text()",
          "old_line_content": "auto TokenizedBuffer::GetKind(Token token) const -> TokenKind {",
          "new_line_content": "  llvm::StringRef source_text = source.text();",
          "content_same": false
        },
        {
          "line": 518,
          "old_api": null,
          "new_api": "drop_front",
          "old_text": null,
          "new_text": "source_text.drop_front()",
          "old_line_content": "    // Close any invalid open groups first.",
          "new_line_content": "      source_text = source_text.drop_front();",
          "content_same": false
        },
        {
          "line": 1032,
          "old_api": null,
          "new_api": "seen_error",
          "old_text": null,
          "new_text": "error_tracking_consumer.seen_error()",
          "old_line_content": "",
          "new_line_content": "  if (error_tracking_consumer.seen_error()) {",
          "content_same": false
        },
        {
          "line": 1040,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "",
          "new_line_content": "  return GetTokenInfo(token).kind;",
          "content_same": false
        },
        {
          "line": 529,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "buffer_->GetTokenInfo(opening_token)",
          "old_line_content": "    }",
          "new_line_content": "    if (LLVM_UNLIKELY(buffer_->GetTokenInfo(opening_token).kind !=",
          "content_same": false
        },
        {
          "line": 531,
          "old_api": null,
          "new_api": "CloseInvalidOpenGroups",
          "old_text": null,
          "new_text": "CloseInvalidOpenGroups(kind)",
          "old_line_content": "",
          "new_line_content": "      CloseInvalidOpenGroups(kind);",
          "content_same": false
        },
        {
          "line": 1044,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "",
          "new_line_content": "  return GetTokenInfo(token).token_line;",
          "content_same": false
        },
        {
          "line": 534,
          "old_api": null,
          "new_api": "unmatched_error",
          "old_text": null,
          "new_text": "unmatched_error()",
          "old_line_content": "",
          "new_line_content": "        return unmatched_error();",
          "content_same": false
        },
        {
          "line": 536,
          "old_api": null,
          "new_api": "back",
          "old_text": null,
          "new_text": "open_groups_.back()",
          "old_line_content": "    // open token would invalidate any pointers.",
          "new_line_content": "      opening_token = open_groups_.back();",
          "content_same": false
        },
        {
          "line": 540,
          "old_api": null,
          "new_api": "pop_back",
          "old_text": null,
          "new_text": "open_groups_.pop_back()",
          "old_line_content": "    return token;",
          "new_line_content": "    open_groups_.pop_back();",
          "content_same": false
        },
        {
          "line": 1052,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "  if (token_info.kind == TokenKind::Error) {",
          "new_line_content": "  return GetTokenInfo(token).column + 1;",
          "content_same": false
        },
        {
          "line": 543,
          "old_api": null,
          "new_api": "LexOneCharSymbolToken",
          "old_text": null,
          "new_text": "LexOneCharSymbolToken(source_text, kind)",
          "old_line_content": "  auto LexSymbolToken(llvm::StringRef& source_text) -> LexResult {",
          "new_line_content": "    Token token = LexOneCharSymbolToken(source_text, kind);",
          "content_same": false
        },
        {
          "line": 1056,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "  }",
          "new_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "content_same": false
        },
        {
          "line": 1057,
          "old_api": null,
          "new_api": "fixed_spelling",
          "old_text": null,
          "new_text": "token_info.kind.fixed_spelling()",
          "old_line_content": "",
          "new_line_content": "  llvm::StringRef fixed_spelling = token_info.kind.fixed_spelling();",
          "content_same": false
        },
        {
          "line": 1058,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "fixed_spelling.empty()",
          "old_line_content": "  // Refer back to the source text to preserve oddities like radix or digit",
          "new_line_content": "  if (!fixed_spelling.empty()) {",
          "content_same": false
        },
        {
          "line": 547,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "buffer_->GetTokenInfo(opening_token)",
          "old_line_content": "#define CARBON_SYMBOL_TOKEN(Name, Spelling) \\",
          "new_line_content": "    buffer_->GetTokenInfo(opening_token).closing_token = token;",
          "content_same": false
        },
        {
          "line": 548,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "buffer_->GetTokenInfo(token)",
          "old_line_content": "  .StartsWith(Spelling, TokenKind::Name)",
          "new_line_content": "    buffer_->GetTokenInfo(token).opening_token = opening_token;",
          "content_same": false
        },
        {
          "line": 1063,
          "old_api": null,
          "new_api": "GetLineInfo",
          "old_text": null,
          "new_text": "GetLineInfo(token_info.token_line)",
          "old_line_content": "    int64_t token_start = line_info.start + token_info.column;",
          "new_line_content": "    const auto& line_info = GetLineInfo(token_info.token_line);",
          "content_same": false
        },
        {
          "line": 556,
          "old_api": null,
          "new_api": "llvm::StringSwitch<TokenKind>(source_text)",
          "old_text": null,
          "new_text": "llvm::StringSwitch<TokenKind>(source_text)",
          "old_line_content": "    }",
          "new_line_content": "    TokenKind kind = llvm::StringSwitch<TokenKind>(source_text)",
          "content_same": false
        },
        {
          "line": 1072,
          "old_api": null,
          "new_api": "GetLineInfo",
          "old_text": null,
          "new_text": "GetLineInfo(token_info.token_line)",
          "old_line_content": "  if (token_info.kind == TokenKind::StringLiteral) {",
          "new_line_content": "    const auto& line_info = GetLineInfo(token_info.token_line);",
          "content_same": false
        },
        {
          "line": 1075,
          "old_api": null,
          "new_api": "substr",
          "old_text": null,
          "new_text": "source_->text().substr(token_start)",
          "old_line_content": "    std::optional<StringLiteral> relexed_token =",
          "new_line_content": "        NumericLiteral::Lex(source_->text().substr(token_start));",
          "content_same": false
        },
        {
          "line": 573,
          "old_api": null,
          "new_api": "AddToken",
          "old_text": null,
          "new_text": "buffer_->AddToken(\n        {.kind = kind, .token_line = current_line_, .column = current_column_})",
          "old_line_content": "      -> LexResult {",
          "new_line_content": "    Token token = buffer_->AddToken(",
          "content_same": false
        },
        {
          "line": 1086,
          "old_api": null,
          "new_api": "substr",
          "old_text": null,
          "new_text": "source_->text().substr(token_start)",
          "old_line_content": "    llvm::StringRef suffix =",
          "new_line_content": "        StringLiteral::Lex(source_->text().substr(token_start));",
          "content_same": false
        },
        {
          "line": 575,
          "old_api": null,
          "new_api": "fixed_spelling",
          "old_text": null,
          "new_text": "kind.fixed_spelling().size()",
          "old_line_content": "      // Too short to form one of these tokens.",
          "new_line_content": "    current_column_ += kind.fixed_spelling().size();",
          "content_same": false
        },
        {
          "line": 1094,
          "old_api": null,
          "new_api": "GetLineInfo",
          "old_text": null,
          "new_text": "GetLineInfo(token_info.token_line)",
          "old_line_content": "  }",
          "new_line_content": "    const auto& line_info = GetLineInfo(token_info.token_line);",
          "content_same": false
        },
        {
          "line": 586,
          "old_api": null,
          "new_api": "LexResult::NoMatch()",
          "old_text": null,
          "new_text": "LexResult::NoMatch()",
          "old_line_content": "        kind = TokenKind::IntegerTypeLiteral;",
          "new_line_content": "      return LexResult::NoMatch();",
          "content_same": false
        },
        {
          "line": 1098,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "suffix.size()",
          "old_line_content": "}",
          "new_line_content": "    return llvm::StringRef(suffix.data() - 1, suffix.size() + 1);",
          "content_same": false
        },
        {
          "line": 590,
          "old_api": null,
          "new_api": "LexResult::NoMatch()",
          "old_text": null,
          "new_text": "LexResult::NoMatch()",
          "old_line_content": "        break;",
          "new_line_content": "      return LexResult::NoMatch();",
          "content_same": false
        },
        {
          "line": 1103,
          "old_api": null,
          "new_api": "llvm::StringRef()",
          "old_text": null,
          "new_text": "llvm::StringRef()",
          "old_line_content": "  return token_info.id;",
          "new_line_content": "    return llvm::StringRef();",
          "content_same": false
        },
        {
          "line": 594,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "word.front()",
          "old_line_content": "      default:",
          "new_line_content": "    switch (word.front()) {",
          "content_same": false
        },
        {
          "line": 1106,
          "old_api": null,
          "new_api": "CARBON_CHECK",
          "old_text": null,
          "new_text": "CARBON_CHECK(token_info.kind == TokenKind::Identifier)",
          "old_line_content": "auto TokenizedBuffer::GetIntegerLiteral(Token token) const",
          "new_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::Identifier) << token_info.kind;",
          "content_same": false
        },
        {
          "line": 1107,
          "old_api": null,
          "new_api": "GetIdentifierText",
          "old_text": null,
          "new_text": "GetIdentifierText(token_info.id)",
          "old_line_content": "    -> const llvm::APInt& {",
          "new_line_content": "  return GetIdentifierText(token_info.id);",
          "content_same": false
        },
        {
          "line": 1111,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "}",
          "new_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "content_same": false
        },
        {
          "line": 1112,
          "old_api": null,
          "new_api": "CARBON_CHECK",
          "old_text": null,
          "new_text": "CARBON_CHECK(token_info.kind == TokenKind::Identifier)",
          "old_line_content": "",
          "new_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::Identifier) << token_info.kind;",
          "content_same": false
        },
        {
          "line": 605,
          "old_api": null,
          "new_api": "LexResult::NoMatch()",
          "old_text": null,
          "new_text": "LexResult::NoMatch()",
          "old_line_content": "    }",
          "new_line_content": "        return LexResult::NoMatch();",
          "content_same": false
        },
        {
          "line": 1118,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "  // safely look at the second character to determine whether we have a",
          "new_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "content_same": false
        },
        {
          "line": 1119,
          "old_api": null,
          "new_api": "CARBON_CHECK",
          "old_text": null,
          "new_text": "CARBON_CHECK(token_info.kind == TokenKind::IntegerLiteral)",
          "old_line_content": "  // decimal or hexadecimal literal.",
          "new_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::IntegerLiteral) << token_info.kind;",
          "content_same": false
        },
        {
          "line": 609,
          "old_api": null,
          "new_api": "CanLexInteger",
          "old_text": null,
          "new_text": "CanLexInteger(emitter_, suffix)",
          "old_line_content": "    }",
          "new_line_content": "    if (!CanLexInteger(emitter_, suffix)) {",
          "content_same": false
        },
        {
          "line": 610,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "buffer_->AddToken(\n          {.kind = TokenKind::Error,\n           .token_line = current_line_,\n           .column = column,\n           .error_length = static_cast<int32_t>(word.size())})",
          "old_line_content": "",
          "new_line_content": "      return buffer_->AddToken(",
          "content_same": false
        },
        {
          "line": 1124,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "",
          "new_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "content_same": false
        },
        {
          "line": 1125,
          "old_api": null,
          "new_api": "CARBON_CHECK",
          "old_text": null,
          "new_text": "CARBON_CHECK(token_info.kind == TokenKind::RealLiteral)",
          "old_line_content": "  return {.mantissa = literal_int_storage_[token_info.literal_index],",
          "new_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::RealLiteral) << token_info.kind;",
          "content_same": false
        },
        {
          "line": 617,
          "old_api": null,
          "new_api": "getAsInteger",
          "old_text": null,
          "new_text": "suffix.getAsInteger(10, suffix_value)",
          "old_line_content": "  }",
          "new_line_content": "    if (suffix.getAsInteger(10, suffix_value)) {",
          "content_same": false
        },
        {
          "line": 618,
          "old_api": null,
          "new_api": "LexResult::NoMatch()",
          "old_text": null,
          "new_text": "LexResult::NoMatch()",
          "old_line_content": "",
          "new_line_content": "      return LexResult::NoMatch();",
          "content_same": false
        },
        {
          "line": 1130,
          "old_api": null,
          "new_api": "GetLineInfo",
          "old_text": null,
          "new_text": "GetLineInfo(token_info.token_line)",
          "old_line_content": "auto TokenizedBuffer::GetStringLiteral(Token token) const -> llvm::StringRef {",
          "new_line_content": "  const auto& line_info = GetLineInfo(token_info.token_line);",
          "content_same": false
        },
        {
          "line": 624,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "buffer_->literal_int_storage_.size()",
          "old_line_content": "",
          "new_line_content": "        buffer_->literal_int_storage_.size();",
          "content_same": false
        },
        {
          "line": 625,
          "old_api": null,
          "new_api": "std::move(suffix_value)",
          "old_text": null,
          "new_text": "std::move(suffix_value)",
          "old_line_content": "    do {",
          "new_line_content": "    buffer_->literal_int_storage_.push_back(std::move(suffix_value));",
          "content_same": false
        },
        {
          "line": 1141,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "}",
          "new_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "content_same": false
        },
        {
          "line": 1142,
          "old_api": null,
          "new_api": "CARBON_CHECK",
          "old_text": null,
          "new_text": "CARBON_CHECK(token_info.kind == TokenKind::StringLiteral)",
          "old_line_content": "",
          "new_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::StringLiteral) << token_info.kind;",
          "content_same": false
        },
        {
          "line": 631,
          "old_api": null,
          "new_api": "gnu::noinline",
          "old_text": null,
          "new_text": "gnu::noinline",
          "old_line_content": "",
          "new_line_content": "  [[gnu::noinline]] auto CloseInvalidOpenGroups(TokenKind kind) -> void {",
          "content_same": false
        },
        {
          "line": 1148,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "  return opening_token_info.closing_token;",
          "new_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "content_same": false
        },
        {
          "line": 637,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "buffer_->GetTokenInfo(opening_token)",
          "old_line_content": "",
          "new_line_content": "      TokenKind opening_kind = buffer_->GetTokenInfo(opening_token).kind;",
          "content_same": false
        },
        {
          "line": 1149,
          "old_api": null,
          "new_api": "is_sized_type_literal",
          "old_text": null,
          "new_text": "token_info.kind.is_sized_type_literal()",
          "old_line_content": "}",
          "new_line_content": "  CARBON_CHECK(token_info.kind.is_sized_type_literal()) << token_info.kind;",
          "content_same": false
        },
        {
          "line": 642,
          "old_api": null,
          "new_api": "pop_back",
          "old_text": null,
          "new_text": "open_groups_.pop_back()",
          "old_line_content": "      // TODO: do a smarter backwards scan for where to put the closing",
          "new_line_content": "      open_groups_.pop_back();",
          "content_same": false
        },
        {
          "line": 643,
          "old_api": null,
          "new_api": "CARBON_DIAGNOSTIC",
          "old_text": null,
          "new_text": "CARBON_DIAGNOSTIC(\n          MismatchedClosing, Error,\n          \"Closing symbol does not match most recent opening symbol.\")",
          "old_line_content": "      // token.",
          "new_line_content": "      CARBON_DIAGNOSTIC(",
          "content_same": false
        },
        {
          "line": 1155,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(opening_token)",
          "old_line_content": "      << closing_token_info.kind;",
          "new_line_content": "  const auto& opening_token_info = GetTokenInfo(opening_token);",
          "content_same": false
        },
        {
          "line": 1156,
          "old_api": null,
          "new_api": "is_opening_symbol",
          "old_text": null,
          "new_text": "opening_token_info.kind.is_opening_symbol()",
          "old_line_content": "  return closing_token_info.opening_token;",
          "new_line_content": "  CARBON_CHECK(opening_token_info.kind.is_opening_symbol())",
          "content_same": false
        },
        {
          "line": 648,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "buffer_->tokens().empty()",
          "old_line_content": "           .token_line = current_line_,",
          "new_line_content": "      CARBON_CHECK(!buffer_->tokens().empty())",
          "content_same": false
        },
        {
          "line": 1163,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(closing_token)",
          "old_line_content": "",
          "new_line_content": "  const auto& closing_token_info = GetTokenInfo(closing_token);",
          "content_same": false
        },
        {
          "line": 1164,
          "old_api": null,
          "new_api": "is_closing_symbol",
          "old_text": null,
          "new_text": "closing_token_info.kind.is_closing_symbol()",
          "old_line_content": "auto TokenizedBuffer::HasTrailingWhitespace(Token token) const -> bool {",
          "new_line_content": "  CARBON_CHECK(closing_token_info.kind.is_closing_symbol())",
          "content_same": false
        },
        {
          "line": 655,
          "old_api": null,
          "new_api": "closing_symbol",
          "old_text": null,
          "new_text": "opening_kind.closing_symbol()",
          "old_line_content": "  }",
          "new_line_content": "          {.kind = opening_kind.closing_symbol(),",
          "content_same": false
        },
        {
          "line": 656,
          "old_api": null,
          "new_api": "HasTrailingWhitespace",
          "old_text": null,
          "new_text": "buffer_->HasTrailingWhitespace(prev_token)",
          "old_line_content": "",
          "new_line_content": "           .has_trailing_space = buffer_->HasTrailingWhitespace(prev_token),",
          "content_same": false
        },
        {
          "line": 1170,
          "old_api": null,
          "new_api": "TokenIterator",
          "old_text": null,
          "new_text": "TokenIterator(token)",
          "old_line_content": "}",
          "new_line_content": "  auto it = TokenIterator(token);",
          "content_same": false
        },
        {
          "line": 1171,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(*(it - 1))",
          "old_line_content": "",
          "new_line_content": "  return it == tokens().begin() || GetTokenInfo(*(it - 1)).has_trailing_space;",
          "content_same": false
        },
        {
          "line": 660,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "buffer_->GetTokenInfo(opening_token)",
          "old_line_content": "    if (insert_result.second) {",
          "new_line_content": "      TokenInfo& opening_token_info = buffer_->GetTokenInfo(opening_token);",
          "content_same": false
        },
        {
          "line": 1175,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "",
          "new_line_content": "  return GetTokenInfo(token).has_trailing_space;",
          "content_same": false
        },
        {
          "line": 664,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "open_groups_.empty()",
          "old_line_content": "  }",
          "new_line_content": "    } while (!open_groups_.empty());",
          "content_same": false
        },
        {
          "line": 1179,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "GetTokenInfo(token)",
          "old_line_content": "  return next;",
          "new_line_content": "  return GetTokenInfo(token).is_recovery;",
          "content_same": false
        },
        {
          "line": 668,
          "old_api": null,
          "new_api": "insert",
          "old_text": null,
          "new_text": "buffer_->identifier_map_.insert(\n        {text, Identifier(buffer_->identifier_infos_.size())})",
          "old_line_content": "      // TODO: Need to add support for Unicode lexing.",
          "new_line_content": "    auto insert_result = buffer_->identifier_map_.insert(",
          "content_same": false
        },
        {
          "line": 677,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "source_text.front()",
          "old_line_content": "",
          "new_line_content": "    if (static_cast<unsigned char>(source_text.front()) > 0x7F) {",
          "content_same": false
        },
        {
          "line": 681,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "source_text.front()",
          "old_line_content": "        << \"Must have at least one character!\";",
          "new_line_content": "    CARBON_CHECK(IsAlpha(source_text.front()) || source_text.front() == '_');",
          "content_same": false
        },
        {
          "line": 1193,
          "old_api": null,
          "new_api": "CARBON_CHECK",
          "old_text": null,
          "new_text": "CARBON_CHECK(line.index > 0)",
          "old_line_content": "  return identifier_infos_[identifier.index].text;",
          "new_line_content": "  CARBON_CHECK(line.index > 0);",
          "content_same": false
        },
        {
          "line": 1194,
          "old_api": null,
          "new_api": "Line",
          "old_text": null,
          "new_text": "Line(line.index - 1)",
          "old_line_content": "}",
          "new_line_content": "  return Line(line.index - 1);",
          "content_same": false
        },
        {
          "line": 689,
          "old_api": null,
          "new_api": "ScanForIdentifierPrefix",
          "old_text": null,
          "new_text": "ScanForIdentifierPrefix(source_text)",
          "old_line_content": "      return result;",
          "new_line_content": "    llvm::StringRef identifier_text = ScanForIdentifierPrefix(source_text);",
          "content_same": false
        },
        {
          "line": 690,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "identifier_text.empty()",
          "old_line_content": "    }",
          "new_line_content": "    CARBON_CHECK(!identifier_text.empty())",
          "content_same": false
        },
        {
          "line": 694,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "identifier_text.size()",
          "old_line_content": "#define CARBON_KEYWORD_TOKEN(Name, Spelling) .Case(Spelling, TokenKind::Name)",
          "new_line_content": "    source_text = source_text.drop_front(identifier_text.size());",
          "content_same": false
        },
        {
          "line": 1207,
          "old_api": null,
          "new_api": "std::max(widths.index, index)",
          "old_text": null,
          "new_text": "std::max(widths.index, index)",
          "old_line_content": "//",
          "new_line_content": "  index = std::max(widths.index, index);",
          "content_same": false
        },
        {
          "line": 1208,
          "old_api": null,
          "new_api": "std::max(widths.kind, kind)",
          "old_text": null,
          "new_text": "std::max(widths.kind, kind)",
          "old_line_content": "// This routine requires its argument to be *non-negative*.",
          "new_line_content": "  kind = std::max(widths.kind, kind);",
          "content_same": false
        },
        {
          "line": 1209,
          "old_api": null,
          "new_api": "std::max(widths.column, column)",
          "old_text": null,
          "new_text": "std::max(widths.column, column)",
          "old_line_content": "static auto ComputeDecimalPrintedWidth(int number) -> int {",
          "new_line_content": "  column = std::max(widths.column, column);",
          "content_same": false
        },
        {
          "line": 1211,
          "old_api": null,
          "new_api": "std::max(widths.indent, indent)",
          "old_text": null,
          "new_text": "std::max(widths.indent, indent)",
          "old_line_content": "  if (number == 0) {",
          "new_line_content": "  indent = std::max(widths.indent, indent);",
          "content_same": false
        },
        {
          "line": 703,
          "old_api": null,
          "new_api": "llvm::StringSwitch<TokenKind>(identifier_text)",
          "old_text": null,
          "new_text": "llvm::StringSwitch<TokenKind>(identifier_text)",
          "old_line_content": "    // Otherwise we have a generic identifier.",
          "new_line_content": "    TokenKind kind = llvm::StringSwitch<TokenKind>(identifier_text)",
          "content_same": false
        },
        {
          "line": 706,
          "old_api": null,
          "new_api": "Default",
          "old_text": null,
          "new_text": "                         .Default(TokenKind::Error)",
          "old_line_content": "                              .column = identifier_column,",
          "new_line_content": "                         .Default(TokenKind::Error);",
          "content_same": false
        },
        {
          "line": 708,
          "old_api": null,
          "new_api": "AddToken",
          "old_text": null,
          "new_text": "buffer_->AddToken({.kind = kind,\n                                .token_line = current_line_,\n                                .column = identifier_column})",
          "old_line_content": "  }",
          "new_line_content": "      return buffer_->AddToken({.kind = kind,",
          "content_same": false
        },
        {
          "line": 714,
          "old_api": null,
          "new_api": "AddToken",
          "old_text": null,
          "new_text": "buffer_->AddToken({.kind = TokenKind::Identifier,\n                              .token_line = current_line_,\n                              .column = identifier_column,\n                              .id = GetOrCreateIdentifier(identifier_text)})",
          "old_line_content": "      }",
          "new_line_content": "    return buffer_->AddToken({.kind = TokenKind::Identifier,",
          "content_same": false
        },
        {
          "line": 717,
          "old_api": null,
          "new_api": "GetOrCreateIdentifier",
          "old_text": null,
          "new_text": "GetOrCreateIdentifier(identifier_text)",
          "old_line_content": "        case '\\t':",
          "new_line_content": "                              .id = GetOrCreateIdentifier(identifier_text)});",
          "content_same": false
        },
        {
          "line": 1231,
          "old_api": null,
          "new_api": "name",
          "old_text": null,
          "new_text": "GetKind(token).name().size()",
          "old_line_content": "    return;",
          "new_line_content": "  widths.kind = GetKind(token).name().size();",
          "content_same": false
        },
        {
          "line": 1232,
          "old_api": null,
          "new_api": "GetLineNumber",
          "old_text": null,
          "new_text": "GetLineNumber(token)",
          "old_line_content": "  }",
          "new_line_content": "  widths.line = ComputeDecimalPrintedWidth(GetLineNumber(token));",
          "content_same": false
        },
        {
          "line": 721,
          "old_api": null,
          "new_api": "take_while",
          "old_text": null,
          "new_text": "source_text.take_while([](char c) {\n      if (IsAlnum(c)) {\n        return false;\n      }\n      switch (c) {\n        case '_':\n        case '\\t':\n        case '\\n':\n          return false;\n        default:\n          break;\n      }\n      return llvm::StringSwitch<bool>(llvm::StringRef(&c, 1))\n#define CARBON_SYMBOL_TOKEN(Name, Spelling) .StartsWith(Spelling, false)\n#include \"toolchain/lex/token_kind.def\"\n          .Default(true);\n    })",
          "old_line_content": "          break;",
          "new_line_content": "    llvm::StringRef error_text = source_text.take_while([](char c) {",
          "content_same": false
        },
        {
          "line": 722,
          "old_api": null,
          "new_api": "IsAlnum",
          "old_text": null,
          "new_text": "IsAlnum(c)",
          "old_line_content": "      }",
          "new_line_content": "      if (IsAlnum(c)) {",
          "content_same": false
        },
        {
          "line": 1233,
          "old_api": null,
          "new_api": "GetColumnNumber",
          "old_text": null,
          "new_text": "GetColumnNumber(token)",
          "old_line_content": "",
          "new_line_content": "  widths.column = ComputeDecimalPrintedWidth(GetColumnNumber(token));",
          "content_same": false
        },
        {
          "line": 1235,
          "old_api": null,
          "new_api": "GetLine",
          "old_text": null,
          "new_text": "GetLine(token)",
          "old_line_content": "                << \"  tokens: [\\n\";",
          "new_line_content": "      ComputeDecimalPrintedWidth(GetIndentColumnNumber(GetLine(token)));",
          "content_same": false
        },
        {
          "line": 733,
          "old_api": null,
          "new_api": "llvm::StringRef(&c, 1)",
          "old_text": null,
          "new_text": "llvm::StringRef(&c, 1)",
          "old_line_content": "",
          "new_line_content": "      return llvm::StringSwitch<bool>(llvm::StringRef(&c, 1))",
          "content_same": false
        },
        {
          "line": 736,
          "old_api": null,
          "new_api": "Default",
          "old_text": null,
          "new_text": "          .Default(true)",
          "old_line_content": "         .token_line = current_line_,",
          "new_line_content": "          .Default(true);",
          "content_same": false
        },
        {
          "line": 1248,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "token_infos_.size()",
          "old_line_content": "}",
          "new_line_content": "  widths.index = ComputeDecimalPrintedWidth((token_infos_.size()));",
          "content_same": false
        },
        {
          "line": 1249,
          "old_api": null,
          "new_api": "tokens",
          "old_text": null,
          "new_text": "tokens()",
          "old_line_content": "",
          "new_line_content": "  for (Token token : tokens()) {",
          "content_same": false
        },
        {
          "line": 1250,
          "old_api": null,
          "new_api": "GetTokenPrintWidths",
          "old_text": null,
          "new_text": "GetTokenPrintWidths(token)",
          "old_line_content": "auto TokenizedBuffer::PrintToken(llvm::raw_ostream& output_stream,",
          "new_line_content": "    widths.Widen(GetTokenPrintWidths(token));",
          "content_same": false
        },
        {
          "line": 1253,
          "old_api": null,
          "new_api": "tokens",
          "old_text": null,
          "new_text": "tokens()",
          "old_line_content": "}",
          "new_line_content": "  for (Token token : tokens()) {",
          "content_same": false
        },
        {
          "line": 1254,
          "old_api": null,
          "new_api": "PrintToken",
          "old_text": null,
          "new_text": "PrintToken(output_stream, token, widths)",
          "old_line_content": "",
          "new_line_content": "    PrintToken(output_stream, token, widths);",
          "content_same": false
        },
        {
          "line": 748,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "error_text.size()",
          "old_line_content": "  auto LexStartOfFile(llvm::StringRef& /*source_text*/) -> void {",
          "new_line_content": "         .error_length = static_cast<int32_t>(error_text.size())});",
          "content_same": false
        },
        {
          "line": 749,
          "old_api": null,
          "new_api": "CARBON_DIAGNOSTIC",
          "old_text": null,
          "new_text": "CARBON_DIAGNOSTIC(UnrecognizedCharacters, Error,\n                      \"Encountered unrecognized characters while parsing.\")",
          "old_line_content": "    // Before lexing any source text, add the start-of-file token so that code",
          "new_line_content": "    CARBON_DIAGNOSTIC(UnrecognizedCharacters, Error,",
          "content_same": false
        },
        {
          "line": 1262,
          "old_api": null,
          "new_api": "PrintToken",
          "old_text": null,
          "new_text": "PrintToken(output_stream, token, {})",
          "old_line_content": "  // Output the main chunk using one format string. We have to do the",
          "new_line_content": "  PrintToken(output_stream, token, {});",
          "content_same": false
        },
        {
          "line": 751,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "error_text.begin()",
          "old_line_content": "    // start-of-file always has trailing space because it *is* whitespace.",
          "new_line_content": "    emitter_.Emit(error_text.begin(), UnrecognizedCharacters);",
          "content_same": false
        },
        {
          "line": 753,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "error_text.size()",
          "old_line_content": "                       .has_trailing_space = true,",
          "new_line_content": "    current_column_ += error_text.size();",
          "content_same": false
        },
        {
          "line": 754,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "error_text.size()",
          "old_line_content": "                       .token_line = current_line_,",
          "new_line_content": "    source_text = source_text.drop_front(error_text.size());",
          "content_same": false
        },
        {
          "line": 1267,
          "old_api": null,
          "new_api": "GetTokenPrintWidths",
          "old_text": null,
          "new_text": "GetTokenPrintWidths(token)",
          "old_line_content": "      \"spelling: '{5}'\",",
          "new_line_content": "  widths.Widen(GetTokenPrintWidths(token));",
          "content_same": false
        },
        {
          "line": 1270,
          "old_api": null,
          "new_api": "GetTokenText",
          "old_text": null,
          "new_text": "GetTokenText(token)",
          "old_line_content": "                          widths.kind + 2),",
          "new_line_content": "  llvm::StringRef token_text = GetTokenText(token);",
          "content_same": false
        },
        {
          "line": 762,
          "old_api": null,
          "new_api": "AddToken",
          "old_text": null,
          "new_text": "buffer_->AddToken({.kind = TokenKind::StartOfFile,\n                       .has_trailing_space = true,\n                       .token_line = current_line_,\n                       .column = current_column_})",
          "old_line_content": "    // re-pin the last line to be the prior one so that diagnostics and editors",
          "new_line_content": "    buffer_->AddToken({.kind = TokenKind::StartOfFile,",
          "content_same": false
        },
        {
          "line": 1275,
          "old_api": null,
          "new_api": "name",
          "old_text": null,
          "new_text": "llvm::formatv(\n      \"    { index: {0}, kind: {1}, line: {2}, column: {3}, indent: {4}, \"\n      \"spelling: '{5}'\",\n      llvm::format_decimal(token_index, widths.index),\n      llvm::right_justify(llvm::formatv(\"'{0}'\", token_info.kind.name()).str(),\n                          widths.kind + 2),\n      llvm::format_decimal(GetLineNumber(token_info.token_line), widths.line),\n      llvm::format_decimal(GetColumnNumber(token), widths.column),\n      llvm::format_decimal(GetIndentColumnNumber(token_info.token_line),\n                           widths.indent),\n      token_text)",
          "old_line_content": "      token_text);",
          "new_line_content": "  output_stream << llvm::formatv(",
          "content_same": false
        },
        {
          "line": 1278,
          "old_api": null,
          "new_api": "llvm::format_decimal(token_index, widths.index)",
          "old_text": null,
          "new_text": "llvm::format_decimal(token_index, widths.index)",
          "old_line_content": "    case TokenKind::Identifier:",
          "new_line_content": "      llvm::format_decimal(token_index, widths.index),",
          "content_same": false
        },
        {
          "line": 1281,
          "old_api": null,
          "new_api": "GetLineNumber",
          "old_text": null,
          "new_text": "GetLineNumber(token_info.token_line)",
          "old_line_content": "    case TokenKind::IntegerLiteral:",
          "new_line_content": "      llvm::format_decimal(GetLineNumber(token_info.token_line), widths.line),",
          "content_same": false
        },
        {
          "line": 1282,
          "old_api": null,
          "new_api": "GetColumnNumber",
          "old_text": null,
          "new_text": "GetColumnNumber(token)",
          "old_line_content": "      output_stream << \", value: `\";",
          "new_line_content": "      llvm::format_decimal(GetColumnNumber(token), widths.column),",
          "content_same": false
        },
        {
          "line": 1289,
          "old_api": null,
          "new_api": "GetIdentifier",
          "old_text": null,
          "new_text": "GetIdentifier(token)",
          "old_line_content": "    case TokenKind::StringLiteral:",
          "new_line_content": "      output_stream << \", identifier: \" << GetIdentifier(token).index;",
          "content_same": false
        },
        {
          "line": 778,
          "old_api": null,
          "new_api": "GetPrevLine",
          "old_text": null,
          "new_text": "buffer_->GetPrevLine(current_line_)",
          "old_line_content": "",
          "new_line_content": "      current_line_ = buffer_->GetPrevLine(current_line_);",
          "content_same": false
        },
        {
          "line": 779,
          "old_api": null,
          "new_api": "GetLineInfo",
          "old_text": null,
          "new_text": "buffer_->GetLineInfo(current_line_)",
          "old_line_content": "    // Close any open groups. We do this after marking whitespace, it will",
          "new_line_content": "      current_line_info_ = &buffer_->GetLineInfo(current_line_);",
          "content_same": false
        },
        {
          "line": 1297,
          "old_api": null,
          "new_api": "GetRealLiteral",
          "old_text": null,
          "new_text": "GetRealLiteral(token)",
          "old_line_content": "        output_stream << \", opening_token: \"",
          "new_line_content": "      output_stream << \", value: `\" << GetRealLiteral(token) << \"`\";",
          "content_same": false
        },
        {
          "line": 787,
          "old_api": null,
          "new_api": "NoteWhitespace",
          "old_text": null,
          "new_text": "NoteWhitespace()",
          "old_line_content": "                       .column = current_column_});",
          "new_line_content": "    NoteWhitespace();",
          "content_same": false
        },
        {
          "line": 1300,
          "old_api": null,
          "new_api": "GetStringLiteral",
          "old_text": null,
          "new_text": "GetStringLiteral(token)",
          "old_line_content": "      break;",
          "new_line_content": "      output_stream << \", value: `\" << GetStringLiteral(token) << \"`\";",
          "content_same": false
        },
        {
          "line": 791,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "open_groups_.empty()",
          "old_line_content": "  // lexer methods. These are named static member functions so that they show up",
          "new_line_content": "    if (!open_groups_.empty()) {",
          "content_same": false
        },
        {
          "line": 792,
          "old_api": null,
          "new_api": "CloseInvalidOpenGroups",
          "old_text": null,
          "new_text": "CloseInvalidOpenGroups(TokenKind::Error)",
          "old_line_content": "  // helpfully in profiles and backtraces, but they tend to not contain the",
          "new_line_content": "      CloseInvalidOpenGroups(TokenKind::Error);",
          "content_same": false
        },
        {
          "line": 1303,
          "old_api": null,
          "new_api": "is_opening_symbol",
          "old_text": null,
          "new_text": "token_info.kind.is_opening_symbol()",
          "old_line_content": "  if (token_info.has_trailing_space) {",
          "new_line_content": "      if (token_info.kind.is_opening_symbol()) {",
          "content_same": false
        },
        {
          "line": 1305,
          "old_api": null,
          "new_api": "GetMatchedClosingToken",
          "old_text": null,
          "new_text": "GetMatchedClosingToken(token)",
          "old_line_content": "  }",
          "new_line_content": "                      << GetMatchedClosingToken(token).index;",
          "content_same": false
        },
        {
          "line": 795,
          "old_api": null,
          "new_api": "AddToken",
          "old_text": null,
          "new_text": "buffer_->AddToken({.kind = TokenKind::EndOfFile,\n                       .token_line = current_line_,\n                       .column = current_column_})",
          "old_line_content": "  // build efficient dispatch tables out of them. All of them end by doing a",
          "new_line_content": "    buffer_->AddToken({.kind = TokenKind::EndOfFile,",
          "content_same": false
        },
        {
          "line": 1306,
          "old_api": null,
          "new_api": "is_closing_symbol",
          "old_text": null,
          "new_text": "token_info.kind.is_closing_symbol()",
          "old_line_content": "  if (token_info.is_recovery) {",
          "new_line_content": "      } else if (token_info.kind.is_closing_symbol()) {",
          "content_same": false
        },
        {
          "line": 1308,
          "old_api": null,
          "new_api": "GetMatchedOpeningToken",
          "old_text": null,
          "new_text": "GetMatchedOpeningToken(token)",
          "old_line_content": "  }",
          "new_line_content": "                      << GetMatchedOpeningToken(token).index;",
          "content_same": false
        },
        {
          "line": 813,
          "old_api": null,
          "new_api": "LexEndOfFile",
          "old_text": null,
          "new_text": "lexer.LexEndOfFile(source_text)",
          "old_line_content": "  }",
          "new_line_content": "      lexer.LexEndOfFile(source_text);",
          "content_same": false
        },
        {
          "line": 1332,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "line_infos_.push_back(info)",
          "old_line_content": "}",
          "new_line_content": "  line_infos_.push_back(info);",
          "content_same": false
        },
        {
          "line": 821,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "static_cast<unsigned char>(\n        source_text.front())",
          "old_line_content": "    LexResult result = lexer.LexMethod(source_text);                          \\",
          "new_line_content": "    [[clang::musttail]] return DispatchTable[static_cast<unsigned char>(",
          "content_same": false
        },
        {
          "line": 822,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "source_text.front()",
          "old_line_content": "    CARBON_CHECK(result) << \"Failed to form a token!\";                        \\",
          "new_line_content": "        source_text.front())](lexer, source_text);",
          "content_same": false
        },
        {
          "line": 1333,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "line_infos_.size()",
          "old_line_content": "",
          "new_line_content": "  return Line(static_cast<int>(line_infos_.size()) - 1);",
          "content_same": false
        },
        {
          "line": 314,
          "old_api": null,
          "new_api": "drop_front",
          "old_text": null,
          "new_text": "source_text.drop_front(ws_count)",
          "old_line_content": "  auto LexCommentOrSlash(llvm::StringRef& source_text) -> void {",
          "new_line_content": "    source_text = source_text.drop_front(ws_count);",
          "content_same": false
        },
        {
          "line": 318,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "source_text.front()",
          "old_line_content": "    // max-munch rule -- if the next character is another `/` then we lex it as",
          "new_line_content": "    CARBON_DCHECK(source_text.front() == '\\n');",
          "content_same": false
        },
        {
          "line": 319,
          "old_api": null,
          "new_api": "NoteWhitespace",
          "old_text": null,
          "new_text": "NoteWhitespace()",
          "old_line_content": "    // a comment start. If it isn't, then we lex as a slash.",
          "new_line_content": "    NoteWhitespace();",
          "content_same": false
        },
        {
          "line": 1345,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "token_infos_.push_back(info)",
          "old_line_content": "    const char* loc) -> DiagnosticLocation {",
          "new_line_content": "  token_infos_.push_back(info);",
          "content_same": false
        },
        {
          "line": 1347,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "token_infos_.size()",
          "old_line_content": "      << \"location not within buffer\";",
          "new_line_content": "  return Token(static_cast<int>(token_infos_.size()) - 1);",
          "content_same": false
        },
        {
          "line": 325,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "source_text.front()",
          "old_line_content": "    // This code path should produce a token, make sure that happens.",
          "new_line_content": "    CARBON_DCHECK(source_text.front() == '/');",
          "content_same": false
        },
        {
          "line": 330,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "source_text.size()",
          "old_line_content": "  auto LexComment(llvm::StringRef& source_text) -> void {",
          "new_line_content": "    if (source_text.size() > 1 && source_text[1] == '/') {",
          "content_same": false
        },
        {
          "line": 1356,
          "old_api": null,
          "new_api": "text",
          "old_text": null,
          "new_text": "buffer_->source_->text()",
          "old_line_content": "",
          "new_line_content": "  CARBON_CHECK(StringRefContainsPointer(buffer_->source_->text(), loc))",
          "content_same": false
        },
        {
          "line": 336,
          "old_api": null,
          "new_api": "LexSymbolToken",
          "old_text": null,
          "new_text": "LexSymbolToken(source_text)",
          "old_line_content": "                        \"Trailing comments are not permitted.\");",
          "new_line_content": "    LexResult result = LexSymbolToken(source_text);",
          "content_same": false
        },
        {
          "line": 337,
          "old_api": null,
          "new_api": "CARBON_CHECK",
          "old_text": null,
          "new_text": "CARBON_CHECK(result)",
          "old_line_content": "",
          "new_line_content": "    CARBON_CHECK(result) << \"Failed to form a token!\";",
          "content_same": false
        },
        {
          "line": 1363,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "std::partition_point(\n      buffer_->line_infos_.begin(), buffer_->line_infos_.end(),\n      [offset](const LineInfo& line) { return line.start <= offset; })",
          "old_line_content": "",
          "new_line_content": "  const auto* line_it = std::partition_point(",
          "content_same": false
        },
        {
          "line": 1364,
          "old_api": null,
          "new_api": "end",
          "old_text": null,
          "new_text": "buffer_->line_infos_.end()",
          "old_line_content": "  // Start by grabbing the line from the buffer. If the line isn't fully lexed,",
          "new_line_content": "      buffer_->line_infos_.begin(), buffer_->line_infos_.end(),",
          "content_same": false
        },
        {
          "line": 341,
          "old_api": null,
          "new_api": "startswith",
          "old_text": null,
          "new_text": "source_text.startswith(\"//\")",
          "old_line_content": "    // The introducer '//' must be followed by whitespace or EOF.",
          "new_line_content": "    CARBON_DCHECK(source_text.startswith(\"//\"));",
          "content_same": false
        },
        {
          "line": 1371,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "buffer_->line_infos_.begin()",
          "old_line_content": "        << \"Currently we assume no unlexed newlines prior to the error column, \"",
          "new_line_content": "  int line_number = line_it - buffer_->line_infos_.begin();",
          "content_same": false
        },
        {
          "line": 348,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "source_text.begin()",
          "old_line_content": "",
          "new_line_content": "      emitter_.Emit(source_text.begin(), TrailingComment);",
          "content_same": false
        },
        {
          "line": 352,
          "old_api": null,
          "new_api": "IsSpace",
          "old_text": null,
          "new_text": "IsSpace(source_text[2])",
          "old_line_content": "    // This may be the end of the file in which case we immediately return.",
          "new_line_content": "    if (source_text.size() > 2 && !IsSpace(source_text[2])) {",
          "content_same": false
        },
        {
          "line": 1378,
          "old_api": null,
          "new_api": "substr",
          "old_text": null,
          "new_text": "buffer_->source_->text().substr(line_it->start, line_it->length)",
          "old_line_content": "    if (end_newline_pos != llvm::StringRef::npos) {",
          "new_line_content": "      buffer_->source_->text().substr(line_it->start, line_it->length);",
          "content_same": false
        },
        {
          "line": 355,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "source_text.begin()",
          "old_line_content": "      return;",
          "new_line_content": "      emitter_.Emit(source_text.begin() + 2,",
          "content_same": false
        },
        {
          "line": 1380,
          "old_api": null,
          "new_api": "take_front",
          "old_text": null,
          "new_text": "line.take_front(column_number).count('\\n')",
          "old_line_content": "    }",
          "new_line_content": "    CARBON_CHECK(line.take_front(column_number).count('\\n') == 0)",
          "content_same": false
        },
        {
          "line": 871,
          "old_api": null,
          "new_api": "CreateLines",
          "old_text": null,
          "new_text": "CreateLines(source_text)",
          "old_line_content": "  }",
          "new_line_content": "    CreateLines(source_text);",
          "content_same": false
        },
        {
          "line": 361,
          "old_api": null,
          "new_api": "drop_front",
          "old_text": null,
          "new_text": "source_text.drop_front(current_line_info_->length - current_column_)",
          "old_line_content": "  }",
          "new_line_content": "        source_text.drop_front(current_line_info_->length - current_column_);",
          "content_same": false
        },
        {
          "line": 873,
          "old_api": null,
          "new_api": "LexStartOfFile",
          "old_text": null,
          "new_text": "LexStartOfFile(source_text)",
          "old_line_content": " private:",
          "new_line_content": "    LexStartOfFile(source_text);",
          "content_same": false
        },
        {
          "line": 363,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "source_text.empty()",
          "old_line_content": "  auto LexNumericLiteral(llvm::StringRef& source_text) -> LexResult {",
          "new_line_content": "    if (source_text.empty()) {",
          "content_same": false
        },
        {
          "line": 1387,
          "old_api": null,
          "new_api": "find",
          "old_text": null,
          "new_text": "line.find('\\n', column_number)",
          "old_line_content": "}",
          "new_line_content": "    auto end_newline_pos = line.find('\\n', column_number);",
          "content_same": false
        },
        {
          "line": 877,
          "old_api": null,
          "new_api": "DispatchNext",
          "old_text": null,
          "new_text": "DispatchNext(*this, source_text)",
          "old_line_content": "",
          "new_line_content": "    DispatchNext(*this, source_text);",
          "content_same": false
        },
        {
          "line": 1389,
          "old_api": null,
          "new_api": "take_front",
          "old_text": null,
          "new_text": "line.take_front(end_newline_pos)",
          "old_line_content": "auto TokenLocationTranslator::GetLocation(Token token) -> DiagnosticLocation {",
          "new_line_content": "      line = line.take_front(end_newline_pos);",
          "content_same": false
        },
        {
          "line": 879,
          "old_api": null,
          "new_api": "empty",
          "old_text": null,
          "new_text": "source_text.empty()",
          "old_line_content": "  // correct lexer routine based on the first byte of source text.",
          "new_line_content": "    CARBON_CHECK(source_text.empty())",
          "content_same": false
        },
        {
          "line": 1393,
          "old_api": null,
          "new_api": "filename",
          "old_text": null,
          "new_text": "buffer_->source_->filename()",
          "old_line_content": "  const char* token_start =",
          "new_line_content": "  return {.file_name = buffer_->source_->filename(),",
          "content_same": false
        },
        {
          "line": 374,
          "old_api": null,
          "new_api": "NumericLiteral::Lex(source_text)",
          "old_text": null,
          "new_text": "NumericLiteral::Lex(source_text)",
          "old_line_content": "    if (!set_indent_) {",
          "new_line_content": "    std::optional<NumericLiteral> literal = NumericLiteral::Lex(source_text);",
          "content_same": false
        },
        {
          "line": 376,
          "old_api": null,
          "new_api": "LexError",
          "old_text": null,
          "new_text": "LexError(source_text)",
          "old_line_content": "      set_indent_ = true;",
          "new_line_content": "      return LexError(source_text);",
          "content_same": false
        },
        {
          "line": 1401,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "buffer_->GetTokenInfo(token)",
          "old_line_content": "}",
          "new_line_content": "  const auto& token_info = buffer_->GetTokenInfo(token);",
          "content_same": false
        },
        {
          "line": 1402,
          "old_api": null,
          "new_api": "GetLineInfo",
          "old_text": null,
          "new_text": "buffer_->GetLineInfo(token_info.token_line)",
          "old_line_content": "",
          "new_line_content": "  const auto& line_info = buffer_->GetLineInfo(token_info.token_line);",
          "content_same": false
        },
        {
          "line": 1404,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "buffer_->source_->text().begin()",
          "old_line_content": "",
          "new_line_content": "      buffer_->source_->text().begin() + line_info.start + token_info.column;",
          "content_same": false
        },
        {
          "line": 1409,
          "old_api": null,
          "new_api": "GetLocation",
          "old_text": null,
          "new_text": "TokenizedBuffer::SourceBufferLocationTranslator(buffer_).GetLocation(\n      token_start)",
          "old_line_content": "",
          "new_line_content": "  return TokenizedBuffer::SourceBufferLocationTranslator(buffer_).GetLocation(",
          "content_same": false
        },
        {
          "line": 389,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "VariantMatch(\n        literal->ComputeValue(emitter_),\n        [&](NumericLiteral::IntegerValue&& value) {\n          auto token = buffer_->AddToken({.kind = TokenKind::IntegerLiteral,\n                                          .token_line = current_line_,\n                                          .column = int_column});\n          buffer_->GetTokenInfo(token).literal_index =\n              buffer_->literal_int_storage_.size();\n          buffer_->literal_int_storage_.push_back(std::move(value.value));\n          return token;\n        },\n        [&](NumericLiteral::RealValue&& value) {\n          auto token = buffer_->AddToken({.kind = TokenKind::RealLiteral,\n                                          .token_line = current_line_,\n                                          .column = int_column});\n          buffer_->GetTokenInfo(token).literal_index =\n              buffer_->literal_int_storage_.size();\n          buffer_->literal_int_storage_.push_back(std::move(value.mantissa));\n          buffer_->literal_int_storage_.push_back(std::move(value.exponent));\n          CARBON_CHECK(buffer_->GetRealLiteral(token).is_decimal ==\n                       (value.radix == NumericLiteral::Radix::Decimal));\n          return token;\n        },\n        [&](NumericLiteral::UnrecoverableError) {\n          auto token = buffer_->AddToken({\n              .kind = TokenKind::Error,\n              .token_line = current_line_,\n              .column = int_column,\n              .error_length = token_size,\n          });\n          return token;\n        })",
          "old_line_content": "        },",
          "new_line_content": "    return VariantMatch(",
          "content_same": false
        },
        {
          "line": 390,
          "old_api": null,
          "new_api": "ComputeValue",
          "old_text": null,
          "new_text": "literal->ComputeValue(emitter_)",
          "old_line_content": "        [&](NumericLiteral::RealValue&& value) {",
          "new_line_content": "        literal->ComputeValue(emitter_),",
          "content_same": false
        },
        {
          "line": 392,
          "old_api": null,
          "new_api": "AddToken",
          "old_text": null,
          "new_text": "buffer_->AddToken({.kind = TokenKind::IntegerLiteral,\n                                          .token_line = current_line_,\n                                          .column = int_column})",
          "old_line_content": "                                          .token_line = current_line_,",
          "new_line_content": "          auto token = buffer_->AddToken({.kind = TokenKind::IntegerLiteral,",
          "content_same": false
        },
        {
          "line": 401,
          "old_api": null,
          "new_api": "AddToken",
          "old_text": null,
          "new_text": "buffer_->AddToken({.kind = TokenKind::RealLiteral,\n                                          .token_line = current_line_,\n                                          .column = int_column})",
          "old_line_content": "        },",
          "new_line_content": "          auto token = buffer_->AddToken({.kind = TokenKind::RealLiteral,",
          "content_same": false
        },
        {
          "line": 404,
          "old_api": null,
          "new_api": "GetTokenInfo",
          "old_text": null,
          "new_text": "buffer_->GetTokenInfo(token)",
          "old_line_content": "              .kind = TokenKind::Error,",
          "new_line_content": "          buffer_->GetTokenInfo(token).literal_index =",
          "content_same": false
        },
        {
          "line": 405,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "buffer_->literal_int_storage_.size()",
          "old_line_content": "              .token_line = current_line_,",
          "new_line_content": "              buffer_->literal_int_storage_.size();",
          "content_same": false
        },
        {
          "line": 406,
          "old_api": null,
          "new_api": "std::move(value.mantissa)",
          "old_text": null,
          "new_text": "std::move(value.mantissa)",
          "old_line_content": "              .column = int_column,",
          "new_line_content": "          buffer_->literal_int_storage_.push_back(std::move(value.mantissa));",
          "content_same": false
        },
        {
          "line": 407,
          "old_api": null,
          "new_api": "std::move(value.exponent)",
          "old_text": null,
          "new_text": "std::move(value.exponent)",
          "old_line_content": "              .error_length = token_size,",
          "new_line_content": "          buffer_->literal_int_storage_.push_back(std::move(value.exponent));",
          "content_same": false
        },
        {
          "line": 408,
          "old_api": null,
          "new_api": "GetRealLiteral",
          "old_text": null,
          "new_text": "buffer_->GetRealLiteral(token)",
          "old_line_content": "          });",
          "new_line_content": "          CARBON_CHECK(buffer_->GetRealLiteral(token).is_decimal ==",
          "content_same": false
        },
        {
          "line": 413,
          "old_api": null,
          "new_api": "AddToken",
          "old_text": null,
          "new_text": "buffer_->AddToken({\n              .kind = TokenKind::Error,\n              .token_line = current_line_,\n              .column = int_column,\n              .error_length = token_size,\n          })",
          "old_line_content": "  auto LexStringLiteral(llvm::StringRef& source_text) -> LexResult {",
          "new_line_content": "          auto token = buffer_->AddToken({",
          "content_same": false
        },
        {
          "line": 424,
          "old_api": null,
          "new_api": "StringLiteral::Lex(source_text)",
          "old_text": null,
          "new_text": "StringLiteral::Lex(source_text)",
          "old_line_content": "    if (!set_indent_) {",
          "new_line_content": "    std::optional<StringLiteral> literal = StringLiteral::Lex(source_text);",
          "content_same": false
        },
        {
          "line": 426,
          "old_api": null,
          "new_api": "LexError",
          "old_text": null,
          "new_text": "LexError(source_text)",
          "old_line_content": "      set_indent_ = true;",
          "new_line_content": "      return LexError(source_text);",
          "content_same": false
        },
        {
          "line": 431,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "literal->text().size()",
          "old_line_content": "      current_column_ += literal_size;",
          "new_line_content": "    int literal_size = literal->text().size();",
          "content_same": false
        },
        {
          "line": 432,
          "old_api": null,
          "new_api": "drop_front",
          "old_text": null,
          "new_text": "source_text.drop_front(literal_size)",
          "old_line_content": "    } else {",
          "new_line_content": "    source_text = source_text.drop_front(literal_size);",
          "content_same": false
        },
        {
          "line": 440,
          "old_api": null,
          "new_api": "is_multi_line",
          "old_text": null,
          "new_text": "literal->is_multi_line()",
          "old_line_content": "        } else {",
          "new_line_content": "    if (!literal->is_multi_line()) {",
          "content_same": false
        },
        {
          "line": 443,
          "old_api": null,
          "new_api": "text",
          "old_text": null,
          "new_text": "literal->text()",
          "old_line_content": "      }",
          "new_line_content": "      for (char c : literal->text()) {",
          "content_same": false
        },
        {
          "line": 445,
          "old_api": null,
          "new_api": "HandleNewline",
          "old_text": null,
          "new_text": "HandleNewline()",
          "old_line_content": "",
          "new_line_content": "          HandleNewline();",
          "content_same": false
        },
        {
          "line": 456,
          "old_api": null,
          "new_api": "is_terminated",
          "old_text": null,
          "new_text": "literal->is_terminated()",
          "old_line_content": "    } else {",
          "new_line_content": "    if (literal->is_terminated()) {",
          "content_same": false
        },
        {
          "line": 458,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "buffer_->AddToken({.kind = TokenKind::StringLiteral,\n                             .token_line = string_line,\n                             .column = string_column,\n                             .literal_index = static_cast<int32_t>(\n                                 buffer_->literal_string_storage_.size())})",
          "old_line_content": "                        \"String is missing a terminator.\");",
          "new_line_content": "          buffer_->AddToken({.kind = TokenKind::StringLiteral,",
          "content_same": false
        },
        {
          "line": 461,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "static_cast<int32_t>(\n                                 buffer_->literal_string_storage_.size())",
          "old_line_content": "                                .token_line = string_line,",
          "new_line_content": "                             .literal_index = static_cast<int32_t>(",
          "content_same": false
        },
        {
          "line": 462,
          "old_api": null,
          "new_api": "size",
          "old_text": null,
          "new_text": "buffer_->literal_string_storage_.size()",
          "old_line_content": "                                .column = string_column,",
          "new_line_content": "                                 buffer_->literal_string_storage_.size())});",
          "content_same": false
        },
        {
          "line": 463,
          "old_api": null,
          "new_api": "push_back",
          "old_text": null,
          "new_text": "buffer_->literal_string_storage_.push_back(\n          literal->ComputeValue(emitter_))",
          "old_line_content": "                                .error_length = literal_size});",
          "new_line_content": "      buffer_->literal_string_storage_.push_back(",
          "content_same": false
        },
        {
          "line": 464,
          "old_api": null,
          "new_api": "ComputeValue",
          "old_text": null,
          "new_text": "literal->ComputeValue(emitter_)",
          "old_line_content": "    }",
          "new_line_content": "          literal->ComputeValue(emitter_));",
          "content_same": false
        },
        {
          "line": 467,
          "old_api": null,
          "new_api": "CARBON_DIAGNOSTIC",
          "old_text": null,
          "new_text": "CARBON_DIAGNOSTIC(UnterminatedString, Error,\n                        \"String is missing a terminator.\")",
          "old_line_content": "  auto LexOneCharSymbolToken(llvm::StringRef& source_text, TokenKind kind)",
          "new_line_content": "      CARBON_DIAGNOSTIC(UnterminatedString, Error,",
          "content_same": false
        },
        {
          "line": 469,
          "old_api": null,
          "new_api": "begin",
          "old_text": null,
          "new_text": "literal->text().begin()",
          "old_line_content": "    // Verify in a debug build that the incoming token kind is correct.",
          "new_line_content": "      emitter_.Emit(literal->text().begin(), UnterminatedString);",
          "content_same": false
        },
        {
          "line": 480,
          "old_api": null,
          "new_api": "CARBON_DCHECK",
          "old_text": null,
          "new_text": "CARBON_DCHECK(kind != TokenKind::Error)",
          "old_line_content": "    }",
          "new_line_content": "    CARBON_DCHECK(kind != TokenKind::Error);",
          "content_same": false
        },
        {
          "line": 481,
          "old_api": null,
          "new_api": "fixed_spelling",
          "old_text": null,
          "new_text": "kind.fixed_spelling().size()",
          "old_line_content": "",
          "new_line_content": "    CARBON_DCHECK(kind.fixed_spelling().size() == 1);",
          "content_same": false
        },
        {
          "line": 483,
          "old_api": null,
          "new_api": "front",
          "old_text": null,
          "new_text": "source_text.front()",
          "old_line_content": "        {.kind = kind, .token_line = current_line_, .column = current_column_});",
          "new_line_content": "        << \"Source text starts with '\" << source_text.front()",
          "content_same": false
        },
        {
          "line": 484,
          "old_api": null,
          "new_api": "fixed_spelling",
          "old_text": null,
          "new_text": "kind.fixed_spelling()",
          "old_line_content": "    ++current_column_;",
          "new_line_content": "        << \"' instead of the spelling '\" << kind.fixed_spelling()",
          "content_same": false
        },
        {
          "line": 495,
          "old_api": null,
          "new_api": "drop_front",
          "old_text": null,
          "new_text": "source_text.drop_front()",
          "old_line_content": "",
          "new_line_content": "    source_text = source_text.drop_front();",
          "content_same": false
        },
        {
          "line": 1007,
          "old_api": null,
          "new_api": "constexpr",
          "old_text": null,
          "new_text": "constexpr",
          "old_line_content": "#define CARBON_CLOSING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, OpeningName) \\",
          "new_line_content": "constexpr TokenizedBuffer::Lexer::DispatchTableT",
          "content_same": false
        },
        {
          "line": 1008,
          "old_api": null,
          "new_api": "MakeDispatchTable",
          "old_text": null,
          "new_text": "MakeDispatchTable()",
          "old_line_content": "  table[(Spelling)[0]] = TokenKind::TokenName;",
          "new_line_content": "    TokenizedBuffer::Lexer::DispatchTable = MakeDispatchTable();",
          "content_same": false
        },
        {
          "line": 1010,
          "old_api": null,
          "new_api": "constexpr",
          "old_text": null,
          "new_text": "constexpr",
          "old_line_content": "      return table;",
          "new_line_content": "constexpr std::array<TokenKind, 256>",
          "content_same": false
        },
        {
          "line": 1011,
          "old_api": null,
          "new_api": "[] {\n      std::array<TokenKind, 256> table = {};\n#define CARBON_ONE_CHAR_SYMBOL_TOKEN(TokenName, Spelling) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#define CARBON_OPENING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, ClosingName) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#define CARBON_CLOSING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, OpeningName) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#include \"toolchain/lex/token_kind.def\"\n      return table;\n    }()",
          "old_text": null,
          "new_text": "[] {\n      std::array<TokenKind, 256> table = {};\n#define CARBON_ONE_CHAR_SYMBOL_TOKEN(TokenName, Spelling) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#define CARBON_OPENING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, ClosingName) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#define CARBON_CLOSING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, OpeningName) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#include \"toolchain/lex/token_kind.def\"\n      return table;\n    }()",
          "old_line_content": "    }();",
          "new_line_content": "    TokenizedBuffer::Lexer::OneCharTokenKindTable = [] {",
          "content_same": false
        },
        {
          "line": 501,
          "old_api": null,
          "new_api": "LexOneCharSymbolToken",
          "old_text": null,
          "new_text": "LexOneCharSymbolToken(source_text, kind)",
          "old_line_content": "          \"Closing symbol without a corresponding opening symbol.\");",
          "new_line_content": "    Token token = LexOneCharSymbolToken(source_text, kind);",
          "content_same": false
        },
        {
          "line": 509,
          "old_api": null,
          "new_api": "CARBON_DIAGNOSTIC",
          "old_text": null,
          "new_text": "CARBON_DIAGNOSTIC(\n          UnmatchedClosing, Error,\n          \"Closing symbol without a corresponding opening symbol.\")",
          "old_line_content": "      return token;",
          "new_line_content": "      CARBON_DIAGNOSTIC(",
          "content_same": false
        }
      ],
      "deletions": [
        {
          "line": 514,
          "old_api": "unmatched_error",
          "new_api": null,
          "old_text": "unmatched_error()",
          "new_text": null,
          "old_line_content": "      return unmatched_error();",
          "new_line_content": "                                       .token_line = current_line_,",
          "content_same": false
        },
        {
          "line": 517,
          "old_api": "back",
          "new_api": null,
          "old_text": "open_groups_.back()",
          "new_text": null,
          "old_line_content": "    Token opening_token = open_groups_.back();",
          "new_line_content": "      ++current_column_;",
          "content_same": false
        },
        {
          "line": 519,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "buffer_->GetTokenInfo(opening_token)",
          "new_text": null,
          "old_line_content": "    if (LLVM_UNLIKELY(buffer_->GetTokenInfo(opening_token).kind !=",
          "new_line_content": "      return token;",
          "content_same": false
        },
        {
          "line": 520,
          "old_api": "opening_symbol",
          "new_api": null,
          "old_text": "kind.opening_symbol()",
          "new_text": null,
          "old_line_content": "                      kind.opening_symbol())) {",
          "new_line_content": "    };",
          "content_same": false
        },
        {
          "line": 521,
          "old_api": "CloseInvalidOpenGroups",
          "new_api": null,
          "old_text": "CloseInvalidOpenGroups(kind)",
          "new_text": null,
          "old_line_content": "      CloseInvalidOpenGroups(kind);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1034,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  return GetTokenInfo(token).token_line;",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 1020,
          "old_api": "Dispatch",
          "new_api": null,
          "old_text": "lexer.Dispatch(source_text)",
          "new_text": null,
          "old_line_content": "  lexer.Dispatch(source_text);",
          "new_line_content": "      return table;",
          "content_same": false
        },
        {
          "line": 526,
          "old_api": "back",
          "new_api": null,
          "old_text": "open_groups_.back()",
          "new_text": null,
          "old_line_content": "      opening_token = open_groups_.back();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1038,
          "old_api": "GetLine",
          "new_api": null,
          "old_text": "GetLine(token)",
          "new_text": null,
          "old_line_content": "  return GetLineNumber(GetLine(token));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 528,
          "old_api": "opening_symbol",
          "new_api": null,
          "old_text": "kind.opening_symbol()",
          "new_text": null,
          "old_line_content": "                    kind.opening_symbol());",
          "new_line_content": "    // Close any invalid open groups first.",
          "content_same": false
        },
        {
          "line": 1042,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  return GetTokenInfo(token).column + 1;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1046,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1047,
          "old_api": "fixed_spelling",
          "new_api": null,
          "old_text": "token_info.kind.fixed_spelling()",
          "new_text": null,
          "old_line_content": "  llvm::StringRef fixed_spelling = token_info.kind.fixed_spelling();",
          "new_line_content": "auto TokenizedBuffer::GetLineNumber(Token token) const -> int {",
          "content_same": false
        },
        {
          "line": 1053,
          "old_api": "GetLineInfo",
          "new_api": null,
          "old_text": "GetLineInfo(token_info.token_line)",
          "new_text": null,
          "old_line_content": "    const auto& line_info = GetLineInfo(token_info.token_line);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1055,
          "old_api": "substr",
          "new_api": null,
          "old_text": "source_->text().substr(token_start, token_info.error_length)",
          "new_text": null,
          "old_line_content": "    return source_->text().substr(token_start, token_info.error_length);",
          "new_line_content": "auto TokenizedBuffer::GetTokenText(Token token) const -> llvm::StringRef {",
          "content_same": false
        },
        {
          "line": 546,
          "old_api": "llvm::StringSwitch<TokenKind>(source_text)",
          "new_api": null,
          "old_text": "llvm::StringSwitch<TokenKind>(source_text)",
          "new_text": null,
          "old_line_content": "    TokenKind kind = llvm::StringSwitch<TokenKind>(source_text)",
          "new_line_content": "    // open token would invalidate any pointers.",
          "content_same": false
        },
        {
          "line": 1062,
          "old_api": "GetLineInfo",
          "new_api": null,
          "old_text": "GetLineInfo(token_info.token_line)",
          "new_text": null,
          "old_line_content": "    const auto& line_info = GetLineInfo(token_info.token_line);",
          "new_line_content": "  if (token_info.kind == TokenKind::Error) {",
          "content_same": false
        },
        {
          "line": 553,
          "old_api": "Default",
          "new_api": null,
          "old_text": "                         .Default(TokenKind::Error)",
          "new_text": null,
          "old_line_content": "                         .Default(TokenKind::Error);",
          "new_line_content": "  auto LexSymbolToken(llvm::StringRef& source_text) -> LexResult {",
          "content_same": false
        },
        {
          "line": 1066,
          "old_api": "CARBON_CHECK",
          "new_api": null,
          "old_text": "CARBON_CHECK(relexed_token)",
          "new_text": null,
          "old_line_content": "    CARBON_CHECK(relexed_token) << \"Could not reform numeric literal token.\";",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 555,
          "old_api": "LexError",
          "new_api": null,
          "old_text": "LexError(source_text)",
          "new_text": null,
          "old_line_content": "      return LexError(source_text);",
          "new_line_content": "    // dispatch. We only lex the multi-character tokens here.",
          "content_same": false
        },
        {
          "line": 1067,
          "old_api": "text",
          "new_api": null,
          "old_text": "relexed_token->text()",
          "new_text": null,
          "old_line_content": "    return relexed_token->text();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1073,
          "old_api": "GetLineInfo",
          "new_api": null,
          "old_text": "GetLineInfo(token_info.token_line)",
          "new_text": null,
          "old_line_content": "    const auto& line_info = GetLineInfo(token_info.token_line);",
          "new_line_content": "    int64_t token_start = line_info.start + token_info.column;",
          "content_same": false
        },
        {
          "line": 566,
          "old_api": "fixed_spelling",
          "new_api": null,
          "old_text": "kind.fixed_spelling().size()",
          "new_text": null,
          "old_line_content": "    source_text = source_text.drop_front(kind.fixed_spelling().size());",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1078,
          "old_api": "text",
          "new_api": null,
          "old_text": "relexed_token->text()",
          "new_text": null,
          "old_line_content": "    return relexed_token->text();",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 1084,
          "old_api": "GetLineInfo",
          "new_api": null,
          "old_text": "GetLineInfo(token_info.token_line)",
          "new_text": null,
          "old_line_content": "    const auto& line_info = GetLineInfo(token_info.token_line);",
          "new_line_content": "    int64_t token_start = line_info.start + token_info.column;",
          "content_same": false
        },
        {
          "line": 574,
          "old_api": "size",
          "new_api": null,
          "old_text": "word.size()",
          "new_text": null,
          "old_line_content": "    if (word.size() < 2) {",
          "new_line_content": "        {.kind = kind, .token_line = current_line_, .column = current_column_});",
          "content_same": false
        },
        {
          "line": 580,
          "old_api": "LexResult::NoMatch()",
          "new_api": null,
          "old_text": "LexResult::NoMatch()",
          "new_text": null,
          "old_line_content": "      return LexResult::NoMatch();",
          "new_line_content": "  // Given a word that has already been lexed, determine whether it is a type",
          "content_same": false
        },
        {
          "line": 1096,
          "old_api": "CARBON_CHECK",
          "new_api": null,
          "old_text": "CARBON_CHECK(token_info.kind == TokenKind::Identifier)",
          "new_text": null,
          "old_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::Identifier) << token_info.kind;",
          "new_line_content": "    llvm::StringRef suffix =",
          "content_same": false
        },
        {
          "line": 1101,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "new_line_content": "  if (token_info.kind == TokenKind::StartOfFile ||",
          "content_same": false
        },
        {
          "line": 1102,
          "old_api": "CARBON_CHECK",
          "new_api": null,
          "old_text": "CARBON_CHECK(token_info.kind == TokenKind::Identifier)",
          "new_text": null,
          "old_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::Identifier) << token_info.kind;",
          "new_line_content": "      token_info.kind == TokenKind::EndOfFile) {",
          "content_same": false
        },
        {
          "line": 595,
          "old_api": "LexResult::NoMatch()",
          "new_api": null,
          "old_text": "LexResult::NoMatch()",
          "new_text": null,
          "old_line_content": "        return LexResult::NoMatch();",
          "new_line_content": "      case 'i':",
          "content_same": false
        },
        {
          "line": 1108,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1109,
          "old_api": "CARBON_CHECK",
          "new_api": null,
          "old_text": "CARBON_CHECK(token_info.kind == TokenKind::IntegerLiteral)",
          "new_text": null,
          "old_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::IntegerLiteral) << token_info.kind;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 598,
          "old_api": "substr",
          "new_api": null,
          "old_text": "word.substr(1)",
          "new_text": null,
          "old_line_content": "    llvm::StringRef suffix = word.substr(1);",
          "new_line_content": "      case 'u':",
          "content_same": false
        },
        {
          "line": 599,
          "old_api": "CanLexInteger",
          "new_api": null,
          "old_text": "CanLexInteger(emitter_, suffix)",
          "new_text": null,
          "old_line_content": "    if (!CanLexInteger(emitter_, suffix)) {",
          "new_line_content": "        kind = TokenKind::UnsignedIntegerTypeLiteral;",
          "content_same": false
        },
        {
          "line": 600,
          "old_api": "size",
          "new_api": null,
          "old_text": "buffer_->AddToken(\n          {.kind = TokenKind::Error,\n           .token_line = current_line_,\n           .column = column,\n           .error_length = static_cast<int32_t>(word.size())})",
          "new_text": null,
          "old_line_content": "      return buffer_->AddToken(",
          "new_line_content": "        break;",
          "content_same": false
        },
        {
          "line": 1114,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1115,
          "old_api": "CARBON_CHECK",
          "new_api": null,
          "old_text": "CARBON_CHECK(token_info.kind == TokenKind::RealLiteral)",
          "new_text": null,
          "old_line_content": "  CARBON_CHECK(token_info.kind == TokenKind::RealLiteral) << token_info.kind;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 604,
          "old_api": "size",
          "new_api": null,
          "old_text": "word.size()",
          "new_text": null,
          "old_line_content": "           .error_length = static_cast<int32_t>(word.size())});",
          "new_line_content": "      default:",
          "content_same": false
        },
        {
          "line": 607,
          "old_api": "getAsInteger",
          "new_api": null,
          "old_text": "suffix.getAsInteger(10, suffix_value)",
          "new_text": null,
          "old_line_content": "    if (suffix.getAsInteger(10, suffix_value)) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1120,
          "old_api": "GetLineInfo",
          "new_api": null,
          "old_text": "GetLineInfo(token_info.token_line)",
          "new_text": null,
          "old_line_content": "  const auto& line_info = GetLineInfo(token_info.token_line);",
          "new_line_content": "  return literal_int_storage_[token_info.literal_index];",
          "content_same": false
        },
        {
          "line": 1122,
          "old_api": "text",
          "new_api": null,
          "old_text": "source_->text()",
          "new_text": null,
          "old_line_content": "  char second_char = source_->text()[token_start + 1];",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 611,
          "old_api": "AddToken",
          "new_api": null,
          "old_text": "buffer_->AddToken(\n        {.kind = *kind, .token_line = current_line_, .column = column})",
          "new_text": null,
          "old_line_content": "    auto token = buffer_->AddToken(",
          "new_line_content": "          {.kind = TokenKind::Error,",
          "content_same": false
        },
        {
          "line": 613,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "buffer_->GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "    buffer_->GetTokenInfo(token).literal_index =",
          "new_line_content": "           .column = column,",
          "content_same": false
        },
        {
          "line": 615,
          "old_api": "std::move(suffix_value)",
          "new_api": null,
          "old_text": "std::move(suffix_value)",
          "new_text": null,
          "old_line_content": "    buffer_->literal_int_storage_.push_back(std::move(suffix_value));",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1131,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "new_line_content": "  int64_t token_start = line_info.start + token_info.column;",
          "content_same": false
        },
        {
          "line": 622,
          "old_api": "is_closing_symbol",
          "new_api": null,
          "old_text": "kind.is_closing_symbol()",
          "new_text": null,
          "old_line_content": "    CARBON_CHECK(kind.is_closing_symbol() || kind == TokenKind::Error);",
          "new_line_content": "        {.kind = *kind, .token_line = current_line_, .column = column});",
          "content_same": false
        },
        {
          "line": 626,
          "old_api": "back",
          "new_api": null,
          "old_text": "open_groups_.back()",
          "new_text": null,
          "old_line_content": "      Token opening_token = open_groups_.back();",
          "new_line_content": "    return token;",
          "content_same": false
        },
        {
          "line": 627,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "buffer_->GetTokenInfo(opening_token)",
          "new_text": null,
          "old_line_content": "      TokenKind opening_kind = buffer_->GetTokenInfo(opening_token).kind;",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 628,
          "old_api": "closing_symbol",
          "new_api": null,
          "old_text": "opening_kind.closing_symbol()",
          "new_text": null,
          "old_line_content": "      if (kind == opening_kind.closing_symbol()) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1138,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 1139,
          "old_api": "is_sized_type_literal",
          "new_api": null,
          "old_text": "token_info.kind.is_sized_type_literal()",
          "new_text": null,
          "old_line_content": "  CARBON_CHECK(token_info.kind.is_sized_type_literal()) << token_info.kind;",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1145,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(opening_token)",
          "new_text": null,
          "old_line_content": "  const auto& opening_token_info = GetTokenInfo(opening_token);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1146,
          "old_api": "is_opening_symbol",
          "new_api": null,
          "old_text": "opening_token_info.kind.is_opening_symbol()",
          "new_text": null,
          "old_line_content": "  CARBON_CHECK(opening_token_info.kind.is_opening_symbol())",
          "new_line_content": "auto TokenizedBuffer::GetTypeLiteralSize(Token token) const",
          "content_same": false
        },
        {
          "line": 640,
          "old_api": "end",
          "new_api": null,
          "old_text": "buffer_->tokens().end()",
          "new_text": null,
          "old_line_content": "      Token prev_token = buffer_->tokens().end()[-1];",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 1153,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(closing_token)",
          "new_text": null,
          "old_line_content": "  const auto& closing_token_info = GetTokenInfo(closing_token);",
          "new_line_content": "auto TokenizedBuffer::GetMatchedClosingToken(Token opening_token) const",
          "content_same": false
        },
        {
          "line": 1154,
          "old_api": "is_closing_symbol",
          "new_api": null,
          "old_text": "closing_token_info.kind.is_closing_symbol()",
          "new_text": null,
          "old_line_content": "  CARBON_CHECK(closing_token_info.kind.is_closing_symbol())",
          "new_line_content": "    -> Token {",
          "content_same": false
        },
        {
          "line": 644,
          "old_api": "closing_symbol",
          "new_api": null,
          "old_text": "buffer_->AddToken(\n          {.kind = opening_kind.closing_symbol(),\n           .has_trailing_space = buffer_->HasTrailingWhitespace(prev_token),\n           .is_recovery = true,\n           .token_line = current_line_,\n           .column = current_column_})",
          "new_text": null,
          "old_line_content": "      Token closing_token = buffer_->AddToken(",
          "new_line_content": "          MismatchedClosing, Error,",
          "content_same": false
        },
        {
          "line": 645,
          "old_api": "closing_symbol",
          "new_api": null,
          "old_text": "opening_kind.closing_symbol()",
          "new_text": null,
          "old_line_content": "          {.kind = opening_kind.closing_symbol(),",
          "new_line_content": "          \"Closing symbol does not match most recent opening symbol.\");",
          "content_same": false
        },
        {
          "line": 1160,
          "old_api": "TokenIterator",
          "new_api": null,
          "old_text": "TokenIterator(token)",
          "new_text": null,
          "old_line_content": "  auto it = TokenIterator(token);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1161,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(*(it - 1))",
          "new_text": null,
          "old_line_content": "  return it == tokens().begin() || GetTokenInfo(*(it - 1)).has_trailing_space;",
          "new_line_content": "auto TokenizedBuffer::GetMatchedOpeningToken(Token closing_token) const",
          "content_same": false
        },
        {
          "line": 651,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "buffer_->GetTokenInfo(closing_token)",
          "new_text": null,
          "old_line_content": "      TokenInfo& closing_token_info = buffer_->GetTokenInfo(closing_token);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1165,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  return GetTokenInfo(token).has_trailing_space;",
          "new_line_content": "      << closing_token_info.kind;",
          "content_same": false
        },
        {
          "line": 1169,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  return GetTokenInfo(token).is_recovery;",
          "new_line_content": "auto TokenizedBuffer::HasLeadingWhitespace(Token token) const -> bool {",
          "content_same": false
        },
        {
          "line": 658,
          "old_api": "insert",
          "new_api": null,
          "old_text": "buffer_->identifier_map_.insert(\n        {text, Identifier(buffer_->identifier_infos_.size())})",
          "new_text": null,
          "old_line_content": "    auto insert_result = buffer_->identifier_map_.insert(",
          "new_line_content": "           .token_line = current_line_,",
          "content_same": false
        },
        {
          "line": 659,
          "old_api": "size",
          "new_api": null,
          "old_text": "buffer_->identifier_infos_.size()",
          "new_text": null,
          "old_line_content": "        {text, Identifier(buffer_->identifier_infos_.size())});",
          "new_line_content": "           .column = current_column_});",
          "content_same": false
        },
        {
          "line": 1178,
          "old_api": "size",
          "new_api": null,
          "old_text": "line_infos_.size()",
          "new_text": null,
          "old_line_content": "  CARBON_DCHECK(static_cast<size_t>(next.index) < line_infos_.size());",
          "new_line_content": "auto TokenizedBuffer::IsRecoveryToken(Token token) const -> bool {",
          "content_same": false
        },
        {
          "line": 667,
          "old_api": "front",
          "new_api": null,
          "old_text": "source_text.front()",
          "new_text": null,
          "old_line_content": "    if (static_cast<unsigned char>(source_text.front()) > 0x7F) {",
          "new_line_content": "  auto GetOrCreateIdentifier(llvm::StringRef text) -> Identifier {",
          "content_same": false
        },
        {
          "line": 1183,
          "old_api": "CARBON_CHECK",
          "new_api": null,
          "old_text": "CARBON_CHECK(line.index > 0)",
          "new_text": null,
          "old_line_content": "  CARBON_CHECK(line.index > 0);",
          "new_line_content": "  return line.index + 1;",
          "content_same": false
        },
        {
          "line": 1184,
          "old_api": "Line",
          "new_api": null,
          "old_text": "Line(line.index - 1)",
          "new_text": null,
          "old_line_content": "  return Line(line.index - 1);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 680,
          "old_api": "empty",
          "new_api": null,
          "old_text": "identifier_text.empty()",
          "new_text": null,
          "old_line_content": "    CARBON_CHECK(!identifier_text.empty())",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 683,
          "old_api": "size",
          "new_api": null,
          "old_text": "identifier_text.size()",
          "new_text": null,
          "old_line_content": "    current_column_ += identifier_text.size();",
          "new_line_content": "    if (!set_indent_) {",
          "content_same": false
        },
        {
          "line": 684,
          "old_api": "size",
          "new_api": null,
          "old_text": "identifier_text.size()",
          "new_text": null,
          "old_line_content": "    source_text = source_text.drop_front(identifier_text.size());",
          "new_line_content": "      current_line_info_->indent = current_column_;",
          "content_same": false
        },
        {
          "line": 1197,
          "old_api": "std::max(widths.index, index)",
          "new_api": null,
          "old_text": "std::max(widths.index, index)",
          "new_text": null,
          "old_line_content": "  index = std::max(widths.index, index);",
          "new_line_content": "auto TokenizedBuffer::GetIndentColumnNumber(Line line) const -> int {",
          "content_same": false
        },
        {
          "line": 1199,
          "old_api": "std::max(widths.column, column)",
          "new_api": null,
          "old_text": "std::max(widths.column, column)",
          "new_text": null,
          "old_line_content": "  column = std::max(widths.column, column);",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 688,
          "old_api": "LexWordAsTypeLiteralToken",
          "new_api": null,
          "old_text": "LexWordAsTypeLiteralToken(identifier_text, identifier_column)",
          "new_text": null,
          "old_line_content": "            LexWordAsTypeLiteralToken(identifier_text, identifier_column)) {",
          "new_line_content": "    // Take the valid characters off the front of the source buffer.",
          "content_same": false
        },
        {
          "line": 1200,
          "old_api": "std::max(widths.line, line)",
          "new_api": null,
          "old_text": "std::max(widths.line, line)",
          "new_text": null,
          "old_line_content": "  line = std::max(widths.line, line);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1201,
          "old_api": "std::max(widths.indent, indent)",
          "new_api": null,
          "old_text": "std::max(widths.indent, indent)",
          "new_text": null,
          "old_line_content": "  indent = std::max(widths.indent, indent);",
          "new_line_content": "auto TokenizedBuffer::GetIdentifierText(Identifier identifier) const",
          "content_same": false
        },
        {
          "line": 696,
          "old_api": "Default",
          "new_api": null,
          "old_text": "                         .Default(TokenKind::Error)",
          "new_text": null,
          "old_line_content": "                         .Default(TokenKind::Error);",
          "new_line_content": "    // Check if the text is a type literal, and if so form such a literal.",
          "content_same": false
        },
        {
          "line": 1215,
          "old_api": "std::log10(number)",
          "new_api": null,
          "old_text": "std::log10(number)",
          "new_text": null,
          "old_line_content": "  return static_cast<int>(std::log10(number)) + 1;",
          "new_line_content": "// the number of digits needed is is one more than the log-base-10 of the",
          "content_same": false
        },
        {
          "line": 704,
          "old_api": "AddToken",
          "new_api": null,
          "old_text": "buffer_->AddToken({.kind = TokenKind::Identifier,\n                              .token_line = current_line_,\n                              .column = identifier_column,\n                              .id = GetOrCreateIdentifier(identifier_text)})",
          "new_text": null,
          "old_line_content": "    return buffer_->AddToken({.kind = TokenKind::Identifier,",
          "new_line_content": "#define CARBON_KEYWORD_TOKEN(Name, Spelling) .Case(Spelling, TokenKind::Name)",
          "content_same": false
        },
        {
          "line": 707,
          "old_api": "GetOrCreateIdentifier",
          "new_api": null,
          "old_text": "GetOrCreateIdentifier(identifier_text)",
          "new_text": null,
          "old_line_content": "                              .id = GetOrCreateIdentifier(identifier_text)});",
          "new_line_content": "    if (kind != TokenKind::Error) {",
          "content_same": false
        },
        {
          "line": 1221,
          "old_api": "name",
          "new_api": null,
          "old_text": "GetKind(token).name().size()",
          "new_text": null,
          "old_line_content": "  widths.kind = GetKind(token).name().size();",
          "new_line_content": "  if (number == 0) {",
          "content_same": false
        },
        {
          "line": 1222,
          "old_api": "GetLineNumber",
          "new_api": null,
          "old_text": "GetLineNumber(token)",
          "new_text": null,
          "old_line_content": "  widths.line = ComputeDecimalPrintedWidth(GetLineNumber(token));",
          "new_line_content": "    return 1;",
          "content_same": false
        },
        {
          "line": 711,
          "old_api": "take_while",
          "new_api": null,
          "old_text": "source_text.take_while([](char c) {\n      if (IsAlnum(c)) {\n        return false;\n      }\n      switch (c) {\n        case '_':\n        case '\\t':\n        case '\\n':\n          return false;\n        default:\n          break;\n      }\n      return llvm::StringSwitch<bool>(llvm::StringRef(&c, 1))\n#define CARBON_SYMBOL_TOKEN(Name, Spelling) .StartsWith(Spelling, false)\n#include \"toolchain/lex/token_kind.def\"\n          .Default(true);\n    })",
          "new_text": null,
          "old_line_content": "    llvm::StringRef error_text = source_text.take_while([](char c) {",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 712,
          "old_api": "IsAlnum",
          "new_api": null,
          "old_text": "IsAlnum(c)",
          "new_text": null,
          "old_line_content": "      if (IsAlnum(c)) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1223,
          "old_api": "GetColumnNumber",
          "new_api": null,
          "old_text": "GetColumnNumber(token)",
          "new_text": null,
          "old_line_content": "  widths.column = ComputeDecimalPrintedWidth(GetColumnNumber(token));",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 1234,
          "old_api": "filename",
          "new_api": null,
          "old_text": "source_->filename()",
          "new_text": null,
          "old_line_content": "  output_stream << \"- filename: \" << source_->filename() << \"\\n\"",
          "new_line_content": "  widths.indent =",
          "content_same": false
        },
        {
          "line": 723,
          "old_api": "llvm::StringRef(&c, 1)",
          "new_api": null,
          "old_text": "llvm::StringRef(&c, 1)",
          "new_text": null,
          "old_line_content": "      return llvm::StringSwitch<bool>(llvm::StringRef(&c, 1))",
          "new_line_content": "        return false;",
          "content_same": false
        },
        {
          "line": 726,
          "old_api": "Default",
          "new_api": null,
          "old_text": "          .Default(true)",
          "new_text": null,
          "old_line_content": "          .Default(true);",
          "new_line_content": "        case '_':",
          "content_same": false
        },
        {
          "line": 1238,
          "old_api": "size",
          "new_api": null,
          "old_text": "token_infos_.size()",
          "new_text": null,
          "old_line_content": "  widths.index = ComputeDecimalPrintedWidth((token_infos_.size()));",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 728,
          "old_api": "empty",
          "new_api": null,
          "old_text": "error_text.empty()",
          "new_text": null,
          "old_line_content": "    if (error_text.empty()) {",
          "new_line_content": "        case '\\n':",
          "content_same": false
        },
        {
          "line": 1239,
          "old_api": "tokens",
          "new_api": null,
          "old_text": "tokens()",
          "new_text": null,
          "old_line_content": "  for (Token token : tokens()) {",
          "new_line_content": "auto TokenizedBuffer::Print(llvm::raw_ostream& output_stream) const -> void {",
          "content_same": false
        },
        {
          "line": 731,
          "old_api": "take_front",
          "new_api": null,
          "old_text": "source_text.take_front(1)",
          "new_text": null,
          "old_line_content": "      error_text = source_text.take_front(1);",
          "new_line_content": "          break;",
          "content_same": false
        },
        {
          "line": 1243,
          "old_api": "tokens",
          "new_api": null,
          "old_text": "tokens()",
          "new_text": null,
          "old_line_content": "  for (Token token : tokens()) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 734,
          "old_api": "size",
          "new_api": null,
          "old_text": "buffer_->AddToken(\n        {.kind = TokenKind::Error,\n         .token_line = current_line_,\n         .column = current_column_,\n         .error_length = static_cast<int32_t>(error_text.size())})",
          "new_text": null,
          "old_line_content": "    auto token = buffer_->AddToken(",
          "new_line_content": "#define CARBON_SYMBOL_TOKEN(Name, Spelling) .StartsWith(Spelling, false)",
          "content_same": false
        },
        {
          "line": 739,
          "old_api": "CARBON_DIAGNOSTIC",
          "new_api": null,
          "old_text": "CARBON_DIAGNOSTIC(UnrecognizedCharacters, Error,\n                      \"Encountered unrecognized characters while parsing.\")",
          "new_text": null,
          "old_line_content": "    CARBON_DIAGNOSTIC(UnrecognizedCharacters, Error,",
          "new_line_content": "      // TODO: Reimplement this to use the lexer properly. In the meantime,",
          "content_same": false
        },
        {
          "line": 1252,
          "old_api": "PrintToken",
          "new_api": null,
          "old_text": "PrintToken(output_stream, token, {})",
          "new_text": null,
          "old_line_content": "  PrintToken(output_stream, token, {});",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 743,
          "old_api": "size",
          "new_api": null,
          "old_text": "error_text.size()",
          "new_text": null,
          "old_line_content": "    current_column_ += error_text.size();",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1257,
          "old_api": "GetTokenPrintWidths",
          "new_api": null,
          "old_text": "GetTokenPrintWidths(token)",
          "new_text": null,
          "old_line_content": "  widths.Widen(GetTokenPrintWidths(token));",
          "new_line_content": "  output_stream << \"  ]\\n\";",
          "content_same": false
        },
        {
          "line": 1259,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  const auto& token_info = GetTokenInfo(token);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1260,
          "old_api": "GetTokenText",
          "new_api": null,
          "old_text": "GetTokenText(token)",
          "new_text": null,
          "old_line_content": "  llvm::StringRef token_text = GetTokenText(token);",
          "new_line_content": "auto TokenizedBuffer::PrintToken(llvm::raw_ostream& output_stream,",
          "content_same": false
        },
        {
          "line": 752,
          "old_api": "AddToken",
          "new_api": null,
          "old_text": "buffer_->AddToken({.kind = TokenKind::StartOfFile,\n                       .has_trailing_space = true,\n                       .token_line = current_line_,\n                       .column = current_column_})",
          "new_text": null,
          "old_line_content": "    buffer_->AddToken({.kind = TokenKind::StartOfFile,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1265,
          "old_api": "name",
          "new_api": null,
          "old_text": "llvm::formatv(\n      \"    { index: {0}, kind: {1}, line: {2}, column: {3}, indent: {4}, \"\n      \"spelling: '{5}'\",\n      llvm::format_decimal(token_index, widths.index),\n      llvm::right_justify(llvm::formatv(\"'{0}'\", token_info.kind.name()).str(),\n                          widths.kind + 2),\n      llvm::format_decimal(GetLineNumber(token_info.token_line), widths.line),\n      llvm::format_decimal(GetColumnNumber(token), widths.column),\n      llvm::format_decimal(GetIndentColumnNumber(token_info.token_line),\n                           widths.indent),\n      token_text)",
          "new_text": null,
          "old_line_content": "  output_stream << llvm::formatv(",
          "new_line_content": "auto TokenizedBuffer::PrintToken(llvm::raw_ostream& output_stream, Token token,",
          "content_same": false
        },
        {
          "line": 1268,
          "old_api": "llvm::format_decimal(token_index, widths.index)",
          "new_api": null,
          "old_text": "llvm::format_decimal(token_index, widths.index)",
          "new_text": null,
          "old_line_content": "      llvm::format_decimal(token_index, widths.index),",
          "new_line_content": "  int token_index = token.index;",
          "content_same": false
        },
        {
          "line": 759,
          "old_api": "empty",
          "new_api": null,
          "old_text": "source_text.empty()",
          "new_text": null,
          "old_line_content": "    CARBON_DCHECK(source_text.empty());",
          "new_line_content": "    // Before lexing any source text, add the start-of-file token so that code",
          "content_same": false
        },
        {
          "line": 1271,
          "old_api": "GetLineNumber",
          "new_api": null,
          "old_text": "GetLineNumber(token_info.token_line)",
          "new_text": null,
          "old_line_content": "      llvm::format_decimal(GetLineNumber(token_info.token_line), widths.line),",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1272,
          "old_api": "GetColumnNumber",
          "new_api": null,
          "old_text": "GetColumnNumber(token)",
          "new_text": null,
          "old_line_content": "      llvm::format_decimal(GetColumnNumber(token), widths.column),",
          "new_line_content": "  // Output the main chunk using one format string. We have to do the",
          "content_same": false
        },
        {
          "line": 1273,
          "old_api": "GetIndentColumnNumber",
          "new_api": null,
          "old_text": "GetIndentColumnNumber(token_info.token_line)",
          "new_text": null,
          "old_line_content": "      llvm::format_decimal(GetIndentColumnNumber(token_info.token_line),",
          "new_line_content": "  // justification manually in order to use the dynamically computed widths",
          "content_same": false
        },
        {
          "line": 767,
          "old_api": "GetLineNumber",
          "new_api": null,
          "old_text": "buffer_->GetLineNumber(current_line_)",
          "new_text": null,
          "old_line_content": "    if (current_column_ == 0 && buffer_->GetLineNumber(current_line_) != 1) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 768,
          "old_api": "GetPrevLine",
          "new_api": null,
          "old_text": "buffer_->GetPrevLine(current_line_)",
          "new_text": null,
          "old_line_content": "      current_line_ = buffer_->GetPrevLine(current_line_);",
          "new_line_content": "  auto LexEndOfFile(llvm::StringRef& source_text) -> void {",
          "content_same": false
        },
        {
          "line": 1287,
          "old_api": "GetRealLiteral",
          "new_api": null,
          "old_text": "GetRealLiteral(token)",
          "new_text": null,
          "old_line_content": "      output_stream << \", value: `\" << GetRealLiteral(token) << \"`\";",
          "new_line_content": "  switch (token_info.kind) {",
          "content_same": false
        },
        {
          "line": 1290,
          "old_api": "GetStringLiteral",
          "new_api": null,
          "old_text": "GetStringLiteral(token)",
          "new_text": null,
          "old_line_content": "      output_stream << \", value: `\" << GetStringLiteral(token) << \"`\";",
          "new_line_content": "      break;",
          "content_same": false
        },
        {
          "line": 781,
          "old_api": "empty",
          "new_api": null,
          "old_text": "open_groups_.empty()",
          "new_text": null,
          "old_line_content": "    if (!open_groups_.empty()) {",
          "new_line_content": "    } else {",
          "content_same": false
        },
        {
          "line": 782,
          "old_api": "CloseInvalidOpenGroups",
          "new_api": null,
          "old_text": "CloseInvalidOpenGroups(TokenKind::Error)",
          "new_text": null,
          "old_line_content": "      CloseInvalidOpenGroups(TokenKind::Error);",
          "new_line_content": "      // Update the line length as this is also the end of a line.",
          "content_same": false
        },
        {
          "line": 1295,
          "old_api": "GetMatchedClosingToken",
          "new_api": null,
          "old_text": "GetMatchedClosingToken(token)",
          "new_text": null,
          "old_line_content": "                      << GetMatchedClosingToken(token).index;",
          "new_line_content": "      break;",
          "content_same": false
        },
        {
          "line": 1296,
          "old_api": "is_closing_symbol",
          "new_api": null,
          "old_text": "token_info.kind.is_closing_symbol()",
          "new_text": null,
          "old_line_content": "      } else if (token_info.kind.is_closing_symbol()) {",
          "new_line_content": "    case TokenKind::RealLiteral:",
          "content_same": false
        },
        {
          "line": 785,
          "old_api": "AddToken",
          "new_api": null,
          "old_text": "buffer_->AddToken({.kind = TokenKind::EndOfFile,\n                       .token_line = current_line_,\n                       .column = current_column_})",
          "new_text": null,
          "old_line_content": "    buffer_->AddToken({.kind = TokenKind::EndOfFile,",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1298,
          "old_api": "GetMatchedOpeningToken",
          "new_api": null,
          "old_text": "GetMatchedOpeningToken(token)",
          "new_text": null,
          "old_line_content": "                      << GetMatchedOpeningToken(token).index;",
          "new_line_content": "      break;",
          "content_same": false
        },
        {
          "line": 802,
          "old_api": "empty",
          "new_api": null,
          "old_text": "source_text.empty()",
          "new_text": null,
          "old_line_content": "    if (LLVM_UNLIKELY(source_text.empty())) {",
          "new_line_content": "  // helpfully in profiles and backtraces, but they tend to not contain the",
          "content_same": false
        },
        {
          "line": 803,
          "old_api": "LexEndOfFile",
          "new_api": null,
          "old_text": "lexer.LexEndOfFile(source_text)",
          "new_text": null,
          "old_line_content": "      lexer.LexEndOfFile(source_text);",
          "new_line_content": "  // interesting logic and simply delegate to the relevant methods. All of their",
          "content_same": false
        },
        {
          "line": 1322,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "line_infos_.push_back(info)",
          "new_text": null,
          "old_line_content": "  line_infos_.push_back(info);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 811,
          "old_api": "front",
          "new_api": null,
          "old_text": "static_cast<unsigned char>(\n        source_text.front())",
          "new_text": null,
          "old_line_content": "    [[clang::musttail]] return DispatchTable[static_cast<unsigned char>(",
          "new_line_content": "    // and this is the loop exit.",
          "content_same": false
        },
        {
          "line": 1323,
          "old_api": "size",
          "new_api": null,
          "old_text": "line_infos_.size()",
          "new_text": null,
          "old_line_content": "  return Line(static_cast<int>(line_infos_.size()) - 1);",
          "new_line_content": "auto TokenizedBuffer::GetLineInfo(Line line) -> LineInfo& {",
          "content_same": false
        },
        {
          "line": 304,
          "old_api": "drop_front",
          "new_api": null,
          "old_text": "source_text.drop_front()",
          "new_text": null,
          "old_line_content": "    source_text = source_text.drop_front();",
          "new_line_content": "    // due to indentation. We don't expect *huge* runs, so just use a scalar",
          "content_same": false
        },
        {
          "line": 309,
          "old_api": "NoteWhitespace",
          "new_api": null,
          "old_text": "NoteWhitespace()",
          "new_text": null,
          "old_line_content": "    NoteWhitespace();",
          "new_line_content": "    while (ws_count < size &&",
          "content_same": false
        },
        {
          "line": 310,
          "old_api": "drop_front",
          "new_api": null,
          "old_text": "source_text.drop_front()",
          "new_text": null,
          "old_line_content": "    source_text = source_text.drop_front();",
          "new_line_content": "           (source_text[ws_count] == ' ' || source_text[ws_count] == '\\t')) {",
          "content_same": false
        },
        {
          "line": 311,
          "old_api": "HandleNewline",
          "new_api": null,
          "old_text": "HandleNewline()",
          "new_text": null,
          "old_line_content": "    HandleNewline();",
          "new_line_content": "      ++ws_count;",
          "content_same": false
        },
        {
          "line": 1335,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "token_infos_.push_back(info)",
          "new_text": null,
          "old_line_content": "  token_infos_.push_back(info);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1336,
          "old_api": "expected_parse_tree_size",
          "new_api": null,
          "old_text": "info.kind.expected_parse_tree_size()",
          "new_text": null,
          "old_line_content": "  expected_parse_tree_size_ += info.kind.expected_parse_tree_size();",
          "new_line_content": "auto TokenizedBuffer::GetTokenInfo(Token token) -> TokenInfo& {",
          "content_same": false
        },
        {
          "line": 1337,
          "old_api": "size",
          "new_api": null,
          "old_text": "token_infos_.size()",
          "new_text": null,
          "old_line_content": "  return Token(static_cast<int>(token_infos_.size()) - 1);",
          "new_line_content": "  return token_infos_[token.index];",
          "content_same": false
        },
        {
          "line": 315,
          "old_api": "front",
          "new_api": null,
          "old_text": "source_text.front()",
          "new_text": null,
          "old_line_content": "    CARBON_DCHECK(source_text.front() == '/');",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 1348,
          "old_api": "begin",
          "new_api": null,
          "old_text": "buffer_->source_->text().begin()",
          "new_text": null,
          "old_line_content": "  int64_t offset = loc - buffer_->source_->text().begin();",
          "new_line_content": "}",
          "content_same": false
        },
        {
          "line": 326,
          "old_api": "LexSymbolToken",
          "new_api": null,
          "old_text": "LexSymbolToken(source_text)",
          "new_text": null,
          "old_line_content": "    LexResult result = LexSymbolToken(source_text);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 327,
          "old_api": "CARBON_CHECK",
          "new_api": null,
          "old_text": "CARBON_CHECK(result)",
          "new_text": null,
          "old_line_content": "    CARBON_CHECK(result) << \"Failed to form a token!\";",
          "new_line_content": "    // Both comments and slash symbols start with a `/`. We disambiguate with a",
          "content_same": false
        },
        {
          "line": 1353,
          "old_api": "begin",
          "new_api": null,
          "old_text": "std::partition_point(\n      buffer_->line_infos_.begin(), buffer_->line_infos_.end(),\n      [offset](const LineInfo& line) { return line.start <= offset; })",
          "new_text": null,
          "old_line_content": "  const auto* line_it = std::partition_point(",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1354,
          "old_api": "end",
          "new_api": null,
          "old_text": "buffer_->line_infos_.end()",
          "new_text": null,
          "old_line_content": "      buffer_->line_infos_.begin(), buffer_->line_infos_.end(),",
          "new_line_content": "auto TokenizedBuffer::SourceBufferLocationTranslator::GetLocation(",
          "content_same": false
        },
        {
          "line": 335,
          "old_api": "CARBON_DIAGNOSTIC",
          "new_api": null,
          "old_text": "CARBON_DIAGNOSTIC(TrailingComment, Error,\n                        \"Trailing comments are not permitted.\")",
          "new_text": null,
          "old_line_content": "      CARBON_DIAGNOSTIC(TrailingComment, Error,",
          "new_line_content": "    // This code path should produce a token, make sure that happens.",
          "content_same": false
        },
        {
          "line": 1361,
          "old_api": "begin",
          "new_api": null,
          "old_text": "buffer_->line_infos_.begin()",
          "new_text": null,
          "old_line_content": "  int line_number = line_it - buffer_->line_infos_.begin();",
          "new_line_content": "  // inspect `line.length` here because it is not necessarily correct for the",
          "content_same": false
        },
        {
          "line": 338,
          "old_api": "begin",
          "new_api": null,
          "old_text": "source_text.begin()",
          "new_text": null,
          "old_line_content": "      emitter_.Emit(source_text.begin(), TrailingComment);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 342,
          "old_api": "IsSpace",
          "new_api": null,
          "old_text": "IsSpace(source_text[2])",
          "new_text": null,
          "old_line_content": "    if (source_text.size() > 2 && !IsSpace(source_text[2])) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 343,
          "old_api": "CARBON_DIAGNOSTIC",
          "new_api": null,
          "old_text": "CARBON_DIAGNOSTIC(NoWhitespaceAfterCommentIntroducer, Error,\n                        \"Whitespace is required after '//'.\")",
          "new_text": null,
          "old_line_content": "      CARBON_DIAGNOSTIC(NoWhitespaceAfterCommentIntroducer, Error,",
          "new_line_content": "    // Any comment must be the only non-whitespace on the line.",
          "content_same": false
        },
        {
          "line": 1369,
          "old_api": "static_cast<int32_t>(llvm::StringRef::npos)",
          "new_api": null,
          "old_text": "static_cast<int32_t>(llvm::StringRef::npos)",
          "new_text": null,
          "old_line_content": "  if (line_it->length == static_cast<int32_t>(llvm::StringRef::npos)) {",
          "new_line_content": "      << \"location precedes the start of the first line\";",
          "content_same": false
        },
        {
          "line": 1370,
          "old_api": "take_front",
          "new_api": null,
          "old_text": "line.take_front(column_number).count('\\n')",
          "new_text": null,
          "old_line_content": "    CARBON_CHECK(line.take_front(column_number).count('\\n') == 0)",
          "new_line_content": "  --line_it;",
          "content_same": false
        },
        {
          "line": 861,
          "old_api": "CreateLines",
          "new_api": null,
          "old_text": "CreateLines(source_text)",
          "new_text": null,
          "old_line_content": "    CreateLines(source_text);",
          "new_line_content": "    [[clang::musttail]] return DispatchNext(lexer, source_text);              \\",
          "content_same": false
        },
        {
          "line": 1373,
          "old_api": "filename",
          "new_api": null,
          "old_text": "buffer_->source_->filename()",
          "new_text": null,
          "old_line_content": "        << buffer_->source_->filename() << \":\" << line_number << \":\"",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 351,
          "old_api": "drop_front",
          "new_api": null,
          "old_text": "source_text.drop_front(current_line_info_->length - current_column_)",
          "new_text": null,
          "old_line_content": "        source_text.drop_front(current_line_info_->length - current_column_);",
          "new_line_content": "    // The introducer '//' must be followed by whitespace or EOF.",
          "content_same": false
        },
        {
          "line": 863,
          "old_api": "LexStartOfFile",
          "new_api": null,
          "old_text": "LexStartOfFile(source_text)",
          "new_text": null,
          "old_line_content": "    LexStartOfFile(source_text);",
          "new_line_content": "  CARBON_DISPATCH_LEX_NON_TOKEN(LexHorizontalWhitespace)",
          "content_same": false
        },
        {
          "line": 1377,
          "old_api": "find",
          "new_api": null,
          "old_text": "line.find('\\n', column_number)",
          "new_text": null,
          "old_line_content": "    auto end_newline_pos = line.find('\\n', column_number);",
          "new_line_content": "  llvm::StringRef line =",
          "content_same": false
        },
        {
          "line": 867,
          "old_api": "DispatchNext",
          "new_api": null,
          "old_text": "DispatchNext(*this, source_text)",
          "new_text": null,
          "old_line_content": "    DispatchNext(*this, source_text);",
          "new_line_content": "  // The main entry point for dispatching through the lexer's table. This method",
          "content_same": false
        },
        {
          "line": 869,
          "old_api": "empty",
          "new_api": null,
          "old_text": "source_text.empty()",
          "new_text": null,
          "old_line_content": "    CARBON_CHECK(source_text.empty())",
          "new_line_content": "  auto Dispatch(llvm::StringRef& source_text) -> void {",
          "content_same": false
        },
        {
          "line": 360,
          "old_api": "LexVerticalWhitespace",
          "new_api": null,
          "old_text": "LexVerticalWhitespace(source_text)",
          "new_text": null,
          "old_line_content": "    LexVerticalWhitespace(source_text);",
          "new_line_content": "    source_text =",
          "content_same": false
        },
        {
          "line": 364,
          "old_api": "NumericLiteral::Lex(source_text)",
          "new_api": null,
          "old_text": "NumericLiteral::Lex(source_text)",
          "new_text": null,
          "old_line_content": "    std::optional<NumericLiteral> literal = NumericLiteral::Lex(source_text);",
          "new_line_content": "      // Finished lexing.",
          "content_same": false
        },
        {
          "line": 366,
          "old_api": "LexError",
          "new_api": null,
          "old_text": "LexError(source_text)",
          "new_text": null,
          "old_line_content": "      return LexError(source_text);",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 1391,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "buffer_->GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "  const auto& token_info = buffer_->GetTokenInfo(token);",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 1392,
          "old_api": "GetLineInfo",
          "new_api": null,
          "old_text": "buffer_->GetLineInfo(token_info.token_line)",
          "new_text": null,
          "old_line_content": "  const auto& line_info = buffer_->GetLineInfo(token_info.token_line);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1394,
          "old_api": "begin",
          "new_api": null,
          "old_text": "buffer_->source_->text().begin()",
          "new_text": null,
          "old_line_content": "      buffer_->source_->text().begin() + line_info.start + token_info.column;",
          "new_line_content": "          .line = line,",
          "content_same": false
        },
        {
          "line": 372,
          "old_api": "drop_front",
          "new_api": null,
          "old_text": "source_text.drop_front(token_size)",
          "new_text": null,
          "old_line_content": "    source_text = source_text.drop_front(token_size);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1399,
          "old_api": "GetLocation",
          "new_api": null,
          "old_text": "TokenizedBuffer::SourceBufferLocationTranslator(buffer_).GetLocation(\n      token_start)",
          "new_text": null,
          "old_line_content": "  return TokenizedBuffer::SourceBufferLocationTranslator(buffer_).GetLocation(",
          "new_line_content": "auto TokenLocationTranslator::GetLocation(Token token) -> DiagnosticLocation {",
          "content_same": false
        },
        {
          "line": 379,
          "old_api": "size",
          "new_api": null,
          "old_text": "VariantMatch(\n        literal->ComputeValue(emitter_),\n        [&](NumericLiteral::IntegerValue&& value) {\n          auto token = buffer_->AddToken({.kind = TokenKind::IntegerLiteral,\n                                          .token_line = current_line_,\n                                          .column = int_column});\n          buffer_->GetTokenInfo(token).literal_index =\n              buffer_->literal_int_storage_.size();\n          buffer_->literal_int_storage_.push_back(std::move(value.value));\n          return token;\n        },\n        [&](NumericLiteral::RealValue&& value) {\n          auto token = buffer_->AddToken({.kind = TokenKind::RealLiteral,\n                                          .token_line = current_line_,\n                                          .column = int_column});\n          buffer_->GetTokenInfo(token).literal_index =\n              buffer_->literal_int_storage_.size();\n          buffer_->literal_int_storage_.push_back(std::move(value.mantissa));\n          buffer_->literal_int_storage_.push_back(std::move(value.exponent));\n          CARBON_CHECK(buffer_->GetRealLiteral(token).is_decimal ==\n                       (value.radix == NumericLiteral::Radix::Decimal));\n          return token;\n        },\n        [&](NumericLiteral::UnrecoverableError) {\n          auto token = buffer_->AddToken({\n              .kind = TokenKind::Error,\n              .token_line = current_line_,\n              .column = int_column,\n              .error_length = token_size,\n          });\n          return token;\n        })",
          "new_text": null,
          "old_line_content": "    return VariantMatch(",
          "new_line_content": "    int int_column = current_column_;",
          "content_same": false
        },
        {
          "line": 385,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "buffer_->GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "          buffer_->GetTokenInfo(token).literal_index =",
          "new_line_content": "      current_line_info_->indent = int_column;",
          "content_same": false
        },
        {
          "line": 386,
          "old_api": "size",
          "new_api": null,
          "old_text": "buffer_->literal_int_storage_.size()",
          "new_text": null,
          "old_line_content": "              buffer_->literal_int_storage_.size();",
          "new_line_content": "      set_indent_ = true;",
          "content_same": false
        },
        {
          "line": 387,
          "old_api": "std::move(value.value)",
          "new_api": null,
          "old_text": "std::move(value.value)",
          "new_text": null,
          "old_line_content": "          buffer_->literal_int_storage_.push_back(std::move(value.value));",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 391,
          "old_api": "AddToken",
          "new_api": null,
          "old_text": "buffer_->AddToken({.kind = TokenKind::RealLiteral,\n                                          .token_line = current_line_,\n                                          .column = int_column})",
          "new_text": null,
          "old_line_content": "          auto token = buffer_->AddToken({.kind = TokenKind::RealLiteral,",
          "new_line_content": "        [&](NumericLiteral::IntegerValue&& value) {",
          "content_same": false
        },
        {
          "line": 394,
          "old_api": "GetTokenInfo",
          "new_api": null,
          "old_text": "buffer_->GetTokenInfo(token)",
          "new_text": null,
          "old_line_content": "          buffer_->GetTokenInfo(token).literal_index =",
          "new_line_content": "                                          .column = int_column});",
          "content_same": false
        },
        {
          "line": 398,
          "old_api": "GetRealLiteral",
          "new_api": null,
          "old_text": "buffer_->GetRealLiteral(token)",
          "new_text": null,
          "old_line_content": "          CARBON_CHECK(buffer_->GetRealLiteral(token).is_decimal ==",
          "new_line_content": "          return token;",
          "content_same": false
        },
        {
          "line": 403,
          "old_api": "AddToken",
          "new_api": null,
          "old_text": "buffer_->AddToken({\n              .kind = TokenKind::Error,\n              .token_line = current_line_,\n              .column = int_column,\n              .error_length = token_size,\n          })",
          "new_text": null,
          "old_line_content": "          auto token = buffer_->AddToken({",
          "new_line_content": "                                          .column = int_column});",
          "content_same": false
        },
        {
          "line": 414,
          "old_api": "StringLiteral::Lex(source_text)",
          "new_api": null,
          "old_text": "StringLiteral::Lex(source_text)",
          "new_text": null,
          "old_line_content": "    std::optional<StringLiteral> literal = StringLiteral::Lex(source_text);",
          "new_line_content": "              .kind = TokenKind::Error,",
          "content_same": false
        },
        {
          "line": 416,
          "old_api": "LexError",
          "new_api": null,
          "old_text": "LexError(source_text)",
          "new_text": null,
          "old_line_content": "      return LexError(source_text);",
          "new_line_content": "              .column = int_column,",
          "content_same": false
        },
        {
          "line": 421,
          "old_api": "size",
          "new_api": null,
          "old_text": "literal->text().size()",
          "new_text": null,
          "old_line_content": "    int literal_size = literal->text().size();",
          "new_line_content": "  }",
          "content_same": false
        },
        {
          "line": 422,
          "old_api": "drop_front",
          "new_api": null,
          "old_text": "source_text.drop_front(literal_size)",
          "new_text": null,
          "old_line_content": "    source_text = source_text.drop_front(literal_size);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 430,
          "old_api": "is_multi_line",
          "new_api": null,
          "old_text": "literal->is_multi_line()",
          "new_text": null,
          "old_line_content": "    if (!literal->is_multi_line()) {",
          "new_line_content": "    int string_column = current_column_;",
          "content_same": false
        },
        {
          "line": 433,
          "old_api": "text",
          "new_api": null,
          "old_text": "literal->text()",
          "new_text": null,
          "old_line_content": "      for (char c : literal->text()) {",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 435,
          "old_api": "HandleNewline",
          "new_api": null,
          "old_text": "HandleNewline()",
          "new_text": null,
          "old_line_content": "          HandleNewline();",
          "new_line_content": "      current_line_info_->indent = string_column;",
          "content_same": false
        },
        {
          "line": 446,
          "old_api": "is_terminated",
          "new_api": null,
          "old_text": "literal->is_terminated()",
          "new_text": null,
          "old_line_content": "    if (literal->is_terminated()) {",
          "new_line_content": "          // The indentation of all lines in a multi-line string literal is",
          "content_same": false
        },
        {
          "line": 448,
          "old_api": "size",
          "new_api": null,
          "old_text": "buffer_->AddToken({.kind = TokenKind::StringLiteral,\n                             .token_line = string_line,\n                             .column = string_column,\n                             .literal_index = static_cast<int32_t>(\n                                 buffer_->literal_string_storage_.size())})",
          "new_text": null,
          "old_line_content": "          buffer_->AddToken({.kind = TokenKind::StringLiteral,",
          "new_line_content": "          current_line_info_->indent = string_column;",
          "content_same": false
        },
        {
          "line": 451,
          "old_api": "size",
          "new_api": null,
          "old_text": "static_cast<int32_t>(\n                                 buffer_->literal_string_storage_.size())",
          "new_text": null,
          "old_line_content": "                             .literal_index = static_cast<int32_t>(",
          "new_line_content": "          ++current_column_;",
          "content_same": false
        },
        {
          "line": 452,
          "old_api": "size",
          "new_api": null,
          "old_text": "buffer_->literal_string_storage_.size()",
          "new_text": null,
          "old_line_content": "                                 buffer_->literal_string_storage_.size())});",
          "new_line_content": "        }",
          "content_same": false
        },
        {
          "line": 453,
          "old_api": "push_back",
          "new_api": null,
          "old_text": "buffer_->literal_string_storage_.push_back(\n          literal->ComputeValue(emitter_))",
          "new_text": null,
          "old_line_content": "      buffer_->literal_string_storage_.push_back(",
          "new_line_content": "      }",
          "content_same": false
        },
        {
          "line": 454,
          "old_api": "ComputeValue",
          "new_api": null,
          "old_text": "literal->ComputeValue(emitter_)",
          "new_text": null,
          "old_line_content": "          literal->ComputeValue(emitter_));",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 457,
          "old_api": "CARBON_DIAGNOSTIC",
          "new_api": null,
          "old_text": "CARBON_DIAGNOSTIC(UnterminatedString, Error,\n                        \"String is missing a terminator.\")",
          "new_text": null,
          "old_line_content": "      CARBON_DIAGNOSTIC(UnterminatedString, Error,",
          "new_line_content": "      auto token =",
          "content_same": false
        },
        {
          "line": 459,
          "old_api": "begin",
          "new_api": null,
          "old_text": "literal->text().begin()",
          "new_text": null,
          "old_line_content": "      emitter_.Emit(literal->text().begin(), UnterminatedString);",
          "new_line_content": "                             .token_line = string_line,",
          "content_same": false
        },
        {
          "line": 460,
          "old_api": "AddToken",
          "new_api": null,
          "old_text": "buffer_->AddToken({.kind = TokenKind::Error,\n                                .token_line = string_line,\n                                .column = string_column,\n                                .error_length = literal_size})",
          "new_text": null,
          "old_line_content": "      return buffer_->AddToken({.kind = TokenKind::Error,",
          "new_line_content": "                             .column = string_column,",
          "content_same": false
        },
        {
          "line": 471,
          "old_api": "fixed_spelling",
          "new_api": null,
          "old_text": "kind.fixed_spelling().size()",
          "new_text": null,
          "old_line_content": "    CARBON_DCHECK(kind.fixed_spelling().size() == 1);",
          "new_line_content": "                                .token_line = string_line,",
          "content_same": false
        },
        {
          "line": 472,
          "old_api": "fixed_spelling",
          "new_api": null,
          "old_text": "kind.fixed_spelling().front()",
          "new_text": null,
          "old_line_content": "    CARBON_DCHECK(source_text.front() == kind.fixed_spelling().front())",
          "new_line_content": "                                .column = string_column,",
          "content_same": false
        },
        {
          "line": 473,
          "old_api": "front",
          "new_api": null,
          "old_text": "source_text.front()",
          "new_text": null,
          "old_line_content": "        << \"Source text starts with '\" << source_text.front()",
          "new_line_content": "                                .error_length = literal_size});",
          "content_same": false
        },
        {
          "line": 474,
          "old_api": "fixed_spelling",
          "new_api": null,
          "old_text": "kind.fixed_spelling()",
          "new_text": null,
          "old_line_content": "        << \"' instead of the spelling '\" << kind.fixed_spelling()",
          "new_line_content": "    }",
          "content_same": false
        },
        {
          "line": 485,
          "old_api": "drop_front",
          "new_api": null,
          "old_text": "source_text.drop_front()",
          "new_text": null,
          "old_line_content": "    source_text = source_text.drop_front();",
          "new_line_content": "        << \"' of the incoming token kind '\" << kind << \"'\";",
          "content_same": false
        },
        {
          "line": 997,
          "old_api": "constexpr",
          "new_api": null,
          "old_text": "constexpr",
          "new_text": null,
          "old_line_content": "constexpr TokenizedBuffer::Lexer::DispatchTableT",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 998,
          "old_api": "MakeDispatchTable",
          "new_api": null,
          "old_text": "MakeDispatchTable()",
          "new_text": null,
          "old_line_content": "    TokenizedBuffer::Lexer::DispatchTable = MakeDispatchTable();",
          "new_line_content": "  Line current_line_ = Line::Invalid;",
          "content_same": false
        },
        {
          "line": 1000,
          "old_api": "constexpr",
          "new_api": null,
          "old_text": "constexpr",
          "new_text": null,
          "old_line_content": "constexpr std::array<TokenKind, 256>",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 1001,
          "old_api": "[] {\n      std::array<TokenKind, 256> table = {};\n#define CARBON_ONE_CHAR_SYMBOL_TOKEN(TokenName, Spelling) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#define CARBON_OPENING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, ClosingName) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#define CARBON_CLOSING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, OpeningName) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#include \"toolchain/lex/token_kind.def\"\n      return table;\n    }()",
          "new_api": null,
          "old_text": "[] {\n      std::array<TokenKind, 256> table = {};\n#define CARBON_ONE_CHAR_SYMBOL_TOKEN(TokenName, Spelling) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#define CARBON_OPENING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, ClosingName) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#define CARBON_CLOSING_GROUP_SYMBOL_TOKEN(TokenName, Spelling, OpeningName) \\\n  table[(Spelling)[0]] = TokenKind::TokenName;\n#include \"toolchain/lex/token_kind.def\"\n      return table;\n    }()",
          "new_text": null,
          "old_line_content": "    TokenizedBuffer::Lexer::OneCharTokenKindTable = [] {",
          "new_line_content": "  int current_column_ = 0;",
          "content_same": false
        },
        {
          "line": 491,
          "old_api": "LexOneCharSymbolToken",
          "new_api": null,
          "old_text": "LexOneCharSymbolToken(source_text, kind)",
          "new_text": null,
          "old_line_content": "    Token token = LexOneCharSymbolToken(source_text, kind);",
          "new_line_content": "",
          "content_same": false
        },
        {
          "line": 499,
          "old_api": "CARBON_DIAGNOSTIC",
          "new_api": null,
          "old_text": "CARBON_DIAGNOSTIC(\n          UnmatchedClosing, Error,\n          \"Closing symbol without a corresponding opening symbol.\")",
          "new_text": null,
          "old_line_content": "      CARBON_DIAGNOSTIC(",
          "new_line_content": "  auto LexOpeningSymbolToken(llvm::StringRef& source_text, TokenKind kind)",
          "content_same": false
        },
        {
          "line": 503,
          "old_api": "AddToken",
          "new_api": null,
          "old_text": "buffer_->AddToken({.kind = TokenKind::Error,\n                                       .token_line = current_line_,\n                                       .column = current_column_,\n                                       .error_length = 1})",
          "new_text": null,
          "old_line_content": "      Token token = buffer_->AddToken({.kind = TokenKind::Error,",
          "new_line_content": "    return token;",
          "content_same": false
        },
        {
          "line": 1019,
          "old_api": "text",
          "new_api": null,
          "old_text": "source.text()",
          "new_text": null,
          "old_line_content": "  llvm::StringRef source_text = source.text();",
          "new_line_content": "#include \"toolchain/lex/token_kind.def\"",
          "content_same": false
        },
        {
          "line": 508,
          "old_api": "drop_front",
          "new_api": null,
          "old_text": "source_text.drop_front()",
          "new_text": null,
          "old_line_content": "      source_text = source_text.drop_front();",
          "new_line_content": "    auto unmatched_error = [&] {",
          "content_same": false
        },
        {
          "line": 1022,
          "old_api": "seen_error",
          "new_api": null,
          "old_text": "error_tracking_consumer.seen_error()",
          "new_text": null,
          "old_line_content": "  if (error_tracking_consumer.seen_error()) {",
          "new_line_content": "",
          "content_same": false
        }
      ]
    },
    "api_summary": {
      "total_replacements": 71,
      "total_additions": 199,
      "total_deletions": 198,
      "total_api_changes": 468
    },
    "non_api_changes": {
      "has_non_api_changes": true,
      "evidence": {
        "total_diff_lines": 12,
        "api_related_lines": 468,
        "non_api_lines": 6,
        "non_api_line_numbers": [
          303,
          305,
          306,
          307,
          312,
          313
        ]
      }
    },
    "api_calls_before": 426,
    "api_calls_after": 427,
    "diff_info": {
      "added_lines": 12,
      "removed_lines": 2,
      "total_diff_lines": 26
    }
  }
}